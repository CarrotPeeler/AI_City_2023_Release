[2023-09-03 23:48:50,075] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-03 23:48:50,152] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.9, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f201e3160a0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-09-03 23:48:55,688] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-03 23:48:55,688] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-03 23:48:55,823] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-03 23:48:55,824] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-03 23:48:55,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4270625114440918 seconds
[2023-09-03 23:48:56,951] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-03 23:48:56,955] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-09-03 23:48:56,955] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-09-03 23:48:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-03 23:48:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-03 23:48:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-03 23:48:56,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-03 23:48:56,972] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-03 23:48:56,972] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-03 23:48:56,972] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-03 23:48:56,972] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-03 23:48:56,972] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1fd03f9400>
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-03 23:48:56,973] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   world_size ................... 2
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-03 23:48:56,974] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-03 23:48:56,975] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:32:01  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 12.0102 (12.0102 -- 12.0102)  data: 6.3209 (6.3209 -- 6.3209)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 2.7730 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5932 (1.6020)  time: 0.6427 (0.4947 -- 1.9583)  data: 0.0341 (0.0003 -- 0.6522)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000001  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4586 (1.5416)  time: 1.0630 (0.5080 -- 4.3765)  data: 0.0020 (0.0006 -- 0.0091)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000001  loss: 2.7724 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4177 (1.5245)  time: 0.7591 (0.5151 -- 3.6327)  data: 0.0015 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 2.7722 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4507 (1.5164)  time: 0.9986 (0.5175 -- 5.4563)  data: 0.0029 (0.0007 -- 0.0148)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 2.7720 (2.7724)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4762 (1.5102)  time: 0.7069 (0.5020 -- 3.2189)  data: 0.0012 (0.0004 -- 0.0055)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000002  loss: 2.7712 (2.7722)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5028 (1.5103)  time: 0.9646 (0.5154 -- 3.5629)  data: 0.0016 (0.0003 -- 0.0038)  max mem: 16413
[2023-09-03 23:50:57,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:50:57,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:50:57,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-09-03 23:50:57,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 2.7704 (2.7720)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4507 (1.4999)  time: 0.7452 (0.5114 -- 2.9839)  data: 0.0020 (0.0004 -- 0.0131)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 2.7694 (2.7716)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3405 (1.4864)  time: 0.7393 (0.4896 -- 3.7612)  data: 0.0011 (0.0001 -- 0.0070)  max mem: 16413
Epoch: [0] Total time: 0:02:23 (0.8992 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 2.7694 (2.7716)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3405 (1.4864)
Val:  [ 0/27]  eta: 0:01:21  loss: 2.7630 (2.7630)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 3.0343 (3.0343 -- 3.0343)  data: 2.6095 (2.6095 -- 2.6095)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.7632 (2.7633)  acc1: 33.3333 (34.3434)  acc5: 88.8889 (86.8687)  time: 0.4711 (0.1985 -- 3.0343)  data: 0.2382 (0.0002 -- 2.6095)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7632 (2.7628)  acc1: 33.3333 (37.0370)  acc5: 88.8889 (87.8307)  time: 0.2036 (0.1688 -- 0.2343)  data: 0.0007 (0.0001 -- 0.0030)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7632 (2.7632)  acc1: 33.3333 (36.5145)  acc5: 85.7143 (85.4772)  time: 0.1884 (0.1654 -- 0.2228)  data: 0.0005 (0.0001 -- 0.0030)  max mem: 16413
Val: Total time: 0:00:08 (0.3008 s / it)
* Acc@1 36.929 Acc@5 84.855 loss 2.763
Accuracy of the network on the 482 val images: 36.93%
[2023-09-03 23:51:29,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-03 23:51:29,156] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-03 23:51:29,156] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-03 23:51:29,156] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-03 23:51:30,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-03 23:51:30,042] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 36.93%
Epoch: [1]  [  0/160]  eta: 0:19:08  lr: 0.000009  min_lr: 0.000002  loss: 2.7688 (2.7688)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6519 (1.6519)  time: 7.1801 (7.1801 -- 7.1801)  data: 6.6572 (6.6572 -- 6.6572)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:34  lr: 0.000011  min_lr: 0.000003  loss: 2.7666 (2.7664)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5032 (1.5045)  time: 0.7967 (0.5229 -- 2.2635)  data: 0.2552 (0.0008 -- 1.7268)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:03  lr: 0.000012  min_lr: 0.000003  loss: 2.7646 (2.7654)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5721 (1.5276)  time: 0.9489 (0.5179 -- 3.7322)  data: 0.1591 (0.0004 -- 1.7157)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000003  loss: 2.7588 (2.7630)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5791 (1.5424)  time: 0.8197 (0.5142 -- 2.8105)  data: 0.2057 (0.0003 -- 2.2715)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000004  loss: 2.7520 (2.7602)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5535 (1.5353)  time: 0.9204 (0.5234 -- 3.4235)  data: 0.1627 (0.0004 -- 1.5057)  max mem: 16413
[2023-09-03 23:53:00,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:53:00,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-09-03 23:53:00,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:53:00,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 2.7454 (2.7569)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5927 (1.5492)  time: 0.9117 (0.5169 -- 2.7678)  data: 0.2185 (0.0002 -- 2.2330)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 2.7287 (2.7524)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5866 (1.5583)  time: 0.8384 (0.5046 -- 3.2070)  data: 0.2580 (0.0005 -- 2.6843)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000004  loss: 2.7182 (2.7472)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7254 (1.5778)  time: 0.8516 (0.5077 -- 3.1632)  data: 0.2485 (0.0003 -- 2.6212)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 2.7118 (2.7432)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7071 (1.5897)  time: 0.6825 (0.4943 -- 1.9302)  data: 0.1051 (0.0002 -- 1.4166)  max mem: 16413
Epoch: [1] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 2.7118 (2.7436)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7071 (1.5897)
Val:  [ 0/27]  eta: 0:01:01  loss: 2.6280 (2.6280)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2622 (2.2622 -- 2.2622)  data: 2.0456 (2.0456 -- 2.0456)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.6541 (2.6528)  acc1: 33.3333 (31.3131)  acc5: 100.0000 (87.8788)  time: 0.4082 (0.1982 -- 2.2622)  data: 0.1986 (0.0008 -- 2.0456)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6437 (2.6472)  acc1: 33.3333 (35.4497)  acc5: 100.0000 (89.4180)  time: 0.2194 (0.1692 -- 0.4450)  data: 0.0198 (0.0001 -- 0.2530)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6473 (2.6523)  acc1: 33.3333 (34.8548)  acc5: 77.7778 (85.4772)  time: 0.2055 (0.1332 -- 0.4450)  data: 0.0191 (0.0001 -- 0.2530)  max mem: 16413
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 36.722 Acc@5 85.477 loss 2.651
Accuracy of the network on the 482 val images: 36.72%
Max accuracy: 36.93%
Epoch: [2]  [  0/160]  eta: 0:22:37  lr: 0.000019  min_lr: 0.000005  loss: 2.7352 (2.7352)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7063 (1.7063)  time: 8.4848 (8.4848 -- 8.4848)  data: 7.9675 (7.9675 -- 7.9675)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:39  lr: 0.000020  min_lr: 0.000005  loss: 2.6983 (2.7030)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6724 (1.6310)  time: 0.7689 (0.5285 -- 4.2861)  data: 0.2270 (0.0004 -- 3.7665)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:10  lr: 0.000021  min_lr: 0.000005  loss: 2.6854 (2.6957)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6855 (1.6551)  time: 1.0424 (0.5221 -- 4.9030)  data: 0.5018 (0.0004 -- 4.3782)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000006  loss: 2.6758 (2.6865)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7116 (1.6909)  time: 0.7445 (0.5240 -- 3.3701)  data: 0.1990 (0.0002 -- 2.8432)  max mem: 16413
[2023-09-03 23:55:03,202] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:55:03,202] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-09-03 23:55:03,202] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:55:03,203] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000006  loss: 2.6638 (2.6811)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7271 (1.7062)  time: 0.8730 (0.5289 -- 2.8585)  data: 0.3344 (0.0005 -- 2.3262)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 2.6424 (2.6763)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8337 (1.7508)  time: 0.9559 (0.5326 -- 2.3392)  data: 0.1780 (0.0003 -- 1.6874)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 2.6381 (2.6711)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7977 (1.7721)  time: 0.9012 (0.5355 -- 2.9417)  data: 0.3218 (0.0006 -- 2.4047)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 2.6168 (2.6622)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7575 (1.7921)  time: 0.7733 (0.5276 -- 2.6567)  data: 0.2219 (0.0005 -- 2.1117)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 2.5879 (2.6537)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8779 (1.8025)  time: 0.7409 (0.4928 -- 1.8190)  data: 0.2176 (0.0002 -- 1.2687)  max mem: 16413
Epoch: [2] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 2.5879 (2.6525)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8779 (1.8025)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.4043 (2.4043)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3667 (2.3667 -- 2.3667)  data: 2.1187 (2.1187 -- 2.1187)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4694 (2.4621)  acc1: 44.4444 (37.3737)  acc5: 100.0000 (92.9293)  time: 0.4124 (0.1986 -- 2.3667)  data: 0.1960 (0.0008 -- 2.1187)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4340 (2.4468)  acc1: 44.4444 (38.0952)  acc5: 100.0000 (93.1217)  time: 0.2179 (0.1692 -- 0.4216)  data: 0.0136 (0.0001 -- 0.2310)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4699 (2.4592)  acc1: 33.3333 (36.9295)  acc5: 88.8889 (90.0415)  time: 0.2026 (0.1324 -- 0.4216)  data: 0.0132 (0.0001 -- 0.2310)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 40.456 Acc@5 89.419 loss 2.462
Accuracy of the network on the 482 val images: 40.46%
[2023-09-03 23:56:31,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-03 23:56:31,458] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-03 23:56:31,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-03 23:56:31,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-03 23:56:32,985] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-03 23:56:32,986] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.46%
Epoch: [3]  [  0/160]  eta: 0:16:56  lr: 0.000028  min_lr: 0.000007  loss: 2.5740 (2.5740)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0964 (2.0964)  time: 6.3522 (6.3522 -- 6.3522)  data: 5.7931 (5.7931 -- 5.7931)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:38  lr: 0.000029  min_lr: 0.000007  loss: 2.6133 (2.6090)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0797 (2.1448)  time: 0.8712 (0.5331 -- 3.3989)  data: 0.2843 (0.0008 -- 2.8547)  max mem: 16413
[2023-09-03 23:57:09,368] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:57:09,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-09-03 23:57:09,369] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:57:09,370] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:04  lr: 0.000031  min_lr: 0.000008  loss: 2.5646 (2.5863)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0055 (2.0918)  time: 0.9350 (0.5281 -- 3.9384)  data: 0.3533 (0.0004 -- 3.4158)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000008  loss: 2.5671 (2.5727)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0716 (2.1330)  time: 0.8886 (0.5276 -- 3.7053)  data: 0.3460 (0.0003 -- 3.1676)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000008  loss: 2.5528 (2.5666)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1136 (2.1381)  time: 0.9559 (0.5180 -- 3.5082)  data: 0.2248 (0.0002 -- 2.3174)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000009  loss: 2.5614 (2.5636)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0274 (2.1378)  time: 0.8926 (0.5133 -- 4.3694)  data: 0.0013 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 2.4914 (2.5507)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3489 (2.1932)  time: 0.7977 (0.5185 -- 2.9427)  data: 0.0023 (0.0002 -- 0.0128)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 2.5293 (2.5456)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1701 (2.1995)  time: 0.8990 (0.5176 -- 4.8665)  data: 0.0025 (0.0004 -- 0.0125)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 2.5049 (2.5398)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8030 (2.2862)  time: 0.6761 (0.4948 -- 2.6803)  data: 0.0586 (0.0002 -- 0.8185)  max mem: 16413
Epoch: [3] Total time: 0:02:24 (0.9009 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 2.5049 (2.5405)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8030 (2.2862)
Val:  [ 0/27]  eta: 0:00:58  loss: 2.1814 (2.1814)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.1653 (2.1653 -- 2.1653)  data: 1.9250 (1.9250 -- 1.9250)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.2839 (2.2681)  acc1: 44.4444 (40.4040)  acc5: 88.8889 (89.8990)  time: 0.3928 (0.2011 -- 2.1653)  data: 0.1768 (0.0010 -- 1.9250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.2367 (2.2450)  acc1: 44.4444 (38.6243)  acc5: 88.8889 (88.8889)  time: 0.2220 (0.1698 -- 0.5188)  data: 0.0175 (0.0001 -- 0.3046)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2523 (2.2669)  acc1: 33.3333 (36.9295)  acc5: 77.7778 (86.7220)  time: 0.2062 (0.1337 -- 0.5188)  data: 0.0169 (0.0001 -- 0.3046)  max mem: 16413
Val: Total time: 0:00:07 (0.2814 s / it)
* Acc@1 39.627 Acc@5 86.929 loss 2.264
Accuracy of the network on the 482 val images: 39.63%
Max accuracy: 40.46%
[2023-09-03 23:59:10,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:59:10,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-09-03 23:59:10,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-03 23:59:10,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:19  lr: 0.000038  min_lr: 0.000010  loss: 2.3978 (2.3978)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1578 (2.1578)  time: 5.3709 (5.3709 -- 5.3709)  data: 4.2755 (4.2755 -- 4.2755)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:47  lr: 0.000039  min_lr: 0.000010  loss: 2.4536 (2.4288)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5072 (2.5807)  time: 0.9911 (0.5250 -- 3.2307)  data: 0.3465 (0.0009 -- 2.6910)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:00  lr: 0.000040  min_lr: 0.000010  loss: 2.4488 (2.4517)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5635 (2.6340)  time: 0.8027 (0.5146 -- 3.5939)  data: 0.2593 (0.0006 -- 3.0728)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000010  loss: 2.3903 (2.4350)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4132 (2.6176)  time: 0.9692 (0.5217 -- 3.4198)  data: 0.4298 (0.0005 -- 2.8659)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 2.4392 (2.4261)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8991 (2.7581)  time: 0.8419 (0.5250 -- 2.1823)  data: 0.1412 (0.0004 -- 1.6566)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:57  lr: 0.000043  min_lr: 0.000011  loss: 2.4560 (2.4240)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6882 (2.8876)  time: 0.9419 (0.5219 -- 4.2943)  data: 0.4017 (0.0004 -- 3.7927)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 2.3864 (2.4156)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3792 (3.0085)  time: 0.8591 (0.5274 -- 3.4724)  data: 0.0501 (0.0007 -- 0.8417)  max mem: 16413
[2023-09-04 00:01:05,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:01:05,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-09-04 00:01:05,247] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:01:05,247] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.4339 (2.4188)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4253 (3.0694)  time: 0.9464 (0.5203 -- 3.6739)  data: 0.4131 (0.0004 -- 3.1689)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.4421 (2.4194)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3078 (3.1796)  time: 0.6086 (0.4945 -- 2.0963)  data: 0.0932 (0.0002 -- 1.5720)  max mem: 16413
Epoch: [4] Total time: 0:02:24 (0.9004 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.4421 (2.4344)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3078 (3.1796)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.9663 (1.9663)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3935 (2.3935 -- 2.3935)  data: 2.1730 (2.1730 -- 2.1730)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.0349 (2.0385)  acc1: 44.4444 (42.4242)  acc5: 100.0000 (91.9192)  time: 0.4099 (0.1970 -- 2.3935)  data: 0.1987 (0.0008 -- 2.1730)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.0064 (2.0088)  acc1: 44.4444 (41.7989)  acc5: 88.8889 (91.0053)  time: 0.2140 (0.1701 -- 0.3641)  data: 0.0123 (0.0001 -- 0.1746)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0556 (2.0402)  acc1: 44.4444 (39.8340)  acc5: 88.8889 (87.5519)  time: 0.2012 (0.1323 -- 0.3641)  data: 0.0121 (0.0001 -- 0.1746)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 42.739 Acc@5 89.004 loss 2.027
Accuracy of the network on the 482 val images: 42.74%
[2023-09-04 00:01:36,605] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:01:36,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:01:36,607] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:01:36,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:01:38,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:01:38,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.74%
Epoch: [5]  [  0/160]  eta: 0:17:09  lr: 0.000047  min_lr: 0.000012  loss: 2.3275 (2.3275)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1470 (3.1470)  time: 6.4368 (6.4368 -- 6.4368)  data: 5.0929 (5.0929 -- 5.0929)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000012  loss: 2.3578 (2.3700)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7608 (3.8435)  time: 0.8843 (0.5191 -- 3.7200)  data: 0.0643 (0.0003 -- 1.2555)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 2.3211 (2.3413)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5325 (3.9711)  time: 0.9018 (0.5300 -- 4.4770)  data: 0.0030 (0.0007 -- 0.0158)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000012  loss: 2.3692 (2.3529)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5576 (3.9044)  time: 0.9955 (0.5196 -- 3.8615)  data: 0.0011 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 2.2621 (2.3400)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5795 (3.8782)  time: 0.7332 (0.5293 -- 2.9748)  data: 0.0014 (0.0002 -- 0.0045)  max mem: 16413
[2023-09-04 00:03:07,053] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:03:07,054] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-09-04 00:03:07,054] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:03:07,055] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:54  lr: 0.000047  min_lr: 0.000012  loss: 2.3813 (2.3432)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7228 (3.9437)  time: 0.7904 (0.5314 -- 2.4855)  data: 0.0022 (0.0003 -- 0.0156)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:35  lr: 0.000047  min_lr: 0.000012  loss: 2.3054 (2.3443)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5950 (4.0059)  time: 0.7929 (0.5406 -- 2.5912)  data: 0.0049 (0.0007 -- 0.0509)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3386 (2.3455)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5287 (4.0924)  time: 0.9516 (0.5464 -- 2.6995)  data: 0.0414 (0.0002 -- 0.7920)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2688 (2.3347)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8471 (4.1180)  time: 0.7165 (0.4948 -- 2.4269)  data: 0.0009 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [5] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2688 (2.3406)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8471 (4.1180)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.7666 (1.7666)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.2607 (2.2607 -- 2.2607)  data: 2.0543 (2.0543 -- 2.0543)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.8553 (1.8343)  acc1: 44.4444 (46.4646)  acc5: 100.0000 (94.9495)  time: 0.4268 (0.2082 -- 2.2607)  data: 0.2074 (0.0006 -- 2.0543)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8302 (1.7999)  acc1: 44.4444 (46.5608)  acc5: 100.0000 (94.1799)  time: 0.2238 (0.1689 -- 0.4558)  data: 0.0161 (0.0001 -- 0.2180)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8553 (1.8315)  acc1: 44.4444 (45.6432)  acc5: 88.8889 (92.9461)  time: 0.2055 (0.1334 -- 0.4558)  data: 0.0158 (0.0001 -- 0.2180)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 49.585 Acc@5 92.739 loss 1.804
Accuracy of the network on the 482 val images: 49.59%
[2023-09-04 00:04:07,240] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:04:07,242] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:04:07,242] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:04:07,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:04:08,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:04:08,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 49.59%
Epoch: [6]  [  0/160]  eta: 0:17:47  lr: 0.000047  min_lr: 0.000012  loss: 2.5361 (2.5361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0419 (5.0419)  time: 6.6739 (6.6739 -- 6.6739)  data: 6.1285 (6.1285 -- 6.1285)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000012  loss: 2.3619 (2.4184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8173 (4.3918)  time: 0.9044 (0.5207 -- 2.4836)  data: 0.2306 (0.0003 -- 1.4824)  max mem: 16413
[2023-09-04 00:04:51,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1913801307802163e-05, 1.1913801307802163e-05, 1.3237557008669071e-05, 1.3237557008669071e-05, 1.4708396676298964e-05, 1.4708396676298964e-05, 1.634266297366552e-05, 1.634266297366552e-05, 1.8158514415183906e-05, 1.8158514415183906e-05, 2.017612712798212e-05, 2.017612712798212e-05, 2.2417919031091242e-05, 2.2417919031091242e-05, 2.490879892343471e-05, 2.490879892343471e-05, 2.7676443248260792e-05, 2.7676443248260792e-05, 3.0751603609178656e-05, 3.0751603609178656e-05, 3.416844845464296e-05, 3.416844845464296e-05, 3.7964942727381054e-05, 3.7964942727381054e-05, 4.218326969709006e-05, 4.218326969709006e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 00:04:51,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.583193329361407, CurrSamplesPerSec=22.183506642272807, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 2.3084 (2.3488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0585 (4.4108)  time: 0.9108 (0.5187 -- 4.7084)  data: 0.3464 (0.0005 -- 4.1736)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000012  loss: 2.2798 (2.3183)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8115 (4.4271)  time: 0.9441 (0.5258 -- 3.5997)  data: 0.3950 (0.0005 -- 3.0842)  max mem: 16413
[2023-09-04 00:05:12,673] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:05:12,673] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-09-04 00:05:12,674] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:05:12,674] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.2574 (2.3088)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2473 (4.6382)  time: 0.7636 (0.5317 -- 3.9658)  data: 0.2100 (0.0003 -- 3.4351)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.2912 (2.3092)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1864 (4.7379)  time: 0.9106 (0.5122 -- 3.7012)  data: 0.3486 (0.0004 -- 3.1557)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1891 (2.2988)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6079 (4.7815)  time: 0.7957 (0.5170 -- 3.0456)  data: 0.2246 (0.0006 -- 2.5206)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.2135 (2.2875)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6571 (4.8448)  time: 0.9172 (0.5223 -- 3.1085)  data: 0.3655 (0.0006 -- 2.5920)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2139 (2.2816)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6308 (4.8805)  time: 0.7234 (0.4949 -- 3.7590)  data: 0.2049 (0.0001 -- 3.2318)  max mem: 16413
Epoch: [6] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2139 (2.2812)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6308 (4.8805)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.6615 (1.6615)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4276 (2.4276 -- 2.4276)  data: 2.1974 (2.1974 -- 2.1974)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6615 (1.6848)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (93.9394)  time: 0.4133 (0.1937 -- 2.4276)  data: 0.2006 (0.0005 -- 2.1974)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5846 (1.6696)  acc1: 55.5556 (53.9683)  acc5: 100.0000 (93.1217)  time: 0.2147 (0.1689 -- 0.4395)  data: 0.0116 (0.0001 -- 0.2190)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6467 (1.7047)  acc1: 44.4444 (51.8672)  acc5: 88.8889 (92.5311)  time: 0.1994 (0.1330 -- 0.4395)  data: 0.0113 (0.0001 -- 0.2190)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 55.809 Acc@5 92.324 loss 1.675
Accuracy of the network on the 482 val images: 55.81%
[2023-09-04 00:06:39,809] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:06:39,811] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:06:39,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:06:39,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:06:41,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:06:41,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.81%
Epoch: [7]  [  0/160]  eta: 0:20:51  lr: 0.000047  min_lr: 0.000012  loss: 1.8493 (1.8493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8296 (4.8296)  time: 7.8195 (7.8195 -- 7.8195)  data: 7.2934 (7.2934 -- 7.2934)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:57  lr: 0.000047  min_lr: 0.000012  loss: 2.3164 (2.2820)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2237 (4.8349)  time: 0.9386 (0.5260 -- 3.5905)  data: 0.2297 (0.0005 -- 3.0578)  max mem: 16413
[2023-09-04 00:07:16,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:07:16,334] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-09-04 00:07:16,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:07:16,336] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2036 (2.2410)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1387 (4.7625)  time: 0.7335 (0.5122 -- 2.6947)  data: 0.0026 (0.0003 -- 0.0087)  max mem: 16413
[2023-09-04 00:07:34,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1175
[2023-09-04 00:07:34,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-09-04 00:07:34,537] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1175
[2023-09-04 00:07:34,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-09-04 00:07:34,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:35  lr: 0.000047  min_lr: 0.000012  loss: 2.3513 (2.2678)  loss_scale: 65536.0000 (45123.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9853 (4.7636)  time: 0.8349 (0.5323 -- 2.9299)  data: 0.0022 (0.0004 -- 0.0083)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.3380 (2.2821)  loss_scale: 32768.0000 (42072.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5561 (4.6765)  time: 0.9553 (0.5242 -- 3.7233)  data: 0.1418 (0.0002 -- 1.2659)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.1142 (2.2606)  loss_scale: 32768.0000 (40230.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3480 (4.7088)  time: 0.8429 (0.5293 -- 3.4045)  data: 0.1245 (0.0005 -- 2.0158)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.2138 (2.2545)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0695 (4.7573)  time: 0.8556 (0.5370 -- 3.6641)  data: 0.0018 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3134 (2.2481)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3348 (4.7494)  time: 0.8060 (0.5246 -- 3.2236)  data: 0.0776 (0.0005 -- 1.4892)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2666 (2.2520)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6235 (4.7476)  time: 0.6955 (0.4940 -- 3.4276)  data: 0.0778 (0.0002 -- 1.5430)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2666 (2.2497)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6235 (4.7476)
Val:  [ 0/27]  eta: 0:00:56  loss: 1.5497 (1.5497)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1040 (2.1040 -- 2.1040)  data: 1.9046 (1.9046 -- 1.9046)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.5474 (1.5811)  acc1: 66.6667 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.3982 (0.1976 -- 2.1040)  data: 0.1863 (0.0010 -- 1.9046)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5079 (1.5744)  acc1: 66.6667 (58.2011)  acc5: 100.0000 (94.7090)  time: 0.2280 (0.1691 -- 0.3543)  data: 0.0232 (0.0001 -- 0.1377)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5701 (1.6123)  acc1: 55.5556 (56.8465)  acc5: 88.8889 (93.7759)  time: 0.2129 (0.1328 -- 0.3543)  data: 0.0221 (0.0001 -- 0.1377)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 59.544 Acc@5 92.946 loss 1.584
Accuracy of the network on the 482 val images: 59.54%
[2023-09-04 00:09:09,234] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:09:09,235] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:09:09,235] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:09:09,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:09:10,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:09:10,630] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.54%
Epoch: [8]  [  0/160]  eta: 0:18:40  lr: 0.000047  min_lr: 0.000012  loss: 2.0471 (2.0471)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0924 (7.0924)  time: 7.0027 (7.0027 -- 7.0027)  data: 5.4299 (5.4299 -- 5.4299)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:42  lr: 0.000047  min_lr: 0.000012  loss: 2.1911 (2.1754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5067 (5.1057)  time: 0.8678 (0.5219 -- 3.1569)  data: 0.0678 (0.0008 -- 0.7084)  max mem: 16413
[2023-09-04 00:09:38,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:09:38,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:09:38,694] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:09:38,694] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000012  loss: 2.1176 (2.1368)  loss_scale: 65536.0000 (46354.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0608 (5.2088)  time: 0.9712 (0.5098 -- 5.5036)  data: 0.0168 (0.0002 -- 0.3131)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 2.0975 (2.1641)  loss_scale: 65536.0000 (52643.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5077 (5.0427)  time: 0.8881 (0.5163 -- 3.8249)  data: 0.0026 (0.0002 -- 0.0132)  max mem: 16413
[2023-09-04 00:10:26,528] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1356
[2023-09-04 00:10:26,528] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1356
[2023-09-04 00:10:26,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:10:26,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:10:26,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 2.2269 (2.1742)  loss_scale: 65536.0000 (53804.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4884 (4.9486)  time: 0.8234 (0.5229 -- 4.1264)  data: 0.0015 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.2267 (2.1959)  loss_scale: 32768.0000 (49638.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3641 (4.8608)  time: 0.8201 (0.5226 -- 2.3358)  data: 0.0920 (0.0002 -- 1.8095)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1336 (2.2004)  loss_scale: 32768.0000 (46850.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5363 (4.8213)  time: 0.8675 (0.5419 -- 3.0660)  data: 0.1306 (0.0002 -- 0.8695)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3554 (2.2139)  loss_scale: 32768.0000 (44852.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5671 (4.9179)  time: 0.8748 (0.5091 -- 4.1672)  data: 0.1128 (0.0003 -- 1.1865)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2542 (2.2166)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8445 (4.8768)  time: 0.6832 (0.4952 -- 2.7344)  data: 0.1710 (0.0001 -- 2.1970)  max mem: 16413
Epoch: [8] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2542 (2.2078)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8445 (4.8768)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.4556 (1.4556)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4426 (2.4426 -- 2.4426)  data: 2.1904 (2.1904 -- 2.1904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4556 (1.4859)  acc1: 44.4444 (54.5455)  acc5: 100.0000 (95.9596)  time: 0.4151 (0.1974 -- 2.4426)  data: 0.2003 (0.0006 -- 2.1904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3673 (1.4708)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (94.7090)  time: 0.2155 (0.1695 -- 0.5008)  data: 0.0173 (0.0001 -- 0.3296)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5402 (1.5128)  acc1: 55.5556 (55.6017)  acc5: 100.0000 (94.6058)  time: 0.1983 (0.1327 -- 0.5008)  data: 0.0168 (0.0001 -- 0.3296)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 60.581 Acc@5 94.191 loss 1.480
Accuracy of the network on the 482 val images: 60.58%
[2023-09-04 00:11:40,788] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:11:40,790] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:11:40,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:11:40,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:11:42,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:11:42,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.58%
Epoch: [9]  [  0/160]  eta: 0:17:54  lr: 0.000047  min_lr: 0.000012  loss: 1.9692 (1.9692)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3843 (5.3843)  time: 6.7182 (6.7182 -- 6.7182)  data: 5.1579 (5.1579 -- 5.1579)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:02:59  lr: 0.000047  min_lr: 0.000012  loss: 2.1554 (2.1704)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1607 (5.2795)  time: 1.0121 (0.5235 -- 4.2723)  data: 0.0686 (0.0003 -- 1.3399)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 2.1565 (2.1836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9513 (5.3110)  time: 0.8080 (0.5288 -- 3.7371)  data: 0.0018 (0.0005 -- 0.0064)  max mem: 16413
[2023-09-04 00:12:30,579] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:12:30,580] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:12:30,580] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:12:30,580] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:12:32,190] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1488
[2023-09-04 00:12:32,190] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1488
[2023-09-04 00:12:32,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:12:32,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:12:32,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 2.2156 (2.1691)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8177 (5.1507)  time: 0.9638 (0.5230 -- 5.1335)  data: 0.0025 (0.0003 -- 0.0156)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.2785 (2.1740)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6656 (5.2941)  time: 0.7579 (0.5239 -- 2.6803)  data: 0.0013 (0.0005 -- 0.0040)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.2061 (2.1744)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6749 (5.2868)  time: 0.9756 (0.5163 -- 3.8904)  data: 0.0020 (0.0005 -- 0.0127)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.1244 (2.1699)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8134 (5.3943)  time: 0.7946 (0.5213 -- 3.1747)  data: 0.0016 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0336 (2.1532)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7836 (5.4347)  time: 0.9426 (0.5113 -- 3.6060)  data: 0.0018 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0496 (2.1529)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7217 (5.5374)  time: 0.6615 (0.4943 -- 2.7757)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [9] Total time: 0:02:24 (0.9033 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0496 (2.1612)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7217 (5.5374)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.4292 (1.4292)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2489 (2.2489 -- 2.2489)  data: 2.0419 (2.0419 -- 2.0419)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.3364 (1.4253)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (95.9596)  time: 0.4038 (0.2055 -- 2.2489)  data: 0.1895 (0.0005 -- 2.0419)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3272 (1.4145)  acc1: 55.5556 (56.0847)  acc5: 100.0000 (94.7090)  time: 0.2206 (0.1692 -- 0.4534)  data: 0.0149 (0.0001 -- 0.2513)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4603 (1.4430)  acc1: 55.5556 (57.2614)  acc5: 100.0000 (94.6058)  time: 0.2040 (0.1328 -- 0.4534)  data: 0.0147 (0.0001 -- 0.2513)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 58.921 Acc@5 94.191 loss 1.418
Accuracy of the network on the 482 val images: 58.92%
Max accuracy: 60.58%
Epoch: [10]  [  0/160]  eta: 0:23:20  lr: 0.000047  min_lr: 0.000012  loss: 2.4840 (2.4840)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5585 (6.5585)  time: 8.7558 (8.7558 -- 8.7558)  data: 8.2063 (8.2063 -- 8.2063)  max mem: 16413
[2023-09-04 00:14:39,253] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:14:39,253] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:14:39,254] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:14:39,255] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 20/160]  eta: 0:02:56  lr: 0.000047  min_lr: 0.000012  loss: 2.0707 (2.1344)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9782 (5.2876)  time: 0.8870 (0.5220 -- 4.2322)  data: 0.2568 (0.0004 -- 3.6929)  max mem: 16413
[2023-09-04 00:14:42,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1623
[2023-09-04 00:14:42,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1623
[2023-09-04 00:14:42,512] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:14:42,512] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:14:42,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 40/160]  eta: 0:02:13  lr: 0.000047  min_lr: 0.000012  loss: 2.1667 (2.1692)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9692 (5.1527)  time: 0.9479 (0.5285 -- 3.5640)  data: 0.0021 (0.0004 -- 0.0052)  max mem: 16413
[2023-09-04 00:15:14,586] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1658
[2023-09-04 00:15:14,586] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1658
[2023-09-04 00:15:14,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:15:14,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:15:14,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 1.9484 (2.1121)  loss_scale: 32768.0000 (35185.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6067 (5.4881)  time: 0.7927 (0.5116 -- 3.1633)  data: 0.0015 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9295 (2.0853)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1602 (5.4756)  time: 0.8807 (0.5248 -- 3.6885)  data: 0.0019 (0.0004 -- 0.0086)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.2167 (2.1136)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3693 (5.4987)  time: 0.8578 (0.5213 -- 3.9407)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0995 (2.1125)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7196 (5.4428)  time: 0.8620 (0.5268 -- 3.6890)  data: 0.0015 (0.0005 -- 0.0031)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.2144 (2.1281)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1807 (5.5429)  time: 0.8187 (0.5167 -- 3.5757)  data: 0.0017 (0.0004 -- 0.0096)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1453 (2.1369)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3678 (5.5956)  time: 0.6473 (0.4941 -- 3.0321)  data: 0.0008 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [10] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1453 (2.1423)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3678 (5.5956)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.3995 (1.3995)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3028 (2.3028 -- 2.3028)  data: 2.0913 (2.0913 -- 2.0913)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2601 (1.3286)  acc1: 44.4444 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4177 (0.1960 -- 2.3028)  data: 0.2076 (0.0008 -- 2.0913)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2364 (1.3203)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (94.7090)  time: 0.2176 (0.1687 -- 0.3903)  data: 0.0168 (0.0001 -- 0.1660)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2601 (1.3722)  acc1: 55.5556 (59.7510)  acc5: 88.8889 (92.9461)  time: 0.2049 (0.1333 -- 0.3903)  data: 0.0161 (0.0001 -- 0.1660)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 61.203 Acc@5 93.568 loss 1.362
Accuracy of the network on the 482 val images: 61.20%
[2023-09-04 00:16:44,164] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:16:44,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:16:44,166] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:16:44,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:16:45,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:16:45,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.20%
Epoch: [11]  [  0/160]  eta: 0:19:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3870 (2.3870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6866 (4.6866)  time: 7.2401 (7.2401 -- 7.2401)  data: 5.4179 (5.4179 -- 5.4179)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:34  lr: 0.000047  min_lr: 0.000012  loss: 2.1945 (2.2348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6696 (5.6539)  time: 0.7951 (0.5306 -- 2.8683)  data: 0.1605 (0.0007 -- 1.1710)  max mem: 16413
[2023-09-04 00:17:15,954] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:17:15,954] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:17:15,954] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:17:15,955] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 2.1278 (2.1748)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3719 (5.7060)  time: 0.9457 (0.5174 -- 3.0042)  data: 0.0178 (0.0004 -- 0.3202)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 2.2108 (2.1808)  loss_scale: 32768.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0686 (5.9045)  time: 0.9106 (0.5158 -- 3.8645)  data: 0.0150 (0.0003 -- 0.2752)  max mem: 16413
[2023-09-04 00:17:55,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1832
[2023-09-04 00:17:55,329] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:17:55,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 00:17:55,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1832
[2023-09-04 00:17:55,329] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [11]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 2.2165 (2.1759)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1291 (6.2050)  time: 0.8292 (0.5270 -- 3.3488)  data: 0.0518 (0.0002 -- 0.9839)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.1602 (2.1624)  loss_scale: 16384.0000 (23683.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8418 (6.2424)  time: 0.8942 (0.5304 -- 3.5420)  data: 0.3096 (0.0003 -- 3.0043)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.1295 (2.1575)  loss_scale: 16384.0000 (22477.2231)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8994 (6.2241)  time: 0.9238 (0.5192 -- 4.4531)  data: 0.3781 (0.0005 -- 3.9377)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0381 (2.1340)  loss_scale: 16384.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5864 (6.1694)  time: 0.8314 (0.5292 -- 2.8975)  data: 0.2639 (0.0003 -- 1.9992)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9950 (2.1233)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6214 (6.2042)  time: 0.7024 (0.4952 -- 2.0395)  data: 0.1792 (0.0001 -- 1.5480)  max mem: 16413
Epoch: [11] Total time: 0:02:23 (0.8961 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9950 (2.1064)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6214 (6.2042)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.2793 (1.2793)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2369 (2.2369 -- 2.2369)  data: 1.9966 (1.9966 -- 1.9966)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.2144 (1.3036)  acc1: 66.6667 (56.5657)  acc5: 100.0000 (97.9798)  time: 0.3974 (0.2009 -- 2.2369)  data: 0.1830 (0.0008 -- 1.9966)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1906 (1.2522)  acc1: 66.6667 (59.7884)  acc5: 100.0000 (97.3545)  time: 0.2253 (0.1685 -- 0.5426)  data: 0.0200 (0.0001 -- 0.3617)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2220 (1.3105)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (95.8506)  time: 0.2095 (0.1355 -- 0.5426)  data: 0.0197 (0.0001 -- 0.3617)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 62.863 Acc@5 94.606 loss 1.280
Accuracy of the network on the 482 val images: 62.86%
[2023-09-04 00:19:16,768] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:19:16,771] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:19:16,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:19:16,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:19:18,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:19:18,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [12]  [  0/160]  eta: 0:19:29  lr: 0.000047  min_lr: 0.000012  loss: 1.4699 (1.4699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6367 (9.6367)  time: 7.3074 (7.3074 -- 7.3074)  data: 4.9524 (4.9524 -- 4.9524)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8978 (1.9541)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2682 (6.3259)  time: 0.8177 (0.5232 -- 2.5530)  data: 0.1531 (0.0004 -- 1.0007)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0025 (2.0163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9926 (6.0718)  time: 0.8784 (0.5112 -- 3.1177)  data: 0.0805 (0.0004 -- 1.4932)  max mem: 16413
[2023-09-04 00:20:00,259] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:20:00,259] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:20:00,259] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:20:00,259] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 2.0294 (2.0160)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3059 (5.9601)  time: 0.9617 (0.5277 -- 3.6930)  data: 0.4071 (0.0003 -- 3.1474)  max mem: 16413
[2023-09-04 00:20:33,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[1.1871674843346339e-05, 1.1871674843346339e-05, 1.3190749825940376e-05, 1.3190749825940376e-05, 1.4656388695489305e-05, 1.4656388695489305e-05, 1.628487632832145e-05, 1.628487632832145e-05, 1.8094307031468277e-05, 1.8094307031468277e-05, 2.010478559052031e-05, 2.010478559052031e-05, 2.2338650656133676e-05, 2.2338650656133676e-05, 2.4820722951259638e-05, 2.4820722951259638e-05, 2.7578581056955155e-05, 2.7578581056955155e-05, 3.064286784106128e-05, 3.064286784106128e-05, 3.4047630934512536e-05, 3.4047630934512536e-05, 3.783070103834726e-05, 3.783070103834726e-05, 4.203411226483028e-05, 4.203411226483028e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 00:20:33,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=16.967161619299663, CurrSamplesPerSec=22.659073371936085, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 2.0712 (2.0183)  loss_scale: 32768.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8834 (5.9493)  time: 0.7586 (0.5285 -- 2.4706)  data: 0.0224 (0.0004 -- 0.4155)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.3446 (2.0665)  loss_scale: 32768.0000 (26117.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5989 (5.9556)  time: 0.9179 (0.5241 -- 2.5345)  data: 0.1148 (0.0009 -- 0.8881)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.2376 (2.0839)  loss_scale: 32768.0000 (27216.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6462 (6.0017)  time: 0.8071 (0.5285 -- 2.0143)  data: 0.0702 (0.0003 -- 1.3731)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8656 (2.0582)  loss_scale: 32768.0000 (28003.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7953 (5.8795)  time: 0.8577 (0.5473 -- 3.3062)  data: 0.1239 (0.0007 -- 1.1274)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1400 (2.0715)  loss_scale: 32768.0000 (28569.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8563 (5.8554)  time: 0.7624 (0.4969 -- 2.9182)  data: 0.0552 (0.0002 -- 0.8353)  max mem: 16413
Epoch: [12] Total time: 0:02:21 (0.8875 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1400 (2.0774)  loss_scale: 32768.0000 (28569.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8563 (5.8554)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.2180 (1.2180)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5803 (2.5803 -- 2.5803)  data: 2.3446 (2.3446 -- 2.3446)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2150 (1.2653)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4376 (0.2025 -- 2.5803)  data: 0.2200 (0.0007 -- 2.3446)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1626 (1.2326)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (95.2381)  time: 0.2095 (0.1695 -- 0.2972)  data: 0.0042 (0.0001 -- 0.0660)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2267 (1.2580)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (95.4357)  time: 0.1940 (0.1326 -- 0.2972)  data: 0.0039 (0.0001 -- 0.0660)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 64.315 Acc@5 95.021 loss 1.233
Accuracy of the network on the 482 val images: 64.32%
[2023-09-04 00:21:48,338] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:21:48,340] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:21:48,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:21:48,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:21:49,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:21:49,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.32%
Epoch: [13]  [  0/160]  eta: 0:20:55  lr: 0.000047  min_lr: 0.000012  loss: 1.6737 (1.6737)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0518 (5.0518)  time: 7.8472 (7.8472 -- 7.8472)  data: 6.5264 (6.5264 -- 6.5264)  max mem: 16413
[2023-09-04 00:22:06,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:22:06,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:22:06,024] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:22:06,025] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:22:07,696] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2092
[2023-09-04 00:22:07,696] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2092
[2023-09-04 00:22:07,696] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:22:07,696] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:22:07,696] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 20/160]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000012  loss: 2.2056 (2.1548)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9383 (6.3913)  time: 0.8554 (0.5210 -- 3.8205)  data: 0.0334 (0.0007 -- 0.4027)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 2.1064 (2.1483)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9229 (6.4711)  time: 0.9130 (0.5232 -- 3.3891)  data: 0.0023 (0.0003 -- 0.0167)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 2.1254 (2.1329)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9613 (6.4363)  time: 0.8693 (0.5286 -- 3.6557)  data: 0.0430 (0.0003 -- 0.8315)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:19  lr: 0.000047  min_lr: 0.000012  loss: 2.1302 (2.1358)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8675 (6.2998)  time: 0.9967 (0.5087 -- 4.6305)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.9440 (2.1156)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5468 (6.3832)  time: 0.7159 (0.5153 -- 2.8557)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0282 (2.0988)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8784 (6.4756)  time: 0.8807 (0.5310 -- 3.1067)  data: 0.0014 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1282 (2.0784)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4856 (6.3785)  time: 0.8505 (0.5186 -- 5.3999)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
[2023-09-04 00:23:59,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:23:59,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:23:59,784] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:23:59,784] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:24:05,544] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2227
[2023-09-04 00:24:05,544] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:24:05,544] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2227
[2023-09-04 00:24:05,544] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:24:05,544] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 00:24:06,608] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2229
[2023-09-04 00:24:06,608] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2229
[2023-09-04 00:24:06,608] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:24:06,608] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:24:06,608] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9718 (2.0594)  loss_scale: 16384.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5616 (6.3848)  time: 0.7048 (0.4959 -- 3.0224)  data: 0.0008 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [13] Total time: 0:02:23 (0.8942 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9718 (2.0694)  loss_scale: 16384.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5616 (6.3848)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.1719 (1.1719)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5915 (2.5915 -- 2.5915)  data: 2.3644 (2.3644 -- 2.3644)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0894 (1.2195)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4315 (0.2013 -- 2.5915)  data: 0.2165 (0.0006 -- 2.3644)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0611 (1.1814)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2098 (0.1685 -- 0.2301)  data: 0.0051 (0.0001 -- 0.0431)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1220 (1.2158)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.0207)  time: 0.1948 (0.1330 -- 0.2301)  data: 0.0045 (0.0001 -- 0.0431)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 64.315 Acc@5 94.191 loss 1.196
Accuracy of the network on the 482 val images: 64.32%
Max accuracy: 64.32%
Epoch: [14]  [  0/160]  eta: 0:23:12  lr: 0.000047  min_lr: 0.000012  loss: 1.9681 (1.9681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8003 (5.8003)  time: 8.7044 (8.7044 -- 8.7044)  data: 8.1895 (8.1895 -- 8.1895)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000012  loss: 2.0163 (1.9872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1844 (6.9841)  time: 0.8072 (0.5149 -- 3.3652)  data: 0.1694 (0.0004 -- 2.4082)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000012  loss: 2.0042 (1.9838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4641 (6.5721)  time: 0.9761 (0.5285 -- 4.5685)  data: 0.0014 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 2.0078 (1.9990)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9349 (6.4151)  time: 0.8284 (0.5138 -- 5.0761)  data: 0.1215 (0.0002 -- 2.1339)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:19  lr: 0.000047  min_lr: 0.000012  loss: 2.0310 (2.0172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1346 (6.3967)  time: 0.9884 (0.5260 -- 4.0121)  data: 0.0015 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.0716 (2.0217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3590 (6.5124)  time: 0.6926 (0.5070 -- 2.4536)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
[2023-09-04 00:26:12,114] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:26:12,114] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:26:12,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:26:12,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9271 (2.0103)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2697 (6.4600)  time: 1.0107 (0.5200 -- 3.2491)  data: 0.0017 (0.0004 -- 0.0113)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0608 (2.0101)  loss_scale: 32768.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9050 (6.5203)  time: 0.8723 (0.5222 -- 4.3822)  data: 0.0014 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9165 (2.0052)  loss_scale: 32768.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7483 (6.4802)  time: 0.5953 (0.4962 -- 1.4266)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [14] Total time: 0:02:23 (0.8977 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9165 (2.0404)  loss_scale: 32768.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7483 (6.4802)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.1270 (1.1270)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3607 (2.3607 -- 2.3607)  data: 2.1182 (2.1182 -- 2.1182)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1270 (1.2057)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4352 (0.2033 -- 2.3607)  data: 0.2171 (0.0005 -- 2.1182)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0603 (1.1649)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (95.7672)  time: 0.2217 (0.1700 -- 0.4748)  data: 0.0137 (0.0001 -- 0.2568)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1401 (1.1988)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.4357)  time: 0.2050 (0.1330 -- 0.4748)  data: 0.0132 (0.0001 -- 0.2568)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 65.768 Acc@5 94.398 loss 1.174
Accuracy of the network on the 482 val images: 65.77%
[2023-09-04 00:26:52,114] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:26:52,116] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:26:52,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:26:52,116] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:26:53,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:26:53,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.77%
Epoch: [15]  [  0/160]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000012  loss: 2.0131 (2.0131)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3061 (7.3061)  time: 6.6839 (6.6839 -- 6.6839)  data: 6.1619 (6.1619 -- 6.1619)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:58  lr: 0.000047  min_lr: 0.000012  loss: 2.0654 (2.0405)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3762 (6.9937)  time: 1.0013 (0.5178 -- 3.0922)  data: 0.3772 (0.0003 -- 2.5707)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 2.1050 (2.0458)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9693 (6.7631)  time: 0.8029 (0.5345 -- 2.3634)  data: 0.2523 (0.0002 -- 1.8364)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0835 (2.0550)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4580 (6.7568)  time: 0.8431 (0.5267 -- 2.7910)  data: 0.2937 (0.0004 -- 2.2798)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9330 (2.0455)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7290 (6.5538)  time: 0.9263 (0.5303 -- 3.2929)  data: 0.3732 (0.0009 -- 2.7612)  max mem: 16413
[2023-09-04 00:28:15,648] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:28:15,648] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:28:15,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:28:15,649] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.0644 (2.0510)  loss_scale: 65536.0000 (37634.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6047 (6.5259)  time: 0.8727 (0.5234 -- 3.4242)  data: 0.3321 (0.0006 -- 2.8944)  max mem: 16413
[2023-09-04 00:28:38,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2510
[2023-09-04 00:28:38,535] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:28:38,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2510
[2023-09-04 00:28:38,536] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:28:38,536] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8956 (2.0388)  loss_scale: 32768.0000 (39267.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8619 (6.5550)  time: 0.9311 (0.5220 -- 2.4888)  data: 0.3857 (0.0005 -- 1.9757)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.2123 (2.0588)  loss_scale: 32768.0000 (38345.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1197 (6.5533)  time: 0.8539 (0.5152 -- 5.1914)  data: 0.3071 (0.0004 -- 4.6722)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2250 (2.0778)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6572 (6.5093)  time: 0.6539 (0.4948 -- 2.3848)  data: 0.1273 (0.0002 -- 1.8382)  max mem: 16413
Epoch: [15] Total time: 0:02:23 (0.8992 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2250 (2.0367)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6572 (6.5093)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.0600 (1.0600)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3479 (2.3479 -- 2.3479)  data: 2.1107 (2.1107 -- 2.1107)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0600 (1.1200)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4261 (0.2001 -- 2.3479)  data: 0.2026 (0.0006 -- 2.1107)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9638 (1.1223)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (94.7090)  time: 0.2244 (0.1695 -- 0.3610)  data: 0.0160 (0.0001 -- 0.1020)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1307 (1.1678)  acc1: 55.5556 (60.9959)  acc5: 100.0000 (94.6058)  time: 0.2077 (0.1324 -- 0.3610)  data: 0.0157 (0.0001 -- 0.1020)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 64.523 Acc@5 94.398 loss 1.123
Accuracy of the network on the 482 val images: 64.52%
Max accuracy: 65.77%
Epoch: [16]  [  0/160]  eta: 0:19:04  lr: 0.000047  min_lr: 0.000012  loss: 2.1067 (2.1067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9051 (7.9051)  time: 7.1557 (7.1557 -- 7.1557)  data: 6.6134 (6.6134 -- 6.6134)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9706 (2.0468)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5772 (6.7940)  time: 0.8243 (0.5203 -- 3.3840)  data: 0.2719 (0.0005 -- 2.6605)  max mem: 16413
[2023-09-04 00:30:04,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2597
[2023-09-04 00:30:04,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2597
[2023-09-04 00:30:04,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:30:04,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 00:30:04,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [16]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 2.0607 (2.0283)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1495 (6.4999)  time: 0.9559 (0.5236 -- 3.4705)  data: 0.3683 (0.0006 -- 2.9121)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9072 (2.0121)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6978 (6.4082)  time: 0.8058 (0.5316 -- 2.1015)  data: 0.1696 (0.0006 -- 1.2272)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.9966 (2.0183)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1834 (6.5520)  time: 0.9048 (0.5166 -- 2.6262)  data: 0.1283 (0.0003 -- 2.0927)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 2.0825 (2.0208)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7350 (6.6852)  time: 0.8739 (0.5106 -- 2.4181)  data: 0.2587 (0.0007 -- 1.8860)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 2.1911 (2.0375)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0303 (6.7321)  time: 0.8226 (0.5316 -- 3.1548)  data: 0.1001 (0.0001 -- 0.9255)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0184 (2.0380)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0064 (6.6663)  time: 0.9322 (0.5254 -- 3.3540)  data: 0.2860 (0.0007 -- 2.8406)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0909 (2.0503)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1277 (6.5734)  time: 0.6932 (0.4947 -- 2.1882)  data: 0.0833 (0.0001 -- 1.6532)  max mem: 16413
Epoch: [16] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0909 (2.0457)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1277 (6.5734)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.0557 (1.0557)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3215 (2.3215 -- 2.3215)  data: 2.1229 (2.1229 -- 2.1229)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0557 (1.1485)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4199 (0.2127 -- 2.3215)  data: 0.1984 (0.0006 -- 2.1229)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9618 (1.1176)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (95.2381)  time: 0.2255 (0.1709 -- 0.3766)  data: 0.0122 (0.0001 -- 0.1818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1130 (1.1471)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (94.6058)  time: 0.2066 (0.1328 -- 0.3766)  data: 0.0119 (0.0001 -- 0.1818)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 64.315 Acc@5 94.191 loss 1.095
Accuracy of the network on the 482 val images: 64.32%
Max accuracy: 65.77%
Epoch: [17]  [  0/160]  eta: 0:19:57  lr: 0.000046  min_lr: 0.000012  loss: 2.5465 (2.5465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7294 (3.7294)  time: 7.4824 (7.4824 -- 7.4824)  data: 4.3127 (4.3127 -- 4.3127)  max mem: 16413
[2023-09-04 00:32:09,616] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:32:09,617] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:32:09,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:32:09,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 20/160]  eta: 0:02:53  lr: 0.000046  min_lr: 0.000012  loss: 1.8288 (1.9297)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8625 (5.6140)  time: 0.9274 (0.5289 -- 3.4274)  data: 0.2172 (0.0004 -- 2.8673)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.9006 (1.9416)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9075 (5.6504)  time: 0.8400 (0.5180 -- 3.9214)  data: 0.2917 (0.0003 -- 3.3844)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.8889 (1.9641)  loss_scale: 32768.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8436 (5.8812)  time: 0.9020 (0.5280 -- 3.8105)  data: 0.3548 (0.0005 -- 3.2676)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.9771 (1.9651)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1417 (6.1701)  time: 0.8722 (0.5208 -- 3.2738)  data: 0.2594 (0.0003 -- 2.7457)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.9974 (1.9779)  loss_scale: 32768.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0218 (6.1488)  time: 0.8037 (0.5185 -- 3.1714)  data: 0.2606 (0.0003 -- 2.6633)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8619 (1.9661)  loss_scale: 32768.0000 (31955.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9243 (6.3069)  time: 1.0043 (0.5166 -- 5.1658)  data: 0.0360 (0.0004 -- 0.6759)  max mem: 16413
[2023-09-04 00:33:50,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2841
[2023-09-04 00:33:50,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2841
[2023-09-04 00:33:50,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:33:50,993] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:33:50,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7979 (1.9420)  loss_scale: 16384.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9548 (6.1807)  time: 0.8474 (0.5184 -- 2.7370)  data: 0.0023 (0.0003 -- 0.0100)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7859 (1.9353)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4820 (6.2076)  time: 0.6593 (0.4964 -- 2.3968)  data: 0.0009 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [17] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7859 (1.9716)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4820 (6.2076)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.9942 (0.9942)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3979 (2.3979 -- 2.3979)  data: 2.1918 (2.1918 -- 2.1918)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9942 (1.1286)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4209 (0.2045 -- 2.3979)  data: 0.2011 (0.0010 -- 2.1918)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9550 (1.0659)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2191 (0.1691 -- 0.3451)  data: 0.0100 (0.0001 -- 0.1623)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0810 (1.1185)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (94.6058)  time: 0.2005 (0.1329 -- 0.3451)  data: 0.0095 (0.0001 -- 0.1623)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 65.768 Acc@5 93.983 loss 1.096
Accuracy of the network on the 482 val images: 65.77%
[2023-09-04 00:34:25,980] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:34:25,981] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:34:25,981] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:34:25,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:34:27,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:34:27,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.77%
Epoch: [18]  [  0/160]  eta: 0:18:32  lr: 0.000046  min_lr: 0.000012  loss: 1.9110 (1.9110)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9745 (7.9745)  time: 6.9533 (6.9533 -- 6.9533)  data: 6.4111 (6.4111 -- 6.4111)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000012  loss: 1.9715 (1.9677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7100 (7.0703)  time: 0.8764 (0.5326 -- 4.0371)  data: 0.3023 (0.0008 -- 3.5074)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.9855 (1.9716)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0199 (6.9447)  time: 0.8994 (0.5320 -- 3.5956)  data: 0.3070 (0.0003 -- 3.0590)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 2.1300 (2.0299)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2549 (6.7309)  time: 0.9870 (0.5098 -- 4.8233)  data: 0.4452 (0.0003 -- 4.3095)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.9137 (1.9980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4820 (6.7791)  time: 0.7871 (0.5255 -- 3.2213)  data: 0.2439 (0.0002 -- 2.6949)  max mem: 16413
[2023-09-04 00:35:54,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:35:54,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:35:54,914] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:35:54,915] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 2.0801 (2.0107)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9073 (6.6582)  time: 0.8814 (0.5134 -- 4.4319)  data: 0.3438 (0.0003 -- 3.9234)  max mem: 16413
[2023-09-04 00:36:19,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[1.1769696168306417e-05, 1.1769696168306417e-05, 1.307744018700713e-05, 1.307744018700713e-05, 1.4530489096674585e-05, 1.4530489096674585e-05, 1.6144987885193987e-05, 1.6144987885193987e-05, 1.7938875427993317e-05, 1.7938875427993317e-05, 1.9932083808881464e-05, 1.9932083808881464e-05, 2.214675978764607e-05, 2.214675978764607e-05, 2.4607510875162297e-05, 2.4607510875162297e-05, 2.734167875018033e-05, 2.734167875018033e-05, 3.037964305575592e-05, 3.037964305575592e-05, 3.3755158950839915e-05, 3.3755158950839915e-05, 3.75057321675999e-05, 3.75057321675999e-05, 4.167303574177767e-05, 4.167303574177767e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 00:36:19,203] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.19435524107593, CurrSamplesPerSec=23.258090736392752, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.1253 (2.0248)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7102 (6.8791)  time: 0.8413 (0.5208 -- 3.8330)  data: 0.2889 (0.0003 -- 3.3138)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0698 (2.0331)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0022 (6.7730)  time: 0.8853 (0.5252 -- 3.4882)  data: 0.3386 (0.0004 -- 2.9612)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9326 (2.0273)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0310 (6.6369)  time: 0.6484 (0.4976 -- 2.0506)  data: 0.1271 (0.0002 -- 1.5451)  max mem: 16413
Epoch: [18] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9326 (2.0029)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0310 (6.6369)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.9587 (0.9587)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3205 (2.3205 -- 2.3205)  data: 2.0875 (2.0875 -- 2.0875)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9587 (1.0960)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4358 (0.1958 -- 2.3205)  data: 0.2181 (0.0004 -- 2.0875)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8898 (1.0620)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (95.7672)  time: 0.2229 (0.1689 -- 0.5474)  data: 0.0189 (0.0001 -- 0.3034)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0474 (1.1092)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.0207)  time: 0.2087 (0.1334 -- 0.5474)  data: 0.0186 (0.0001 -- 0.3034)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 69.502 Acc@5 94.606 loss 1.068
Accuracy of the network on the 482 val images: 69.50%
[2023-09-04 00:36:57,659] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:36:57,661] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:36:57,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:36:57,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:36:58,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:36:58,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.50%
Epoch: [19]  [  0/160]  eta: 0:18:09  lr: 0.000046  min_lr: 0.000012  loss: 1.6143 (1.6143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3146 (7.3146)  time: 6.8112 (6.8112 -- 6.8112)  data: 6.2730 (6.2730 -- 6.2730)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9156 (1.8797)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9010 (6.0475)  time: 0.8390 (0.5229 -- 3.1486)  data: 0.2384 (0.0002 -- 2.5970)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000012  loss: 1.9221 (1.9178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0078 (6.3285)  time: 0.8272 (0.5327 -- 4.3661)  data: 0.1758 (0.0009 -- 1.8444)  max mem: 16413
[2023-09-04 00:37:56,427] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:37:56,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:37:56,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:37:56,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 2.1393 (1.9894)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3464 (6.1285)  time: 0.9236 (0.5312 -- 3.7632)  data: 0.3415 (0.0006 -- 3.2510)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000012  loss: 1.9242 (1.9651)  loss_scale: 65536.0000 (42072.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4472 (6.3027)  time: 0.8525 (0.5308 -- 2.3299)  data: 0.2885 (0.0009 -- 1.7650)  max mem: 16413
[2023-09-04 00:38:16,888] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3124
[2023-09-04 00:38:16,888] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:38:16,888] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 00:38:16,888] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3124
[2023-09-04 00:38:16,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.8760 (1.9593)  loss_scale: 32768.0000 (41203.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5474 (6.3634)  time: 0.9157 (0.5086 -- 3.0050)  data: 0.2956 (0.0006 -- 2.4604)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0644 (1.9771)  loss_scale: 32768.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7461 (6.4754)  time: 0.9073 (0.5281 -- 3.2708)  data: 0.2218 (0.0005 -- 2.7209)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9606 (1.9798)  loss_scale: 32768.0000 (38810.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1713 (6.4537)  time: 0.8337 (0.5237 -- 3.8424)  data: 0.2601 (0.0004 -- 3.3243)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0861 (1.9745)  loss_scale: 32768.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7107 (6.3486)  time: 0.6582 (0.4954 -- 1.9725)  data: 0.0864 (0.0002 -- 1.4489)  max mem: 16413
Epoch: [19] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0861 (1.9896)  loss_scale: 32768.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7107 (6.3486)
[2023-09-04 00:39:20,317] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-09-04 00:39:20,319] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-09-04 00:39:20,321] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-09-04 00:39:20,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-09-04 00:39:21,379] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-09-04 00:39:21,379] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 1.2175 (1.2175)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3583 (2.3583 -- 2.3583)  data: 2.1058 (2.1058 -- 2.1058)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1419 (1.0912)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4132 (0.2055 -- 2.3583)  data: 0.1924 (0.0007 -- 2.1058)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9291 (1.0396)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (95.7672)  time: 0.2166 (0.1699 -- 0.3270)  data: 0.0059 (0.0001 -- 0.1041)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0300 (1.1118)  acc1: 66.6667 (62.6556)  acc5: 100.0000 (93.7759)  time: 0.2009 (0.1325 -- 0.3270)  data: 0.0057 (0.0001 -- 0.1041)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 65.560 Acc@5 93.361 loss 1.070
Accuracy of the network on the 482 val images: 65.56%
Max accuracy: 69.50%
Epoch: [20]  [  0/160]  eta: 0:17:23  lr: 0.000046  min_lr: 0.000012  loss: 2.3636 (2.3636)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4150 (4.4150)  time: 6.5237 (6.5237 -- 6.5237)  data: 4.3810 (4.3810 -- 4.3810)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000012  loss: 1.8937 (1.9775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3553 (6.2676)  time: 0.9080 (0.5201 -- 4.1788)  data: 0.0731 (0.0003 -- 1.4237)  max mem: 16413
[2023-09-04 00:40:09,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3237
[2023-09-04 00:40:09,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3237
[2023-09-04 00:40:09,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:40:09,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:40:09,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 2.1506 (2.0285)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9156 (6.1779)  time: 0.9077 (0.5173 -- 4.3068)  data: 0.0234 (0.0003 -- 0.4231)  max mem: 16413
Epoch: [20]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 2.0034 (2.0223)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0657 (6.2094)  time: 0.8535 (0.5144 -- 2.6802)  data: 0.2295 (0.0003 -- 2.1608)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 2.0063 (2.0062)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5922 (6.0455)  time: 0.9483 (0.5062 -- 5.1224)  data: 0.4014 (0.0004 -- 4.5838)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0012 (2.0055)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5507 (6.0510)  time: 0.7620 (0.5311 -- 2.0654)  data: 0.2120 (0.0006 -- 1.5513)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9990 (2.0018)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7178 (5.9812)  time: 0.8284 (0.5288 -- 3.8902)  data: 0.2841 (0.0005 -- 3.3662)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9871 (1.9928)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2565 (6.0742)  time: 0.9969 (0.5186 -- 2.8693)  data: 0.2123 (0.0005 -- 2.1063)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0613 (1.9980)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9885 (6.0820)  time: 0.6250 (0.4938 -- 1.8707)  data: 0.1029 (0.0003 -- 1.2909)  max mem: 16413
Epoch: [20] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0613 (1.9889)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9885 (6.0820)
Val:  [ 0/27]  eta: 0:01:08  loss: 1.0786 (1.0786)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.5329 (2.5329 -- 2.5329)  data: 2.3014 (2.3014 -- 2.3014)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0098 (1.0711)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4369 (0.1940 -- 2.5329)  data: 0.2290 (0.0006 -- 2.3014)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9342 (1.0177)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (95.7672)  time: 0.2194 (0.1689 -- 0.4459)  data: 0.0243 (0.0001 -- 0.2662)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0083 (1.0731)  acc1: 66.6667 (62.2407)  acc5: 100.0000 (94.6058)  time: 0.2056 (0.1325 -- 0.4459)  data: 0.0241 (0.0001 -- 0.2662)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 67.012 Acc@5 94.191 loss 1.024
Accuracy of the network on the 482 val images: 67.01%
Max accuracy: 69.50%
Epoch: [21]  [  0/160]  eta: 0:25:29  lr: 0.000046  min_lr: 0.000012  loss: 1.9625 (1.9625)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4483 (7.4483)  time: 9.5606 (9.5606 -- 9.5606)  data: 9.0292 (9.0292 -- 9.0292)  max mem: 16413
[2023-09-04 00:42:12,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:42:12,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:42:12,601] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:42:12,601] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 20/160]  eta: 0:02:46  lr: 0.000046  min_lr: 0.000012  loss: 2.0389 (2.0220)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6465 (6.4758)  time: 0.7691 (0.5249 -- 3.3166)  data: 0.2281 (0.0004 -- 2.7609)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:06  lr: 0.000046  min_lr: 0.000012  loss: 1.8663 (1.9723)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3083 (6.4501)  time: 0.9164 (0.5134 -- 3.8900)  data: 0.3711 (0.0006 -- 3.3365)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000012  loss: 2.1320 (1.9881)  loss_scale: 32768.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9845 (6.4369)  time: 0.7538 (0.5304 -- 2.7505)  data: 0.1801 (0.0004 -- 2.1982)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.8766 (1.9655)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8954 (6.4020)  time: 0.9643 (0.5270 -- 4.2317)  data: 0.1694 (0.0004 -- 2.6535)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0174 (1.9739)  loss_scale: 32768.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0392 (6.3939)  time: 0.7647 (0.5332 -- 3.2448)  data: 0.2126 (0.0009 -- 2.7148)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9253 (1.9691)  loss_scale: 32768.0000 (31955.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5661 (6.3039)  time: 0.9691 (0.5329 -- 3.5686)  data: 0.3059 (0.0005 -- 2.3307)  max mem: 16413
[2023-09-04 00:44:03,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:44:03,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:44:03,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:44:03,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:44:07,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3499
[2023-09-04 00:44:07,920] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:44:07,921] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 00:44:07,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3499
[2023-09-04 00:44:07,921] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7982 (1.9571)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3248 (6.3826)  time: 0.8187 (0.5196 -- 3.1833)  data: 0.0327 (0.0002 -- 0.3606)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0109 (1.9612)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0992 (6.4240)  time: 0.6699 (0.4963 -- 1.8226)  data: 0.0040 (0.0002 -- 0.0637)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0109 (1.9586)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0992 (6.4240)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8536 (0.8536)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4535 (2.4535 -- 2.4535)  data: 2.2342 (2.2342 -- 2.2342)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8687 (1.0391)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4232 (0.1986 -- 2.4535)  data: 0.2044 (0.0005 -- 2.2342)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9103 (0.9957)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (95.2381)  time: 0.2174 (0.1703 -- 0.2659)  data: 0.0093 (0.0001 -- 0.0853)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9567 (1.0259)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (94.6058)  time: 0.1989 (0.1328 -- 0.2659)  data: 0.0089 (0.0001 -- 0.0853)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 71.369 Acc@5 94.606 loss 0.992
Accuracy of the network on the 482 val images: 71.37%
[2023-09-04 00:44:29,112] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:44:29,113] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:44:29,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:44:29,114] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:44:30,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:44:30,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.37%
Epoch: [22]  [  0/160]  eta: 0:20:11  lr: 0.000046  min_lr: 0.000012  loss: 1.7481 (1.7481)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0247 (4.0247)  time: 7.5714 (7.5714 -- 7.5714)  data: 6.9904 (6.9904 -- 6.9904)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000012  loss: 1.9446 (1.9113)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7174 (5.9732)  time: 0.8185 (0.5326 -- 3.2246)  data: 0.2592 (0.0006 -- 2.6973)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 2.0695 (1.9826)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9170 (6.2882)  time: 0.9336 (0.5343 -- 2.4029)  data: 0.2373 (0.0003 -- 1.5166)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 2.1338 (2.0350)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8630 (6.5864)  time: 0.8211 (0.5165 -- 2.8442)  data: 0.1738 (0.0004 -- 2.3268)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.9611 (2.0142)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6886 (6.6502)  time: 0.8846 (0.5242 -- 3.0046)  data: 0.3148 (0.0005 -- 2.4744)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0018 (2.0075)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1027 (6.7319)  time: 0.8229 (0.5167 -- 2.2491)  data: 0.2790 (0.0006 -- 1.6857)  max mem: 16413
[2023-09-04 00:46:11,247] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:46:11,248] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:46:11,251] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:46:11,252] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:46:12,561] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3629
[2023-09-04 00:46:12,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:46:12,562] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3629
[2023-09-04 00:46:12,562] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 00:46:12,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0464 (2.0070)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1698 (6.7735)  time: 1.0346 (0.5250 -- 3.8196)  data: 0.2714 (0.0004 -- 3.3051)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0629 (2.0097)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1674 (6.7274)  time: 0.8173 (0.5301 -- 3.4351)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
[2023-09-04 00:46:53,010] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3679
[2023-09-04 00:46:53,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:46:53,010] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3679
[2023-09-04 00:46:53,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:46:53,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0494 (2.0039)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5807 (6.6492)  time: 0.6525 (0.4882 -- 3.0149)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [22] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0494 (1.9738)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5807 (6.6492)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9650 (0.9650)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4358 (2.4358 -- 2.4358)  data: 2.2014 (2.2014 -- 2.2014)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9235 (1.0560)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4321 (0.1963 -- 2.4358)  data: 0.2139 (0.0005 -- 2.2014)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9235 (0.9988)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.8254)  time: 0.2159 (0.1701 -- 0.3408)  data: 0.0091 (0.0001 -- 0.1275)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9637 (1.0409)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2006 (0.1334 -- 0.3408)  data: 0.0081 (0.0001 -- 0.1275)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 67.635 Acc@5 94.813 loss 1.006
Accuracy of the network on the 482 val images: 67.63%
Max accuracy: 71.37%
Epoch: [23]  [  0/160]  eta: 0:21:07  lr: 0.000046  min_lr: 0.000012  loss: 2.1886 (2.1886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1945 (7.1945)  time: 7.9222 (7.9222 -- 7.9222)  data: 7.4020 (7.4020 -- 7.4020)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000012  loss: 1.9291 (1.9301)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0015 (6.0169)  time: 0.8930 (0.5107 -- 4.8442)  data: 0.3515 (0.0003 -- 4.3279)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.9875 (1.9427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1192 (6.4399)  time: 0.8612 (0.5304 -- 4.3877)  data: 0.3111 (0.0003 -- 3.8407)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000012  loss: 2.1723 (2.0079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0850 (6.4365)  time: 1.0249 (0.5082 -- 4.1774)  data: 0.4883 (0.0001 -- 3.6418)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 2.1278 (2.0132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3433 (6.2714)  time: 0.7753 (0.5076 -- 3.2672)  data: 0.2397 (0.0002 -- 2.7473)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 2.0191 (2.0117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7189 (6.2390)  time: 0.9098 (0.5269 -- 3.1430)  data: 0.3687 (0.0004 -- 2.6181)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9830 (2.0166)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8252 (6.2265)  time: 0.7202 (0.5329 -- 2.3126)  data: 0.1706 (0.0003 -- 1.7676)  max mem: 16413
[2023-09-04 00:48:59,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:48:59,331] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:48:59,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:48:59,332] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9410 (2.0059)  loss_scale: 32768.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0128 (6.2213)  time: 0.9064 (0.5262 -- 2.4316)  data: 0.3315 (0.0006 -- 1.8977)  max mem: 16413
[2023-09-04 00:49:19,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3829
[2023-09-04 00:49:19,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3829
[2023-09-04 00:49:19,151] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:49:19,151] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:49:19,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8518 (2.0021)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6093 (6.2625)  time: 0.7761 (0.4966 -- 2.3773)  data: 0.1764 (0.0002 -- 1.8693)  max mem: 16413
Epoch: [23] Total time: 0:02:23 (0.8999 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8518 (1.9950)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6093 (6.2625)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.2283 (1.2283)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2814 (2.2814 -- 2.2814)  data: 2.0630 (2.0630 -- 2.0630)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9383 (1.0695)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4184 (0.2034 -- 2.2814)  data: 0.2031 (0.0008 -- 2.0630)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9318 (1.0169)  acc1: 66.6667 (67.7249)  acc5: 100.0000 (95.7672)  time: 0.2311 (0.1690 -- 0.5313)  data: 0.0257 (0.0001 -- 0.3403)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0055 (1.0696)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (94.6058)  time: 0.2146 (0.1326 -- 0.5313)  data: 0.0253 (0.0001 -- 0.3403)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 69.087 Acc@5 94.398 loss 1.033
Accuracy of the network on the 482 val images: 69.09%
Max accuracy: 71.37%
Epoch: [24]  [  0/160]  eta: 0:20:01  lr: 0.000046  min_lr: 0.000012  loss: 1.6297 (1.6297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2098 (6.2098)  time: 7.5070 (7.5070 -- 7.5070)  data: 6.3348 (6.3348 -- 6.3348)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000012  loss: 1.9460 (1.9078)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9351 (6.8475)  time: 0.8973 (0.5156 -- 4.0128)  data: 0.1142 (0.0005 -- 1.3278)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000012  loss: 1.7651 (1.8567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4875 (6.4827)  time: 0.8363 (0.5182 -- 3.0525)  data: 0.1389 (0.0006 -- 1.1943)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:42  lr: 0.000046  min_lr: 0.000012  loss: 1.8316 (1.8705)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6031 (6.5285)  time: 1.0154 (0.5159 -- 5.1500)  data: 0.3889 (0.0003 -- 4.6205)  max mem: 16413
[2023-09-04 00:50:39,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3907
[2023-09-04 00:50:39,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3907
[2023-09-04 00:50:39,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 00:50:39,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 00:50:39,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9225 (1.8965)  loss_scale: 8192.0000 (14968.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7224 (6.7494)  time: 0.8638 (0.5112 -- 4.1154)  data: 0.3301 (0.0003 -- 3.6011)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.6370 (1.8598)  loss_scale: 8192.0000 (13626.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7205 (6.8285)  time: 0.8877 (0.5180 -- 3.9063)  data: 0.3408 (0.0004 -- 3.3801)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8006 (1.8619)  loss_scale: 8192.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9463 (6.7817)  time: 0.8025 (0.5218 -- 3.6482)  data: 0.2597 (0.0003 -- 3.1368)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9925 (1.8827)  loss_scale: 8192.0000 (12084.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4389 (6.7187)  time: 0.8831 (0.5255 -- 3.1588)  data: 0.3382 (0.0005 -- 2.6396)  max mem: 16413
[2023-09-04 00:51:56,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[1.1608898360300739e-05, 1.1608898360300739e-05, 1.2898775955889711e-05, 1.2898775955889711e-05, 1.4331973284321898e-05, 1.4331973284321898e-05, 1.5924414760357666e-05, 1.5924414760357666e-05, 1.7693794178175183e-05, 1.7693794178175183e-05, 1.9659771309083537e-05, 1.9659771309083537e-05, 2.1844190343426154e-05, 2.1844190343426154e-05, 2.4271322603806833e-05, 2.4271322603806833e-05, 2.6968136226452038e-05, 2.6968136226452038e-05, 2.9964595807168928e-05, 2.9964595807168928e-05, 3.329399534129881e-05, 3.329399534129881e-05, 3.6993328156998675e-05, 3.6993328156998675e-05, 4.1103697952220754e-05, 4.1103697952220754e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 00:51:56,489] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.782765013441555, CurrSamplesPerSec=24.398361344721124, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9638 (1.8906)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4481 (6.6975)  time: 0.7367 (0.4949 -- 2.8608)  data: 0.2185 (0.0002 -- 2.3494)  max mem: 16413
Epoch: [24] Total time: 0:02:23 (0.8974 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9638 (1.9037)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4481 (6.6975)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.9874 (0.9874)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3851 (2.3851 -- 2.3851)  data: 2.1323 (2.1323 -- 2.1323)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8968 (0.9875)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4186 (0.2032 -- 2.3851)  data: 0.2009 (0.0006 -- 2.1323)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8963 (0.9486)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1697 -- 0.3190)  data: 0.0127 (0.0001 -- 0.1346)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9082 (0.9936)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (95.8506)  time: 0.2046 (0.1330 -- 0.3190)  data: 0.0124 (0.0001 -- 0.1346)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 71.162 Acc@5 95.643 loss 0.942
Accuracy of the network on the 482 val images: 71.16%
Max accuracy: 71.37%
Epoch: [25]  [  0/160]  eta: 0:19:50  lr: 0.000046  min_lr: 0.000012  loss: 2.1798 (2.1798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6456 (4.6456)  time: 7.4376 (7.4376 -- 7.4376)  data: 6.8783 (6.8783 -- 6.8783)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:03:03  lr: 0.000046  min_lr: 0.000012  loss: 2.0119 (2.0395)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0937 (6.6275)  time: 1.0016 (0.5253 -- 3.5562)  data: 0.2683 (0.0007 -- 2.5401)  max mem: 16413
[2023-09-04 00:52:45,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:52:45,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 00:52:45,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:52:45,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [25]  [ 40/160]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000012  loss: 1.9443 (1.9771)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4626 (6.3188)  time: 0.8160 (0.5208 -- 3.3175)  data: 0.0013 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 2.0602 (2.0103)  loss_scale: 16384.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4826 (6.2761)  time: 0.8898 (0.5344 -- 3.6620)  data: 0.0025 (0.0002 -- 0.0088)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.8801 (1.9857)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0176 (6.2888)  time: 0.7830 (0.5232 -- 3.3780)  data: 0.0019 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.8836 (1.9724)  loss_scale: 16384.0000 (13464.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8692 (6.2622)  time: 0.9209 (0.5241 -- 3.7635)  data: 0.0019 (0.0002 -- 0.0150)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.8922 (1.9553)  loss_scale: 16384.0000 (13946.7107)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6666 (6.2044)  time: 0.8086 (0.5208 -- 2.6477)  data: 0.0015 (0.0002 -- 0.0067)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0238 (1.9568)  loss_scale: 16384.0000 (14292.4255)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3952 (6.2897)  time: 0.8264 (0.5356 -- 3.0653)  data: 0.0019 (0.0007 -- 0.0031)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0948 (1.9491)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3783 (6.3259)  time: 0.7135 (0.4950 -- 3.2207)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [25] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0948 (1.9538)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3783 (6.3259)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.9612 (0.9612)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1762 (2.1762 -- 2.1762)  data: 1.9536 (1.9536 -- 1.9536)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9612 (1.0020)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (96.9697)  time: 0.4147 (0.1980 -- 2.1762)  data: 0.1953 (0.0008 -- 1.9536)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7925 (0.9379)  acc1: 77.7778 (70.3704)  acc5: 100.0000 (96.2963)  time: 0.2286 (0.1711 -- 0.3502)  data: 0.0189 (0.0001 -- 0.0968)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9291 (0.9930)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (95.4357)  time: 0.2138 (0.1333 -- 0.3502)  data: 0.0186 (0.0001 -- 0.0968)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 72.407 Acc@5 95.228 loss 0.952
Accuracy of the network on the 482 val images: 72.41%
[2023-09-04 00:54:34,122] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:54:34,124] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:54:34,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:54:34,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:54:35,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:54:35,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.41%
Epoch: [26]  [  0/160]  eta: 0:19:28  lr: 0.000046  min_lr: 0.000012  loss: 1.1263 (1.1263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0518 (7.0518)  time: 7.3020 (7.3020 -- 7.3020)  data: 5.4105 (5.4105 -- 5.4105)  max mem: 16413
[2023-09-04 00:54:45,111] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:54:45,112] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 00:54:45,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:54:45,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 20/160]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000012  loss: 2.0597 (2.0419)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5782 (5.7764)  time: 0.8269 (0.5307 -- 2.8498)  data: 0.0418 (0.0004 -- 0.6249)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:11  lr: 0.000046  min_lr: 0.000012  loss: 1.9629 (2.0070)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2821 (5.6301)  time: 1.0472 (0.5260 -- 4.6323)  data: 0.0083 (0.0005 -- 0.1208)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 1.9916 (2.0082)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3131 (5.5872)  time: 0.8389 (0.5206 -- 3.8623)  data: 0.0014 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000012  loss: 1.8900 (1.9857)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9940 (5.7357)  time: 0.9045 (0.5223 -- 3.0232)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 2.0947 (1.9849)  loss_scale: 32768.0000 (32119.1287)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6867 (6.0012)  time: 0.8120 (0.5268 -- 3.1728)  data: 0.0023 (0.0003 -- 0.0156)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000012  loss: 2.0744 (1.9830)  loss_scale: 32768.0000 (32226.3802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5223 (6.1810)  time: 0.9585 (0.5182 -- 3.5277)  data: 0.0012 (0.0002 -- 0.0030)  max mem: 16413
[2023-09-04 00:56:39,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:56:39,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 00:56:39,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:56:39,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 00:56:43,599] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4299
[2023-09-04 00:56:43,599] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4299
[2023-09-04 00:56:43,599] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 00:56:43,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 00:56:43,600] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.9472 (1.9765)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4902 (6.0926)  time: 0.6783 (0.5155 -- 2.5695)  data: 0.0014 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 2.0283 (1.9760)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1394 (6.1561)  time: 0.7505 (0.4933 -- 3.4818)  data: 0.0010 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [26] Total time: 0:02:23 (0.8944 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 2.0283 (1.9627)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1394 (6.1561)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9428 (0.9428)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3520 (2.3520 -- 2.3520)  data: 2.1455 (2.1455 -- 2.1455)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8963 (0.9902)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (97.9798)  time: 0.4146 (0.2089 -- 2.3520)  data: 0.1970 (0.0005 -- 2.1455)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8109 (0.9339)  acc1: 77.7778 (70.8995)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1696 -- 0.3089)  data: 0.0116 (0.0001 -- 0.1287)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8782 (0.9686)  acc1: 77.7778 (70.5394)  acc5: 100.0000 (96.2656)  time: 0.2023 (0.1326 -- 0.3089)  data: 0.0109 (0.0001 -- 0.1287)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 73.859 Acc@5 95.643 loss 0.926
Accuracy of the network on the 482 val images: 73.86%
[2023-09-04 00:57:06,429] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 00:57:06,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 00:57:06,431] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 00:57:06,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 00:57:07,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 00:57:07,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.86%
Epoch: [27]  [  0/160]  eta: 0:23:20  lr: 0.000045  min_lr: 0.000012  loss: 1.9244 (1.9244)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8824 (6.8824)  time: 8.7513 (8.7513 -- 8.7513)  data: 4.7552 (4.7552 -- 4.7552)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000012  loss: 1.8933 (1.8476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8481 (6.6780)  time: 0.8947 (0.5247 -- 4.6902)  data: 0.0084 (0.0004 -- 0.1378)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000012  loss: 1.8110 (1.8476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4359 (6.0731)  time: 0.8866 (0.5213 -- 3.5431)  data: 0.0263 (0.0004 -- 0.5043)  max mem: 16413
[2023-09-04 00:57:59,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4368
[2023-09-04 00:57:59,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4368
[2023-09-04 00:57:59,682] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:57:59,682] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 00:57:59,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [ 60/160]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000012  loss: 1.9233 (1.8636)  loss_scale: 16384.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8573 (6.5197)  time: 0.8647 (0.5261 -- 3.8619)  data: 0.0016 (0.0003 -- 0.0088)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000012  loss: 1.9493 (1.8885)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2643 (6.5898)  time: 0.8990 (0.5386 -- 3.4216)  data: 0.0016 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000012  loss: 1.9569 (1.9175)  loss_scale: 16384.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0274 (6.5496)  time: 0.7039 (0.5346 -- 2.5139)  data: 0.0016 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000012  loss: 2.0451 (1.9248)  loss_scale: 16384.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5004 (6.7030)  time: 0.9055 (0.5393 -- 2.6872)  data: 0.1802 (0.0008 -- 2.1636)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.9086 (1.9234)  loss_scale: 16384.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4765 (6.6345)  time: 0.9207 (0.5290 -- 3.6472)  data: 0.3595 (0.0004 -- 3.1312)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 2.0091 (1.9336)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7748 (6.6555)  time: 0.6797 (0.4953 -- 3.7314)  data: 0.1622 (0.0002 -- 3.2303)  max mem: 16413
Epoch: [27] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 2.0091 (1.9395)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7748 (6.6555)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.9177 (0.9177)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3020 (2.3020 -- 2.3020)  data: 2.0847 (2.0847 -- 2.0847)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9177 (1.0614)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4080 (0.2036 -- 2.3020)  data: 0.1905 (0.0007 -- 2.0847)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7707 (0.9598)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (96.2963)  time: 0.2219 (0.1689 -- 0.3766)  data: 0.0145 (0.0001 -- 0.1759)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8743 (1.0213)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (95.4357)  time: 0.2045 (0.1332 -- 0.3766)  data: 0.0142 (0.0001 -- 0.1759)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 69.295 Acc@5 95.021 loss 0.981
Accuracy of the network on the 482 val images: 69.29%
Max accuracy: 73.86%
Epoch: [28]  [  0/160]  eta: 0:17:53  lr: 0.000045  min_lr: 0.000012  loss: 1.9349 (1.9349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6818 (8.6818)  time: 6.7066 (6.7066 -- 6.7066)  data: 5.3181 (5.3181 -- 5.3181)  max mem: 16413
[2023-09-04 01:00:02,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:00:02,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:00:02,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:00:02,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 20/160]  eta: 0:02:47  lr: 0.000045  min_lr: 0.000012  loss: 1.8178 (1.7942)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6156 (6.7031)  time: 0.9229 (0.5283 -- 3.4718)  data: 0.2810 (0.0010 -- 2.9453)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000012  loss: 2.1085 (1.8782)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7377 (6.6501)  time: 0.9138 (0.5184 -- 3.2169)  data: 0.0877 (0.0005 -- 1.0072)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:34  lr: 0.000045  min_lr: 0.000011  loss: 1.7784 (1.8661)  loss_scale: 32768.0000 (28201.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8661 (6.4732)  time: 0.7103 (0.5223 -- 3.6866)  data: 0.0023 (0.0002 -- 0.0157)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.8623 (1.8749)  loss_scale: 32768.0000 (29329.3827)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7301 (6.3660)  time: 1.0075 (0.5193 -- 3.9836)  data: 0.1561 (0.0007 -- 2.0074)  max mem: 16413
[2023-09-04 01:00:58,925] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4564
[2023-09-04 01:00:58,925] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4564
[2023-09-04 01:00:58,925] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:00:58,925] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:00:58,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 2.0233 (1.9054)  loss_scale: 16384.0000 (27252.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7199 (6.4310)  time: 0.8417 (0.5207 -- 4.2188)  data: 0.0354 (0.0004 -- 0.6806)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.8863 (1.9003)  loss_scale: 16384.0000 (25456.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9869 (6.5200)  time: 0.8618 (0.5268 -- 3.7682)  data: 0.0029 (0.0004 -- 0.0325)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.7940 (1.8967)  loss_scale: 16384.0000 (24169.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3211 (6.6739)  time: 0.8624 (0.5292 -- 4.0893)  data: 0.0123 (0.0003 -- 0.2205)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8360 (1.8971)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7799 (6.6860)  time: 0.6869 (0.4957 -- 3.1512)  data: 0.0013 (0.0002 -- 0.0124)  max mem: 16413
Epoch: [28] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8360 (1.9198)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7799 (6.6860)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.8084 (0.8084)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3230 (2.3230 -- 2.3230)  data: 2.1047 (2.1047 -- 2.1047)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8084 (1.0139)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4133 (0.2119 -- 2.3230)  data: 0.1926 (0.0005 -- 2.1047)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7836 (0.9139)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (95.7672)  time: 0.2241 (0.1696 -- 0.4371)  data: 0.0153 (0.0001 -- 0.2201)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8010 (0.9753)  acc1: 77.7778 (70.9544)  acc5: 100.0000 (95.0207)  time: 0.2067 (0.1350 -- 0.4371)  data: 0.0148 (0.0001 -- 0.2201)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 73.651 Acc@5 95.228 loss 0.934
Accuracy of the network on the 482 val images: 73.65%
Max accuracy: 73.86%
Epoch: [29]  [  0/160]  eta: 0:21:21  lr: 0.000045  min_lr: 0.000011  loss: 2.6865 (2.6865)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6151 (4.6151)  time: 8.0123 (8.0123 -- 8.0123)  data: 7.4657 (7.4657 -- 7.4657)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000011  loss: 2.1161 (2.0953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7278 (5.8192)  time: 0.8046 (0.5172 -- 2.6860)  data: 0.2634 (0.0002 -- 2.1694)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000011  loss: 1.7817 (1.9822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7343 (5.8213)  time: 0.9965 (0.5347 -- 3.9200)  data: 0.4466 (0.0005 -- 3.3653)  max mem: 16413
[2023-09-04 01:03:01,874] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:03:01,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:03:01,875] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:03:01,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 60/160]  eta: 0:01:38  lr: 0.000045  min_lr: 0.000011  loss: 1.9442 (1.9981)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5452 (5.8576)  time: 0.8097 (0.5223 -- 4.1168)  data: 0.2640 (0.0004 -- 3.5885)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.9662 (1.9779)  loss_scale: 32768.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2688 (6.1580)  time: 0.8796 (0.5254 -- 3.8825)  data: 0.3356 (0.0004 -- 3.3541)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.7025 (1.9498)  loss_scale: 32768.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2523 (6.1492)  time: 0.8835 (0.5215 -- 4.0405)  data: 0.3338 (0.0004 -- 3.5022)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8591 (1.9366)  loss_scale: 32768.0000 (25591.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0481 (6.1780)  time: 0.8301 (0.5233 -- 2.6921)  data: 0.2854 (0.0004 -- 2.1657)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8358 (1.9224)  loss_scale: 32768.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9882 (6.1708)  time: 0.7823 (0.5295 -- 3.1813)  data: 0.0966 (0.0003 -- 1.8945)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.9875 (1.9295)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9323 (6.2271)  time: 0.8053 (0.4963 -- 3.3018)  data: 0.0010 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [29] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.9875 (1.9071)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9323 (6.2271)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0392 (1.0392)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4183 (2.4183 -- 2.4183)  data: 2.2028 (2.2028 -- 2.2028)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0127 (0.9532)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (97.9798)  time: 0.4092 (0.1854 -- 2.4183)  data: 0.2016 (0.0006 -- 2.2028)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7630 (0.8760)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (97.3545)  time: 0.2187 (0.1697 -- 0.5316)  data: 0.0181 (0.0001 -- 0.3451)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8350 (0.9308)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (96.6805)  time: 0.2068 (0.1327 -- 0.5316)  data: 0.0179 (0.0001 -- 0.3451)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 75.104 Acc@5 96.473 loss 0.903
Accuracy of the network on the 482 val images: 75.10%
[2023-09-04 01:04:40,188] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:04:40,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:04:40,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:04:40,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:04:41,605] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:04:41,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.10%
Epoch: [30]  [  0/160]  eta: 0:16:21  lr: 0.000045  min_lr: 0.000011  loss: 1.6513 (1.6513)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5768 (5.5768)  time: 6.1322 (6.1322 -- 6.1322)  data: 5.5728 (5.5728 -- 5.5728)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:11  lr: 0.000045  min_lr: 0.000011  loss: 1.8014 (1.8457)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1873 (6.9128)  time: 1.1333 (0.5028 -- 5.7900)  data: 0.5606 (0.0003 -- 5.2866)  max mem: 16413
[2023-09-04 01:05:10,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:05:10,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:05:10,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:05:10,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:05:17,216] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4827
[2023-09-04 01:05:17,216] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4827
[2023-09-04 01:05:17,216] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:05:17,216] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:05:17,216] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 40/160]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000011  loss: 1.8497 (1.8610)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0631 (6.5889)  time: 0.7898 (0.5049 -- 3.5736)  data: 0.2547 (0.0003 -- 3.0591)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000045  min_lr: 0.000011  loss: 1.7538 (1.8366)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8496 (6.8502)  time: 0.9682 (0.5082 -- 4.2902)  data: 0.2733 (0.0002 -- 3.0592)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000011  loss: 1.6927 (1.8047)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9853 (6.7389)  time: 0.7423 (0.5158 -- 2.9882)  data: 0.0088 (0.0003 -- 0.1530)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8772 (1.8338)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8314 (6.8169)  time: 0.8725 (0.5297 -- 3.1945)  data: 0.2315 (0.0002 -- 2.1633)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8689 (1.8451)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7392 (7.0083)  time: 0.9237 (0.5187 -- 3.7909)  data: 0.0216 (0.0001 -- 0.4064)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.9544 (1.8652)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1007 (6.9528)  time: 0.7088 (0.5328 -- 1.5733)  data: 0.0674 (0.0008 -- 0.4814)  max mem: 16413
[2023-09-04 01:07:03,712] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:07:03,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:07:03,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:07:03,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.9122 (1.8774)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8649 (6.9579)  time: 0.7611 (0.4964 -- 2.3766)  data: 0.0939 (0.0002 -- 0.9812)  max mem: 16413
Epoch: [30] Total time: 0:02:23 (0.8976 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.9122 (1.8756)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8649 (6.9579)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.6285 (0.6285)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2219 (2.2219 -- 2.2219)  data: 1.9811 (1.9811 -- 1.9811)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5827 (0.8911)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (97.9798)  time: 0.4281 (0.2029 -- 2.2219)  data: 0.2104 (0.0006 -- 1.9811)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5935 (0.8493)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2363 (0.1697 -- 0.5244)  data: 0.0336 (0.0001 -- 0.3364)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7788 (0.9093)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.4357)  time: 0.2191 (0.1327 -- 0.5244)  data: 0.0329 (0.0001 -- 0.3364)  max mem: 16413
Val: Total time: 0:00:07 (0.2938 s / it)
* Acc@1 76.141 Acc@5 95.851 loss 0.862
Accuracy of the network on the 482 val images: 76.14%
[2023-09-04 01:07:13,162] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:07:13,164] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:07:13,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:07:13,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:07:14,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:07:14,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.14%
Epoch: [31]  [  0/160]  eta: 0:20:16  lr: 0.000045  min_lr: 0.000011  loss: 1.1397 (1.1397)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9739 (7.9739)  time: 7.6027 (7.6027 -- 7.6027)  data: 7.0474 (7.0474 -- 7.0474)  max mem: 16413
[2023-09-04 01:07:31,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4970
[2023-09-04 01:07:31,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:07:31,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 01:07:31,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4970
[2023-09-04 01:07:31,336] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [31]  [ 20/160]  eta: 0:02:43  lr: 0.000045  min_lr: 0.000011  loss: 2.0321 (1.8744)  loss_scale: 32768.0000 (48371.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3159 (7.0124)  time: 0.8474 (0.5331 -- 3.7053)  data: 0.1818 (0.0004 -- 2.0118)  max mem: 16413
[2023-09-04 01:07:54,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.1390910354167532e-05, 1.1390910354167532e-05, 1.2656567060186148e-05, 1.2656567060186148e-05, 1.4062852289095717e-05, 1.4062852289095717e-05, 1.5625391432328577e-05, 1.5625391432328577e-05, 1.736154603592064e-05, 1.736154603592064e-05, 1.9290606706578487e-05, 1.9290606706578487e-05, 2.1434007451753873e-05, 2.1434007451753873e-05, 2.381556383528208e-05, 2.381556383528208e-05, 2.6461737594757868e-05, 2.6461737594757868e-05, 2.9401930660842074e-05, 2.9401930660842074e-05, 3.2668811845380086e-05, 3.2668811845380086e-05, 3.629867982820009e-05, 3.629867982820009e-05, 4.033186647577788e-05, 4.033186647577788e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 01:07:54,898] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.46959954412456, CurrSamplesPerSec=21.65960037990203, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000011  loss: 1.9868 (1.9312)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9426 (6.6470)  time: 0.9027 (0.5206 -- 3.4169)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8950 (1.9099)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0324 (6.5903)  time: 0.8471 (0.5375 -- 2.5981)  data: 0.0564 (0.0005 -- 0.5850)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.9492 (1.9110)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5130 (6.4736)  time: 0.9058 (0.5387 -- 3.1852)  data: 0.0017 (0.0005 -- 0.0073)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 2.1237 (1.9336)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9871 (6.5035)  time: 0.7921 (0.5191 -- 2.5051)  data: 0.1189 (0.0004 -- 1.9858)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8595 (1.9328)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6179 (6.5533)  time: 1.0128 (0.5249 -- 4.3427)  data: 0.0020 (0.0003 -- 0.0062)  max mem: 16413
[2023-09-04 01:09:20,185] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:09:20,185] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:09:20,185] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:09:20,185] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000011  loss: 1.7484 (1.9169)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4398 (6.5263)  time: 0.6192 (0.5221 -- 1.7912)  data: 0.0013 (0.0003 -- 0.0025)  max mem: 16413
[2023-09-04 01:09:21,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5102
[2023-09-04 01:09:21,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5102
[2023-09-04 01:09:21,833] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:09:21,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:09:21,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 2.0180 (1.9129)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6702 (6.4727)  time: 0.7716 (0.5002 -- 1.9937)  data: 0.0815 (0.0002 -- 1.2906)  max mem: 16413
Epoch: [31] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 2.0180 (1.9124)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6702 (6.4727)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.7549 (0.7549)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2179 (2.2179 -- 2.2179)  data: 2.0055 (2.0055 -- 2.0055)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7549 (0.9649)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.3965 (0.1991 -- 2.2179)  data: 0.1849 (0.0010 -- 2.0055)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7429 (0.8741)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (96.8254)  time: 0.2281 (0.1700 -- 0.4369)  data: 0.0233 (0.0001 -- 0.2242)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8439 (0.9247)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (96.2656)  time: 0.2125 (0.1326 -- 0.4369)  data: 0.0230 (0.0001 -- 0.2242)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 76.556 Acc@5 96.266 loss 0.857
Accuracy of the network on the 482 val images: 76.56%
[2023-09-04 01:09:43,402] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:09:43,404] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:09:43,404] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:09:43,404] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:09:44,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:09:44,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.56%
Epoch: [32]  [  0/160]  eta: 0:21:53  lr: 0.000045  min_lr: 0.000011  loss: 2.3665 (2.3665)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9905 (4.9905)  time: 8.2064 (8.2064 -- 8.2064)  data: 7.6688 (7.6688 -- 7.6688)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8443 (1.8993)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9525 (6.4210)  time: 0.9186 (0.5245 -- 4.8832)  data: 0.2771 (0.0004 -- 2.9912)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000011  loss: 1.9178 (1.9355)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9052 (6.4072)  time: 0.8152 (0.5261 -- 3.9327)  data: 0.1111 (0.0003 -- 1.4420)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000011  loss: 1.8052 (1.9141)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7213 (6.3123)  time: 0.9787 (0.5066 -- 5.7637)  data: 0.0898 (0.0003 -- 1.7682)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8401 (1.9059)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4731 (6.3177)  time: 0.8655 (0.5247 -- 3.4044)  data: 0.0067 (0.0003 -- 0.1100)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 2.0606 (1.9271)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9760 (6.3236)  time: 0.8347 (0.5083 -- 2.6638)  data: 0.2298 (0.0003 -- 2.1488)  max mem: 16413
[2023-09-04 01:11:29,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:11:29,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:11:29,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:11:29,441] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 2.0840 (1.9449)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9936 (6.2606)  time: 0.7984 (0.5312 -- 3.2184)  data: 0.2553 (0.0005 -- 2.6826)  max mem: 16413
[2023-09-04 01:11:37,780] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5241
[2023-09-04 01:11:37,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:11:37,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 01:11:37,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5241
[2023-09-04 01:11:37,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8294 (1.9420)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5428 (6.3111)  time: 0.9272 (0.5407 -- 4.0040)  data: 0.3694 (0.0003 -- 3.4813)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.9826 (1.9385)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5844 (6.2601)  time: 0.7031 (0.4936 -- 2.6077)  data: 0.1840 (0.0002 -- 2.0971)  max mem: 16413
Epoch: [32] Total time: 0:02:24 (0.9030 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.9826 (1.8992)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5844 (6.2601)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6276 (0.6276)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3420 (2.3420 -- 2.3420)  data: 2.1024 (2.1024 -- 2.1024)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6598 (0.9124)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (97.9798)  time: 0.4179 (0.2011 -- 2.3420)  data: 0.1971 (0.0009 -- 2.1024)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7579 (0.8550)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.3545)  time: 0.2213 (0.1711 -- 0.4231)  data: 0.0152 (0.0001 -- 0.2367)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8854 (0.9120)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (96.6805)  time: 0.2031 (0.1325 -- 0.4231)  data: 0.0149 (0.0001 -- 0.2367)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 76.556 Acc@5 96.473 loss 0.853
Accuracy of the network on the 482 val images: 76.56%
[2023-09-04 01:12:17,019] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:12:17,021] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:12:17,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:12:17,021] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:12:18,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:12:18,422] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.56%
Epoch: [33]  [  0/160]  eta: 0:20:56  lr: 0.000045  min_lr: 0.000011  loss: 2.2145 (2.2145)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5286 (8.5286)  time: 7.8542 (7.8542 -- 7.8542)  data: 5.1503 (5.1503 -- 5.1503)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:46  lr: 0.000045  min_lr: 0.000011  loss: 1.9592 (1.9225)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4603 (6.6947)  time: 0.8579 (0.5265 -- 3.4271)  data: 0.0162 (0.0003 -- 0.2181)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000011  loss: 1.8637 (1.8835)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5894 (6.6932)  time: 0.8879 (0.5247 -- 2.8672)  data: 0.0116 (0.0003 -- 0.1723)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 2.0409 (1.9149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7857 (6.8387)  time: 0.8663 (0.5149 -- 3.3246)  data: 0.2677 (0.0006 -- 2.8048)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.8843 (1.9102)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7445 (6.7157)  time: 0.8767 (0.5234 -- 3.1467)  data: 0.0487 (0.0003 -- 0.9422)  max mem: 16413
[2023-09-04 01:13:44,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:13:44,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:13:44,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:13:44,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.6842 (1.8930)  loss_scale: 65536.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3673 (6.6960)  time: 0.8180 (0.5286 -- 3.1524)  data: 0.1407 (0.0003 -- 1.4238)  max mem: 16413
[2023-09-04 01:14:08,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5397
[2023-09-04 01:14:08,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5397
[2023-09-04 01:14:08,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:14:08,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:14:08,018] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.9215 (1.8869)  loss_scale: 65536.0000 (40079.8678)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2347 (6.8204)  time: 0.9366 (0.5223 -- 5.3844)  data: 0.0018 (0.0003 -- 0.0131)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000011  loss: 1.7320 (1.8704)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7912 (6.8213)  time: 0.6604 (0.5280 -- 2.0717)  data: 0.0103 (0.0006 -- 0.1759)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8426 (1.8626)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4059 (6.9705)  time: 0.7597 (0.4942 -- 1.7918)  data: 0.2221 (0.0002 -- 1.2500)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8788 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8426 (1.8560)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4059 (6.9705)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.6380 (0.6380)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2538 (2.2538 -- 2.2538)  data: 2.0431 (2.0431 -- 2.0431)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6733 (0.9798)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (94.9495)  time: 0.4143 (0.2006 -- 2.2538)  data: 0.1963 (0.0007 -- 2.0431)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7168 (0.8676)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (95.7672)  time: 0.2282 (0.1709 -- 0.4191)  data: 0.0190 (0.0001 -- 0.2208)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7750 (0.9206)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (94.6058)  time: 0.2111 (0.1353 -- 0.4191)  data: 0.0187 (0.0001 -- 0.2208)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 76.141 Acc@5 94.813 loss 0.878
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 76.56%
Epoch: [34]  [  0/160]  eta: 0:18:02  lr: 0.000044  min_lr: 0.000011  loss: 1.7586 (1.7586)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2159 (8.2159)  time: 6.7635 (6.7635 -- 6.7635)  data: 5.5160 (5.5160 -- 5.5160)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:53  lr: 0.000044  min_lr: 0.000011  loss: 1.8937 (1.8875)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5409 (6.8240)  time: 0.9652 (0.5140 -- 3.7311)  data: 0.1028 (0.0004 -- 0.8138)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:07  lr: 0.000044  min_lr: 0.000011  loss: 1.8918 (1.8861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7981 (7.0145)  time: 0.8675 (0.5174 -- 4.2862)  data: 0.0022 (0.0001 -- 0.0125)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:36  lr: 0.000044  min_lr: 0.000011  loss: 1.9258 (1.8846)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3328 (6.7280)  time: 0.7651 (0.5162 -- 2.0586)  data: 0.0067 (0.0003 -- 0.1082)  max mem: 16413
[2023-09-04 01:15:53,995] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5510
[2023-09-04 01:15:53,995] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:15:53,995] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5510
[2023-09-04 01:15:53,995] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:15:53,996] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000011  loss: 1.9045 (1.8950)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0521 (6.7192)  time: 0.9003 (0.5200 -- 3.0328)  data: 0.1462 (0.0006 -- 2.5116)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.8983 (1.8889)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1841 (6.6462)  time: 0.9579 (0.5283 -- 3.4384)  data: 0.4137 (0.0007 -- 2.8812)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.6512 (1.8590)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4716 (6.6603)  time: 0.8508 (0.5116 -- 3.0817)  data: 0.3096 (0.0005 -- 2.5372)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.9240 (1.8542)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4591 (6.6643)  time: 0.9133 (0.5286 -- 3.8859)  data: 0.3635 (0.0002 -- 3.3680)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9008 (1.8599)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2172 (6.7089)  time: 0.7220 (0.4959 -- 3.8859)  data: 0.2068 (0.0002 -- 3.3680)  max mem: 16413
Epoch: [34] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.9008 (1.8862)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2172 (6.7089)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.5634 (0.5634)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2215 (2.2215 -- 2.2215)  data: 1.9698 (1.9698 -- 1.9698)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6411 (0.8839)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4118 (0.1896 -- 2.2215)  data: 0.1913 (0.0005 -- 1.9698)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7010 (0.8324)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2241 (0.1694 -- 0.3460)  data: 0.0125 (0.0001 -- 0.1259)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7332 (0.8787)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.0207)  time: 0.2096 (0.1329 -- 0.3460)  data: 0.0123 (0.0001 -- 0.1259)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 78.008 Acc@5 95.643 loss 0.830
Accuracy of the network on the 482 val images: 78.01%
[2023-09-04 01:17:16,352] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:17:16,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:17:16,353] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:17:16,353] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:17:17,885] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:17:17,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.01%
Epoch: [35]  [  0/160]  eta: 0:19:58  lr: 0.000044  min_lr: 0.000011  loss: 1.4904 (1.4904)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4190 (9.4190)  time: 7.4919 (7.4919 -- 7.4919)  data: 6.9498 (6.9498 -- 6.9498)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:44  lr: 0.000044  min_lr: 0.000011  loss: 1.8161 (1.7678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5083 (6.0864)  time: 0.8573 (0.5272 -- 5.1240)  data: 0.1276 (0.0007 -- 2.4438)  max mem: 16413
[2023-09-04 01:17:58,337] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:17:58,338] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:17:58,339] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:17:58,340] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 40/160]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000011  loss: 1.9779 (1.8759)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9718 (6.2689)  time: 0.8807 (0.5278 -- 2.9829)  data: 0.1335 (0.0004 -- 1.2915)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.9088 (1.8723)  loss_scale: 32768.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8127 (6.2341)  time: 0.9784 (0.5367 -- 2.7606)  data: 0.1104 (0.0007 -- 2.1762)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:17  lr: 0.000044  min_lr: 0.000011  loss: 1.6396 (1.8556)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6550 (6.4624)  time: 0.8172 (0.5185 -- 4.3818)  data: 0.0014 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.9783 (1.8714)  loss_scale: 32768.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3075 (6.4364)  time: 0.9490 (0.5201 -- 3.6836)  data: 0.0019 (0.0003 -- 0.0129)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8846 (1.8614)  loss_scale: 32768.0000 (27487.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1236 (6.4314)  time: 0.7909 (0.5298 -- 3.0879)  data: 0.0019 (0.0002 -- 0.0134)  max mem: 16413
[2023-09-04 01:19:11,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5721
[2023-09-04 01:19:11,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5721
[2023-09-04 01:19:11,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:19:11,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:19:11,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.9189 (1.8691)  loss_scale: 16384.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6153 (6.3298)  time: 0.9187 (0.5092 -- 4.5335)  data: 0.0019 (0.0004 -- 0.0162)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8369 (1.8792)  loss_scale: 16384.0000 (24780.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0040 (6.3208)  time: 0.5596 (0.4940 -- 1.2594)  data: 0.0010 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [35] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8369 (1.8580)  loss_scale: 16384.0000 (24780.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0040 (6.3208)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4476 (0.4476)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4119 (2.4119 -- 2.4119)  data: 2.1967 (2.1967 -- 2.1967)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5807 (0.8842)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4255 (0.2079 -- 2.4119)  data: 0.2020 (0.0009 -- 2.1967)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6230 (0.8124)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2205 (0.1702 -- 0.3104)  data: 0.0083 (0.0001 -- 0.1262)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8005 (0.8626)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.0207)  time: 0.2020 (0.1335 -- 0.3104)  data: 0.0078 (0.0001 -- 0.1262)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 77.593 Acc@5 95.643 loss 0.831
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.01%
Epoch: [36]  [  0/160]  eta: 0:21:40  lr: 0.000044  min_lr: 0.000011  loss: 1.9965 (1.9965)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5967 (5.5967)  time: 8.1309 (8.1309 -- 8.1309)  data: 7.5829 (7.5829 -- 7.5829)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:41  lr: 0.000044  min_lr: 0.000011  loss: 1.6317 (1.6888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7786 (6.0268)  time: 0.8063 (0.5156 -- 3.2813)  data: 0.2556 (0.0006 -- 2.7668)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9105 (1.7808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5585 (6.5197)  time: 0.8406 (0.5256 -- 3.9398)  data: 0.2950 (0.0002 -- 3.4122)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000011  loss: 1.7413 (1.7808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1937 (6.5351)  time: 0.9756 (0.5243 -- 3.9936)  data: 0.4304 (0.0009 -- 3.4709)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 2.0517 (1.8292)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1370 (6.4796)  time: 0.8313 (0.5332 -- 3.4256)  data: 0.2839 (0.0004 -- 2.8936)  max mem: 16413
[2023-09-04 01:21:13,707] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:21:13,707] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:21:13,707] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:21:13,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 2.0714 (1.8668)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2053 (6.5073)  time: 0.8584 (0.5154 -- 3.5779)  data: 0.3151 (0.0009 -- 3.0462)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 2.1452 (1.9073)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2707 (6.4793)  time: 0.9542 (0.5181 -- 4.8251)  data: 0.4142 (0.0004 -- 4.3150)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.6522 (1.8928)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2159 (6.4620)  time: 0.8753 (0.5090 -- 3.5816)  data: 0.3350 (0.0004 -- 3.0481)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 2.0322 (1.9019)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5902 (6.3594)  time: 0.6295 (0.4981 -- 2.6682)  data: 0.1060 (0.0002 -- 2.1050)  max mem: 16413
Epoch: [36] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 2.0322 (1.8737)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5902 (6.3594)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4333 (0.4333)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3234 (2.3234 -- 2.3234)  data: 2.0908 (2.0908 -- 2.0908)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5984 (0.8465)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4099 (0.2056 -- 2.3234)  data: 0.1961 (0.0008 -- 2.0908)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5984 (0.7722)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2262 (0.1703 -- 0.4138)  data: 0.0253 (0.0001 -- 0.2277)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6914 (0.8311)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (94.6058)  time: 0.2101 (0.1327 -- 0.4138)  data: 0.0245 (0.0001 -- 0.2277)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 76.556 Acc@5 95.643 loss 0.805
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 78.01%
Epoch: [37]  [  0/160]  eta: 0:18:12  lr: 0.000044  min_lr: 0.000011  loss: 2.3272 (2.3272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1478 (6.1478)  time: 6.8258 (6.8258 -- 6.8258)  data: 6.2830 (6.2830 -- 6.2830)  max mem: 16413
Epoch: [37]  [ 20/160]  eta: 0:02:55  lr: 0.000044  min_lr: 0.000011  loss: 1.9260 (1.9431)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6735 (6.5401)  time: 0.9752 (0.5141 -- 3.1270)  data: 0.3216 (0.0004 -- 2.5971)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:02  lr: 0.000044  min_lr: 0.000011  loss: 1.8439 (1.8958)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8992 (6.3727)  time: 0.7683 (0.5237 -- 2.6097)  data: 0.1946 (0.0002 -- 2.0904)  max mem: 16413
[2023-09-04 01:23:20,261] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:23:20,261] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:23:20,262] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:23:20,262] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 60/160]  eta: 0:01:42  lr: 0.000044  min_lr: 0.000011  loss: 1.9564 (1.9167)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8722 (6.2834)  time: 1.0492 (0.5203 -- 4.3927)  data: 0.5104 (0.0004 -- 3.8520)  max mem: 16413
[2023-09-04 01:23:37,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=29, lr=[1.1117940440317657e-05, 1.1117940440317657e-05, 1.2353267155908508e-05, 1.2353267155908508e-05, 1.3725852395453895e-05, 1.3725852395453895e-05, 1.5250947106059886e-05, 1.5250947106059886e-05, 1.6945496784510984e-05, 1.6945496784510984e-05, 1.882832976056776e-05, 1.882832976056776e-05, 2.092036640063084e-05, 2.092036640063084e-05, 2.324485155625649e-05, 2.324485155625649e-05, 2.5827612840284987e-05, 2.5827612840284987e-05, 2.869734760031665e-05, 2.869734760031665e-05, 3.1885941778129613e-05, 3.1885941778129613e-05, 3.542882419792179e-05, 3.542882419792179e-05, 3.93653602199131e-05, 3.93653602199131e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 01:23:37,198] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.502173837230075, CurrSamplesPerSec=21.35319048560478, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:18  lr: 0.000044  min_lr: 0.000011  loss: 1.7912 (1.9006)  loss_scale: 65536.0000 (42072.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6401 (6.2149)  time: 0.8202 (0.5207 -- 4.7274)  data: 0.2712 (0.0003 -- 4.1297)  max mem: 16413
[2023-09-04 01:23:41,107] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6003
[2023-09-04 01:23:41,107] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6003
[2023-09-04 01:23:41,107] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:23:41,107] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:23:41,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 2.0071 (1.9128)  loss_scale: 32768.0000 (40878.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9294 (6.2780)  time: 0.9170 (0.5253 -- 3.5063)  data: 0.3722 (0.0005 -- 2.9746)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.9475 (1.9001)  loss_scale: 32768.0000 (39538.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8814 (6.4676)  time: 0.8177 (0.5205 -- 3.8417)  data: 0.2739 (0.0004 -- 3.3349)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8106 (1.8810)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4093 (6.4353)  time: 0.9311 (0.5287 -- 3.4000)  data: 0.3874 (0.0004 -- 2.8671)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9123 (1.8809)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0928 (6.4538)  time: 0.6302 (0.4956 -- 2.2023)  data: 0.1067 (0.0002 -- 1.6707)  max mem: 16413
Epoch: [37] Total time: 0:02:24 (0.9028 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.9123 (1.8848)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0928 (6.4538)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5031 (0.5031)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3286 (2.3286 -- 2.3286)  data: 2.0882 (2.0882 -- 2.0882)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5381 (0.7915)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4076 (0.1936 -- 2.3286)  data: 0.1928 (0.0006 -- 2.0882)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6630 (0.7527)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2266 (0.1694 -- 0.6165)  data: 0.0224 (0.0001 -- 0.4111)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7597 (0.8029)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2107 (0.1324 -- 0.6165)  data: 0.0220 (0.0001 -- 0.4111)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 79.253 Acc@5 96.266 loss 0.799
Accuracy of the network on the 482 val images: 79.25%
[2023-09-04 01:24:50,964] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:24:50,966] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:24:50,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:24:50,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:24:52,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:24:52,368] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.25%
Epoch: [38]  [  0/160]  eta: 0:15:58  lr: 0.000044  min_lr: 0.000011  loss: 1.8815 (1.8815)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4085 (6.4085)  time: 5.9904 (5.9904 -- 5.9904)  data: 5.4587 (5.4587 -- 5.4587)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:32  lr: 0.000044  min_lr: 0.000011  loss: 1.9060 (1.8604)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9585 (6.3987)  time: 0.8422 (0.5309 -- 2.1366)  data: 0.2554 (0.0010 -- 1.5944)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:01:59  lr: 0.000044  min_lr: 0.000011  loss: 1.9315 (1.8646)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8252 (6.9619)  time: 0.9006 (0.5213 -- 3.0562)  data: 0.3580 (0.0006 -- 2.5114)  max mem: 16413
[2023-09-04 01:25:43,986] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:25:43,986] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:25:43,986] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:25:43,986] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:25:45,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6135
[2023-09-04 01:25:45,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6135
[2023-09-04 01:25:45,697] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:25:45,697] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:25:45,697] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.7895 (1.8383)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0801 (6.8587)  time: 0.9578 (0.5248 -- 4.3285)  data: 0.3049 (0.0004 -- 3.8069)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.7811 (1.8507)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1713 (6.6779)  time: 0.8836 (0.5107 -- 4.0190)  data: 0.0434 (0.0002 -- 0.8411)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.9886 (1.8800)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6193 (6.6769)  time: 0.8932 (0.5253 -- 3.1479)  data: 0.0020 (0.0004 -- 0.0058)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.8982 (1.8970)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6041 (6.7167)  time: 0.8622 (0.5321 -- 3.9705)  data: 0.1668 (0.0004 -- 1.9494)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8272 (1.8868)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5418 (6.7686)  time: 0.8296 (0.5319 -- 2.4883)  data: 0.1975 (0.0005 -- 1.9597)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.9230 (1.8878)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8265 (6.6947)  time: 0.6739 (0.4943 -- 3.2534)  data: 0.1545 (0.0002 -- 2.7239)  max mem: 16413
Epoch: [38] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.9230 (1.8734)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8265 (6.6947)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5578 (0.5578)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3148 (2.3148 -- 2.3148)  data: 2.1006 (2.1006 -- 2.1006)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5949 (0.7970)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (97.9798)  time: 0.4087 (0.2055 -- 2.3148)  data: 0.1932 (0.0010 -- 2.1006)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6092 (0.7463)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2193 (0.1698 -- 0.3534)  data: 0.0135 (0.0001 -- 0.1582)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7580 (0.7995)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.2022 (0.1329 -- 0.3534)  data: 0.0126 (0.0001 -- 0.1582)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 79.253 Acc@5 95.851 loss 0.776
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 79.25%
Epoch: [39]  [  0/160]  eta: 0:17:54  lr: 0.000043  min_lr: 0.000011  loss: 2.0548 (2.0548)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8876 (5.8876)  time: 6.7168 (6.7168 -- 6.7168)  data: 4.1493 (4.1493 -- 4.1493)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:39  lr: 0.000043  min_lr: 0.000011  loss: 1.9481 (1.8939)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6679 (6.2733)  time: 0.8590 (0.5347 -- 2.9884)  data: 0.1250 (0.0005 -- 2.4670)  max mem: 16413
[2023-09-04 01:27:48,791] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:27:48,791] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:27:48,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:27:48,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:27:50,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6265
[2023-09-04 01:27:50,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6265
[2023-09-04 01:27:50,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:27:50,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:27:50,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 01:28:01,912] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6278
[2023-09-04 01:28:01,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:28:01,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 01:28:01,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6278
[2023-09-04 01:28:01,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [39]  [ 40/160]  eta: 0:01:58  lr: 0.000043  min_lr: 0.000011  loss: 1.7627 (1.8895)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7179 (6.0152)  time: 0.8323 (0.5209 -- 3.0161)  data: 0.1943 (0.0002 -- 2.4982)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:35  lr: 0.000043  min_lr: 0.000011  loss: 1.9560 (1.9039)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2901 (6.2506)  time: 0.8732 (0.5269 -- 2.0531)  data: 0.2583 (0.0004 -- 1.4807)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:13  lr: 0.000043  min_lr: 0.000011  loss: 2.0892 (1.9273)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1883 (6.2493)  time: 0.8147 (0.5294 -- 1.9740)  data: 0.1258 (0.0005 -- 1.3805)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000011  loss: 1.8330 (1.9129)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0436 (6.2793)  time: 0.9604 (0.5329 -- 3.6644)  data: 0.2082 (0.0007 -- 1.8593)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.7913 (1.8967)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2358 (6.3231)  time: 0.7915 (0.5258 -- 2.0111)  data: 0.0639 (0.0002 -- 0.8383)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8312 (1.8970)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2783 (6.4456)  time: 1.0264 (0.5261 -- 5.0480)  data: 0.4830 (0.0005 -- 4.5340)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8763 (1.8825)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5909 (6.4631)  time: 0.6250 (0.4966 -- 2.0883)  data: 0.1006 (0.0001 -- 1.5423)  max mem: 16413
Epoch: [39] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8763 (1.8708)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5909 (6.4631)
[2023-09-04 01:29:44,341] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-09-04 01:29:44,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-09-04 01:29:44,346] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-09-04 01:29:44,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-09-04 01:29:45,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-09-04 01:29:45,406] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3975 (0.3975)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4849 (2.4849 -- 2.4849)  data: 2.2542 (2.2542 -- 2.2542)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6034 (0.8240)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4259 (0.1983 -- 2.4849)  data: 0.2073 (0.0009 -- 2.2542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6365 (0.7695)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (94.1799)  time: 0.2161 (0.1695 -- 0.3655)  data: 0.0094 (0.0001 -- 0.1573)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7984 (0.8259)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (94.6058)  time: 0.1975 (0.1327 -- 0.3655)  data: 0.0084 (0.0001 -- 0.1573)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 76.349 Acc@5 95.851 loss 0.803
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 79.25%
Epoch: [40]  [  0/160]  eta: 0:17:42  lr: 0.000043  min_lr: 0.000011  loss: 1.2955 (1.2955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8083 (6.8083)  time: 6.6411 (6.6411 -- 6.6411)  data: 5.0018 (5.0018 -- 5.0018)  max mem: 16413
[2023-09-04 01:30:04,498] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:30:04,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:30:04,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:30:04,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [ 20/160]  eta: 0:02:37  lr: 0.000043  min_lr: 0.000011  loss: 1.8994 (1.8465)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1073 (6.1104)  time: 0.8505 (0.5274 -- 4.1161)  data: 0.0841 (0.0008 -- 0.7944)  max mem: 16413
Epoch: [40]  [ 40/160]  eta: 0:01:58  lr: 0.000043  min_lr: 0.000011  loss: 1.8839 (1.8596)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0031 (6.2394)  time: 0.8432 (0.5214 -- 2.4797)  data: 0.0579 (0.0003 -- 0.8922)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:35  lr: 0.000043  min_lr: 0.000011  loss: 1.7724 (1.8289)  loss_scale: 32768.0000 (30887.8689)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7868 (6.3795)  time: 0.8900 (0.5120 -- 2.6507)  data: 0.1732 (0.0004 -- 2.1246)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.7631 (1.8179)  loss_scale: 32768.0000 (31352.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0179 (6.4848)  time: 0.8911 (0.5328 -- 2.5976)  data: 0.0638 (0.0004 -- 0.6929)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:54  lr: 0.000043  min_lr: 0.000011  loss: 1.9418 (1.8275)  loss_scale: 32768.0000 (31632.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4047 (6.6329)  time: 0.7823 (0.5159 -- 2.2366)  data: 0.1766 (0.0003 -- 1.6888)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 2.0614 (1.8450)  loss_scale: 32768.0000 (31820.1653)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9913 (6.7146)  time: 0.8821 (0.5207 -- 3.0740)  data: 0.3088 (0.0005 -- 2.5206)  max mem: 16413
[2023-09-04 01:31:50,362] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6527
[2023-09-04 01:31:50,362] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6527
[2023-09-04 01:31:50,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:31:50,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:31:50,362] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [140/160]  eta: 0:00:17  lr: 0.000043  min_lr: 0.000011  loss: 1.9300 (1.8585)  loss_scale: 16384.0000 (30327.8298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3201 (6.7001)  time: 0.8593 (0.5063 -- 3.0075)  data: 0.3211 (0.0003 -- 2.4861)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.7416 (1.8430)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6107 (6.7675)  time: 0.7279 (0.4954 -- 2.5331)  data: 0.2056 (0.0002 -- 1.9845)  max mem: 16413
Epoch: [40] Total time: 0:02:20 (0.8796 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.7416 (1.8450)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6107 (6.7675)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4244 (0.4244)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2933 (2.2933 -- 2.2933)  data: 2.0606 (2.0606 -- 2.0606)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5878 (0.8255)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4083 (0.1990 -- 2.2933)  data: 0.1929 (0.0005 -- 2.0606)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5878 (0.7428)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1695 -- 0.4297)  data: 0.0222 (0.0001 -- 0.2162)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7425 (0.8012)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2126 (0.1331 -- 0.4297)  data: 0.0219 (0.0001 -- 0.2162)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 79.876 Acc@5 95.851 loss 0.757
Accuracy of the network on the 482 val images: 79.88%
[2023-09-04 01:32:21,742] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:32:21,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:32:21,743] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:32:21,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:32:23,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:32:23,158] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.88%
Epoch: [41]  [  0/160]  eta: 0:22:13  lr: 0.000043  min_lr: 0.000011  loss: 2.3826 (2.3826)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0063 (6.0063)  time: 8.3327 (8.3327 -- 8.3327)  data: 7.8139 (7.8139 -- 7.8139)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:42  lr: 0.000043  min_lr: 0.000011  loss: 1.8606 (1.8793)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5166 (6.8210)  time: 0.8045 (0.5240 -- 2.9516)  data: 0.0707 (0.0004 -- 0.6036)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:03  lr: 0.000043  min_lr: 0.000011  loss: 1.8953 (1.8945)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1734 (6.8050)  time: 0.8828 (0.5398 -- 2.7463)  data: 0.2720 (0.0003 -- 1.7078)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.8781 (1.8851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5871 (6.8127)  time: 0.8501 (0.5249 -- 3.3151)  data: 0.0524 (0.0004 -- 0.8138)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000011  loss: 2.0161 (1.8860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1957 (6.7249)  time: 0.9071 (0.5322 -- 4.2050)  data: 0.0793 (0.0003 -- 0.9365)  max mem: 16413
[2023-09-04 01:33:53,894] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:33:53,894] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:33:53,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:33:53,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.8711 (1.8657)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2558 (6.6949)  time: 0.9205 (0.5226 -- 3.3132)  data: 0.2150 (0.0008 -- 2.4023)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.7395 (1.8485)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5332 (6.8327)  time: 0.8641 (0.5370 -- 2.5322)  data: 0.0572 (0.0004 -- 0.7806)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8907 (1.8429)  loss_scale: 32768.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3043 (6.9176)  time: 0.8064 (0.5270 -- 2.1784)  data: 0.2261 (0.0003 -- 1.6405)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8342 (1.8523)  loss_scale: 32768.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6607 (6.9157)  time: 0.7291 (0.4993 -- 1.9339)  data: 0.1342 (0.0002 -- 1.1463)  max mem: 16413
Epoch: [41] Total time: 0:02:23 (0.8945 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8342 (1.8474)  loss_scale: 32768.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6607 (6.9157)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5193 (0.5193)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2531 (2.2531 -- 2.2531)  data: 1.9998 (1.9998 -- 1.9998)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5588 (0.7653)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4168 (0.1967 -- 2.2531)  data: 0.2012 (0.0004 -- 1.9998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6000 (0.7217)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2241 (0.1702 -- 0.4081)  data: 0.0202 (0.0001 -- 0.2024)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7506 (0.7700)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.4357)  time: 0.2088 (0.1330 -- 0.4081)  data: 0.0200 (0.0001 -- 0.2024)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 80.913 Acc@5 95.851 loss 0.738
Accuracy of the network on the 482 val images: 80.91%
[2023-09-04 01:34:54,081] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:34:54,083] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:34:54,083] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:34:54,083] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:34:55,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:34:55,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.91%
Epoch: [42]  [  0/160]  eta: 0:21:03  lr: 0.000043  min_lr: 0.000011  loss: 2.0979 (2.0979)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6880 (5.6880)  time: 7.8999 (7.8999 -- 7.8999)  data: 6.1465 (6.1465 -- 6.1465)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:42  lr: 0.000043  min_lr: 0.000011  loss: 1.8591 (1.8948)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9335 (6.0944)  time: 0.8232 (0.5324 -- 3.2969)  data: 0.0399 (0.0004 -- 0.7695)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:05  lr: 0.000043  min_lr: 0.000011  loss: 1.8659 (1.8241)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9995 (6.3625)  time: 0.9282 (0.5322 -- 4.6793)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:39  lr: 0.000043  min_lr: 0.000011  loss: 1.7651 (1.8311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4653 (6.4237)  time: 0.8793 (0.5376 -- 2.5937)  data: 0.1064 (0.0003 -- 2.0750)  max mem: 16413
[2023-09-04 01:35:58,224] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:35:58,224] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:35:58,224] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:35:58,224] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:36:02,770] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6789
[2023-09-04 01:36:02,770] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6789
[2023-09-04 01:36:02,770] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:36:02,771] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:36:02,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000011  loss: 1.9324 (1.8516)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9270 (6.3467)  time: 0.8663 (0.5243 -- 3.8869)  data: 0.2277 (0.0001 -- 3.3543)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.7615 (1.8364)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4765 (6.3617)  time: 0.9041 (0.5353 -- 3.6000)  data: 0.3540 (0.0007 -- 3.0633)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.7102 (1.8178)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9128 (6.4684)  time: 0.7096 (0.5295 -- 1.9708)  data: 0.0477 (0.0006 -- 0.7783)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.7432 (1.8036)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5724 (6.5340)  time: 0.9990 (0.5233 -- 2.8899)  data: 0.2837 (0.0008 -- 2.3537)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6347 (1.7960)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0534 (6.5102)  time: 0.7387 (0.4950 -- 3.8102)  data: 0.2237 (0.0002 -- 3.3110)  max mem: 16413
Epoch: [42] Total time: 0:02:24 (0.9026 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6347 (1.8174)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0534 (6.5102)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3539 (0.3539)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2741 (2.2741 -- 2.2741)  data: 2.0292 (2.0292 -- 2.0292)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5828 (0.8181)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (94.9495)  time: 0.4099 (0.1939 -- 2.2741)  data: 0.1935 (0.0004 -- 2.0292)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6143 (0.7476)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2260 (0.1695 -- 0.4286)  data: 0.0217 (0.0001 -- 0.2323)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7131 (0.8043)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.2096 (0.1335 -- 0.4286)  data: 0.0214 (0.0001 -- 0.2323)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 78.631 Acc@5 96.266 loss 0.785
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 80.91%
Epoch: [43]  [  0/160]  eta: 0:17:22  lr: 0.000043  min_lr: 0.000011  loss: 1.9958 (1.9958)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6311 (4.6311)  time: 6.5159 (6.5159 -- 6.5159)  data: 4.7159 (4.7159 -- 4.7159)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:44  lr: 0.000043  min_lr: 0.000011  loss: 1.9019 (1.8854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1169 (6.0084)  time: 0.9103 (0.5211 -- 2.4129)  data: 0.2813 (0.0002 -- 1.8673)  max mem: 16413
[2023-09-04 01:38:07,234] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:38:07,234] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:38:07,235] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:38:07,235] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [43]  [ 40/160]  eta: 0:01:58  lr: 0.000043  min_lr: 0.000011  loss: 1.8260 (1.8138)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7299 (6.3044)  time: 0.7956 (0.5255 -- 3.1936)  data: 0.2072 (0.0005 -- 2.3603)  max mem: 16413
[2023-09-04 01:38:20,088] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6930
[2023-09-04 01:38:20,088] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6930
[2023-09-04 01:38:20,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:38:20,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:38:20,089] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [43]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.8004 (1.8164)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1170 (6.4980)  time: 0.9592 (0.5247 -- 4.9802)  data: 0.2505 (0.0005 -- 2.8416)  max mem: 16413
[2023-09-04 01:38:28,667] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6942
[2023-09-04 01:38:28,667] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:38:28,667] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6942
[2023-09-04 01:38:28,667] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:38:28,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000011  loss: 1.7988 (1.7921)  loss_scale: 16384.0000 (33779.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0365 (6.5136)  time: 0.7782 (0.5249 -- 2.9220)  data: 0.1797 (0.0004 -- 2.3368)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.8077 (1.8076)  loss_scale: 16384.0000 (30334.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6575 (6.5642)  time: 0.9160 (0.5232 -- 3.1339)  data: 0.3656 (0.0005 -- 2.6053)  max mem: 16413
[2023-09-04 01:39:18,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=37, lr=[1.079275389402428e-05, 1.079275389402428e-05, 1.199194877113809e-05, 1.199194877113809e-05, 1.3324387523486763e-05, 1.3324387523486763e-05, 1.4804875026096405e-05, 1.4804875026096405e-05, 1.6449861140107117e-05, 1.6449861140107117e-05, 1.8277623489007908e-05, 1.8277623489007908e-05, 2.0308470543342117e-05, 2.0308470543342117e-05, 2.2564967270380128e-05, 2.2564967270380128e-05, 2.5072185855977923e-05, 2.5072185855977923e-05, 2.785798428441991e-05, 2.785798428441991e-05, 3.095331587157768e-05, 3.095331587157768e-05, 3.439257319064187e-05, 3.439257319064187e-05, 3.8213970211824294e-05, 3.8213970211824294e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 01:39:18,787] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.628416975151467, CurrSamplesPerSec=22.490821873429034, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.6233 (1.7877)  loss_scale: 16384.0000 (28028.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8297 (6.5037)  time: 0.8954 (0.5254 -- 4.2322)  data: 0.3458 (0.0005 -- 3.6993)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8428 (1.7945)  loss_scale: 16384.0000 (26377.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0877 (6.6591)  time: 1.0110 (0.5177 -- 4.9882)  data: 0.4699 (0.0003 -- 4.4820)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8461 (1.7978)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0464 (6.6362)  time: 0.5998 (0.4950 -- 1.6392)  data: 0.0847 (0.0001 -- 1.1236)  max mem: 16413
Epoch: [43] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8461 (1.8192)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0464 (6.6362)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2984 (0.2984)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2640 (2.2640 -- 2.2640)  data: 2.0430 (2.0430 -- 2.0430)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5036 (0.7406)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4032 (0.2023 -- 2.2640)  data: 0.1880 (0.0009 -- 2.0430)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5456 (0.6858)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1688 -- 0.4342)  data: 0.0181 (0.0001 -- 0.1958)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6077 (0.7635)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2099 (0.1331 -- 0.4342)  data: 0.0178 (0.0001 -- 0.1958)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.750
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 80.91%
Epoch: [44]  [  0/160]  eta: 0:22:14  lr: 0.000042  min_lr: 0.000011  loss: 1.7065 (1.7065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3143 (7.3143)  time: 8.3383 (8.3383 -- 8.3383)  data: 7.8099 (7.8099 -- 7.8099)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:48  lr: 0.000042  min_lr: 0.000011  loss: 1.7905 (1.7643)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4988 (6.4965)  time: 0.8434 (0.5221 -- 2.8922)  data: 0.2160 (0.0004 -- 2.3349)  max mem: 16413
[2023-09-04 01:40:34,448] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:40:34,448] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:40:34,449] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:40:34,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [ 40/160]  eta: 0:02:06  lr: 0.000042  min_lr: 0.000011  loss: 1.6747 (1.7308)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1949 (6.3243)  time: 0.9069 (0.5247 -- 3.1542)  data: 0.3506 (0.0003 -- 2.6372)  max mem: 16413
[2023-09-04 01:40:50,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7090
[2023-09-04 01:40:50,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7090
[2023-09-04 01:40:50,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:40:50,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:40:50,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.7950 (1.7746)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0057 (6.3834)  time: 0.8614 (0.5285 -- 3.4838)  data: 0.3147 (0.0003 -- 2.9624)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 1.8568 (1.7897)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3767 (6.5503)  time: 0.8645 (0.5265 -- 4.4351)  data: 0.3186 (0.0003 -- 3.8842)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.7116 (1.7759)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6896 (6.5509)  time: 0.8786 (0.5149 -- 4.2219)  data: 0.3283 (0.0003 -- 3.7052)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.8569 (1.7905)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0433 (6.8276)  time: 0.7460 (0.5370 -- 2.2362)  data: 0.1539 (0.0004 -- 1.7086)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7153 (1.8025)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3438 (6.8897)  time: 1.0208 (0.5366 -- 3.4842)  data: 0.0069 (0.0004 -- 0.1039)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8539 (1.8151)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8853 (7.0386)  time: 0.5993 (0.4957 -- 1.6124)  data: 0.0697 (0.0002 -- 1.1047)  max mem: 16413
Epoch: [44] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8539 (1.8321)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8853 (7.0386)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3080 (0.3080)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3370 (2.3370 -- 2.3370)  data: 2.1206 (2.1206 -- 2.1206)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4927 (0.7568)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4105 (0.2023 -- 2.3370)  data: 0.1938 (0.0003 -- 2.1206)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5713 (0.7022)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1694 -- 0.4258)  data: 0.0144 (0.0001 -- 0.2220)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6444 (0.7775)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2023 (0.1331 -- 0.4258)  data: 0.0141 (0.0001 -- 0.2220)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.739
Accuracy of the network on the 482 val images: 81.33%
[2023-09-04 01:42:28,904] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 01:42:28,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 01:42:28,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 01:42:28,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 01:42:30,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 01:42:30,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.33%
Epoch: [45]  [  0/160]  eta: 0:21:12  lr: 0.000042  min_lr: 0.000011  loss: 1.9686 (1.9686)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3235 (8.3235)  time: 7.9541 (7.9541 -- 7.9541)  data: 5.4350 (5.4350 -- 5.4350)  max mem: 16413
[2023-09-04 01:42:53,120] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:42:53,120] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:42:53,120] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:42:53,120] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [45]  [ 20/160]  eta: 0:02:35  lr: 0.000042  min_lr: 0.000011  loss: 1.8748 (1.8341)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3828 (6.7267)  time: 0.7707 (0.5250 -- 2.4028)  data: 0.0761 (0.0004 -- 0.9663)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:01:59  lr: 0.000042  min_lr: 0.000011  loss: 1.9531 (1.8675)  loss_scale: 32768.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6367 (7.0116)  time: 0.8797 (0.5381 -- 3.0718)  data: 0.2890 (0.0002 -- 2.4931)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:35  lr: 0.000042  min_lr: 0.000011  loss: 1.8112 (1.8449)  loss_scale: 32768.0000 (27664.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4348 (7.1553)  time: 0.8638 (0.5230 -- 3.5503)  data: 0.3168 (0.0003 -- 3.0361)  max mem: 16413
[2023-09-04 01:43:30,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7261
[2023-09-04 01:43:30,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:43:30,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7261
[2023-09-04 01:43:30,098] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:43:30,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [ 80/160]  eta: 0:01:14  lr: 0.000042  min_lr: 0.000011  loss: 1.7623 (1.8260)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0140 (6.9912)  time: 0.8549 (0.5387 -- 2.7343)  data: 0.2999 (0.0007 -- 2.1854)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.9553 (1.8465)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9039 (7.0211)  time: 0.9669 (0.5222 -- 4.0066)  data: 0.3172 (0.0003 -- 3.4604)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.8356 (1.8459)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6059 (7.0375)  time: 0.8166 (0.5234 -- 2.8050)  data: 0.1106 (0.0003 -- 2.1743)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7873 (1.8395)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3687 (6.9566)  time: 0.7959 (0.5160 -- 3.5396)  data: 0.0029 (0.0002 -- 0.0165)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8413 (1.8342)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9356 (6.9030)  time: 0.6860 (0.4963 -- 1.6285)  data: 0.0626 (0.0002 -- 1.0165)  max mem: 16413
Epoch: [45] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8413 (1.7992)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9356 (6.9030)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2911 (0.2911)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4545 (2.4545 -- 2.4545)  data: 2.2362 (2.2362 -- 2.2362)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5222 (0.7358)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4467 (0.1842 -- 2.4545)  data: 0.2310 (0.0009 -- 2.2362)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5530 (0.6905)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2210 (0.1699 -- 0.5057)  data: 0.0154 (0.0001 -- 0.2924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6227 (0.7550)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2036 (0.1330 -- 0.5057)  data: 0.0150 (0.0001 -- 0.2924)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 79.668 Acc@5 96.888 loss 0.722
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 81.33%
Epoch: [46]  [  0/160]  eta: 0:16:44  lr: 0.000042  min_lr: 0.000011  loss: 2.1593 (2.1593)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7069 (5.7069)  time: 6.2754 (6.2754 -- 6.2754)  data: 5.7391 (5.7391 -- 5.7391)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:53  lr: 0.000042  min_lr: 0.000011  loss: 1.7958 (1.8312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7018 (6.8904)  time: 0.9892 (0.5123 -- 3.7759)  data: 0.4064 (0.0004 -- 3.2515)  max mem: 16413
[2023-09-04 01:45:31,413] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:45:31,413] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:45:31,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:45:31,414] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [ 40/160]  eta: 0:02:05  lr: 0.000042  min_lr: 0.000011  loss: 1.6574 (1.8234)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1248 (6.8106)  time: 0.8382 (0.5228 -- 4.7865)  data: 0.2935 (0.0001 -- 4.2443)  max mem: 16413
[2023-09-04 01:45:59,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7420
[2023-09-04 01:45:59,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7420
[2023-09-04 01:45:59,098] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:45:59,098] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 01:45:59,098] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [46]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.9054 (1.8431)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4245 (6.8348)  time: 0.9010 (0.5253 -- 3.5288)  data: 0.3433 (0.0004 -- 2.9867)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.7745 (1.8129)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6506 (6.7950)  time: 0.7627 (0.5176 -- 3.6257)  data: 0.2169 (0.0002 -- 3.0968)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.7000 (1.7959)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7781 (6.8713)  time: 0.9025 (0.5369 -- 3.4122)  data: 0.3532 (0.0003 -- 2.8767)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.9890 (1.8252)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2011 (6.8299)  time: 0.8791 (0.5102 -- 4.2201)  data: 0.3337 (0.0003 -- 3.6977)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7679 (1.8216)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9416 (6.8670)  time: 0.9841 (0.5178 -- 3.5452)  data: 0.4423 (0.0003 -- 3.0323)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.9543 (1.8212)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3681 (6.9147)  time: 0.6142 (0.4965 -- 2.5345)  data: 0.1020 (0.0002 -- 2.0266)  max mem: 16413
Epoch: [46] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.9543 (1.8288)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3681 (6.9147)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3197 (0.3197)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2721 (2.2721 -- 2.2721)  data: 2.0696 (2.0696 -- 2.0696)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5308 (0.7103)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4197 (0.2113 -- 2.2721)  data: 0.1947 (0.0007 -- 2.0696)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5563 (0.6769)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2303 (0.1687 -- 0.5183)  data: 0.0189 (0.0001 -- 0.3019)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5829 (0.7362)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2103 (0.1326 -- 0.5183)  data: 0.0186 (0.0001 -- 0.3019)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 80.498 Acc@5 97.095 loss 0.729
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 81.33%
Epoch: [47]  [  0/160]  eta: 0:20:15  lr: 0.000042  min_lr: 0.000011  loss: 1.6960 (1.6960)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4299 (9.4299)  time: 7.5989 (7.5989 -- 7.5989)  data: 6.3161 (6.3161 -- 6.3161)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:39  lr: 0.000042  min_lr: 0.000011  loss: 1.8494 (1.8151)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6204 (7.2881)  time: 0.8163 (0.5140 -- 3.7205)  data: 0.2662 (0.0002 -- 3.2022)  max mem: 16413
[2023-09-04 01:48:00,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:48:00,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:48:00,906] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:48:00,907] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [ 40/160]  eta: 0:02:03  lr: 0.000042  min_lr: 0.000011  loss: 1.7469 (1.7950)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2316 (7.3950)  time: 0.9160 (0.5166 -- 3.8700)  data: 0.3758 (0.0004 -- 3.3449)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.7038 (1.7737)  loss_scale: 32768.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4784 (7.0921)  time: 0.9176 (0.5370 -- 3.1878)  data: 0.2655 (0.0004 -- 2.6679)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 1.6686 (1.7677)  loss_scale: 32768.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4311 (6.9659)  time: 0.8245 (0.5288 -- 2.5273)  data: 0.0400 (0.0003 -- 0.7709)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.7680 (1.7697)  loss_scale: 32768.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5319 (6.9404)  time: 0.9071 (0.5189 -- 4.1660)  data: 0.0162 (0.0003 -- 0.2939)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.9073 (1.7859)  loss_scale: 32768.0000 (28841.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8149 (6.9008)  time: 0.8271 (0.5203 -- 2.3332)  data: 0.2177 (0.0005 -- 1.8209)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8212 (1.7943)  loss_scale: 32768.0000 (29398.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2687 (6.8887)  time: 0.9281 (0.5270 -- 4.1577)  data: 0.0549 (0.0004 -- 1.0638)  max mem: 16413
[2023-09-04 01:49:50,597] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:49:50,597] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:49:50,597] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:49:50,597] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000011  loss: 1.8096 (1.7803)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9499 (6.9323)  time: 0.6218 (0.4966 -- 1.4455)  data: 0.0030 (0.0003 -- 0.0423)  max mem: 16413
Epoch: [47] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000011  loss: 1.8096 (1.7988)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9499 (6.9323)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4020 (0.4020)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4319 (2.4319 -- 2.4319)  data: 2.2183 (2.2183 -- 2.2183)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6675 (0.7820)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4342 (0.1949 -- 2.4319)  data: 0.2240 (0.0006 -- 2.2183)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5982 (0.7034)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1699 -- 0.4556)  data: 0.0199 (0.0001 -- 0.2367)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6675 (0.7638)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (96.2656)  time: 0.2074 (0.1326 -- 0.4556)  data: 0.0196 (0.0001 -- 0.2367)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 78.631 Acc@5 96.680 loss 0.734
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 81.33%
[2023-09-04 01:50:06,136] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7680
[2023-09-04 01:50:06,136] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7680
[2023-09-04 01:50:06,136] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:50:06,136] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:50:06,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [48]  [  0/160]  eta: 0:17:47  lr: 0.000041  min_lr: 0.000011  loss: 1.5873 (1.5873)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6489 (6.6489)  time: 6.6721 (6.6721 -- 6.6721)  data: 5.7677 (5.7677 -- 5.7677)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000011  loss: 1.8940 (1.8574)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9366 (6.9738)  time: 0.8663 (0.5295 -- 2.9298)  data: 0.2866 (0.0008 -- 2.3791)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:00  lr: 0.000041  min_lr: 0.000011  loss: 1.7235 (1.7679)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4198 (6.8482)  time: 0.8537 (0.5361 -- 3.2702)  data: 0.3018 (0.0008 -- 2.7380)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:38  lr: 0.000041  min_lr: 0.000011  loss: 1.8443 (1.7844)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3748 (6.8394)  time: 0.9365 (0.5292 -- 3.1010)  data: 0.1683 (0.0005 -- 1.2974)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000011  loss: 1.7309 (1.7854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4164 (6.8438)  time: 0.8210 (0.5281 -- 2.8061)  data: 0.0535 (0.0004 -- 0.5197)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000011  loss: 1.7361 (1.7757)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8154 (6.8392)  time: 0.8941 (0.5298 -- 3.9387)  data: 0.2840 (0.0004 -- 3.3785)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.8736 (1.7923)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1770 (6.9076)  time: 0.8662 (0.5325 -- 3.1111)  data: 0.0958 (0.0004 -- 1.8658)  max mem: 16413
[2023-09-04 01:51:57,739] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:51:57,740] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:51:57,740] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:51:57,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:52:05,233] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7819
[2023-09-04 01:52:05,233] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7819
[2023-09-04 01:52:05,234] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:52:05,234] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:52:05,234] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [48]  [140/160]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000010  loss: 1.8441 (1.8037)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5489 (6.8693)  time: 0.7558 (0.5083 -- 2.5482)  data: 0.2133 (0.0005 -- 2.0049)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.8104 (1.7934)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1057 (6.8784)  time: 0.7396 (0.4938 -- 2.7172)  data: 0.2114 (0.0002 -- 2.1658)  max mem: 16413
Epoch: [48] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.8104 (1.7858)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1057 (6.8784)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3136 (0.3136)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2791 (2.2791 -- 2.2791)  data: 2.0540 (2.0540 -- 2.0540)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5772 (0.7524)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4203 (0.2026 -- 2.2791)  data: 0.2041 (0.0007 -- 2.0540)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5772 (0.6724)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2390 (0.1693 -- 0.6573)  data: 0.0331 (0.0001 -- 0.4668)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6066 (0.7322)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2229 (0.1327 -- 0.6573)  data: 0.0326 (0.0001 -- 0.4668)  max mem: 16413
Val: Total time: 0:00:08 (0.2979 s / it)
* Acc@1 80.083 Acc@5 96.888 loss 0.712
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 81.33%
Epoch: [49]  [  0/160]  eta: 0:19:44  lr: 0.000041  min_lr: 0.000010  loss: 2.2356 (2.2356)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7326 (8.7326)  time: 7.4043 (7.4043 -- 7.4043)  data: 6.8389 (6.8389 -- 6.8389)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:46  lr: 0.000041  min_lr: 0.000010  loss: 1.9200 (1.8623)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4136 (7.2305)  time: 0.8749 (0.5212 -- 3.1033)  data: 0.3340 (0.0003 -- 2.5647)  max mem: 16413
[2023-09-04 01:53:08,034] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7876
[2023-09-04 01:53:08,034] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7876
[2023-09-04 01:53:08,035] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:53:08,035] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:53:08,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [ 40/160]  eta: 0:02:09  lr: 0.000041  min_lr: 0.000010  loss: 1.8992 (1.8684)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2595 (6.8409)  time: 0.9712 (0.5217 -- 4.2348)  data: 0.4229 (0.0005 -- 3.7053)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:40  lr: 0.000041  min_lr: 0.000010  loss: 1.8174 (1.8516)  loss_scale: 16384.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8940 (6.6758)  time: 0.8433 (0.5225 -- 4.6544)  data: 0.2960 (0.0002 -- 4.1427)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.8953 (1.8525)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4839 (6.8562)  time: 0.8787 (0.5273 -- 2.8393)  data: 0.1354 (0.0001 -- 1.5203)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000010  loss: 1.8491 (1.8438)  loss_scale: 16384.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5664 (6.8118)  time: 0.9009 (0.5182 -- 2.7710)  data: 0.1600 (0.0003 -- 1.9826)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000010  loss: 1.9553 (1.8386)  loss_scale: 16384.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5061 (6.8974)  time: 0.8477 (0.5153 -- 3.5031)  data: 0.3045 (0.0003 -- 2.9849)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.7051 (1.8232)  loss_scale: 16384.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3895 (6.9405)  time: 0.7765 (0.5328 -- 3.6200)  data: 0.2267 (0.0002 -- 3.1032)  max mem: 16413
[2023-09-04 01:54:51,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[1.0418644962272174e-05, 1.0418644962272174e-05, 1.1576272180302416e-05, 1.1576272180302416e-05, 1.286252464478046e-05, 1.286252464478046e-05, 1.4291694049756068e-05, 1.4291694049756068e-05, 1.587966005528452e-05, 1.587966005528452e-05, 1.764406672809391e-05, 1.764406672809391e-05, 1.960451858677101e-05, 1.960451858677101e-05, 2.1782798429745566e-05, 2.1782798429745566e-05, 2.4203109366383962e-05, 2.4203109366383962e-05, 2.689234374042662e-05, 2.689234374042662e-05, 2.988038193380736e-05, 2.988038193380736e-05, 3.3200424370897066e-05, 3.3200424370897066e-05, 3.688936041210785e-05, 3.688936041210785e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 01:54:51,441] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.864152238806778, CurrSamplesPerSec=24.78518551347678, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.6901 (1.8077)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4224 (6.9565)  time: 0.7286 (0.4953 -- 4.7380)  data: 0.2125 (0.0002 -- 4.2058)  max mem: 16413
Epoch: [49] Total time: 0:02:23 (0.8960 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.6901 (1.8148)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4224 (6.9565)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3459 (0.3459)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4139 (2.4139 -- 2.4139)  data: 2.2067 (2.2067 -- 2.2067)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5293 (0.6934)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4209 (0.1869 -- 2.4139)  data: 0.2088 (0.0004 -- 2.2067)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5777 (0.6519)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2264 (0.1701 -- 0.5218)  data: 0.0218 (0.0001 -- 0.3422)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5806 (0.7019)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2100 (0.1324 -- 0.5218)  data: 0.0216 (0.0001 -- 0.3422)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 81.120 Acc@5 96.680 loss 0.697
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.33%
Epoch: [50]  [  0/160]  eta: 0:21:50  lr: 0.000041  min_lr: 0.000010  loss: 2.1142 (2.1142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1189 (8.1189)  time: 8.1902 (8.1902 -- 8.1902)  data: 5.9563 (5.9563 -- 5.9563)  max mem: 16413
[2023-09-04 01:55:10,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:55:10,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:55:10,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:55:10,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [50]  [ 20/160]  eta: 0:02:38  lr: 0.000041  min_lr: 0.000010  loss: 1.6531 (1.6497)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9825 (6.3117)  time: 0.7804 (0.5316 -- 3.0814)  data: 0.0947 (0.0002 -- 0.7874)  max mem: 16413
[2023-09-04 01:55:40,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8040
[2023-09-04 01:55:40,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8040
[2023-09-04 01:55:40,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:55:40,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 01:55:40,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [50]  [ 40/160]  eta: 0:02:01  lr: 0.000041  min_lr: 0.000010  loss: 1.5730 (1.6788)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5180 (6.3722)  time: 0.8909 (0.5219 -- 2.9343)  data: 0.2585 (0.0003 -- 2.4050)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000010  loss: 1.8067 (1.6996)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1100 (6.5631)  time: 0.9454 (0.5253 -- 3.0311)  data: 0.2992 (0.0004 -- 2.4932)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000010  loss: 1.8833 (1.7239)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9159 (6.8656)  time: 0.8380 (0.5265 -- 2.2918)  data: 0.0753 (0.0006 -- 0.7056)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.6901 (1.7290)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7508 (7.0243)  time: 0.8399 (0.5209 -- 3.1041)  data: 0.1822 (0.0004 -- 2.5780)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.7084 (1.7385)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8405 (7.0656)  time: 0.8844 (0.5203 -- 4.5877)  data: 0.3505 (0.0004 -- 4.0690)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.7098 (1.7525)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2617 (7.1627)  time: 0.8625 (0.5340 -- 4.2775)  data: 0.3241 (0.0004 -- 3.7683)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.7661 (1.7518)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6101 (7.2018)  time: 0.6786 (0.4958 -- 3.7099)  data: 0.1593 (0.0002 -- 3.1681)  max mem: 16413
Epoch: [50] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.7661 (1.7807)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6101 (7.2018)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3392 (0.3392)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4118 (2.4118 -- 2.4118)  data: 2.1684 (2.1684 -- 2.1684)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5761 (0.7588)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4168 (0.1992 -- 2.4118)  data: 0.2023 (0.0010 -- 2.1684)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5501 (0.6565)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2177 (0.1704 -- 0.3184)  data: 0.0118 (0.0001 -- 0.1366)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6388 (0.7188)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2043 (0.1331 -- 0.3184)  data: 0.0115 (0.0001 -- 0.1366)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 80.290 Acc@5 96.680 loss 0.698
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 81.33%
Epoch: [51]  [  0/160]  eta: 0:19:31  lr: 0.000041  min_lr: 0.000010  loss: 1.9698 (1.9698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6301 (5.6301)  time: 7.3213 (7.3213 -- 7.3213)  data: 6.7728 (6.7728 -- 6.7728)  max mem: 16413
[2023-09-04 01:57:45,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:57:45,887] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 01:57:45,893] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:57:45,893] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [51]  [ 20/160]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000010  loss: 1.7238 (1.7762)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4405 (6.9497)  time: 0.8728 (0.5173 -- 3.7682)  data: 0.2182 (0.0003 -- 3.2308)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:05  lr: 0.000041  min_lr: 0.000010  loss: 1.7246 (1.7596)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7131 (7.0368)  time: 0.9010 (0.5229 -- 3.5314)  data: 0.2585 (0.0002 -- 3.0017)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:35  lr: 0.000041  min_lr: 0.000010  loss: 1.6165 (1.7558)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7636 (7.2785)  time: 0.7806 (0.5253 -- 2.9084)  data: 0.2128 (0.0005 -- 2.3575)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000010  loss: 1.8137 (1.7566)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2608 (7.1411)  time: 0.9636 (0.5104 -- 3.7316)  data: 0.4189 (0.0006 -- 3.2197)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.7394 (1.7618)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5964 (7.0388)  time: 0.7552 (0.5391 -- 2.5650)  data: 0.1997 (0.0004 -- 2.0425)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.8997 (1.7840)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1553 (6.9641)  time: 0.9355 (0.5363 -- 3.5153)  data: 0.3867 (0.0007 -- 2.9809)  max mem: 16413
[2023-09-04 01:59:33,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:59:33,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:59:33,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 01:59:33,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 01:59:35,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8299
[2023-09-04 01:59:35,016] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 01:59:35,016] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 01:59:35,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8299
[2023-09-04 01:59:35,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000010  loss: 1.7757 (1.7933)  loss_scale: 32768.0000 (32187.0071)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3130 (6.9019)  time: 0.7431 (0.5266 -- 2.2468)  data: 0.1923 (0.0006 -- 1.6952)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.9004 (1.8009)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3990 (7.0029)  time: 0.7668 (0.4955 -- 3.1769)  data: 0.2405 (0.0003 -- 2.6702)  max mem: 16413
Epoch: [51] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.9004 (1.8027)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3990 (7.0029)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2999 (0.2999)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3718 (2.3718 -- 2.3718)  data: 2.1431 (2.1431 -- 2.1431)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6343 (0.7275)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4185 (0.1992 -- 2.3718)  data: 0.2077 (0.0005 -- 2.1431)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5503 (0.6565)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2222 (0.1697 -- 0.4588)  data: 0.0210 (0.0001 -- 0.2744)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6472 (0.7241)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2077 (0.1330 -- 0.4588)  data: 0.0206 (0.0001 -- 0.2744)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 80.705 Acc@5 96.473 loss 0.698
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.33%
Epoch: [52]  [  0/160]  eta: 0:18:51  lr: 0.000040  min_lr: 0.000010  loss: 1.7696 (1.7696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6172 (7.6172)  time: 7.0691 (7.0691 -- 7.0691)  data: 6.4914 (6.4914 -- 6.4914)  max mem: 16413
[2023-09-04 02:00:14,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8332
[2023-09-04 02:00:14,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8332
[2023-09-04 02:00:14,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:00:14,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:00:14,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [ 20/160]  eta: 0:02:28  lr: 0.000040  min_lr: 0.000010  loss: 1.6107 (1.6432)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9129 (6.3433)  time: 0.7620 (0.5046 -- 1.6939)  data: 0.1216 (0.0004 -- 0.8439)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:01:58  lr: 0.000040  min_lr: 0.000010  loss: 1.7378 (1.7000)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9388 (6.4375)  time: 0.9135 (0.5267 -- 3.2925)  data: 0.0752 (0.0002 -- 0.7069)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000010  loss: 1.8720 (1.7806)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2020 (6.6521)  time: 0.8745 (0.5227 -- 2.0248)  data: 0.1324 (0.0004 -- 1.0764)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000010  loss: 1.8115 (1.7819)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2420 (6.6303)  time: 0.9277 (0.5187 -- 3.2246)  data: 0.0352 (0.0004 -- 0.6628)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.9111 (1.7930)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6314 (6.5114)  time: 0.8575 (0.5250 -- 3.5231)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.7570 (1.7952)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7789 (6.6303)  time: 0.8787 (0.5196 -- 3.6074)  data: 0.0010 (0.0005 -- 0.0023)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.8435 (1.7968)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4564 (6.6147)  time: 0.7954 (0.5183 -- 2.7807)  data: 0.0053 (0.0004 -- 0.0800)  max mem: 16413
[2023-09-04 02:02:05,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:02:05,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:02:05,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:02:05,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7823 (1.7968)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0541 (6.5682)  time: 0.7343 (0.4934 -- 4.3697)  data: 0.0010 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [52] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.7823 (1.8100)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0541 (6.5682)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3030 (0.3030)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4025 (2.4025 -- 2.4025)  data: 2.1875 (2.1875 -- 2.1875)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4585 (0.6868)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (96.9697)  time: 0.4163 (0.1956 -- 2.4025)  data: 0.2088 (0.0004 -- 2.1875)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5267 (0.6397)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2296 (0.1686 -- 0.6696)  data: 0.0300 (0.0001 -- 0.4879)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5983 (0.7035)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2154 (0.1329 -- 0.6696)  data: 0.0297 (0.0001 -- 0.4879)  max mem: 16413
Val: Total time: 0:00:07 (0.2957 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.687
Accuracy of the network on the 482 val images: 82.37%
[2023-09-04 02:02:27,614] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 02:02:27,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 02:02:27,616] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 02:02:27,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 02:02:29,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 02:02:29,075] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [53]  [  0/160]  eta: 0:20:20  lr: 0.000040  min_lr: 0.000010  loss: 2.0904 (2.0904)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6231 (3.6231)  time: 7.6270 (7.6270 -- 7.6270)  data: 7.1005 (7.1005 -- 7.1005)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:44  lr: 0.000040  min_lr: 0.000010  loss: 1.7746 (1.7921)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1631 (7.2207)  time: 0.8538 (0.5277 -- 3.3337)  data: 0.3002 (0.0003 -- 2.7736)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:10  lr: 0.000040  min_lr: 0.000010  loss: 1.6623 (1.7412)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6396 (6.9904)  time: 0.9938 (0.5194 -- 3.8271)  data: 0.4469 (0.0004 -- 3.2922)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000010  loss: 2.0350 (1.8257)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9476 (6.8609)  time: 0.7649 (0.5289 -- 3.4890)  data: 0.2176 (0.0002 -- 2.9524)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:13  lr: 0.000040  min_lr: 0.000010  loss: 1.6914 (1.7918)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3438 (6.7552)  time: 0.7267 (0.5224 -- 2.6937)  data: 0.1762 (0.0003 -- 2.1472)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.8829 (1.8176)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9285 (6.8619)  time: 0.9436 (0.5283 -- 3.1303)  data: 0.3950 (0.0004 -- 2.5971)  max mem: 16413
[2023-09-04 02:04:10,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:04:10,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:04:10,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:04:10,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:04:10,655] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8590
[2023-09-04 02:04:10,655] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8590
[2023-09-04 02:04:10,655] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:04:10,655] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:04:10,656] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.8528 (1.8149)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4479 (6.7165)  time: 0.8737 (0.5193 -- 1.7575)  data: 0.3028 (0.0009 -- 1.2185)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6958 (1.8027)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6265 (6.7713)  time: 0.9265 (0.5270 -- 3.4322)  data: 0.3772 (0.0009 -- 2.9032)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.8373 (1.7953)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6978 (6.8681)  time: 0.6819 (0.4963 -- 3.5135)  data: 0.1710 (0.0002 -- 3.0126)  max mem: 16413
Epoch: [53] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.8373 (1.7766)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6978 (6.8681)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2753 (0.2753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3388 (2.3388 -- 2.3388)  data: 2.1012 (2.1012 -- 2.1012)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5630 (0.7698)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4208 (0.1970 -- 2.3388)  data: 0.2002 (0.0007 -- 2.1012)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5142 (0.6683)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2248 (0.1698 -- 0.4572)  data: 0.0186 (0.0001 -- 0.2685)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5829 (0.7220)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.2096 (0.1330 -- 0.4572)  data: 0.0183 (0.0001 -- 0.2685)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.711
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.37%
Epoch: [54]  [  0/160]  eta: 0:21:14  lr: 0.000040  min_lr: 0.000010  loss: 2.3317 (2.3317)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1991 (6.1991)  time: 7.9632 (7.9632 -- 7.9632)  data: 6.9663 (6.9663 -- 6.9663)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:49  lr: 0.000040  min_lr: 0.000010  loss: 1.7525 (1.7172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4082 (6.9687)  time: 0.8713 (0.5267 -- 3.9737)  data: 0.1862 (0.0003 -- 2.0830)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:03  lr: 0.000040  min_lr: 0.000010  loss: 1.7430 (1.7382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0327 (7.1693)  time: 0.8465 (0.5173 -- 3.2611)  data: 0.1581 (0.0004 -- 2.2727)  max mem: 16413
[2023-09-04 02:05:47,130] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8685
[2023-09-04 02:05:47,130] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8685
[2023-09-04 02:05:47,130] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:05:47,130] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:05:47,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [54]  [ 60/160]  eta: 0:01:39  lr: 0.000040  min_lr: 0.000010  loss: 1.7539 (1.7375)  loss_scale: 16384.0000 (28470.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7302 (7.1212)  time: 0.9285 (0.5228 -- 4.2390)  data: 0.3832 (0.0004 -- 3.7176)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000010  loss: 1.9428 (1.7799)  loss_scale: 16384.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4560 (7.0399)  time: 0.8413 (0.5252 -- 3.2856)  data: 0.2954 (0.0003 -- 2.7606)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000010  loss: 1.8724 (1.7791)  loss_scale: 16384.0000 (23683.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2781 (6.9944)  time: 0.9143 (0.5047 -- 4.4788)  data: 0.3738 (0.0001 -- 3.9555)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.8428 (1.7862)  loss_scale: 16384.0000 (22477.2231)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8845 (6.9788)  time: 0.8298 (0.5244 -- 3.3702)  data: 0.2846 (0.0004 -- 2.8394)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.8417 (1.7888)  loss_scale: 16384.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4242 (6.8795)  time: 0.7880 (0.5272 -- 2.9586)  data: 0.2349 (0.0004 -- 2.4110)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.8098 (1.7849)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8597 (6.8115)  time: 0.7471 (0.4940 -- 4.2921)  data: 0.2267 (0.0002 -- 3.7792)  max mem: 16413
Epoch: [54] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.8098 (1.7768)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8597 (6.8115)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3322 (0.3322)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2515 (2.2515 -- 2.2515)  data: 2.0412 (2.0412 -- 2.0412)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5369 (0.7249)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4183 (0.1973 -- 2.2515)  data: 0.2062 (0.0006 -- 2.0412)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5427 (0.6510)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2328 (0.1688 -- 0.5543)  data: 0.0299 (0.0001 -- 0.3683)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6101 (0.7038)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2177 (0.1322 -- 0.5543)  data: 0.0297 (0.0001 -- 0.3683)  max mem: 16413
Val: Total time: 0:00:07 (0.2923 s / it)
* Acc@1 80.498 Acc@5 96.888 loss 0.690
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 82.37%
Epoch: [55]  [  0/160]  eta: 0:21:10  lr: 0.000040  min_lr: 0.000010  loss: 1.3927 (1.3927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0781 (7.0781)  time: 7.9420 (7.9420 -- 7.9420)  data: 6.2443 (6.2443 -- 6.2443)  max mem: 16413
[2023-09-04 02:07:48,588] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:07:48,588] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:07:48,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:07:48,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [55]  [ 20/160]  eta: 0:02:44  lr: 0.000040  min_lr: 0.000010  loss: 1.8100 (1.8728)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1001 (7.3171)  time: 0.8395 (0.5212 -- 3.4057)  data: 0.1905 (0.0004 -- 2.2317)  max mem: 16413
[2023-09-04 02:08:08,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8837
[2023-09-04 02:08:08,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8837
[2023-09-04 02:08:08,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:08:08,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:08:08,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [ 40/160]  eta: 0:02:01  lr: 0.000040  min_lr: 0.000010  loss: 1.8588 (1.8861)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8857 (6.8729)  time: 0.8357 (0.5210 -- 2.6036)  data: 0.2345 (0.0001 -- 2.0436)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:39  lr: 0.000040  min_lr: 0.000010  loss: 1.7384 (1.8499)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4013 (6.8399)  time: 0.9633 (0.5374 -- 2.7263)  data: 0.4130 (0.0004 -- 2.1395)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:14  lr: 0.000040  min_lr: 0.000010  loss: 1.6693 (1.8074)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4004 (6.9170)  time: 0.7591 (0.5238 -- 2.2077)  data: 0.2125 (0.0005 -- 1.6825)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.8501 (1.8019)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4621 (6.9678)  time: 0.9179 (0.5243 -- 3.9431)  data: 0.3739 (0.0003 -- 3.3984)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.8132 (1.8066)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6935 (7.0731)  time: 0.8403 (0.5130 -- 2.0931)  data: 0.2081 (0.0003 -- 1.5763)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.9518 (1.8200)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1843 (7.0190)  time: 0.8663 (0.5332 -- 1.8616)  data: 0.1916 (0.0005 -- 1.3140)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8373 (1.8302)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1317 (7.0148)  time: 0.7184 (0.4957 -- 2.4608)  data: 0.0675 (0.0002 -- 1.3262)  max mem: 16413
Epoch: [55] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8373 (1.8043)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1317 (7.0148)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4083 (0.4083)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4138 (2.4138 -- 2.4138)  data: 2.1600 (2.1600 -- 2.1600)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4888 (0.7139)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4095 (0.2021 -- 2.4138)  data: 0.1983 (0.0006 -- 2.1600)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5771 (0.6587)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2196 (0.1693 -- 0.4246)  data: 0.0181 (0.0001 -- 0.2265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5943 (0.7103)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2057 (0.1329 -- 0.4246)  data: 0.0176 (0.0001 -- 0.2265)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 81.535 Acc@5 97.095 loss 0.688
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.37%
Epoch: [56]  [  0/160]  eta: 0:17:58  lr: 0.000039  min_lr: 0.000010  loss: 2.1823 (2.1823)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0326 (9.0326)  time: 6.7400 (6.7400 -- 6.7400)  data: 6.1894 (6.1894 -- 6.1894)  max mem: 16413
[2023-09-04 02:10:10,502] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:10:10,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:10:10,502] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:10:10,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [56]  [ 20/160]  eta: 0:02:39  lr: 0.000039  min_lr: 0.000010  loss: 1.8271 (1.8463)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5513 (7.3087)  time: 0.8558 (0.5161 -- 3.5651)  data: 0.3002 (0.0005 -- 3.0193)  max mem: 16413
[2023-09-04 02:10:39,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[9.999403491949061e-06, 9.999403491949061e-06, 1.1110448324387846e-05, 1.1110448324387846e-05, 1.234494258265316e-05, 1.234494258265316e-05, 1.3716602869614624e-05, 1.3716602869614624e-05, 1.524066985512736e-05, 1.524066985512736e-05, 1.6934077616808177e-05, 1.6934077616808177e-05, 1.881564179645353e-05, 1.881564179645353e-05, 2.0906268662726143e-05, 2.0906268662726143e-05, 2.3229187403029046e-05, 2.3229187403029046e-05, 2.5810208225587826e-05, 2.5810208225587826e-05, 2.8678009139542034e-05, 2.8678009139542034e-05, 3.186445459949115e-05, 3.186445459949115e-05, 3.540494955499016e-05, 3.540494955499016e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 02:10:39,983] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.05192865326592, CurrSamplesPerSec=20.394929839767475, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6515 (1.7521)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7354 (7.2481)  time: 0.8717 (0.5278 -- 2.9949)  data: 0.3027 (0.0003 -- 2.4564)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:38  lr: 0.000039  min_lr: 0.000010  loss: 1.7198 (1.7407)  loss_scale: 32768.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3696 (7.0886)  time: 0.9388 (0.5176 -- 3.1276)  data: 0.1418 (0.0003 -- 2.5664)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000039  min_lr: 0.000010  loss: 1.8564 (1.7695)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3416 (7.2307)  time: 0.8025 (0.5285 -- 2.8598)  data: 0.1425 (0.0004 -- 2.3145)  max mem: 16413
[2023-09-04 02:11:29,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9056
[2023-09-04 02:11:29,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9056
[2023-09-04 02:11:29,058] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:11:29,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:11:29,059] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [56]  [100/160]  eta: 0:00:55  lr: 0.000039  min_lr: 0.000010  loss: 1.7260 (1.7290)  loss_scale: 32768.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4989 (7.0899)  time: 0.8239 (0.5216 -- 2.8328)  data: 0.2372 (0.0003 -- 2.2875)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.8603 (1.7456)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0309 (7.1453)  time: 0.8493 (0.5311 -- 3.3022)  data: 0.0297 (0.0002 -- 0.5604)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.6837 (1.7435)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9037 (7.1079)  time: 0.9789 (0.5142 -- 4.7617)  data: 0.0983 (0.0004 -- 1.3246)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8924 (1.7584)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5399 (7.0454)  time: 0.6562 (0.4962 -- 2.4491)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8924 (1.7590)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5399 (7.0454)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3211 (0.3211)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3051 (2.3051 -- 2.3051)  data: 2.0615 (2.0615 -- 2.0615)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4610 (0.7306)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4069 (0.2088 -- 2.3051)  data: 0.1892 (0.0008 -- 2.0615)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5398 (0.6594)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2267 (0.1694 -- 0.5525)  data: 0.0210 (0.0001 -- 0.3558)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5824 (0.7283)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (97.0954)  time: 0.2103 (0.1328 -- 0.5525)  data: 0.0207 (0.0001 -- 0.3558)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 80.290 Acc@5 97.303 loss 0.711
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.37%
Epoch: [57]  [  0/160]  eta: 0:19:08  lr: 0.000039  min_lr: 0.000010  loss: 2.1374 (2.1374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5312 (6.5312)  time: 7.1784 (7.1784 -- 7.1784)  data: 6.6205 (6.6205 -- 6.6205)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:41  lr: 0.000039  min_lr: 0.000010  loss: 1.9918 (1.8792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6918 (7.2279)  time: 0.8553 (0.5299 -- 2.7908)  data: 0.2551 (0.0004 -- 1.6643)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:12  lr: 0.000039  min_lr: 0.000010  loss: 1.5102 (1.7465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7892 (6.8094)  time: 1.0547 (0.5292 -- 4.6574)  data: 0.4680 (0.0004 -- 4.1344)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000039  min_lr: 0.000010  loss: 1.6755 (1.7421)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7413 (6.4947)  time: 0.6667 (0.5122 -- 2.3551)  data: 0.1275 (0.0003 -- 1.8316)  max mem: 16413
[2023-09-04 02:13:34,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:13:34,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:13:34,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:13:34,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [ 80/160]  eta: 0:01:17  lr: 0.000039  min_lr: 0.000010  loss: 1.6828 (1.7316)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4274 (6.4529)  time: 0.9862 (0.5445 -- 4.2755)  data: 0.4321 (0.0008 -- 3.7577)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.7169 (1.7444)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5567 (6.6381)  time: 0.8446 (0.5162 -- 4.0714)  data: 0.3085 (0.0003 -- 3.5377)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.7185 (1.7438)  loss_scale: 32768.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1215 (6.6356)  time: 0.9671 (0.5182 -- 3.8802)  data: 0.4244 (0.0004 -- 3.3655)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.9076 (1.7628)  loss_scale: 32768.0000 (25215.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8113 (6.7189)  time: 0.7665 (0.5278 -- 2.1850)  data: 0.1055 (0.0002 -- 1.1768)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8220 (1.7696)  loss_scale: 32768.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5384 (6.7615)  time: 0.7167 (0.4970 -- 3.7932)  data: 0.0007 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [57] Total time: 0:02:23 (0.8988 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8220 (1.7970)  loss_scale: 32768.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5384 (6.7615)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4060 (0.4060)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3657 (2.3657 -- 2.3657)  data: 2.1530 (2.1530 -- 2.1530)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5648 (0.7432)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4119 (0.2035 -- 2.3657)  data: 0.2004 (0.0005 -- 2.1530)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5648 (0.6605)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2224 (0.1688 -- 0.5385)  data: 0.0208 (0.0001 -- 0.3603)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5917 (0.7091)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.8506)  time: 0.2085 (0.1327 -- 0.5385)  data: 0.0205 (0.0001 -- 0.3603)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 83.195 Acc@5 96.266 loss 0.674
Accuracy of the network on the 482 val images: 83.20%
[2023-09-04 02:15:01,306] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 02:15:01,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 02:15:01,308] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 02:15:01,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 02:15:02,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 02:15:02,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.20%
Epoch: [58]  [  0/160]  eta: 0:26:28  lr: 0.000039  min_lr: 0.000010  loss: 1.7307 (1.7307)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3764 (7.3764)  time: 9.9311 (9.9311 -- 9.9311)  data: 6.7006 (6.7006 -- 6.7006)  max mem: 16413
[2023-09-04 02:15:21,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9291
[2023-09-04 02:15:21,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9291
[2023-09-04 02:15:21,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:15:21,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 02:15:21,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [58]  [ 20/160]  eta: 0:02:49  lr: 0.000039  min_lr: 0.000010  loss: 1.5768 (1.6101)  loss_scale: 16384.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5923 (6.5690)  time: 0.7733 (0.5148 -- 3.1903)  data: 0.0019 (0.0002 -- 0.0108)  max mem: 16413
Epoch: [58]  [ 40/160]  eta: 0:02:06  lr: 0.000039  min_lr: 0.000010  loss: 1.8004 (1.6858)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7447 (7.1260)  time: 0.8960 (0.5241 -- 3.7122)  data: 0.1898 (0.0004 -- 1.4503)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:40  lr: 0.000039  min_lr: 0.000010  loss: 1.6602 (1.6950)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2449 (6.8750)  time: 0.8926 (0.5253 -- 5.5781)  data: 0.1160 (0.0001 -- 1.4820)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:19  lr: 0.000039  min_lr: 0.000010  loss: 1.7130 (1.6955)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4354 (7.1380)  time: 0.9646 (0.5258 -- 4.1200)  data: 0.0017 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 1.9298 (1.7460)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4534 (7.3995)  time: 0.8473 (0.5151 -- 3.8208)  data: 0.0012 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.8955 (1.7819)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5480 (7.3038)  time: 0.8135 (0.5183 -- 2.3504)  data: 0.0024 (0.0003 -- 0.0141)  max mem: 16413
[2023-09-04 02:17:12,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:17:12,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:17:12,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:17:12,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.9327 (1.7918)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7210 (7.1844)  time: 0.7787 (0.5309 -- 2.9276)  data: 0.0024 (0.0006 -- 0.0134)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8178 (1.7916)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8963 (7.0744)  time: 0.7094 (0.4949 -- 3.0303)  data: 0.0008 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [58] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8178 (1.7980)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8963 (7.0744)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3428 (0.3428)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2379 (2.2379 -- 2.2379)  data: 2.0252 (2.0252 -- 2.0252)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5789 (0.6813)  acc1: 77.7778 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4120 (0.1936 -- 2.2379)  data: 0.1983 (0.0004 -- 2.0252)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5034 (0.6120)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2267 (0.1705 -- 0.4709)  data: 0.0224 (0.0001 -- 0.2815)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5786 (0.6663)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2141 (0.1332 -- 0.4709)  data: 0.0221 (0.0001 -- 0.2815)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 83.610 Acc@5 97.095 loss 0.646
Accuracy of the network on the 482 val images: 83.61%
[2023-09-04 02:17:33,570] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 02:17:33,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 02:17:33,572] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 02:17:33,572] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 02:17:34,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 02:17:34,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.61%
Epoch: [59]  [  0/160]  eta: 0:19:50  lr: 0.000039  min_lr: 0.000010  loss: 1.5370 (1.5370)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1483 (6.1483)  time: 7.4429 (7.4429 -- 7.4429)  data: 6.3294 (6.3294 -- 6.3294)  max mem: 16413
[2023-09-04 02:17:45,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9446
[2023-09-04 02:17:45,568] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9446
[2023-09-04 02:17:45,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:17:45,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:17:45,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 20/160]  eta: 0:02:38  lr: 0.000039  min_lr: 0.000010  loss: 1.6308 (1.7463)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8273 (6.8533)  time: 0.8187 (0.5369 -- 3.5127)  data: 0.1691 (0.0004 -- 1.9256)  max mem: 16413
Epoch: [59]  [ 40/160]  eta: 0:02:06  lr: 0.000038  min_lr: 0.000010  loss: 1.9256 (1.8167)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5144 (6.7704)  time: 0.9652 (0.5271 -- 3.5386)  data: 0.4067 (0.0003 -- 3.0171)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:35  lr: 0.000038  min_lr: 0.000010  loss: 1.8590 (1.8150)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4008 (6.7013)  time: 0.7616 (0.5230 -- 1.7989)  data: 0.1810 (0.0002 -- 1.2725)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:14  lr: 0.000038  min_lr: 0.000010  loss: 1.4793 (1.7532)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9215 (6.7082)  time: 0.8535 (0.5288 -- 4.7782)  data: 0.1689 (0.0001 -- 2.7818)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.9236 (1.7927)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2973 (6.6714)  time: 0.9633 (0.5096 -- 4.0946)  data: 0.3829 (0.0003 -- 3.5759)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.8110 (1.7832)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1814 (6.7777)  time: 0.8062 (0.5263 -- 3.4560)  data: 0.2636 (0.0003 -- 2.9364)  max mem: 16413
[2023-09-04 02:19:37,966] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:19:37,966] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:19:38,008] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:19:38,008] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6707 (1.7776)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5262 (6.8539)  time: 0.8867 (0.5199 -- 3.3889)  data: 0.3283 (0.0004 -- 2.8406)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5550 (1.7678)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3584 (6.8923)  time: 0.7283 (0.4963 -- 3.4641)  data: 0.2092 (0.0002 -- 2.9400)  max mem: 16413
Epoch: [59] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5550 (1.7496)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3584 (6.8923)
[2023-09-04 02:19:57,455] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-04 02:19:57,457] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-04 02:19:57,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-04 02:19:57,457] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-04 02:19:58,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-04 02:19:58,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2271 (0.2271)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2627 (2.2627 -- 2.2627)  data: 2.0514 (2.0514 -- 2.0514)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4118 (0.6783)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4130 (0.2088 -- 2.2627)  data: 0.1972 (0.0011 -- 2.0514)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4363 (0.6076)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2289 (0.1710 -- 0.5062)  data: 0.0229 (0.0001 -- 0.2917)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5285 (0.6784)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.4357)  time: 0.2117 (0.1333 -- 0.5062)  data: 0.0225 (0.0001 -- 0.2917)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 83.610 Acc@5 96.266 loss 0.661
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 83.61%
Epoch: [60]  [  0/160]  eta: 0:20:43  lr: 0.000038  min_lr: 0.000010  loss: 2.0071 (2.0071)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5306 (8.5306)  time: 7.7716 (7.7716 -- 7.7716)  data: 7.2059 (7.2059 -- 7.2059)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:48  lr: 0.000038  min_lr: 0.000010  loss: 1.6071 (1.7020)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4731 (7.0758)  time: 0.8719 (0.5212 -- 3.3941)  data: 0.3334 (0.0002 -- 2.8776)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:03  lr: 0.000038  min_lr: 0.000010  loss: 1.8159 (1.7252)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2659 (6.8870)  time: 0.8535 (0.5240 -- 2.9499)  data: 0.1875 (0.0005 -- 2.0223)  max mem: 16413
[2023-09-04 02:21:03,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9657
[2023-09-04 02:21:03,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9657
[2023-09-04 02:21:03,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:21:03,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:21:03,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [60]  [ 60/160]  eta: 0:01:36  lr: 0.000038  min_lr: 0.000010  loss: 1.7912 (1.7469)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4824 (6.9731)  time: 0.8410 (0.5231 -- 2.7734)  data: 0.2373 (0.0005 -- 2.2596)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:17  lr: 0.000038  min_lr: 0.000010  loss: 1.8069 (1.7795)  loss_scale: 16384.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3622 (6.8484)  time: 0.9560 (0.5234 -- 2.6393)  data: 0.1502 (0.0002 -- 2.0437)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000010  loss: 1.7548 (1.7792)  loss_scale: 16384.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1812 (7.0705)  time: 0.7977 (0.5133 -- 2.9260)  data: 0.1022 (0.0004 -- 1.3169)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.9116 (1.7847)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0206 (7.0573)  time: 0.7937 (0.5255 -- 3.3911)  data: 0.2111 (0.0006 -- 2.8224)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.7983 (1.7917)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8075 (7.1285)  time: 0.9070 (0.5201 -- 3.7852)  data: 0.3672 (0.0005 -- 3.2653)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.6697 (1.7866)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1493 (7.0603)  time: 0.7491 (0.4952 -- 2.4889)  data: 0.1280 (0.0002 -- 1.9891)  max mem: 16413
Epoch: [60] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.6697 (1.7749)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1493 (7.0603)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2008 (0.2008)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4075 (2.4075 -- 2.4075)  data: 2.1959 (2.1959 -- 2.1959)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5304 (0.6793)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4152 (0.2044 -- 2.4075)  data: 0.2009 (0.0007 -- 2.1959)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5216 (0.6114)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2204 (0.1697 -- 0.4394)  data: 0.0157 (0.0001 -- 0.2428)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5346 (0.6734)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2047 (0.1325 -- 0.4394)  data: 0.0154 (0.0001 -- 0.2428)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.656
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [61]  [  0/160]  eta: 0:21:00  lr: 0.000038  min_lr: 0.000010  loss: 2.0752 (2.0752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5839 (5.5839)  time: 7.8807 (7.8807 -- 7.8807)  data: 7.3412 (7.3412 -- 7.3412)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:55  lr: 0.000038  min_lr: 0.000010  loss: 1.7461 (1.7211)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1614 (6.6731)  time: 0.9193 (0.5172 -- 4.4572)  data: 0.0880 (0.0002 -- 1.7353)  max mem: 16413
[2023-09-04 02:23:07,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:23:07,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:23:07,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:23:07,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [61]  [ 40/160]  eta: 0:02:10  lr: 0.000038  min_lr: 0.000010  loss: 1.7137 (1.7320)  loss_scale: 32768.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3214 (6.6588)  time: 0.9143 (0.5274 -- 2.8507)  data: 0.0017 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:40  lr: 0.000038  min_lr: 0.000010  loss: 1.6498 (1.7356)  loss_scale: 32768.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5748 (6.7741)  time: 0.8329 (0.5292 -- 4.2595)  data: 0.0024 (0.0004 -- 0.0179)  max mem: 16413
[2023-09-04 02:23:45,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9828
[2023-09-04 02:23:45,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9828
[2023-09-04 02:23:45,295] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:23:45,295] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:23:45,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000038  min_lr: 0.000010  loss: 1.6302 (1.7132)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5206 (6.6356)  time: 0.9637 (0.5151 -- 3.8666)  data: 0.0013 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.8008 (1.7313)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5085 (6.6993)  time: 0.7386 (0.5321 -- 2.8271)  data: 0.0018 (0.0004 -- 0.0077)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000010  loss: 1.6682 (1.7389)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4332 (6.7050)  time: 0.9173 (0.5204 -- 3.9674)  data: 0.0017 (0.0002 -- 0.0120)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6981 (1.7428)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6094 (6.6387)  time: 0.8482 (0.5106 -- 3.7174)  data: 0.0019 (0.0003 -- 0.0141)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.8699 (1.7578)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0444 (6.6896)  time: 0.6470 (0.4937 -- 2.5330)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [61] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.8699 (1.7712)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0444 (6.6896)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2929 (0.2929)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2648 (2.2648 -- 2.2648)  data: 2.0381 (2.0381 -- 2.0381)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5234 (0.6815)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (96.9697)  time: 0.4189 (0.2045 -- 2.2648)  data: 0.2034 (0.0005 -- 2.0381)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5574 (0.6329)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2354 (0.1695 -- 0.6208)  data: 0.0320 (0.0001 -- 0.4371)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6371 (0.6824)  acc1: 85.7143 (82.9876)  acc5: 100.0000 (96.2656)  time: 0.2205 (0.1327 -- 0.6208)  data: 0.0317 (0.0001 -- 0.4371)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 84.647 Acc@5 96.680 loss 0.667
Accuracy of the network on the 482 val images: 84.65%
[2023-09-04 02:25:07,745] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 02:25:07,747] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 02:25:07,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 02:25:07,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 02:25:09,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 02:25:09,161] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.65%
Epoch: [62]  [  0/160]  eta: 0:20:53  lr: 0.000038  min_lr: 0.000010  loss: 1.5296 (1.5296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6002 (3.6002)  time: 7.8335 (7.8335 -- 7.8335)  data: 7.3129 (7.3129 -- 7.3129)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:37  lr: 0.000038  min_lr: 0.000010  loss: 1.7963 (1.8073)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8885 (6.6923)  time: 0.7864 (0.5306 -- 2.4531)  data: 0.2407 (0.0004 -- 1.9373)  max mem: 16413
[2023-09-04 02:25:48,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:25:48,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:25:48,334] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:25:48,334] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [ 40/160]  eta: 0:02:07  lr: 0.000038  min_lr: 0.000010  loss: 1.6664 (1.7485)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0579 (6.6248)  time: 0.9996 (0.5274 -- 4.5260)  data: 0.4534 (0.0004 -- 4.0055)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:39  lr: 0.000038  min_lr: 0.000010  loss: 1.8501 (1.7810)  loss_scale: 32768.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5955 (6.8999)  time: 0.8672 (0.5167 -- 3.9485)  data: 0.3211 (0.0004 -- 3.4266)  max mem: 16413
[2023-09-04 02:26:26,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=54, lr=[9.539276537446474e-06, 9.539276537446474e-06, 1.0599196152718305e-05, 1.0599196152718305e-05, 1.1776884614131447e-05, 1.1776884614131447e-05, 1.3085427349034943e-05, 1.3085427349034943e-05, 1.4539363721149935e-05, 1.4539363721149935e-05, 1.6154848579055485e-05, 1.6154848579055485e-05, 1.794983175450609e-05, 1.794983175450609e-05, 1.9944257505006766e-05, 1.9944257505006766e-05, 2.2160286116674188e-05, 2.2160286116674188e-05, 2.4622540129637982e-05, 2.4622540129637982e-05, 2.7358377921819982e-05, 2.7358377921819982e-05, 3.0398197690911092e-05, 3.0398197690911092e-05, 3.377577521212343e-05, 3.377577521212343e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 02:26:26,307] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.023623500751988, CurrSamplesPerSec=22.26259268220294, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:21  lr: 0.000038  min_lr: 0.000010  loss: 1.7471 (1.7606)  loss_scale: 32768.0000 (25283.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4601 (6.8534)  time: 1.0712 (0.5076 -- 5.1959)  data: 0.1483 (0.0003 -- 1.7148)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:58  lr: 0.000037  min_lr: 0.000010  loss: 1.7084 (1.7571)  loss_scale: 32768.0000 (26765.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8194 (6.8296)  time: 0.7975 (0.4993 -- 4.7490)  data: 0.0010 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000010  loss: 1.8984 (1.7790)  loss_scale: 32768.0000 (27758.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0507 (6.9954)  time: 0.8407 (0.5264 -- 3.8496)  data: 0.0013 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000010  loss: 1.7561 (1.7749)  loss_scale: 32768.0000 (28468.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8277 (6.9553)  time: 0.7561 (0.5251 -- 4.0695)  data: 0.0015 (0.0002 -- 0.0060)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.8254 (1.7750)  loss_scale: 32768.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2112 (6.8828)  time: 0.6478 (0.4967 -- 1.2457)  data: 0.0507 (0.0002 -- 0.6125)  max mem: 16413
Epoch: [62] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.8254 (1.7418)  loss_scale: 32768.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2112 (6.8828)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2468 (0.2468)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2759 (2.2759 -- 2.2759)  data: 2.0237 (2.0237 -- 2.0237)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4184 (0.7021)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4092 (0.2089 -- 2.2759)  data: 0.1850 (0.0007 -- 2.0237)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5187 (0.6429)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2288 (0.1701 -- 0.5478)  data: 0.0185 (0.0001 -- 0.3559)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6020 (0.6898)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2108 (0.1329 -- 0.5478)  data: 0.0183 (0.0001 -- 0.3559)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 82.988 Acc@5 96.680 loss 0.668
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.65%
Epoch: [63]  [  0/160]  eta: 0:20:01  lr: 0.000037  min_lr: 0.000010  loss: 1.9624 (1.9624)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3715 (5.3715)  time: 7.5098 (7.5098 -- 7.5098)  data: 6.3893 (6.3893 -- 6.3893)  max mem: 16413
[2023-09-04 02:27:50,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:27:50,964] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:27:50,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:27:50,964] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:27:59,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10095
[2023-09-04 02:27:59,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:27:59,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10095
[2023-09-04 02:27:59,914] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 02:27:59,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [63]  [ 20/160]  eta: 0:02:58  lr: 0.000037  min_lr: 0.000009  loss: 1.8587 (1.7063)  loss_scale: 32768.0000 (48371.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0359 (6.2737)  time: 0.9609 (0.5304 -- 4.2055)  data: 0.0541 (0.0004 -- 1.0322)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:03  lr: 0.000037  min_lr: 0.000009  loss: 1.7979 (1.7396)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3567 (6.4247)  time: 0.7689 (0.5310 -- 3.4762)  data: 0.0016 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000009  loss: 1.7116 (1.7323)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8565 (6.7657)  time: 0.8816 (0.5291 -- 3.8566)  data: 0.0107 (0.0008 -- 0.1769)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:14  lr: 0.000037  min_lr: 0.000009  loss: 1.7055 (1.7327)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9138 (6.9846)  time: 0.8016 (0.5384 -- 2.5913)  data: 0.2044 (0.0003 -- 1.7485)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.7548 (1.7493)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1725 (7.0408)  time: 0.9920 (0.5330 -- 3.2837)  data: 0.3494 (0.0001 -- 2.7372)  max mem: 16413
[2023-09-04 02:29:16,966] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10183
[2023-09-04 02:29:16,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:29:16,966] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10183
[2023-09-04 02:29:16,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:29:16,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000009  loss: 1.8214 (1.7577)  loss_scale: 16384.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2478 (6.9525)  time: 0.8457 (0.5371 -- 2.9411)  data: 0.1547 (0.0003 -- 2.4153)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.8377 (1.7593)  loss_scale: 16384.0000 (30676.4255)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4171 (7.0034)  time: 0.8734 (0.5217 -- 2.7485)  data: 0.1065 (0.0004 -- 1.1105)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.6681 (1.7546)  loss_scale: 16384.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0986 (7.0368)  time: 0.6384 (0.4947 -- 1.6696)  data: 0.0014 (0.0002 -- 0.0133)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8889 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.6681 (1.7663)  loss_scale: 16384.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0986 (7.0368)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2542 (0.2542)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1519 (2.1519 -- 2.1519)  data: 1.9387 (1.9387 -- 1.9387)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5454 (0.7183)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.3991 (0.1936 -- 2.1519)  data: 0.1824 (0.0007 -- 1.9387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5027 (0.6204)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.3545)  time: 0.2328 (0.1711 -- 0.4895)  data: 0.0258 (0.0001 -- 0.2882)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5280 (0.6807)  acc1: 88.8889 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2137 (0.1330 -- 0.4895)  data: 0.0252 (0.0001 -- 0.2882)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 81.328 Acc@5 96.680 loss 0.668
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 84.65%
Epoch: [64]  [  0/160]  eta: 0:19:16  lr: 0.000037  min_lr: 0.000009  loss: 1.5415 (1.5415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0541 (10.0541)  time: 7.2284 (7.2284 -- 7.2284)  data: 6.6903 (6.6903 -- 6.6903)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:43  lr: 0.000037  min_lr: 0.000009  loss: 1.8087 (1.8376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0871 (7.4870)  time: 0.8649 (0.5163 -- 3.8180)  data: 0.3206 (0.0007 -- 3.2916)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:03  lr: 0.000037  min_lr: 0.000009  loss: 1.6970 (1.8088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2445 (7.1613)  time: 0.8808 (0.5392 -- 3.2309)  data: 0.2683 (0.0004 -- 2.6965)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000009  loss: 1.6846 (1.7592)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4109 (7.3457)  time: 0.9275 (0.5327 -- 2.7422)  data: 0.3305 (0.0004 -- 2.2100)  max mem: 16413
[2023-09-04 02:31:18,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:31:18,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:31:18,718] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:31:18,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000009  loss: 1.7451 (1.7484)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9216 (7.3249)  time: 0.8021 (0.5234 -- 2.4697)  data: 0.2135 (0.0002 -- 1.9226)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.9126 (1.7741)  loss_scale: 32768.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0162 (7.3896)  time: 0.9081 (0.5239 -- 2.8521)  data: 0.3705 (0.0003 -- 2.3339)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.9061 (1.7949)  loss_scale: 32768.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5995 (7.2661)  time: 0.7820 (0.5298 -- 3.1509)  data: 0.2338 (0.0004 -- 2.5858)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.6577 (1.7738)  loss_scale: 32768.0000 (24401.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3330 (7.1548)  time: 0.9953 (0.5165 -- 3.3093)  data: 0.4508 (0.0005 -- 2.7651)  max mem: 16413
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.7972 (1.7694)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3771 (7.1715)  time: 0.6708 (0.4953 -- 3.0778)  data: 0.1536 (0.0001 -- 2.5672)  max mem: 16413
Epoch: [64] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.7972 (1.7736)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3771 (7.1715)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3352 (0.3352)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1780 (2.1780 -- 2.1780)  data: 1.9763 (1.9763 -- 1.9763)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5412 (0.6943)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4465 (0.2010 -- 2.1780)  data: 0.2324 (0.0006 -- 1.9763)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5412 (0.6178)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2351 (0.1693 -- 0.7906)  data: 0.0292 (0.0001 -- 0.5605)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5854 (0.6710)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2209 (0.1335 -- 0.7906)  data: 0.0285 (0.0001 -- 0.5605)  max mem: 16413
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 82.780 Acc@5 96.680 loss 0.678
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.65%
Epoch: [65]  [  0/160]  eta: 0:19:02  lr: 0.000037  min_lr: 0.000009  loss: 1.4913 (1.4913)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0215 (6.0215)  time: 7.1432 (7.1432 -- 7.1432)  data: 4.5333 (4.5333 -- 4.5333)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000009  loss: 1.6868 (1.7138)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1969 (7.0229)  time: 0.8236 (0.5282 -- 4.1402)  data: 0.0019 (0.0007 -- 0.0051)  max mem: 16413
[2023-09-04 02:33:22,036] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:33:22,037] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:33:22,038] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:33:22,038] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [65]  [ 40/160]  eta: 0:02:00  lr: 0.000037  min_lr: 0.000009  loss: 1.3971 (1.6144)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0085 (7.0804)  time: 0.8755 (0.5169 -- 3.3000)  data: 0.0776 (0.0004 -- 1.3063)  max mem: 16413
[2023-09-04 02:33:25,544] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10443
[2023-09-04 02:33:25,544] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10443
[2023-09-04 02:33:25,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:33:25,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:33:25,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [65]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000009  loss: 1.6727 (1.6281)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0828 (6.9407)  time: 0.9764 (0.5265 -- 3.9600)  data: 0.3621 (0.0005 -- 3.4374)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000009  loss: 1.8630 (1.6678)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7105 (6.9546)  time: 0.7805 (0.5267 -- 3.1417)  data: 0.2325 (0.0003 -- 2.6329)  max mem: 16413
[2023-09-04 02:34:11,020] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10495
[2023-09-04 02:34:11,020] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10495
[2023-09-04 02:34:11,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:34:11,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:34:11,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [100/160]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000009  loss: 1.7109 (1.6720)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6053 (6.9753)  time: 0.8742 (0.5185 -- 4.6416)  data: 0.3287 (0.0007 -- 4.1265)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.7062 (1.6897)  loss_scale: 16384.0000 (30059.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0201 (6.9914)  time: 0.8818 (0.5180 -- 3.9001)  data: 0.3142 (0.0003 -- 3.3760)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.6042 (1.6854)  loss_scale: 16384.0000 (28120.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0759 (7.1169)  time: 0.9968 (0.5127 -- 5.2809)  data: 0.4636 (0.0003 -- 4.7682)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.8044 (1.6976)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2019 (7.0728)  time: 0.7013 (0.4933 -- 2.6315)  data: 0.1816 (0.0002 -- 2.1176)  max mem: 16413
Epoch: [65] Total time: 0:02:24 (0.9051 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.8044 (1.7392)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2019 (7.0728)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3411 (0.3411)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2965 (2.2965 -- 2.2965)  data: 2.0651 (2.0651 -- 2.0651)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5791 (0.6745)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4034 (0.2015 -- 2.2965)  data: 0.1907 (0.0007 -- 2.0651)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4498 (0.5909)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2364 (0.1691 -- 0.8221)  data: 0.0335 (0.0001 -- 0.6334)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4703 (0.6482)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2213 (0.1330 -- 0.8221)  data: 0.0328 (0.0001 -- 0.6334)  max mem: 16413
Val: Total time: 0:00:08 (0.2969 s / it)
* Acc@1 83.610 Acc@5 97.095 loss 0.640
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.65%
Epoch: [66]  [  0/160]  eta: 0:17:25  lr: 0.000036  min_lr: 0.000009  loss: 1.0682 (1.0682)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7908 (7.7908)  time: 6.5322 (6.5322 -- 6.5322)  data: 4.9770 (4.9770 -- 4.9770)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:41  lr: 0.000036  min_lr: 0.000009  loss: 1.7909 (1.7369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1596 (6.4753)  time: 0.8879 (0.5122 -- 3.0791)  data: 0.3341 (0.0007 -- 2.5544)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:01:59  lr: 0.000036  min_lr: 0.000009  loss: 1.6848 (1.7425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0096 (6.3253)  time: 0.8195 (0.5085 -- 3.6937)  data: 0.2807 (0.0002 -- 3.1617)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:42  lr: 0.000036  min_lr: 0.000009  loss: 1.5642 (1.7193)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5026 (6.4531)  time: 1.0877 (0.5316 -- 4.7299)  data: 0.5393 (0.0005 -- 4.1963)  max mem: 16413
[2023-09-04 02:36:18,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:36:18,351] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:36:18,354] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:36:18,354] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.8900 (1.7701)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5185 (6.5049)  time: 0.7568 (0.5265 -- 2.8285)  data: 0.2093 (0.0001 -- 2.3082)  max mem: 16413
[2023-09-04 02:36:39,590] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10648
[2023-09-04 02:36:39,590] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10648
[2023-09-04 02:36:39,590] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:36:39,590] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:36:39,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.6545 (1.7597)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6575 (6.4054)  time: 0.9245 (0.5153 -- 4.4333)  data: 0.3808 (0.0002 -- 3.9187)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.8739 (1.7724)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8220 (6.3710)  time: 0.8657 (0.5112 -- 3.9671)  data: 0.3230 (0.0002 -- 3.4477)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.6793 (1.7627)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0537 (6.6097)  time: 0.9680 (0.5104 -- 3.7860)  data: 0.4297 (0.0003 -- 3.2787)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.7015 (1.7463)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0211 (6.6749)  time: 0.5737 (0.4938 -- 1.6719)  data: 0.0577 (0.0001 -- 1.1409)  max mem: 16413
Epoch: [66] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.7015 (1.7710)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0211 (6.6749)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2306 (0.2306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2985 (2.2985 -- 2.2985)  data: 2.0582 (2.0582 -- 2.0582)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5773 (0.6764)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4023 (0.2056 -- 2.2985)  data: 0.1883 (0.0006 -- 2.0582)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5430 (0.6210)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2230 (0.1695 -- 0.5697)  data: 0.0204 (0.0001 -- 0.3923)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5548 (0.6651)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2077 (0.1338 -- 0.5697)  data: 0.0200 (0.0001 -- 0.3923)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 83.195 Acc@5 96.888 loss 0.654
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.65%
Epoch: [67]  [  0/160]  eta: 0:18:07  lr: 0.000036  min_lr: 0.000009  loss: 1.7161 (1.7161)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3225 (4.3225)  time: 6.7988 (6.7988 -- 6.7988)  data: 5.7469 (5.7469 -- 5.7469)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:47  lr: 0.000036  min_lr: 0.000009  loss: 1.6553 (1.7714)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0042 (7.0289)  time: 0.9145 (0.5293 -- 3.6789)  data: 0.3627 (0.0008 -- 3.1245)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:12  lr: 0.000036  min_lr: 0.000009  loss: 1.6063 (1.7244)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5830 (6.9780)  time: 1.0085 (0.5173 -- 4.1326)  data: 0.4639 (0.0003 -- 3.5956)  max mem: 16413
[2023-09-04 02:38:42,375] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:38:42,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:38:42,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:38:42,376] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000009  loss: 1.8568 (1.7291)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2282 (7.1404)  time: 0.6769 (0.5278 -- 2.5043)  data: 0.1254 (0.0005 -- 1.9620)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:18  lr: 0.000036  min_lr: 0.000009  loss: 1.7559 (1.7373)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7932 (7.1424)  time: 1.0373 (0.5151 -- 4.7703)  data: 0.5004 (0.0003 -- 4.2434)  max mem: 16413
[2023-09-04 02:39:05,842] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10802
[2023-09-04 02:39:05,842] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10802
[2023-09-04 02:39:05,843] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:39:05,843] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:39:05,843] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.8029 (1.7524)  loss_scale: 16384.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5723 (7.2907)  time: 0.8401 (0.5154 -- 3.6930)  data: 0.3040 (0.0004 -- 3.1696)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.5777 (1.7346)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6498 (7.2390)  time: 0.8962 (0.5166 -- 3.2505)  data: 0.3505 (0.0004 -- 2.7347)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.8196 (1.7319)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1726 (7.1560)  time: 0.8041 (0.5099 -- 3.9836)  data: 0.2325 (0.0001 -- 3.2766)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.8981 (1.7399)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2723 (7.1138)  time: 0.7197 (0.4943 -- 4.7341)  data: 0.0548 (0.0002 -- 1.0864)  max mem: 16413
Epoch: [67] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.8981 (1.7367)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2723 (7.1138)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2886 (0.2886)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4236 (2.4236 -- 2.4236)  data: 2.1814 (2.1814 -- 2.1814)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4537 (0.6583)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (96.9697)  time: 0.4157 (0.1978 -- 2.4236)  data: 0.2014 (0.0008 -- 2.1814)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5435 (0.6171)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2198 (0.1685 -- 0.4855)  data: 0.0167 (0.0001 -- 0.2966)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5698 (0.6622)  acc1: 85.7143 (82.9876)  acc5: 100.0000 (95.8506)  time: 0.2035 (0.1330 -- 0.4855)  data: 0.0163 (0.0001 -- 0.2966)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 84.232 Acc@5 96.680 loss 0.630
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.65%
Epoch: [68]  [  0/160]  eta: 0:21:10  lr: 0.000036  min_lr: 0.000009  loss: 2.1091 (2.1091)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5813 (5.5813)  time: 7.9395 (7.9395 -- 7.9395)  data: 7.4122 (7.4122 -- 7.4122)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:49  lr: 0.000036  min_lr: 0.000009  loss: 1.6229 (1.6629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6403 (7.8042)  time: 0.8708 (0.5175 -- 4.8060)  data: 0.2674 (0.0003 -- 3.1402)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:13  lr: 0.000036  min_lr: 0.000009  loss: 1.6046 (1.6512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8473 (7.3199)  time: 1.0114 (0.5254 -- 5.2300)  data: 0.0154 (0.0002 -- 0.2716)  max mem: 16413
[2023-09-04 02:41:12,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:41:12,304] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:41:12,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:41:12,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [68]  [ 60/160]  eta: 0:01:39  lr: 0.000036  min_lr: 0.000009  loss: 1.6837 (1.6605)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1431 (7.0646)  time: 0.7570 (0.5136 -- 4.0575)  data: 0.0012 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000009  loss: 1.8680 (1.7111)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2190 (7.0313)  time: 0.8948 (0.5251 -- 3.3889)  data: 0.0018 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000009  loss: 1.7447 (1.7234)  loss_scale: 32768.0000 (24494.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7824 (6.9312)  time: 0.8610 (0.5279 -- 4.0569)  data: 0.0844 (0.0005 -- 0.8430)  max mem: 16413
[2023-09-04 02:42:09,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=60, lr=[9.042925336597765e-06, 9.042925336597765e-06, 1.0047694818441962e-05, 1.0047694818441962e-05, 1.1164105353824401e-05, 1.1164105353824401e-05, 1.2404561504249335e-05, 1.2404561504249335e-05, 1.3782846115832595e-05, 1.3782846115832595e-05, 1.5314273462036215e-05, 1.5314273462036215e-05, 1.701585940226246e-05, 1.701585940226246e-05, 1.890651044695829e-05, 1.890651044695829e-05, 2.1007233829953654e-05, 2.1007233829953654e-05, 2.3341370922170724e-05, 2.3341370922170724e-05, 2.59348565801897e-05, 2.59348565801897e-05, 2.8816507311321884e-05, 2.8816507311321884e-05, 3.2018341457024315e-05, 3.2018341457024315e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 02:42:09,695] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=18.096572561925413, CurrSamplesPerSec=23.029335017123874, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.8340 (1.7326)  loss_scale: 32768.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6286 (6.8688)  time: 0.8524 (0.5272 -- 3.7140)  data: 0.0017 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.9740 (1.7633)  loss_scale: 32768.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3515 (6.9061)  time: 0.8083 (0.5225 -- 2.3385)  data: 0.1107 (0.0003 -- 1.4333)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.7232 (1.7596)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0134 (6.8411)  time: 0.7031 (0.4952 -- 1.6542)  data: 0.0489 (0.0002 -- 0.9582)  max mem: 16413
Epoch: [68] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.7232 (1.7566)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0134 (6.8411)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2367 (0.2367)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3463 (2.3463 -- 2.3463)  data: 2.0991 (2.0991 -- 2.0991)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3957 (0.6821)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4250 (0.1875 -- 2.3463)  data: 0.2149 (0.0003 -- 2.0991)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4707 (0.6181)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2353 (0.1696 -- 0.6139)  data: 0.0335 (0.0001 -- 0.4021)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5414 (0.6661)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (96.2656)  time: 0.2229 (0.1342 -- 0.6139)  data: 0.0326 (0.0001 -- 0.4021)  max mem: 16413
Val: Total time: 0:00:08 (0.2977 s / it)
* Acc@1 83.610 Acc@5 96.680 loss 0.645
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.65%
Epoch: [69]  [  0/160]  eta: 0:19:35  lr: 0.000035  min_lr: 0.000009  loss: 2.2706 (2.2706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0368 (6.0368)  time: 7.3472 (7.3472 -- 7.3472)  data: 5.6105 (5.6105 -- 5.6105)  max mem: 16413
[2023-09-04 02:43:13,519] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:43:13,519] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:43:13,519] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:43:13,520] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [69]  [ 20/160]  eta: 0:02:53  lr: 0.000035  min_lr: 0.000009  loss: 1.8316 (1.8469)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1629 (6.3952)  time: 0.9373 (0.5308 -- 4.0373)  data: 0.3248 (0.0009 -- 3.5115)  max mem: 16413
[2023-09-04 02:43:15,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11063
[2023-09-04 02:43:15,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11063
[2023-09-04 02:43:15,691] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:43:15,691] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:43:15,691] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [69]  [ 40/160]  eta: 0:02:09  lr: 0.000035  min_lr: 0.000009  loss: 1.7077 (1.8075)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7893 (6.8496)  time: 0.9051 (0.5209 -- 3.0252)  data: 0.2346 (0.0003 -- 2.2129)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:37  lr: 0.000035  min_lr: 0.000009  loss: 1.9257 (1.8479)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1888 (7.0120)  time: 0.7788 (0.5351 -- 2.5791)  data: 0.1605 (0.0006 -- 1.7303)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000009  loss: 1.8121 (1.8251)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1142 (7.1843)  time: 0.9984 (0.5176 -- 4.6099)  data: 0.0701 (0.0005 -- 1.3425)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000009  loss: 1.8392 (1.8095)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2245 (7.1059)  time: 0.7871 (0.5114 -- 3.6726)  data: 0.0060 (0.0001 -- 0.0782)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.8003 (1.8143)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5931 (7.1327)  time: 0.9235 (0.5248 -- 3.5986)  data: 0.0522 (0.0004 -- 1.0097)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7449 (1.8065)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0808 (7.0586)  time: 0.8466 (0.5258 -- 3.5487)  data: 0.0018 (0.0003 -- 0.0071)  max mem: 16413
[2023-09-04 02:45:04,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11189
[2023-09-04 02:45:04,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11189
[2023-09-04 02:45:04,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:45:04,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:45:04,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.5294 (1.7865)  loss_scale: 16384.0000 (32460.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6462 (7.0704)  time: 0.5972 (0.4935 -- 1.3497)  data: 0.0017 (0.0002 -- 0.0166)  max mem: 16413
Epoch: [69] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.5294 (1.7632)  loss_scale: 16384.0000 (32460.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6462 (7.0704)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2072 (0.2072)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3771 (2.3771 -- 2.3771)  data: 2.1063 (2.1063 -- 2.1063)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4160 (0.6367)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4292 (0.2021 -- 2.3771)  data: 0.2113 (0.0007 -- 2.1063)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5095 (0.6022)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2244 (0.1693 -- 0.4211)  data: 0.0225 (0.0001 -- 0.2285)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6295 (0.6599)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2094 (0.1330 -- 0.4211)  data: 0.0219 (0.0001 -- 0.2285)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 82.780 Acc@5 96.680 loss 0.663
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.65%
Epoch: [70]  [  0/160]  eta: 0:17:48  lr: 0.000035  min_lr: 0.000009  loss: 1.6826 (1.6826)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9338 (4.9338)  time: 6.6791 (6.6791 -- 6.6791)  data: 6.1232 (6.1232 -- 6.1232)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:52  lr: 0.000035  min_lr: 0.000009  loss: 1.7926 (1.7461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3853 (6.8140)  time: 0.9605 (0.5238 -- 4.1740)  data: 0.3506 (0.0003 -- 3.6575)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:01:59  lr: 0.000035  min_lr: 0.000009  loss: 1.6219 (1.7057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1304 (6.6727)  time: 0.7539 (0.5257 -- 2.7396)  data: 0.2077 (0.0004 -- 2.1910)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:39  lr: 0.000035  min_lr: 0.000009  loss: 1.6381 (1.6919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4680 (6.7745)  time: 0.9795 (0.5250 -- 3.5961)  data: 0.4342 (0.0003 -- 3.0685)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:17  lr: 0.000035  min_lr: 0.000009  loss: 1.8594 (1.7332)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7608 (6.8143)  time: 0.8786 (0.5211 -- 3.5987)  data: 0.0706 (0.0006 -- 1.1795)  max mem: 16413
[2023-09-04 02:46:41,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11287
[2023-09-04 02:46:41,619] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 02:46:41,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11287
[2023-09-04 02:46:41,660] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 02:46:41,660] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [70]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000009  loss: 1.9153 (1.7627)  loss_scale: 8192.0000 (15248.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9420 (6.7473)  time: 0.7814 (0.5249 -- 2.2027)  data: 0.0858 (0.0003 -- 1.6816)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.8062 (1.7646)  loss_scale: 8192.0000 (14082.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0282 (6.6763)  time: 0.8152 (0.5390 -- 2.5560)  data: 0.1491 (0.0002 -- 2.0099)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.8466 (1.7673)  loss_scale: 8192.0000 (13246.6383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7058 (6.7362)  time: 0.8964 (0.5161 -- 1.9654)  data: 0.1557 (0.0008 -- 1.4343)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.8979 (1.7736)  loss_scale: 8192.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8686 (6.7570)  time: 0.6943 (0.4941 -- 1.8171)  data: 0.0971 (0.0002 -- 1.3016)  max mem: 16413
Epoch: [70] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.8979 (1.7472)  loss_scale: 8192.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8686 (6.7570)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2369 (0.2369)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2535 (2.2535 -- 2.2535)  data: 2.0480 (2.0480 -- 2.0480)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4848 (0.6727)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4217 (0.1993 -- 2.2535)  data: 0.2116 (0.0008 -- 2.0480)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4848 (0.6176)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2282 (0.1691 -- 0.4004)  data: 0.0282 (0.0001 -- 0.2129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5367 (0.6805)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2036 (0.1329 -- 0.3762)  data: 0.0171 (0.0001 -- 0.2009)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 81.743 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 84.65%
Epoch: [71]  [  0/160]  eta: 0:21:55  lr: 0.000035  min_lr: 0.000009  loss: 1.2820 (1.2820)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0441 (10.0441)  time: 8.2202 (8.2202 -- 8.2202)  data: 5.8609 (5.8609 -- 5.8609)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:46  lr: 0.000035  min_lr: 0.000009  loss: 1.6900 (1.6937)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8212 (7.8468)  time: 0.8366 (0.5210 -- 3.4159)  data: 0.2192 (0.0003 -- 2.1636)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:07  lr: 0.000035  min_lr: 0.000009  loss: 1.6294 (1.6689)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2833 (7.2131)  time: 0.9329 (0.5115 -- 4.7287)  data: 0.0507 (0.0003 -- 0.9859)  max mem: 16413
[2023-09-04 02:48:47,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:48:47,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:48:47,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 02:48:47,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [71]  [ 60/160]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000009  loss: 1.8501 (1.7018)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9212 (7.2120)  time: 0.9236 (0.5284 -- 4.9822)  data: 0.0025 (0.0006 -- 0.0106)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:15  lr: 0.000035  min_lr: 0.000009  loss: 1.4834 (1.6718)  loss_scale: 16384.0000 (10720.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1942 (6.9998)  time: 0.7214 (0.5260 -- 1.8951)  data: 0.1017 (0.0006 -- 0.7532)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:54  lr: 0.000035  min_lr: 0.000009  loss: 1.8043 (1.6797)  loss_scale: 16384.0000 (11841.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3448 (6.9520)  time: 0.7962 (0.5318 -- 2.6126)  data: 0.2116 (0.0006 -- 2.0654)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.7916 (1.7005)  loss_scale: 16384.0000 (12592.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0562 (6.9237)  time: 0.9888 (0.5307 -- 3.4714)  data: 0.3355 (0.0003 -- 2.9436)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.9651 (1.7294)  loss_scale: 16384.0000 (13130.4397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1047 (7.0372)  time: 0.8597 (0.5076 -- 2.7839)  data: 0.2468 (0.0003 -- 2.2653)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.8125 (1.7459)  loss_scale: 16384.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2957 (6.9876)  time: 0.7261 (0.4950 -- 2.0001)  data: 0.2043 (0.0002 -- 1.4807)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.8125 (1.7200)  loss_scale: 16384.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2957 (6.9876)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1958 (0.1958)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2892 (2.2892 -- 2.2892)  data: 2.0720 (2.0720 -- 2.0720)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4485 (0.6907)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4119 (0.1976 -- 2.2892)  data: 0.2011 (0.0003 -- 2.0720)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4611 (0.6280)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2181 (0.1718 -- 0.3566)  data: 0.0122 (0.0001 -- 0.1325)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5273 (0.6657)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2043 (0.1337 -- 0.3566)  data: 0.0120 (0.0001 -- 0.1325)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 82.158 Acc@5 96.680 loss 0.659
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 84.65%
Epoch: [72]  [  0/160]  eta: 0:19:17  lr: 0.000035  min_lr: 0.000009  loss: 1.3990 (1.3990)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4045 (4.4045)  time: 7.2371 (7.2371 -- 7.2371)  data: 5.7343 (5.7343 -- 5.7343)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:40  lr: 0.000034  min_lr: 0.000009  loss: 1.7008 (1.6997)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2644 (6.6842)  time: 0.8405 (0.5384 -- 3.1163)  data: 0.2889 (0.0007 -- 2.5923)  max mem: 16413
[2023-09-04 02:50:45,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:50:45,298] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:50:45,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:50:45,298] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:50:50,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11550
[2023-09-04 02:50:50,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11550
[2023-09-04 02:50:50,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:50:50,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:50:50,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000009  loss: 1.7628 (1.7187)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5193 (6.7542)  time: 0.8526 (0.5259 -- 2.6071)  data: 0.2317 (0.0004 -- 2.0779)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:40  lr: 0.000034  min_lr: 0.000009  loss: 1.5557 (1.6815)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5658 (6.9900)  time: 1.0071 (0.5304 -- 3.5223)  data: 0.4116 (0.0004 -- 3.0069)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000034  min_lr: 0.000009  loss: 1.7555 (1.6933)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4107 (6.8714)  time: 0.8272 (0.5181 -- 2.9545)  data: 0.1221 (0.0002 -- 1.2565)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000009  loss: 1.5800 (1.6915)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3068 (6.7765)  time: 0.9279 (0.5039 -- 4.6535)  data: 0.3943 (0.0003 -- 4.1609)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.7493 (1.6867)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1964 (6.7076)  time: 0.8802 (0.5093 -- 4.7367)  data: 0.3397 (0.0004 -- 4.1882)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.8554 (1.6991)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7724 (6.8354)  time: 0.9153 (0.5348 -- 4.6691)  data: 0.3703 (0.0005 -- 4.1587)  max mem: 16413
[2023-09-04 02:52:41,388] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:52:41,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:52:41,393] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:52:41,393] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6692 (1.6968)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3799 (6.9066)  time: 0.6128 (0.4943 -- 2.3190)  data: 0.0944 (0.0001 -- 1.8276)  max mem: 16413
Epoch: [72] Total time: 0:02:24 (0.9001 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.6692 (1.7175)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3799 (6.9066)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2134 (0.2134)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3123 (2.3123 -- 2.3123)  data: 2.0988 (2.0988 -- 2.0988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4235 (0.6506)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4338 (0.1965 -- 2.3123)  data: 0.2141 (0.0009 -- 2.0988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5201 (0.6172)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2292 (0.1713 -- 0.4954)  data: 0.0200 (0.0001 -- 0.2390)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6340 (0.6699)  acc1: 85.7143 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2112 (0.1327 -- 0.4954)  data: 0.0193 (0.0001 -- 0.2390)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 81.328 Acc@5 96.888 loss 0.660
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 84.65%
Epoch: [73]  [  0/160]  eta: 0:20:37  lr: 0.000034  min_lr: 0.000009  loss: 2.1483 (2.1483)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2274 (4.2274)  time: 7.7373 (7.7373 -- 7.7373)  data: 6.0782 (6.0782 -- 6.0782)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:48  lr: 0.000034  min_lr: 0.000009  loss: 1.6733 (1.7540)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7432 (6.4993)  time: 0.8785 (0.5413 -- 3.8563)  data: 0.0269 (0.0003 -- 0.5119)  max mem: 16413
Epoch: [73]  [ 40/160]  eta: 0:02:11  lr: 0.000034  min_lr: 0.000009  loss: 1.5823 (1.6816)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3045 (6.5774)  time: 0.9860 (0.5296 -- 4.4338)  data: 0.0013 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000009  loss: 1.7270 (1.7057)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1572 (6.7282)  time: 0.8388 (0.5156 -- 4.7129)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000009  loss: 1.6979 (1.6996)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8238 (6.9185)  time: 0.8477 (0.5080 -- 4.1633)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.8178 (1.7076)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0932 (6.8374)  time: 0.7923 (0.5223 -- 3.0802)  data: 0.0016 (0.0007 -- 0.0061)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.6969 (1.7002)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9868 (6.7728)  time: 0.8664 (0.5232 -- 3.2660)  data: 0.0143 (0.0004 -- 0.2461)  max mem: 16413
[2023-09-04 02:54:45,859] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:54:45,860] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:54:45,861] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:54:45,861] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 02:54:52,541] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11811
[2023-09-04 02:54:52,541] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 02:54:52,541] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11811
[2023-09-04 02:54:52,541] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 02:54:52,541] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7194 (1.7027)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3911 (6.7981)  time: 0.9783 (0.5079 -- 5.0878)  data: 0.0768 (0.0002 -- 1.5161)  max mem: 16413
[2023-09-04 02:55:09,480] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11833
[2023-09-04 02:55:09,480] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11833
[2023-09-04 02:55:09,480] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:55:09,480] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:55:09,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6585 (1.7016)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3910 (6.7814)  time: 0.6085 (0.4831 -- 2.3435)  data: 0.0005 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [73] Total time: 0:02:23 (0.8947 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.6585 (1.7364)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3910 (6.7814)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2163 (0.2163)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2779 (2.2779 -- 2.2779)  data: 2.0520 (2.0520 -- 2.0520)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3769 (0.6173)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (96.9697)  time: 0.4180 (0.1971 -- 2.2779)  data: 0.2026 (0.0007 -- 2.0520)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5459 (0.6044)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2238 (0.1711 -- 0.4257)  data: 0.0193 (0.0001 -- 0.1657)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5638 (0.6522)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2090 (0.1358 -- 0.4257)  data: 0.0190 (0.0001 -- 0.1657)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 85.062 Acc@5 97.303 loss 0.617
Accuracy of the network on the 482 val images: 85.06%
[2023-09-04 02:55:20,201] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 02:55:20,203] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 02:55:20,203] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 02:55:20,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 02:55:21,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 02:55:21,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.06%
Epoch: [74]  [  0/160]  eta: 0:20:38  lr: 0.000034  min_lr: 0.000009  loss: 1.5827 (1.5827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7027 (5.7027)  time: 7.7376 (7.7376 -- 7.7376)  data: 7.1850 (7.1850 -- 7.1850)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:45  lr: 0.000034  min_lr: 0.000009  loss: 1.6506 (1.6894)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1235 (7.3392)  time: 0.8521 (0.5188 -- 2.7288)  data: 0.3136 (0.0004 -- 2.2150)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6465 (1.6579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7671 (7.2623)  time: 0.8219 (0.5279 -- 3.3340)  data: 0.2689 (0.0004 -- 2.7789)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000009  loss: 1.7965 (1.7063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5032 (7.1803)  time: 0.9252 (0.5310 -- 3.6306)  data: 0.2032 (0.0004 -- 3.1206)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000009  loss: 1.8462 (1.7228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4141 (7.0436)  time: 0.8232 (0.5219 -- 1.8594)  data: 0.1190 (0.0007 -- 0.8879)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 1.7733 (1.7340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5507 (7.1581)  time: 0.8493 (0.5438 -- 2.2241)  data: 0.0813 (0.0007 -- 0.9895)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000009  loss: 1.8571 (1.7451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0794 (6.9913)  time: 0.8434 (0.5300 -- 2.5685)  data: 0.2113 (0.0003 -- 2.0333)  max mem: 16413
[2023-09-04 02:57:14,411] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:57:14,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 02:57:14,413] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 02:57:14,413] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7806 (1.7561)  loss_scale: 32768.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6850 (6.9329)  time: 0.8809 (0.5425 -- 2.8031)  data: 0.2781 (0.0009 -- 2.2699)  max mem: 16413
[2023-09-04 02:57:41,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=66, lr=[8.515378090801247e-06, 8.515378090801247e-06, 9.461531212001385e-06, 9.461531212001385e-06, 1.0512812457779315e-05, 1.0512812457779315e-05, 1.1680902730865906e-05, 1.1680902730865906e-05, 1.297878081207323e-05, 1.297878081207323e-05, 1.4420867568970255e-05, 1.4420867568970255e-05, 1.6023186187744725e-05, 1.6023186187744725e-05, 1.780354020860525e-05, 1.780354020860525e-05, 1.9781711342894723e-05, 1.9781711342894723e-05, 2.1979679269883025e-05, 2.1979679269883025e-05, 2.4421865855425585e-05, 2.4421865855425585e-05, 2.7135406506028427e-05, 2.7135406506028427e-05, 3.0150451673364916e-05, 3.0150451673364916e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 02:57:41,888] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=18.06456481529588, CurrSamplesPerSec=24.49452699322371, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6141 (1.7486)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8309 (6.9398)  time: 0.6497 (0.4977 -- 1.8830)  data: 0.1086 (0.0003 -- 1.3082)  max mem: 16413
Epoch: [74] Total time: 0:02:20 (0.8758 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.6141 (1.7527)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8309 (6.9398)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2638 (0.2638)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5520 (2.5520 -- 2.5520)  data: 2.3315 (2.3315 -- 2.3315)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4669 (0.6599)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4284 (0.2049 -- 2.5520)  data: 0.2130 (0.0004 -- 2.3315)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4936 (0.6049)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2119 (0.1684 -- 0.3149)  data: 0.0065 (0.0001 -- 0.1160)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5047 (0.6501)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.1948 (0.1326 -- 0.3149)  data: 0.0061 (0.0001 -- 0.1160)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.613
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.06%
Epoch: [75]  [  0/160]  eta: 0:22:48  lr: 0.000033  min_lr: 0.000009  loss: 1.8171 (1.8171)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5140 (6.5140)  time: 8.5522 (8.5522 -- 8.5522)  data: 5.1255 (5.1255 -- 5.1255)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:55  lr: 0.000033  min_lr: 0.000009  loss: 1.6869 (1.6615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1725 (8.2555)  time: 0.8872 (0.5303 -- 3.1453)  data: 0.0685 (0.0004 -- 1.3270)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000008  loss: 1.8509 (1.7322)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3627 (7.6335)  time: 0.8618 (0.5183 -- 3.6009)  data: 0.1664 (0.0004 -- 1.7575)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000008  loss: 1.7726 (1.7282)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0649 (7.5161)  time: 0.9312 (0.5285 -- 3.4865)  data: 0.0991 (0.0004 -- 1.4608)  max mem: 16413
[2023-09-04 02:58:52,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12062
[2023-09-04 02:58:52,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12062
[2023-09-04 02:58:52,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:58:52,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 02:58:52,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [75]  [ 80/160]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000008  loss: 1.7175 (1.7214)  loss_scale: 16384.0000 (28924.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2581 (7.2272)  time: 0.9263 (0.5198 -- 3.9999)  data: 0.0185 (0.0004 -- 0.3420)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000008  loss: 1.6486 (1.6879)  loss_scale: 16384.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9413 (7.1802)  time: 0.8240 (0.5140 -- 3.0232)  data: 0.0508 (0.0002 -- 0.9962)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6567 (1.6856)  loss_scale: 16384.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7436 (7.2373)  time: 0.7896 (0.5375 -- 3.0787)  data: 0.0017 (0.0005 -- 0.0028)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7407 (1.6906)  loss_scale: 16384.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4045 (7.3629)  time: 0.8879 (0.5288 -- 4.8763)  data: 0.0020 (0.0005 -- 0.0041)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.6582 (1.6895)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9087 (7.3150)  time: 0.7179 (0.4940 -- 3.0557)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [75] Total time: 0:02:24 (0.9034 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.6582 (1.6990)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9087 (7.3150)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2146 (0.2146)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3101 (2.3101 -- 2.3101)  data: 2.1099 (2.1099 -- 2.1099)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4587 (0.6470)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4107 (0.2015 -- 2.3101)  data: 0.1985 (0.0006 -- 2.1099)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5199 (0.6230)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2203 (0.1703 -- 0.3816)  data: 0.0133 (0.0001 -- 0.1894)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5451 (0.6686)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2050 (0.1331 -- 0.3816)  data: 0.0131 (0.0001 -- 0.1894)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.618
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.06%
Epoch: [76]  [  0/160]  eta: 0:18:03  lr: 0.000033  min_lr: 0.000008  loss: 1.8408 (1.8408)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9440 (13.9440)  time: 6.7710 (6.7710 -- 6.7710)  data: 4.2478 (4.2478 -- 4.2478)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:58  lr: 0.000033  min_lr: 0.000008  loss: 1.8490 (1.7762)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0989 (7.8056)  time: 0.9969 (0.5238 -- 4.4527)  data: 0.4503 (0.0005 -- 3.9434)  max mem: 16413
[2023-09-04 03:00:57,393] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:00:57,393] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:00:57,393] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:00:57,393] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [76]  [ 40/160]  eta: 0:02:02  lr: 0.000033  min_lr: 0.000008  loss: 1.3859 (1.6326)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4616 (7.6594)  time: 0.7621 (0.5251 -- 3.0353)  data: 0.2146 (0.0004 -- 2.4910)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000008  loss: 1.6641 (1.6719)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8395 (7.4270)  time: 0.9141 (0.5224 -- 4.2668)  data: 0.3658 (0.0005 -- 3.7270)  max mem: 16413
[2023-09-04 03:01:28,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12229
[2023-09-04 03:01:28,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12229
[2023-09-04 03:01:28,878] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:01:28,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:01:28,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000008  loss: 1.8754 (1.7134)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5031 (7.2514)  time: 0.8119 (0.5262 -- 2.7445)  data: 0.2689 (0.0006 -- 2.1980)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000008  loss: 1.8040 (1.7286)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1992 (7.2058)  time: 0.8621 (0.5299 -- 2.4168)  data: 0.2273 (0.0002 -- 1.9030)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.8165 (1.7358)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3081 (7.2364)  time: 1.0270 (0.5287 -- 4.5306)  data: 0.0057 (0.0003 -- 0.0755)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7941 (1.7378)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0695 (7.2245)  time: 0.7507 (0.5186 -- 2.9866)  data: 0.0370 (0.0002 -- 0.7087)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.5863 (1.7198)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0786 (7.1493)  time: 0.6415 (0.4959 -- 1.8320)  data: 0.0518 (0.0002 -- 0.9706)  max mem: 16413
Epoch: [76] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.5863 (1.7249)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0786 (7.1493)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2325 (0.2325)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3167 (2.3167 -- 2.3167)  data: 2.1118 (2.1118 -- 2.1118)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4273 (0.6518)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4105 (0.1977 -- 2.3167)  data: 0.1943 (0.0011 -- 2.1118)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4345 (0.6064)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2251 (0.1709 -- 0.4128)  data: 0.0179 (0.0001 -- 0.2004)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5826 (0.6354)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.2072 (0.1331 -- 0.4128)  data: 0.0169 (0.0001 -- 0.2004)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 84.855 Acc@5 96.888 loss 0.624
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.06%
Epoch: [77]  [  0/160]  eta: 0:21:30  lr: 0.000033  min_lr: 0.000008  loss: 1.9265 (1.9265)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2905 (7.2905)  time: 8.0643 (8.0643 -- 8.0643)  data: 6.4367 (6.4367 -- 6.4367)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:48  lr: 0.000033  min_lr: 0.000008  loss: 1.7099 (1.7643)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8686 (6.8297)  time: 0.8617 (0.5246 -- 4.2198)  data: 0.2625 (0.0005 -- 2.6692)  max mem: 16413
[2023-09-04 03:03:31,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:03:31,391] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:03:31,395] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:03:31,396] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [ 40/160]  eta: 0:02:04  lr: 0.000033  min_lr: 0.000008  loss: 1.7013 (1.7231)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9075 (6.8045)  time: 0.8634 (0.5219 -- 3.1950)  data: 0.2049 (0.0003 -- 1.8746)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6556 (1.6756)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2033 (6.7909)  time: 0.8509 (0.5356 -- 2.7850)  data: 0.1110 (0.0005 -- 1.4399)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000008  loss: 1.4702 (1.6680)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4028 (6.9430)  time: 0.9016 (0.5293 -- 5.3268)  data: 0.0646 (0.0005 -- 0.7303)  max mem: 16413
[2023-09-04 03:04:10,860] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12403
[2023-09-04 03:04:10,860] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:04:10,860] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12403
[2023-09-04 03:04:10,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 03:04:10,860] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [77]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000008  loss: 1.5813 (1.6697)  loss_scale: 16384.0000 (23683.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5925 (7.1350)  time: 0.8347 (0.5200 -- 3.8068)  data: 0.0018 (0.0006 -- 0.0060)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.6000 (1.6645)  loss_scale: 16384.0000 (22477.2231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0886 (7.1832)  time: 0.8256 (0.5277 -- 1.9018)  data: 0.0109 (0.0004 -- 0.1829)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000033  min_lr: 0.000008  loss: 1.7039 (1.6639)  loss_scale: 16384.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4015 (7.2040)  time: 0.7912 (0.5254 -- 4.2261)  data: 0.0014 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.8089 (1.6861)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8506 (7.1799)  time: 0.7256 (0.4952 -- 2.5450)  data: 0.0012 (0.0002 -- 0.0101)  max mem: 16413
Epoch: [77] Total time: 0:02:20 (0.8792 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.8089 (1.7220)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8506 (7.1799)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2333 (0.2333)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3525 (2.3525 -- 2.3525)  data: 2.1291 (2.1291 -- 2.1291)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4517 (0.6148)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4226 (0.1980 -- 2.3525)  data: 0.2143 (0.0008 -- 2.1291)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4750 (0.5938)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.3545)  time: 0.2227 (0.1696 -- 0.4268)  data: 0.0183 (0.0001 -- 0.2188)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5861 (0.6310)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (96.6805)  time: 0.2091 (0.1333 -- 0.4268)  data: 0.0180 (0.0001 -- 0.2188)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 85.685 Acc@5 97.303 loss 0.609
Accuracy of the network on the 482 val images: 85.68%
[2023-09-04 03:05:19,986] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 03:05:19,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 03:05:19,988] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 03:05:19,988] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 03:05:21,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 03:05:21,514] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.68%
Epoch: [78]  [  0/160]  eta: 0:22:05  lr: 0.000032  min_lr: 0.000008  loss: 1.4432 (1.4432)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7250 (3.7250)  time: 8.2868 (8.2868 -- 8.2868)  data: 7.7458 (7.7458 -- 7.7458)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:35  lr: 0.000032  min_lr: 0.000008  loss: 1.6116 (1.6377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4219 (7.0274)  time: 0.7552 (0.5157 -- 2.2784)  data: 0.1433 (0.0006 -- 1.6601)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000008  loss: 1.8655 (1.7353)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9366 (7.2067)  time: 0.9068 (0.5308 -- 2.9236)  data: 0.3247 (0.0005 -- 2.3982)  max mem: 16413
[2023-09-04 03:06:13,640] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:06:13,640] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:06:13,641] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:06:13,641] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [78]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000008  loss: 1.7340 (1.7382)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9806 (7.2413)  time: 0.8763 (0.5256 -- 2.9918)  data: 0.2465 (0.0009 -- 2.4602)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000008  loss: 1.9031 (1.7537)  loss_scale: 32768.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5611 (7.1002)  time: 0.8451 (0.5255 -- 3.7349)  data: 0.2990 (0.0004 -- 3.1957)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.7410 (1.7517)  loss_scale: 32768.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1183 (7.2581)  time: 0.9104 (0.5247 -- 3.5821)  data: 0.3683 (0.0002 -- 3.0465)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.7561 (1.7510)  loss_scale: 32768.0000 (25726.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5995 (7.1905)  time: 0.9124 (0.5276 -- 4.5193)  data: 0.3607 (0.0003 -- 3.9992)  max mem: 16413
[2023-09-04 03:07:17,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12601
[2023-09-04 03:07:17,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:07:17,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12601
[2023-09-04 03:07:17,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 03:07:17,707] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.7105 (1.7325)  loss_scale: 16384.0000 (24401.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4603 (7.2686)  time: 0.9124 (0.5094 -- 4.3490)  data: 0.3745 (0.0003 -- 3.8490)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5705 (1.7263)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5891 (7.2272)  time: 0.7039 (0.4935 -- 3.5744)  data: 0.1840 (0.0002 -- 3.0399)  max mem: 16413
Epoch: [78] Total time: 0:02:24 (0.9011 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5705 (1.7320)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5891 (7.2272)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2550 (0.2550)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3335 (2.3335 -- 2.3335)  data: 2.1269 (2.1269 -- 2.1269)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4432 (0.5901)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4121 (0.2073 -- 2.3335)  data: 0.1946 (0.0008 -- 2.1269)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4766 (0.5670)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2231 (0.1697 -- 0.5028)  data: 0.0148 (0.0001 -- 0.2803)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5050 (0.6001)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2059 (0.1326 -- 0.5028)  data: 0.0145 (0.0001 -- 0.2803)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.588
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.68%
Epoch: [79]  [  0/160]  eta: 0:25:41  lr: 0.000032  min_lr: 0.000008  loss: 1.9250 (1.9250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3962 (5.3962)  time: 9.6339 (9.6339 -- 9.6339)  data: 9.0858 (9.0858 -- 9.0858)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:41  lr: 0.000032  min_lr: 0.000008  loss: 1.6735 (1.7696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5409 (6.9053)  time: 0.7332 (0.5267 -- 2.5701)  data: 0.1831 (0.0008 -- 2.0347)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:03  lr: 0.000032  min_lr: 0.000008  loss: 1.8059 (1.8050)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8911 (7.2755)  time: 0.8968 (0.5304 -- 3.6161)  data: 0.1753 (0.0003 -- 1.4888)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:35  lr: 0.000032  min_lr: 0.000008  loss: 1.8916 (1.8138)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0072 (7.2872)  time: 0.8116 (0.5340 -- 2.6395)  data: 0.0198 (0.0005 -- 0.3680)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:13  lr: 0.000032  min_lr: 0.000008  loss: 1.7152 (1.7883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4733 (7.2027)  time: 0.7996 (0.5229 -- 1.8344)  data: 0.1153 (0.0003 -- 1.0490)  max mem: 16413
[2023-09-04 03:09:17,967] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:09:17,967] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:09:17,971] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:09:17,971] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [79]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.7801 (1.7849)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7466 (7.1782)  time: 0.9744 (0.5383 -- 2.8752)  data: 0.0286 (0.0002 -- 0.4192)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.4676 (1.7386)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8006 (7.1218)  time: 0.9056 (0.5268 -- 2.9764)  data: 0.0015 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.7083 (1.7294)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1755 (7.1166)  time: 0.8144 (0.5271 -- 2.1994)  data: 0.0591 (0.0001 -- 1.1537)  max mem: 16413
[2023-09-04 03:10:13,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12795
[2023-09-04 03:10:13,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12795
[2023-09-04 03:10:13,016] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:10:13,016] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:10:13,016] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5754 (1.7105)  loss_scale: 32768.0000 (23040.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5774 (7.0495)  time: 0.6828 (0.4839 -- 1.7411)  data: 0.0172 (0.0002 -- 0.3340)  max mem: 16413
Epoch: [79] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5754 (1.7286)  loss_scale: 32768.0000 (23040.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5774 (7.0495)
[2023-09-04 03:10:15,013] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-04 03:10:15,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-04 03:10:15,018] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-04 03:10:15,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-04 03:10:16,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-04 03:10:16,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1783 (0.1783)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3570 (2.3570 -- 2.3570)  data: 2.1372 (2.1372 -- 2.1372)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4384 (0.6447)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4325 (0.2127 -- 2.3570)  data: 0.2077 (0.0011 -- 2.1372)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4384 (0.5859)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2279 (0.1709 -- 0.4206)  data: 0.0191 (0.0001 -- 0.2302)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4916 (0.6200)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2088 (0.1328 -- 0.4206)  data: 0.0186 (0.0001 -- 0.2302)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 84.025 Acc@5 97.095 loss 0.619
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [80]  [  0/160]  eta: 0:21:47  lr: 0.000032  min_lr: 0.000008  loss: 1.9892 (1.9892)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8160 (7.8160)  time: 8.1735 (8.1735 -- 8.1735)  data: 6.1982 (6.1982 -- 6.1982)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:44  lr: 0.000032  min_lr: 0.000008  loss: 1.7052 (1.7504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0234 (7.1156)  time: 0.8218 (0.5140 -- 3.9222)  data: 0.0840 (0.0007 -- 1.4105)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:00  lr: 0.000032  min_lr: 0.000008  loss: 1.6659 (1.7254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4168 (7.4591)  time: 0.8238 (0.5311 -- 2.3943)  data: 0.1743 (0.0005 -- 1.2916)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:34  lr: 0.000032  min_lr: 0.000008  loss: 1.7119 (1.7323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0716 (7.4444)  time: 0.8363 (0.5196 -- 2.5902)  data: 0.1278 (0.0003 -- 1.6024)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:18  lr: 0.000032  min_lr: 0.000008  loss: 1.7787 (1.7469)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6444 (7.2780)  time: 1.0618 (0.5178 -- 4.4296)  data: 0.2874 (0.0002 -- 3.9155)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000008  loss: 1.8360 (1.7510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3119 (7.1550)  time: 0.7819 (0.5148 -- 2.7481)  data: 0.2450 (0.0002 -- 2.2324)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.7175 (1.7623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0951 (7.2429)  time: 0.7893 (0.5244 -- 3.5042)  data: 0.2439 (0.0004 -- 2.9865)  max mem: 16413
[2023-09-04 03:12:16,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:12:16,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:12:16,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:12:16,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.4335 (1.7330)  loss_scale: 32768.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8574 (7.2237)  time: 0.8381 (0.5321 -- 2.3552)  data: 0.2831 (0.0009 -- 1.8325)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.8467 (1.7430)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7277 (7.2152)  time: 0.7746 (0.4955 -- 2.7780)  data: 0.2496 (0.0001 -- 2.2560)  max mem: 16413
Epoch: [80] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.8467 (1.7373)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7277 (7.2152)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1709 (0.1709)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2855 (2.2855 -- 2.2855)  data: 2.0468 (2.0468 -- 2.0468)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4787 (0.5973)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4047 (0.2032 -- 2.2855)  data: 0.1874 (0.0006 -- 2.0468)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4787 (0.5593)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2249 (0.1696 -- 0.5662)  data: 0.0201 (0.0001 -- 0.3788)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5052 (0.5847)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2108 (0.1343 -- 0.5662)  data: 0.0198 (0.0001 -- 0.3788)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 84.232 Acc@5 97.095 loss 0.587
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [81]  [  0/160]  eta: 0:20:53  lr: 0.000031  min_lr: 0.000008  loss: 2.1527 (2.1527)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4504 (5.4504)  time: 7.8339 (7.8339 -- 7.8339)  data: 7.3063 (7.3063 -- 7.3063)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000008  loss: 1.8153 (1.7668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8379 (7.4250)  time: 0.8423 (0.5266 -- 2.6436)  data: 0.3002 (0.0004 -- 2.1159)  max mem: 16413
[2023-09-04 03:13:19,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12982
[2023-09-04 03:13:19,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12982
[2023-09-04 03:13:19,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:13:19,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:13:19,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 03:13:33,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=72, lr=[7.961979027681322e-06, 7.961979027681322e-06, 8.846643364090358e-06, 8.846643364090358e-06, 9.829603737878174e-06, 9.829603737878174e-06, 1.0921781930975751e-05, 1.0921781930975751e-05, 1.2135313256639722e-05, 1.2135313256639722e-05, 1.3483681396266359e-05, 1.3483681396266359e-05, 1.498186821807373e-05, 1.498186821807373e-05, 1.6646520242304144e-05, 1.6646520242304144e-05, 1.849613360256016e-05, 1.849613360256016e-05, 2.0551259558400174e-05, 2.0551259558400174e-05, 2.2834732842666863e-05, 2.2834732842666863e-05, 2.537192538074096e-05, 2.537192538074096e-05, 2.8191028200823287e-05, 2.8191028200823287e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 03:13:33,983] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=18.028622029138774, CurrSamplesPerSec=22.319437318491445, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000008  loss: 1.3876 (1.6814)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1073 (7.4146)  time: 0.8155 (0.5172 -- 2.8819)  data: 0.1455 (0.0004 -- 1.4203)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000008  loss: 1.7584 (1.7340)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3526 (7.1874)  time: 0.9748 (0.5241 -- 3.7622)  data: 0.4220 (0.0004 -- 3.2065)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000008  loss: 1.7418 (1.7177)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2045 (7.0410)  time: 0.9207 (0.5068 -- 4.1102)  data: 0.3783 (0.0004 -- 3.5794)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 1.7612 (1.7184)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1653 (6.8946)  time: 0.7933 (0.5245 -- 3.0269)  data: 0.2472 (0.0002 -- 2.4948)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.5651 (1.6970)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2023 (6.8862)  time: 0.7983 (0.5286 -- 2.7132)  data: 0.2370 (0.0002 -- 2.1719)  max mem: 16413
[2023-09-04 03:14:56,252] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13093
[2023-09-04 03:14:56,252] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13093
[2023-09-04 03:14:56,252] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:14:56,252] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:14:56,252] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.7250 (1.6995)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9944 (6.9963)  time: 0.8990 (0.5195 -- 2.6927)  data: 0.3482 (0.0005 -- 2.1587)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5712 (1.6842)  loss_scale: 8192.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6533 (6.9562)  time: 0.7134 (0.4950 -- 2.6933)  data: 0.1943 (0.0002 -- 2.1822)  max mem: 16413
Epoch: [81] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5712 (1.6841)  loss_scale: 8192.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6533 (6.9562)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2008 (0.2008)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6254 (2.6254 -- 2.6254)  data: 2.3901 (2.3901 -- 2.3901)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4765 (0.6249)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4391 (0.1928 -- 2.6254)  data: 0.2184 (0.0006 -- 2.3901)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4765 (0.5855)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.3545)  time: 0.2133 (0.1694 -- 0.2577)  data: 0.0008 (0.0001 -- 0.0017)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5175 (0.6344)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.1977 (0.1328 -- 0.2577)  data: 0.0005 (0.0001 -- 0.0016)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.624
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 85.68%
Epoch: [82]  [  0/160]  eta: 0:20:43  lr: 0.000031  min_lr: 0.000008  loss: 2.0027 (2.0027)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2978 (6.2978)  time: 7.7737 (7.7737 -- 7.7737)  data: 5.7908 (5.7908 -- 5.7908)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:42  lr: 0.000031  min_lr: 0.000008  loss: 1.8070 (1.7636)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6674 (7.3243)  time: 0.8281 (0.5308 -- 2.1825)  data: 0.1816 (0.0007 -- 1.6512)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:12  lr: 0.000031  min_lr: 0.000008  loss: 1.7540 (1.7402)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6763 (7.3491)  time: 1.0389 (0.5192 -- 4.5963)  data: 0.4880 (0.0003 -- 4.0529)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000008  loss: 1.6504 (1.7218)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5063 (7.4172)  time: 0.7751 (0.5163 -- 3.0611)  data: 0.2281 (0.0003 -- 2.5410)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000008  loss: 1.8303 (1.7496)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6261 (7.6631)  time: 0.7515 (0.5284 -- 4.0172)  data: 0.2012 (0.0004 -- 3.4661)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:54  lr: 0.000031  min_lr: 0.000008  loss: 1.9955 (1.7671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1865 (7.3979)  time: 0.8101 (0.5304 -- 1.8634)  data: 0.1367 (0.0003 -- 0.8551)  max mem: 16413
[2023-09-04 03:16:56,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:16:56,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 03:16:56,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:16:56,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.6662 (1.7469)  loss_scale: 16384.0000 (9478.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1633 (7.2620)  time: 0.9137 (0.5239 -- 2.3470)  data: 0.2318 (0.0005 -- 1.8087)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.5189 (1.7253)  loss_scale: 16384.0000 (10457.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3384 (7.2092)  time: 0.8427 (0.5269 -- 2.1831)  data: 0.2845 (0.0001 -- 1.6582)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5963 (1.7147)  loss_scale: 16384.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5759 (7.1337)  time: 0.7134 (0.4935 -- 2.4035)  data: 0.1252 (0.0001 -- 1.8537)  max mem: 16413
Epoch: [82] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5963 (1.7195)  loss_scale: 16384.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5759 (7.1337)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.2591 (0.2591)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6743 (2.6743 -- 2.6743)  data: 2.4364 (2.4364 -- 2.4364)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4598 (0.6394)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4525 (0.2078 -- 2.6743)  data: 0.2234 (0.0011 -- 2.4364)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4763 (0.5997)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2134 (0.1681 -- 0.2668)  data: 0.0012 (0.0001 -- 0.0075)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4777 (0.6299)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.1899 (0.1330 -- 0.2450)  data: 0.0005 (0.0001 -- 0.0022)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.626
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 85.68%
Epoch: [83]  [  0/160]  eta: 0:21:30  lr: 0.000031  min_lr: 0.000008  loss: 1.6958 (1.6958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2673 (5.2673)  time: 8.0653 (8.0653 -- 8.0653)  data: 7.1340 (7.1340 -- 7.1340)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:35  lr: 0.000031  min_lr: 0.000008  loss: 1.6107 (1.7005)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0098 (6.9561)  time: 0.7661 (0.5331 -- 3.4726)  data: 0.1677 (0.0003 -- 2.0541)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000008  loss: 1.4971 (1.6190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8817 (6.8789)  time: 0.8724 (0.5185 -- 3.2999)  data: 0.1272 (0.0001 -- 1.4277)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000008  loss: 1.7748 (1.6684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5061 (6.8943)  time: 0.9329 (0.5265 -- 2.6959)  data: 0.1215 (0.0006 -- 1.8276)  max mem: 16413
[2023-09-04 03:18:59,178] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:18:59,178] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:18:59,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:18:59,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [83]  [ 80/160]  eta: 0:01:15  lr: 0.000031  min_lr: 0.000008  loss: 1.7764 (1.6849)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3382 (6.8242)  time: 0.8562 (0.5270 -- 2.6934)  data: 0.1857 (0.0003 -- 2.0134)  max mem: 16413
[2023-09-04 03:19:12,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13364
[2023-09-04 03:19:12,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:19:12,939] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 03:19:12,940] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13364
[2023-09-04 03:19:12,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [83]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.9388 (1.7264)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3189 (6.7710)  time: 0.9135 (0.5208 -- 3.6673)  data: 0.2791 (0.0004 -- 3.1428)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000008  loss: 1.5727 (1.7036)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7523 (6.7854)  time: 0.8725 (0.5201 -- 3.7726)  data: 0.3024 (0.0004 -- 3.2287)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.4766 (1.6823)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1433 (6.8509)  time: 0.9842 (0.5088 -- 4.3185)  data: 0.4418 (0.0003 -- 3.8047)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.8838 (1.6986)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2865 (6.9396)  time: 0.6429 (0.4934 -- 2.5395)  data: 0.1318 (0.0002 -- 2.0400)  max mem: 16413
Epoch: [83] Total time: 0:02:24 (0.9025 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.8838 (1.7226)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2865 (6.9396)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2145 (0.2145)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5339 (2.5339 -- 2.5339)  data: 2.2940 (2.2940 -- 2.2940)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3035 (0.5536)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4260 (0.1987 -- 2.5339)  data: 0.2096 (0.0009 -- 2.2940)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4395 (0.5458)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.3545)  time: 0.2140 (0.1685 -- 0.3139)  data: 0.0075 (0.0001 -- 0.1345)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5017 (0.5831)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.0954)  time: 0.1969 (0.1328 -- 0.3139)  data: 0.0072 (0.0001 -- 0.1345)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 84.647 Acc@5 97.095 loss 0.601
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [84]  [  0/160]  eta: 0:20:05  lr: 0.000030  min_lr: 0.000008  loss: 1.8515 (1.8515)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2229 (6.2229)  time: 7.5371 (7.5371 -- 7.5371)  data: 6.8704 (6.8704 -- 6.8704)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:43  lr: 0.000030  min_lr: 0.000008  loss: 1.5913 (1.5860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9184 (6.7751)  time: 0.8498 (0.5155 -- 3.6733)  data: 0.2730 (0.0001 -- 2.9690)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:02  lr: 0.000030  min_lr: 0.000008  loss: 1.7336 (1.6502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5083 (6.8226)  time: 0.8611 (0.5400 -- 3.6488)  data: 0.1470 (0.0008 -- 1.5987)  max mem: 16413
[2023-09-04 03:21:15,875] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:21:15,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:21:15,875] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:21:15,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000008  loss: 1.8260 (1.6991)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0131 (7.0064)  time: 0.8481 (0.5224 -- 2.8621)  data: 0.2788 (0.0008 -- 2.3421)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:14  lr: 0.000030  min_lr: 0.000008  loss: 1.5721 (1.6791)  loss_scale: 32768.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2445 (7.0536)  time: 0.8561 (0.5282 -- 2.7470)  data: 0.2337 (0.0003 -- 2.0943)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000008  loss: 1.6227 (1.6743)  loss_scale: 32768.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8495 (6.9162)  time: 0.9080 (0.5195 -- 4.1787)  data: 0.0264 (0.0004 -- 0.4352)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.7268 (1.6881)  loss_scale: 32768.0000 (25591.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4458 (6.8379)  time: 0.8022 (0.5180 -- 3.0031)  data: 0.0094 (0.0005 -- 0.1539)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:17  lr: 0.000030  min_lr: 0.000008  loss: 1.6738 (1.6879)  loss_scale: 32768.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1324 (6.9360)  time: 0.8302 (0.5198 -- 2.5537)  data: 0.0227 (0.0003 -- 0.2620)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.7015 (1.6789)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7259 (6.8696)  time: 0.7437 (0.4946 -- 2.5177)  data: 0.0156 (0.0001 -- 0.2919)  max mem: 16413
Epoch: [84] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.7015 (1.7149)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7259 (6.8696)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1928 (0.1928)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3374 (2.3374 -- 2.3374)  data: 2.1129 (2.1129 -- 2.1129)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3676 (0.6082)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4087 (0.2032 -- 2.3374)  data: 0.1940 (0.0007 -- 2.1129)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3676 (0.5582)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1688 -- 0.4268)  data: 0.0156 (0.0001 -- 0.2236)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4106 (0.5921)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2054 (0.1339 -- 0.4268)  data: 0.0148 (0.0001 -- 0.2236)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 83.402 Acc@5 96.888 loss 0.613
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 85.68%
Epoch: [85]  [  0/160]  eta: 0:22:20  lr: 0.000030  min_lr: 0.000008  loss: 1.3109 (1.3109)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0464 (6.0464)  time: 8.3767 (8.3767 -- 8.3767)  data: 7.0701 (7.0701 -- 7.0701)  max mem: 16413
[2023-09-04 03:23:03,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-09-04 03:23:03,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-09-04 03:23:03,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:23:03,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:23:03,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [ 20/160]  eta: 0:02:35  lr: 0.000030  min_lr: 0.000008  loss: 1.7328 (1.6712)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3633 (6.2761)  time: 0.7470 (0.5267 -- 1.8614)  data: 0.0631 (0.0004 -- 1.2347)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:01:59  lr: 0.000030  min_lr: 0.000008  loss: 1.7007 (1.6794)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0431 (6.2835)  time: 0.8809 (0.5114 -- 1.9238)  data: 0.1533 (0.0003 -- 1.0497)  max mem: 16413
[2023-09-04 03:23:39,111] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13646
[2023-09-04 03:23:39,111] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13646
[2023-09-04 03:23:39,112] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:23:39,112] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:23:39,112] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [85]  [ 60/160]  eta: 0:01:33  lr: 0.000030  min_lr: 0.000008  loss: 1.3530 (1.6262)  loss_scale: 8192.0000 (15175.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0313 (6.2375)  time: 0.7964 (0.5277 -- 1.9566)  data: 0.1063 (0.0004 -- 1.4197)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:14  lr: 0.000030  min_lr: 0.000008  loss: 1.8157 (1.6691)  loss_scale: 8192.0000 (13451.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2921 (6.2857)  time: 0.9403 (0.5288 -- 2.8348)  data: 0.0884 (0.0008 -- 0.8123)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000008  loss: 1.5849 (1.6439)  loss_scale: 8192.0000 (12409.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0054 (6.3073)  time: 0.8828 (0.5354 -- 2.4556)  data: 0.1331 (0.0004 -- 1.2742)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000008  loss: 1.6663 (1.6573)  loss_scale: 8192.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9820 (6.5577)  time: 0.9487 (0.5174 -- 2.2884)  data: 0.3319 (0.0004 -- 1.7638)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.6716 (1.6672)  loss_scale: 8192.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0331 (6.6419)  time: 0.8239 (0.5271 -- 3.2920)  data: 0.2762 (0.0004 -- 2.7687)  max mem: 16413
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.8083 (1.6840)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3602 (6.7390)  time: 0.7235 (0.4945 -- 1.7067)  data: 0.0806 (0.0002 -- 0.9410)  max mem: 16413
Epoch: [85] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.8083 (1.7176)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3602 (6.7390)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1906 (0.1906)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1729 (2.1729 -- 2.1729)  data: 1.9548 (1.9548 -- 1.9548)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3260 (0.6012)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.3946 (0.2069 -- 2.1729)  data: 0.1786 (0.0005 -- 1.9548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4785 (0.5654)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2217 (0.1706 -- 0.4137)  data: 0.0123 (0.0001 -- 0.2104)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5156 (0.6040)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2062 (0.1333 -- 0.4137)  data: 0.0121 (0.0001 -- 0.2104)  max mem: 16413
Val: Total time: 0:00:07 (0.2816 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.68%
Epoch: [86]  [  0/160]  eta: 0:17:23  lr: 0.000030  min_lr: 0.000008  loss: 1.8666 (1.8666)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1791 (6.1791)  time: 6.5231 (6.5231 -- 6.5231)  data: 5.9851 (5.9851 -- 5.9851)  max mem: 16413
[2023-09-04 03:25:43,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:25:43,189] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 03:25:43,190] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:25:43,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [86]  [ 20/160]  eta: 0:02:44  lr: 0.000030  min_lr: 0.000008  loss: 1.7582 (1.7544)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7665 (7.4775)  time: 0.9070 (0.5233 -- 2.2274)  data: 0.2079 (0.0007 -- 1.6976)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:06  lr: 0.000030  min_lr: 0.000008  loss: 1.6763 (1.6933)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9583 (7.3669)  time: 0.9325 (0.5086 -- 2.7152)  data: 0.1143 (0.0001 -- 1.5563)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000007  loss: 1.5972 (1.6853)  loss_scale: 16384.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6037 (7.5785)  time: 0.9295 (0.5066 -- 4.5107)  data: 0.0013 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000007  loss: 1.8364 (1.7228)  loss_scale: 16384.0000 (14866.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9887 (7.4676)  time: 0.7368 (0.5224 -- 2.6968)  data: 0.0376 (0.0007 -- 0.5464)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 1.6174 (1.7074)  loss_scale: 16384.0000 (15167.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9539 (7.4667)  time: 0.8490 (0.5201 -- 2.8705)  data: 0.2293 (0.0003 -- 2.3582)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000007  loss: 1.8719 (1.7335)  loss_scale: 16384.0000 (15368.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4742 (7.4788)  time: 1.0676 (0.5233 -- 4.0848)  data: 0.5257 (0.0003 -- 3.5502)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.5941 (1.7202)  loss_scale: 16384.0000 (15512.5106)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7618 (7.4169)  time: 0.8140 (0.5211 -- 3.3161)  data: 0.2709 (0.0002 -- 2.7856)  max mem: 16413
[2023-09-04 03:27:36,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:27:36,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:27:36,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:27:36,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.6945 (1.7129)  loss_scale: 32768.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5262 (7.3343)  time: 0.6637 (0.4949 -- 3.5842)  data: 0.1542 (0.0002 -- 3.0735)  max mem: 16413
Epoch: [86] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.6945 (1.6854)  loss_scale: 32768.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5262 (7.3343)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1799 (0.1799)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3156 (2.3156 -- 2.3156)  data: 2.0730 (2.0730 -- 2.0730)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5358 (0.6215)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4063 (0.2017 -- 2.3156)  data: 0.1899 (0.0010 -- 2.0730)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5057 (0.5782)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2249 (0.1692 -- 0.6248)  data: 0.0211 (0.0001 -- 0.4033)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5096 (0.6299)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (97.9253)  time: 0.2083 (0.1332 -- 0.6248)  data: 0.0208 (0.0001 -- 0.4033)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 82.573 Acc@5 97.718 loss 0.622
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 85.68%
Epoch: [87]  [  0/160]  eta: 0:20:30  lr: 0.000029  min_lr: 0.000007  loss: 1.8841 (1.8841)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9123 (7.9123)  time: 7.6899 (7.6899 -- 7.6899)  data: 7.1639 (7.1639 -- 7.1639)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:45  lr: 0.000029  min_lr: 0.000007  loss: 1.6030 (1.6492)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9396 (7.3946)  time: 0.8539 (0.5186 -- 3.8355)  data: 0.2965 (0.0002 -- 3.3188)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:02  lr: 0.000029  min_lr: 0.000007  loss: 1.7677 (1.6575)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1878 (7.1205)  time: 0.8481 (0.5255 -- 3.1283)  data: 0.3052 (0.0003 -- 2.5998)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000007  loss: 1.6996 (1.6564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9131 (7.2011)  time: 0.9023 (0.5231 -- 3.3095)  data: 0.3569 (0.0005 -- 2.7812)  max mem: 16413
[2023-09-04 03:29:10,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=76, lr=[7.38833426229976e-06, 7.38833426229976e-06, 8.20926029144418e-06, 8.20926029144418e-06, 9.121400323826864e-06, 9.121400323826864e-06, 1.0134889248696515e-05, 1.0134889248696515e-05, 1.126098805410724e-05, 1.126098805410724e-05, 1.2512208949008044e-05, 1.2512208949008044e-05, 1.3902454387786715e-05, 1.3902454387786715e-05, 1.5447171541985237e-05, 1.5447171541985237e-05, 1.7163523935539154e-05, 1.7163523935539154e-05, 1.9070582150599057e-05, 1.9070582150599057e-05, 2.1189535722887845e-05, 2.1189535722887845e-05, 2.3543928580986492e-05, 2.3543928580986492e-05, 2.6159920645540545e-05, 2.6159920645540545e-05, 2.906657849504505e-05, 2.906657849504505e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 03:29:10,427] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=18.10174687298724, CurrSamplesPerSec=22.30135935029091, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000007  loss: 2.0022 (1.7256)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7879 (7.1023)  time: 0.8089 (0.5303 -- 3.3064)  data: 0.2252 (0.0004 -- 2.7871)  max mem: 16413
[2023-09-04 03:29:27,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14019
[2023-09-04 03:29:27,865] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:29:27,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14019
[2023-09-04 03:29:27,865] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:29:27,865] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [87]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 1.6540 (1.7076)  loss_scale: 32768.0000 (32443.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8091 (7.0540)  time: 0.8801 (0.5227 -- 2.9341)  data: 0.2385 (0.0003 -- 2.4026)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 1.7491 (1.7147)  loss_scale: 16384.0000 (29789.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7463 (7.0692)  time: 0.8228 (0.5374 -- 2.4491)  data: 0.2682 (0.0005 -- 1.8908)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.7644 (1.7281)  loss_scale: 16384.0000 (27887.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8370 (7.0857)  time: 0.9004 (0.5342 -- 2.8820)  data: 0.3517 (0.0004 -- 2.3642)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.6952 (1.7307)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2321 (7.1565)  time: 0.8308 (0.4959 -- 2.8820)  data: 0.3134 (0.0002 -- 2.3642)  max mem: 16413
Epoch: [87] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.6952 (1.6983)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2321 (7.1565)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1725 (0.1725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2592 (2.2592 -- 2.2592)  data: 2.0289 (2.0289 -- 2.0289)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4275 (0.5734)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (96.9697)  time: 0.3993 (0.1960 -- 2.2592)  data: 0.1908 (0.0006 -- 2.0289)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4376 (0.5390)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.2963)  time: 0.2260 (0.1677 -- 0.5845)  data: 0.0258 (0.0001 -- 0.3587)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5035 (0.5742)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (95.8506)  time: 0.2150 (0.1326 -- 0.5845)  data: 0.0256 (0.0001 -- 0.3587)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 84.025 Acc@5 96.473 loss 0.595
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [88]  [  0/160]  eta: 0:17:10  lr: 0.000029  min_lr: 0.000007  loss: 1.9270 (1.9270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2040 (4.2040)  time: 6.4411 (6.4411 -- 6.4411)  data: 5.7515 (5.7515 -- 5.7515)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:48  lr: 0.000029  min_lr: 0.000007  loss: 1.8203 (1.7808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9688 (7.1015)  time: 0.9386 (0.5364 -- 3.2414)  data: 0.3839 (0.0005 -- 2.6930)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000007  loss: 1.5042 (1.6598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4183 (6.8422)  time: 0.7620 (0.5314 -- 2.4727)  data: 0.2139 (0.0003 -- 1.9239)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:36  lr: 0.000029  min_lr: 0.000007  loss: 1.7737 (1.6826)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2720 (6.8872)  time: 0.9337 (0.5365 -- 3.4185)  data: 0.3821 (0.0006 -- 2.9110)  max mem: 16413
[2023-09-04 03:31:31,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:31:31,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:31:31,006] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:31:31,006] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [88]  [ 80/160]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000007  loss: 1.7133 (1.6821)  loss_scale: 32768.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0372 (6.6745)  time: 0.7798 (0.5342 -- 2.7414)  data: 0.2175 (0.0007 -- 2.2117)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:54  lr: 0.000029  min_lr: 0.000007  loss: 1.6783 (1.6876)  loss_scale: 32768.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3115 (6.8666)  time: 0.8650 (0.5356 -- 3.5755)  data: 0.3099 (0.0008 -- 3.0129)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 1.7711 (1.6991)  loss_scale: 32768.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5408 (6.9076)  time: 0.9904 (0.5186 -- 3.5495)  data: 0.4401 (0.0004 -- 3.0207)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.5512 (1.6827)  loss_scale: 32768.0000 (24866.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6175 (6.9284)  time: 0.8632 (0.5273 -- 3.9299)  data: 0.3199 (0.0004 -- 3.4220)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.7248 (1.6823)  loss_scale: 32768.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0135 (6.9462)  time: 0.7125 (0.4948 -- 2.7701)  data: 0.1980 (0.0002 -- 2.2345)  max mem: 16413
Epoch: [88] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.7248 (1.6866)  loss_scale: 32768.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0135 (6.9462)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1535 (0.1535)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2842 (2.2842 -- 2.2842)  data: 2.0770 (2.0770 -- 2.0770)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4371 (0.6145)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4230 (0.1967 -- 2.2842)  data: 0.2045 (0.0009 -- 2.0770)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4532 (0.5573)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2312 (0.1684 -- 0.5052)  data: 0.0250 (0.0001 -- 0.3247)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4932 (0.5914)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.2149 (0.1325 -- 0.5052)  data: 0.0247 (0.0001 -- 0.3247)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 84.232 Acc@5 96.888 loss 0.605
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [89]  [  0/160]  eta: 0:21:18  lr: 0.000029  min_lr: 0.000007  loss: 1.8589 (1.8589)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2580 (9.2580)  time: 7.9926 (7.9926 -- 7.9926)  data: 7.4540 (7.4540 -- 7.4540)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:45  lr: 0.000028  min_lr: 0.000007  loss: 1.6390 (1.7017)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2788 (6.4169)  time: 0.8406 (0.5166 -- 3.3634)  data: 0.2342 (0.0005 -- 2.7995)  max mem: 16413
[2023-09-04 03:33:33,549] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:33:33,549] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 03:33:33,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:33:33,554] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 03:33:35,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14279
[2023-09-04 03:33:35,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14279
[2023-09-04 03:33:35,242] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:33:35,242] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:33:35,242] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [89]  [ 40/160]  eta: 0:02:04  lr: 0.000028  min_lr: 0.000007  loss: 1.7406 (1.7128)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1140 (6.6896)  time: 0.8795 (0.5154 -- 3.1314)  data: 0.3092 (0.0002 -- 2.5859)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000007  loss: 1.6804 (1.7154)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0522 (6.5880)  time: 0.8131 (0.5161 -- 2.5316)  data: 0.1890 (0.0003 -- 1.9895)  max mem: 16413
[2023-09-04 03:34:05,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14313
[2023-09-04 03:34:05,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14313
[2023-09-04 03:34:05,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:34:05,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:34:05,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [ 80/160]  eta: 0:01:14  lr: 0.000028  min_lr: 0.000007  loss: 1.7005 (1.7119)  loss_scale: 32768.0000 (32363.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3906 (6.6861)  time: 0.8434 (0.5235 -- 2.1933)  data: 0.2961 (0.0009 -- 1.6558)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000007  loss: 1.6155 (1.7040)  loss_scale: 16384.0000 (29199.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8022 (6.6903)  time: 0.8936 (0.5229 -- 4.3098)  data: 0.3508 (0.0004 -- 3.7986)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000007  loss: 1.5768 (1.6768)  loss_scale: 16384.0000 (27080.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6464 (6.7563)  time: 0.8361 (0.5327 -- 2.2897)  data: 0.1989 (0.0003 -- 1.7652)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.9056 (1.7098)  loss_scale: 16384.0000 (25563.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2754 (6.7319)  time: 1.0283 (0.5209 -- 3.5292)  data: 0.4612 (0.0005 -- 3.0084)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.7592 (1.7080)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1228 (6.8103)  time: 0.5326 (0.4945 -- 0.6893)  data: 0.0100 (0.0002 -- 0.1850)  max mem: 16413
Epoch: [89] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.7592 (1.7315)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1228 (6.8103)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1941 (0.1941)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2959 (2.2959 -- 2.2959)  data: 2.0728 (2.0728 -- 2.0728)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3967 (0.5765)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4108 (0.2006 -- 2.2959)  data: 0.1954 (0.0006 -- 2.0728)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4051 (0.5404)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2260 (0.1727 -- 0.5508)  data: 0.0215 (0.0001 -- 0.3490)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4847 (0.5814)  acc1: 85.7143 (84.2324)  acc5: 100.0000 (96.6805)  time: 0.2105 (0.1326 -- 0.5508)  data: 0.0211 (0.0001 -- 0.3490)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 84.440 Acc@5 97.095 loss 0.599
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [90]  [  0/160]  eta: 0:19:53  lr: 0.000028  min_lr: 0.000007  loss: 2.0686 (2.0686)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4438 (9.4438)  time: 7.4597 (7.4597 -- 7.4597)  data: 6.9453 (6.9453 -- 6.9453)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:43  lr: 0.000028  min_lr: 0.000007  loss: 1.7158 (1.7384)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7691 (7.1414)  time: 0.8517 (0.5259 -- 2.5255)  data: 0.1837 (0.0003 -- 1.9755)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:06  lr: 0.000028  min_lr: 0.000007  loss: 1.9285 (1.7975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9428 (7.0664)  time: 0.9365 (0.5218 -- 2.9201)  data: 0.3212 (0.0005 -- 2.3670)  max mem: 16413
[2023-09-04 03:36:08,950] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:36:08,950] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:36:08,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:36:08,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:36:09,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14443
[2023-09-04 03:36:09,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14443
[2023-09-04 03:36:09,497] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:36:09,497] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:36:09,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [90]  [ 60/160]  eta: 0:01:37  lr: 0.000028  min_lr: 0.000007  loss: 1.8180 (1.8175)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7124 (6.9118)  time: 0.8041 (0.5215 -- 3.0917)  data: 0.2548 (0.0003 -- 2.5411)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000007  loss: 1.8054 (1.8105)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9362 (7.0578)  time: 0.8804 (0.5130 -- 3.7310)  data: 0.3339 (0.0003 -- 3.2017)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000007  loss: 1.7135 (1.7825)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6356 (6.9457)  time: 0.8766 (0.5106 -- 3.4951)  data: 0.3287 (0.0004 -- 2.9594)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000007  loss: 1.4831 (1.7586)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6973 (6.9406)  time: 0.8700 (0.5315 -- 3.8372)  data: 0.3194 (0.0007 -- 3.3082)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.7445 (1.7496)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4133 (6.8463)  time: 0.9186 (0.5247 -- 3.9895)  data: 0.3771 (0.0003 -- 3.4497)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.8812 (1.7548)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5656 (6.9819)  time: 0.5681 (0.4970 -- 1.4539)  data: 0.0470 (0.0002 -- 0.9269)  max mem: 16413
Epoch: [90] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.8812 (1.7371)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5656 (6.9819)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1615 (0.1615)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3125 (2.3125 -- 2.3125)  data: 2.0563 (2.0563 -- 2.0563)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3984 (0.5688)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4096 (0.2066 -- 2.3125)  data: 0.1892 (0.0007 -- 2.0563)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3984 (0.5277)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2216 (0.1696 -- 0.4545)  data: 0.0139 (0.0001 -- 0.2496)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4772 (0.5738)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (96.6805)  time: 0.2054 (0.1329 -- 0.4545)  data: 0.0136 (0.0001 -- 0.2496)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 84.025 Acc@5 96.888 loss 0.586
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [91]  [  0/160]  eta: 0:22:40  lr: 0.000028  min_lr: 0.000007  loss: 2.1246 (2.1246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2264 (7.2264)  time: 8.5007 (8.5007 -- 8.5007)  data: 7.0409 (7.0409 -- 7.0409)  max mem: 16413
[2023-09-04 03:38:13,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:38:13,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:38:13,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:38:13,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [91]  [ 20/160]  eta: 0:02:47  lr: 0.000028  min_lr: 0.000007  loss: 1.6488 (1.6412)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6370 (6.9530)  time: 0.8292 (0.5134 -- 5.5414)  data: 0.1253 (0.0004 -- 2.4812)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:08  lr: 0.000028  min_lr: 0.000007  loss: 1.7582 (1.6617)  loss_scale: 32768.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5207 (7.0223)  time: 0.9387 (0.5269 -- 3.9438)  data: 0.0380 (0.0003 -- 0.7297)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:37  lr: 0.000028  min_lr: 0.000007  loss: 1.9121 (1.7297)  loss_scale: 32768.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8188 (7.0070)  time: 0.7828 (0.5258 -- 3.2182)  data: 0.0017 (0.0004 -- 0.0046)  max mem: 16413
[2023-09-04 03:38:56,958] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14621
[2023-09-04 03:38:56,958] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:38:56,958] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14621
[2023-09-04 03:38:56,958] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:38:56,958] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [ 80/160]  eta: 0:01:18  lr: 0.000028  min_lr: 0.000007  loss: 1.6404 (1.7003)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4600 (7.3016)  time: 0.9803 (0.5167 -- 4.6016)  data: 0.0012 (0.0004 -- 0.0022)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000007  loss: 1.6868 (1.7006)  loss_scale: 16384.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8043 (7.3405)  time: 0.7517 (0.5169 -- 4.6225)  data: 0.0018 (0.0004 -- 0.0071)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000007  loss: 1.3983 (1.6714)  loss_scale: 16384.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2318 (7.3735)  time: 1.0127 (0.5122 -- 5.2069)  data: 0.0021 (0.0003 -- 0.0118)  max mem: 16413
[2023-09-04 03:39:55,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14689
[2023-09-04 03:39:55,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14689
[2023-09-04 03:39:55,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:39:55,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 03:39:55,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.5397 (1.6520)  loss_scale: 8192.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7351 (7.2446)  time: 0.7839 (0.4982 -- 3.4540)  data: 0.0013 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.4175 (1.6411)  loss_scale: 8192.0000 (19814.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5133 (7.2993)  time: 0.6761 (0.4955 -- 1.8722)  data: 0.0010 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [91] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.4175 (1.6639)  loss_scale: 8192.0000 (19814.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5133 (7.2993)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2009 (0.2009)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3809 (2.3809 -- 2.3809)  data: 2.1181 (2.1181 -- 2.1181)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3648 (0.5585)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4172 (0.2109 -- 2.3809)  data: 0.1938 (0.0009 -- 2.1181)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4100 (0.5205)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2185 (0.1704 -- 0.3037)  data: 0.0080 (0.0001 -- 0.1103)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4927 (0.5573)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.0954)  time: 0.2013 (0.1329 -- 0.3037)  data: 0.0077 (0.0001 -- 0.1103)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 86.100 Acc@5 96.888 loss 0.573
Accuracy of the network on the 482 val images: 86.10%
[2023-09-04 03:40:24,366] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 03:40:24,368] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 03:40:24,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 03:40:24,368] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 03:40:25,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 03:40:25,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.10%
Epoch: [92]  [  0/160]  eta: 0:21:09  lr: 0.000027  min_lr: 0.000007  loss: 1.7583 (1.7583)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9515 (7.9515)  time: 7.9329 (7.9329 -- 7.9329)  data: 7.3964 (7.3964 -- 7.3964)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:50  lr: 0.000027  min_lr: 0.000007  loss: 1.7940 (1.7119)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5595 (7.4417)  time: 0.8791 (0.5280 -- 2.8952)  data: 0.1050 (0.0006 -- 1.0038)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:05  lr: 0.000027  min_lr: 0.000007  loss: 1.6027 (1.6513)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8484 (7.2474)  time: 0.8713 (0.5095 -- 2.4687)  data: 0.1223 (0.0003 -- 1.9550)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:39  lr: 0.000027  min_lr: 0.000007  loss: 1.5069 (1.6476)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3961 (7.1322)  time: 0.8881 (0.5259 -- 2.3292)  data: 0.1880 (0.0008 -- 1.8190)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:14  lr: 0.000027  min_lr: 0.000007  loss: 1.7815 (1.6900)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3111 (6.9831)  time: 0.7534 (0.5196 -- 2.2040)  data: 0.0889 (0.0007 -- 0.7061)  max mem: 16413
[2023-09-04 03:41:58,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:41:58,816] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 03:41:58,818] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:41:58,818] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [92]  [100/160]  eta: 0:00:55  lr: 0.000027  min_lr: 0.000007  loss: 1.6791 (1.6810)  loss_scale: 8192.0000 (8435.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0072 (7.0280)  time: 0.9189 (0.5278 -- 3.6212)  data: 0.3540 (0.0009 -- 3.0867)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000007  loss: 1.5860 (1.6666)  loss_scale: 16384.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0756 (7.1771)  time: 0.8183 (0.5337 -- 3.2158)  data: 0.2567 (0.0006 -- 2.6751)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.6488 (1.6672)  loss_scale: 16384.0000 (10690.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8322 (7.1527)  time: 0.9861 (0.5147 -- 2.7341)  data: 0.4447 (0.0006 -- 2.1770)  max mem: 16413
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.6669 (1.6691)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1489 (7.0985)  time: 0.6407 (0.4949 -- 1.3458)  data: 0.0361 (0.0002 -- 0.7078)  max mem: 16413
Epoch: [92] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.6669 (1.6844)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1489 (7.0985)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2125 (0.2125)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2872 (2.2872 -- 2.2872)  data: 2.0639 (2.0639 -- 2.0639)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3133 (0.5702)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4099 (0.1935 -- 2.2872)  data: 0.1976 (0.0005 -- 2.0639)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4437 (0.5423)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.3545)  time: 0.2250 (0.1703 -- 0.3525)  data: 0.0211 (0.0001 -- 0.1620)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5383 (0.5862)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (96.6805)  time: 0.2100 (0.1328 -- 0.3525)  data: 0.0208 (0.0001 -- 0.1620)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.892 Acc@5 96.888 loss 0.583
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.10%
Epoch: [93]  [  0/160]  eta: 0:18:51  lr: 0.000027  min_lr: 0.000007  loss: 1.1172 (1.1172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1423 (4.1423)  time: 7.0712 (7.0712 -- 7.0712)  data: 5.4969 (5.4969 -- 5.4969)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:45  lr: 0.000027  min_lr: 0.000007  loss: 1.6696 (1.6588)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6165 (7.3999)  time: 0.8843 (0.5263 -- 4.4483)  data: 0.0578 (0.0003 -- 0.6149)  max mem: 16413
Epoch: [93]  [ 40/160]  eta: 0:02:08  lr: 0.000027  min_lr: 0.000007  loss: 1.5303 (1.6284)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1920 (7.0533)  time: 0.9622 (0.5143 -- 3.6752)  data: 0.0775 (0.0006 -- 0.9385)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:39  lr: 0.000027  min_lr: 0.000007  loss: 1.6039 (1.6316)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6230 (6.8783)  time: 0.8403 (0.5108 -- 3.8782)  data: 0.0803 (0.0003 -- 1.4481)  max mem: 16413
[2023-09-04 03:44:02,623] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:44:02,624] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:44:02,624] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:44:02,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000007  loss: 1.8151 (1.6530)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0971 (6.8922)  time: 0.8676 (0.5118 -- 3.0208)  data: 0.0016 (0.0002 -- 0.0063)  max mem: 16413
[2023-09-04 03:44:22,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14970
[2023-09-04 03:44:22,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14970
[2023-09-04 03:44:22,616] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:44:22,616] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:44:22,617] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000007  loss: 1.5594 (1.6594)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1195 (6.8817)  time: 0.9037 (0.5263 -- 3.1968)  data: 0.0014 (0.0002 -- 0.0035)  max mem: 16413
[2023-09-04 03:44:46,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=83, lr=[6.800255005360772e-06, 6.800255005360772e-06, 7.555838894845303e-06, 7.555838894845303e-06, 8.395376549828112e-06, 8.395376549828112e-06, 9.328196166475682e-06, 9.328196166475682e-06, 1.0364662407195201e-05, 1.0364662407195201e-05, 1.1516291563550223e-05, 1.1516291563550223e-05, 1.2795879515055802e-05, 1.2795879515055802e-05, 1.4217643905617558e-05, 1.4217643905617558e-05, 1.5797382117352843e-05, 1.5797382117352843e-05, 1.7552646797058713e-05, 1.7552646797058713e-05, 1.9502940885620792e-05, 1.9502940885620792e-05, 2.1669934317356435e-05, 2.1669934317356435e-05, 2.4077704797062705e-05, 2.4077704797062705e-05, 2.675300533006967e-05, 2.675300533006967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 03:44:46,816] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=18.142198183999565, CurrSamplesPerSec=21.524235461353225, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000007  loss: 1.7283 (1.6786)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4131 (6.8782)  time: 0.7555 (0.5273 -- 2.6489)  data: 0.0013 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.7683 (1.6842)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1363 (7.0912)  time: 0.9021 (0.5232 -- 2.8145)  data: 0.0016 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.4738 (1.6722)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4050 (7.0319)  time: 0.6919 (0.4959 -- 2.0417)  data: 0.0044 (0.0002 -- 0.0724)  max mem: 16413
Epoch: [93] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.4738 (1.6983)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4050 (7.0319)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1670 (0.1670)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3205 (2.3205 -- 2.3205)  data: 2.0904 (2.0904 -- 2.0904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3722 (0.5554)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4273 (0.1987 -- 2.3205)  data: 0.2094 (0.0007 -- 2.0904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3932 (0.5130)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.3545)  time: 0.2284 (0.1694 -- 0.3989)  data: 0.0205 (0.0001 -- 0.1937)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4659 (0.5636)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (96.6805)  time: 0.2112 (0.1359 -- 0.3989)  data: 0.0191 (0.0001 -- 0.1937)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 86.100 Acc@5 97.095 loss 0.574
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.10%
Epoch: [94]  [  0/160]  eta: 0:18:32  lr: 0.000027  min_lr: 0.000007  loss: 1.7209 (1.7209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1786 (7.1786)  time: 6.9544 (6.9544 -- 6.9544)  data: 6.3358 (6.3358 -- 6.3358)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:48  lr: 0.000027  min_lr: 0.000007  loss: 1.4549 (1.5972)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8400 (6.6610)  time: 0.9187 (0.5182 -- 4.6830)  data: 0.0948 (0.0002 -- 1.8689)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:57  lr: 0.000027  min_lr: 0.000007  loss: 1.7519 (1.6250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6477 (6.6984)  time: 0.7492 (0.5330 -- 1.7658)  data: 0.0273 (0.0002 -- 0.4766)  max mem: 16413
[2023-09-04 03:46:25,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:46:25,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:46:25,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:46:25,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [94]  [ 60/160]  eta: 0:01:37  lr: 0.000027  min_lr: 0.000007  loss: 1.7183 (1.6546)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1258 (6.7089)  time: 0.9500 (0.5355 -- 3.3811)  data: 0.2767 (0.0005 -- 1.6694)  max mem: 16413
[2023-09-04 03:46:34,175] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-09-04 03:46:34,175] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-09-04 03:46:34,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:46:34,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:46:34,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [94]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.5767 (1.6388)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3289 (6.7036)  time: 0.8355 (0.5316 -- 2.7151)  data: 0.2047 (0.0002 -- 2.1819)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000007  loss: 1.7667 (1.6568)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5608 (6.8217)  time: 0.8968 (0.5262 -- 3.6494)  data: 0.3472 (0.0004 -- 3.1187)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 1.6987 (1.6750)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7020 (6.8599)  time: 0.8782 (0.5146 -- 4.1983)  data: 0.3303 (0.0002 -- 3.6606)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.8475 (1.7014)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3667 (6.9432)  time: 0.7962 (0.5313 -- 2.3097)  data: 0.2428 (0.0009 -- 1.7855)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.6940 (1.6999)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5998 (7.0140)  time: 0.7136 (0.4938 -- 1.6192)  data: 0.1369 (0.0002 -- 1.0887)  max mem: 16413
Epoch: [94] Total time: 0:02:21 (0.8825 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.6940 (1.6884)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5998 (7.0140)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1957 (0.1957)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2872 (2.2872 -- 2.2872)  data: 2.0549 (2.0549 -- 2.0549)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3689 (0.5807)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4204 (0.1992 -- 2.2872)  data: 0.2047 (0.0007 -- 2.0549)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4785 (0.5547)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2303 (0.1684 -- 0.5668)  data: 0.0293 (0.0001 -- 0.3857)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5506 (0.5840)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2173 (0.1322 -- 0.5668)  data: 0.0290 (0.0001 -- 0.3857)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.583
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.10%
Epoch: [95]  [  0/160]  eta: 0:20:50  lr: 0.000026  min_lr: 0.000007  loss: 1.7084 (1.7084)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6307 (6.6307)  time: 7.8136 (7.8136 -- 7.8136)  data: 6.5660 (6.5660 -- 6.5660)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:55  lr: 0.000026  min_lr: 0.000007  loss: 1.8922 (1.7849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9077 (6.6044)  time: 0.9260 (0.5232 -- 4.6409)  data: 0.1008 (0.0003 -- 1.9840)  max mem: 16413
[2023-09-04 03:48:35,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:48:35,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:48:35,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:48:35,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [95]  [ 40/160]  eta: 0:02:06  lr: 0.000026  min_lr: 0.000007  loss: 1.6441 (1.7266)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1218 (6.3192)  time: 0.8445 (0.5101 -- 4.4408)  data: 0.0012 (0.0002 -- 0.0027)  max mem: 16413
[2023-09-04 03:48:41,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15245
[2023-09-04 03:48:41,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:48:41,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15245
[2023-09-04 03:48:41,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:48:41,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [ 60/160]  eta: 0:01:37  lr: 0.000026  min_lr: 0.000007  loss: 1.7009 (1.7251)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7846 (6.6075)  time: 0.8202 (0.5247 -- 2.8311)  data: 0.0664 (0.0003 -- 0.7855)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:16  lr: 0.000026  min_lr: 0.000007  loss: 1.6956 (1.7207)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6074 (6.6488)  time: 0.9136 (0.5249 -- 3.0142)  data: 0.1676 (0.0004 -- 2.4801)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000007  loss: 1.6469 (1.7133)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7866 (6.7526)  time: 0.8713 (0.5270 -- 3.3159)  data: 0.3213 (0.0005 -- 2.7787)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 1.5561 (1.6960)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3624 (6.8750)  time: 0.8420 (0.5251 -- 3.9947)  data: 0.2926 (0.0003 -- 3.4576)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.6739 (1.6967)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9782 (6.9100)  time: 0.8631 (0.5234 -- 3.7630)  data: 0.3098 (0.0008 -- 3.2457)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.6722 (1.6939)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0408 (6.8976)  time: 0.6666 (0.4970 -- 2.3851)  data: 0.1373 (0.0002 -- 1.8559)  max mem: 16413
Epoch: [95] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.6722 (1.6957)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0408 (6.8976)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1777 (0.1777)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3419 (2.3419 -- 2.3419)  data: 2.1261 (2.1261 -- 2.1261)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4608 (0.5687)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4160 (0.2082 -- 2.3419)  data: 0.2006 (0.0008 -- 2.1261)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4328 (0.5252)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (96.8254)  time: 0.2271 (0.1709 -- 0.5550)  data: 0.0220 (0.0001 -- 0.3558)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4520 (0.5683)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (96.6805)  time: 0.2102 (0.1331 -- 0.5550)  data: 0.0217 (0.0001 -- 0.3558)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 85.892 Acc@5 97.095 loss 0.581
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.10%
Epoch: [96]  [  0/160]  eta: 0:18:37  lr: 0.000026  min_lr: 0.000007  loss: 1.5880 (1.5880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9515 (5.9515)  time: 6.9813 (6.9813 -- 6.9813)  data: 5.5304 (5.5304 -- 5.5304)  max mem: 16413
[2023-09-04 03:50:42,716] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:50:42,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:50:42,718] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:50:42,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [96]  [ 20/160]  eta: 0:02:38  lr: 0.000026  min_lr: 0.000007  loss: 1.5980 (1.7829)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6765 (7.9931)  time: 0.8403 (0.5255 -- 4.1664)  data: 0.1821 (0.0006 -- 2.1111)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:04  lr: 0.000026  min_lr: 0.000007  loss: 1.5009 (1.6584)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3661 (7.1025)  time: 0.9356 (0.5194 -- 3.1042)  data: 0.2028 (0.0003 -- 2.1306)  max mem: 16413
[2023-09-04 03:51:16,367] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15409
[2023-09-04 03:51:16,367] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15409
[2023-09-04 03:51:16,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:51:16,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:51:16,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [96]  [ 60/160]  eta: 0:01:39  lr: 0.000026  min_lr: 0.000007  loss: 1.6248 (1.6525)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0552 (6.8848)  time: 0.9075 (0.5224 -- 4.7367)  data: 0.0122 (0.0004 -- 0.2196)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:17  lr: 0.000026  min_lr: 0.000007  loss: 1.6414 (1.6601)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6900 (6.9228)  time: 0.8814 (0.5226 -- 3.8984)  data: 0.0012 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000007  loss: 1.5643 (1.6589)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5123 (6.8934)  time: 0.7801 (0.5286 -- 2.0593)  data: 0.0020 (0.0007 -- 0.0072)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 1.6590 (1.6411)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5296 (6.9728)  time: 0.8407 (0.5212 -- 2.8718)  data: 0.0014 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.6907 (1.6542)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5130 (7.0895)  time: 0.9334 (0.5207 -- 4.1645)  data: 0.0018 (0.0003 -- 0.0084)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000006  loss: 1.7966 (1.6773)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5593 (7.0818)  time: 0.7036 (0.4951 -- 4.1152)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [96] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000006  loss: 1.7966 (1.6798)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5593 (7.0818)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1713 (0.1713)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3109 (2.3109 -- 2.3109)  data: 2.1141 (2.1141 -- 2.1141)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3933 (0.6031)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4265 (0.2061 -- 2.3109)  data: 0.2055 (0.0003 -- 2.1141)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3933 (0.5424)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2362 (0.1699 -- 0.4778)  data: 0.0223 (0.0001 -- 0.2962)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4261 (0.5785)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2179 (0.1319 -- 0.4778)  data: 0.0221 (0.0001 -- 0.2962)  max mem: 16413
Val: Total time: 0:00:08 (0.2969 s / it)
* Acc@1 85.270 Acc@5 97.303 loss 0.580
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.10%
Epoch: [97]  [  0/160]  eta: 0:19:51  lr: 0.000026  min_lr: 0.000006  loss: 2.0187 (2.0187)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1050 (10.1050)  time: 7.4448 (7.4448 -- 7.4448)  data: 6.9176 (6.9176 -- 6.9176)  max mem: 16413
[2023-09-04 03:53:19,578] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:53:19,578] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:53:19,579] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:53:19,580] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [97]  [ 20/160]  eta: 0:02:39  lr: 0.000025  min_lr: 0.000006  loss: 1.6145 (1.6875)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4342 (6.9403)  time: 0.8226 (0.5135 -- 4.5174)  data: 0.2489 (0.0004 -- 3.9864)  max mem: 16413
Epoch: [97]  [ 40/160]  eta: 0:02:01  lr: 0.000025  min_lr: 0.000006  loss: 1.8778 (1.7395)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6283 (6.4209)  time: 0.8828 (0.5252 -- 2.7624)  data: 0.1504 (0.0004 -- 0.9887)  max mem: 16413
[2023-09-04 03:53:42,753] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15565
[2023-09-04 03:53:42,753] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15565
[2023-09-04 03:53:42,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:53:42,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 03:53:42,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [97]  [ 60/160]  eta: 0:01:40  lr: 0.000025  min_lr: 0.000006  loss: 1.7507 (1.7292)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5069 (6.6508)  time: 0.9774 (0.5247 -- 3.9369)  data: 0.4023 (0.0004 -- 3.4059)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000006  loss: 1.5070 (1.7019)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8791 (6.6744)  time: 0.8534 (0.5126 -- 4.1911)  data: 0.3151 (0.0002 -- 3.6818)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 1.6365 (1.6821)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0610 (6.6060)  time: 0.9292 (0.5077 -- 3.6901)  data: 0.3926 (0.0004 -- 3.1878)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.7125 (1.6894)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1491 (6.7057)  time: 0.8473 (0.5133 -- 4.2745)  data: 0.3099 (0.0003 -- 3.7636)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.6425 (1.6992)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (6.7414)  time: 0.7640 (0.5210 -- 2.5826)  data: 0.2146 (0.0004 -- 2.0415)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.6739 (1.6912)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (6.8141)  time: 0.6831 (0.4966 -- 3.6669)  data: 0.1565 (0.0002 -- 3.1154)  max mem: 16413
Epoch: [97] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.6739 (1.6873)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (6.8141)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1646 (0.1646)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4437 (2.4437 -- 2.4437)  data: 2.2085 (2.2085 -- 2.2085)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4078 (0.5662)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4205 (0.1982 -- 2.4437)  data: 0.2019 (0.0006 -- 2.2085)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4251 (0.5330)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (96.8254)  time: 0.2190 (0.1696 -- 0.4234)  data: 0.0127 (0.0001 -- 0.2389)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4787 (0.5769)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (96.6805)  time: 0.2028 (0.1331 -- 0.4234)  data: 0.0123 (0.0001 -- 0.2389)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 83.817 Acc@5 97.095 loss 0.588
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 86.10%
Epoch: [98]  [  0/160]  eta: 0:20:05  lr: 0.000025  min_lr: 0.000006  loss: 0.9734 (0.9734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7625 (8.7625)  time: 7.5366 (7.5366 -- 7.5366)  data: 6.1777 (6.1777 -- 6.1777)  max mem: 16413
[2023-09-04 03:55:44,361] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:55:44,362] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 03:55:44,362] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:55:44,362] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000006  loss: 1.6191 (1.6287)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0240 (6.3766)  time: 0.8548 (0.5210 -- 4.2418)  data: 0.3022 (0.0004 -- 3.7035)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:02:05  lr: 0.000025  min_lr: 0.000006  loss: 1.7160 (1.6737)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6062 (6.5158)  time: 0.9192 (0.4965 -- 5.5553)  data: 0.2887 (0.0003 -- 3.3006)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:38  lr: 0.000025  min_lr: 0.000006  loss: 1.6652 (1.6552)  loss_scale: 32768.0000 (29007.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1560 (6.7441)  time: 0.8478 (0.5185 -- 4.9635)  data: 0.3031 (0.0002 -- 4.4402)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000006  loss: 1.6410 (1.6739)  loss_scale: 32768.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7934 (6.7626)  time: 0.9500 (0.5161 -- 3.6727)  data: 0.4112 (0.0003 -- 3.1626)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000006  loss: 1.6125 (1.6473)  loss_scale: 32768.0000 (30496.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1150 (6.8330)  time: 0.7892 (0.5229 -- 3.5877)  data: 0.2476 (0.0001 -- 3.0665)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.8605 (1.6598)  loss_scale: 32768.0000 (30872.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6374 (6.8644)  time: 0.8843 (0.5207 -- 3.8809)  data: 0.2147 (0.0005 -- 2.4682)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.8039 (1.6700)  loss_scale: 32768.0000 (31141.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6992 (6.8657)  time: 0.8006 (0.5287 -- 4.7596)  data: 0.2513 (0.0003 -- 4.2248)  max mem: 16413
[2023-09-04 03:57:39,008] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:57:39,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 03:57:39,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:57:39,010] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 03:57:42,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15829
[2023-09-04 03:57:42,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15829
[2023-09-04 03:57:42,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:57:42,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:57:42,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.6813 (1.6753)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1800 (6.8283)  time: 0.7095 (0.4952 -- 3.1703)  data: 0.1867 (0.0002 -- 2.6454)  max mem: 16413
Epoch: [98] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.6813 (1.6760)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1800 (6.8283)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1639 (0.1639)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3973 (2.3973 -- 2.3973)  data: 2.1861 (2.1861 -- 2.1861)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4855 (0.5622)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4237 (0.2042 -- 2.3973)  data: 0.2138 (0.0004 -- 2.1861)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4554 (0.5318)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1689 -- 0.3547)  data: 0.0174 (0.0001 -- 0.1791)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4776 (0.5889)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (96.2656)  time: 0.2042 (0.1328 -- 0.3547)  data: 0.0165 (0.0001 -- 0.1791)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 84.440 Acc@5 96.888 loss 0.587
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.10%
Epoch: [99]  [  0/160]  eta: 0:18:16  lr: 0.000025  min_lr: 0.000006  loss: 1.9660 (1.9660)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9982 (7.9982)  time: 6.8507 (6.8507 -- 6.8507)  data: 5.4918 (5.4918 -- 5.4918)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:37  lr: 0.000025  min_lr: 0.000006  loss: 1.7201 (1.6625)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8218 (8.5017)  time: 0.8421 (0.5216 -- 2.7922)  data: 0.1141 (0.0009 -- 1.2303)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:02:08  lr: 0.000025  min_lr: 0.000006  loss: 1.7181 (1.7075)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5428 (7.7045)  time: 1.0183 (0.5263 -- 3.6279)  data: 0.1687 (0.0005 -- 2.6452)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000006  loss: 1.7451 (1.7223)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2556 (7.5376)  time: 0.7417 (0.5174 -- 2.9560)  data: 0.0014 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:15  lr: 0.000025  min_lr: 0.000006  loss: 1.6133 (1.6740)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8268 (7.5428)  time: 0.9016 (0.5218 -- 2.7412)  data: 0.1124 (0.0004 -- 1.3043)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000006  loss: 1.7418 (1.6616)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3295 (7.4540)  time: 0.8537 (0.5262 -- 3.0912)  data: 0.3052 (0.0003 -- 2.5467)  max mem: 16413
[2023-09-04 03:59:46,036] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:59:46,037] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 03:59:46,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 03:59:46,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000006  loss: 1.5922 (1.6524)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5615 (7.3131)  time: 0.8180 (0.5285 -- 4.2981)  data: 0.2673 (0.0001 -- 3.7873)  max mem: 16413
[2023-09-04 03:59:54,349] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15967
[2023-09-04 03:59:54,349] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15967
[2023-09-04 03:59:54,350] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:59:54,350] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 03:59:54,350] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 04:00:01,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15973
[2023-09-04 04:00:01,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15973
[2023-09-04 04:00:01,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:00:01,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:00:01,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.6638 (1.6472)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7140 (7.3008)  time: 0.9790 (0.5122 -- 4.3228)  data: 0.0943 (0.0004 -- 1.8153)  max mem: 16413
[2023-09-04 04:00:18,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=90, lr=[6.20369869372962e-06, 6.20369869372962e-06, 6.892998548588467e-06, 6.892998548588467e-06, 7.658887276209406e-06, 7.658887276209406e-06, 8.509874751343784e-06, 8.509874751343784e-06, 9.455416390381983e-06, 9.455416390381983e-06, 1.0506018211535537e-05, 1.0506018211535537e-05, 1.1673353568372818e-05, 1.1673353568372818e-05, 1.2970392853747574e-05, 1.2970392853747574e-05, 1.4411547615275083e-05, 1.4411547615275083e-05, 1.601283068363898e-05, 1.601283068363898e-05, 1.77920340929322e-05, 1.77920340929322e-05, 1.9768926769924666e-05, 1.9768926769924666e-05, 2.1965474188805186e-05, 2.1965474188805186e-05, 2.440608243200576e-05, 2.440608243200576e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 04:00:18,654] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=18.154952311956215, CurrSamplesPerSec=24.69348698102692, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.7643 (1.6441)  loss_scale: 16384.0000 (31846.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5685 (7.2417)  time: 0.6213 (0.4951 -- 2.4863)  data: 0.1013 (0.0002 -- 1.9613)  max mem: 16413
Epoch: [99] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.7643 (1.6673)  loss_scale: 16384.0000 (31846.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5685 (7.2417)
[2023-09-04 04:00:18,660] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-09-04 04:00:18,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-09-04 04:00:18,665] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-09-04 04:00:18,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-09-04 04:00:19,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-09-04 04:00:19,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1583 (0.1583)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4802 (2.4802 -- 2.4802)  data: 2.2472 (2.2472 -- 2.2472)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4717 (0.5631)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4216 (0.2028 -- 2.4802)  data: 0.2100 (0.0009 -- 2.2472)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4688 (0.5306)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2203 (0.1690 -- 0.3093)  data: 0.0140 (0.0001 -- 0.1381)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5530 (0.5833)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2066 (0.1328 -- 0.3093)  data: 0.0135 (0.0001 -- 0.1381)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.594
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 86.10%
Epoch: [100]  [  0/160]  eta: 0:19:06  lr: 0.000024  min_lr: 0.000006  loss: 1.9204 (1.9204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8565 (4.8565)  time: 7.1659 (7.1659 -- 7.1659)  data: 5.0825 (5.0825 -- 5.0825)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:34  lr: 0.000024  min_lr: 0.000006  loss: 1.5365 (1.6675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4319 (6.8118)  time: 0.8033 (0.5152 -- 3.8098)  data: 0.0071 (0.0008 -- 0.0833)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:09  lr: 0.000024  min_lr: 0.000006  loss: 1.4000 (1.6190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3906 (7.2748)  time: 1.0551 (0.5221 -- 4.4773)  data: 0.0155 (0.0003 -- 0.2708)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:40  lr: 0.000024  min_lr: 0.000006  loss: 1.6483 (1.6504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1340 (7.1983)  time: 0.8499 (0.5180 -- 2.6178)  data: 0.0976 (0.0003 -- 1.2453)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000006  loss: 1.8116 (1.6564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4879 (7.3485)  time: 0.9519 (0.5063 -- 4.3093)  data: 0.3751 (0.0003 -- 3.8080)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000006  loss: 1.7073 (1.6717)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5273 (7.2804)  time: 0.8471 (0.5099 -- 3.4205)  data: 0.3058 (0.0003 -- 2.8963)  max mem: 16413
[2023-09-04 04:02:05,988] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:02:05,988] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:02:05,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:02:05,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:02:18,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16117
[2023-09-04 04:02:18,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:02:18,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16117
[2023-09-04 04:02:18,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 04:02:18,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [100]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.6776 (1.6761)  loss_scale: 32768.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3765 (7.2056)  time: 0.8255 (0.5091 -- 2.8498)  data: 0.2060 (0.0001 -- 2.3266)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:19  lr: 0.000024  min_lr: 0.000006  loss: 1.8312 (1.6865)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5947 (7.1844)  time: 1.0074 (0.5269 -- 4.2778)  data: 0.4325 (0.0004 -- 3.7564)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.8613 (1.7027)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2980 (7.1246)  time: 0.7459 (0.4964 -- 4.0698)  data: 0.2359 (0.0002 -- 3.5622)  max mem: 16413
Epoch: [100] Total time: 0:02:24 (0.9053 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.8613 (1.7066)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2980 (7.1246)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1606 (0.1606)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3342 (2.3342 -- 2.3342)  data: 2.0923 (2.0923 -- 2.0923)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4123 (0.5454)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4076 (0.2035 -- 2.3342)  data: 0.1912 (0.0005 -- 2.0923)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4123 (0.5241)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2220 (0.1686 -- 0.4848)  data: 0.0179 (0.0001 -- 0.2908)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4895 (0.5720)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.2075 (0.1331 -- 0.4848)  data: 0.0177 (0.0001 -- 0.2908)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.588
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.10%
Epoch: [101]  [  0/160]  eta: 0:16:34  lr: 0.000024  min_lr: 0.000006  loss: 1.4248 (1.4248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4178 (9.4178)  time: 6.2171 (6.2171 -- 6.2171)  data: 5.6981 (5.6981 -- 5.6981)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:44  lr: 0.000024  min_lr: 0.000006  loss: 1.7356 (1.6681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3004 (8.2411)  time: 0.9265 (0.5076 -- 2.4060)  data: 0.1929 (0.0001 -- 1.8558)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:02  lr: 0.000024  min_lr: 0.000006  loss: 1.6910 (1.6576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8212 (7.5842)  time: 0.8623 (0.5197 -- 4.1254)  data: 0.0316 (0.0003 -- 0.3650)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:40  lr: 0.000024  min_lr: 0.000006  loss: 1.4619 (1.6341)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9198 (7.2267)  time: 0.9668 (0.5260 -- 4.2375)  data: 0.0244 (0.0003 -- 0.4625)  max mem: 16413
Epoch: [101]  [ 80/160]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000006  loss: 1.6512 (1.6304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0398 (7.2385)  time: 0.8862 (0.5133 -- 4.0390)  data: 0.0107 (0.0004 -- 0.1924)  max mem: 16413
[2023-09-04 04:04:23,516] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:04:23,516] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:04:23,519] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:04:23,519] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000006  loss: 1.6867 (1.6431)  loss_scale: 32768.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6638 (7.1645)  time: 0.8461 (0.5246 -- 3.2925)  data: 0.0027 (0.0002 -- 0.0147)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.5187 (1.6346)  loss_scale: 32768.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4173 (7.2216)  time: 0.8334 (0.5224 -- 3.5393)  data: 0.0019 (0.0003 -- 0.0062)  max mem: 16413
[2023-09-04 04:04:58,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16284
[2023-09-04 04:04:58,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16284
[2023-09-04 04:04:58,148] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:04:58,148] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:04:58,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.6896 (1.6391)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2477 (7.2521)  time: 0.9355 (0.5005 -- 3.9901)  data: 0.0011 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.6492 (1.6371)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6199 (7.2820)  time: 0.6191 (0.4930 -- 2.2616)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [101] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.6492 (1.6836)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6199 (7.2820)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1596 (0.1596)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2956 (2.2956 -- 2.2956)  data: 2.0671 (2.0671 -- 2.0671)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4560 (0.5705)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4109 (0.1990 -- 2.2956)  data: 0.1888 (0.0006 -- 2.0671)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4560 (0.5535)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2155 (0.1731 -- 0.3260)  data: 0.0063 (0.0001 -- 0.1119)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4985 (0.5872)  acc1: 85.7143 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.1998 (0.1332 -- 0.3260)  data: 0.0061 (0.0001 -- 0.1119)  max mem: 16413
Val: Total time: 0:00:07 (0.2812 s / it)
* Acc@1 83.817 Acc@5 97.303 loss 0.584
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 86.10%
Epoch: [102]  [  0/160]  eta: 0:20:50  lr: 0.000024  min_lr: 0.000006  loss: 1.9262 (1.9262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4253 (6.4253)  time: 7.8167 (7.8167 -- 7.8167)  data: 7.2603 (7.2603 -- 7.2603)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:39  lr: 0.000024  min_lr: 0.000006  loss: 1.5351 (1.6855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4773 (6.7013)  time: 0.8056 (0.5272 -- 4.1784)  data: 0.2586 (0.0003 -- 3.6423)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:00  lr: 0.000024  min_lr: 0.000006  loss: 1.6664 (1.6766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9088 (7.0061)  time: 0.8683 (0.5340 -- 3.2435)  data: 0.3168 (0.0004 -- 2.7105)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000006  loss: 1.6645 (1.6627)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5013 (6.9223)  time: 0.8880 (0.5089 -- 2.7738)  data: 0.3454 (0.0005 -- 2.2436)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000006  loss: 1.6479 (1.6604)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2789 (6.7995)  time: 0.9395 (0.5287 -- 3.3257)  data: 0.3924 (0.0006 -- 2.7949)  max mem: 16413
[2023-09-04 04:06:59,163] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:06:59,163] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:06:59,164] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:06:59,164] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [102]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.7353 (1.6679)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9238 (6.8489)  time: 0.8666 (0.5178 -- 3.3269)  data: 0.3242 (0.0006 -- 2.7989)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000006  loss: 1.7087 (1.6793)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6086 (6.9117)  time: 0.8224 (0.5296 -- 3.3573)  data: 0.2738 (0.0004 -- 2.8155)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.7264 (1.6740)  loss_scale: 32768.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2422 (6.8502)  time: 0.8472 (0.5141 -- 2.6139)  data: 0.2985 (0.0005 -- 2.0844)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.6634 (1.6674)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5995 (6.8347)  time: 0.7066 (0.4965 -- 4.2453)  data: 0.1874 (0.0002 -- 3.7346)  max mem: 16413
Epoch: [102] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.6634 (1.6696)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5995 (6.8347)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1423 (0.1423)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3479 (2.3479 -- 2.3479)  data: 2.1316 (2.1316 -- 2.1316)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3763 (0.6077)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4204 (0.2041 -- 2.3479)  data: 0.2024 (0.0007 -- 2.1316)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4759 (0.5764)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2191 (0.1688 -- 0.3278)  data: 0.0107 (0.0001 -- 0.1165)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4759 (0.6184)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2024 (0.1330 -- 0.3278)  data: 0.0104 (0.0001 -- 0.1165)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.780 Acc@5 96.680 loss 0.620
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 86.10%
Epoch: [103]  [  0/160]  eta: 0:20:20  lr: 0.000023  min_lr: 0.000006  loss: 1.4461 (1.4461)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1825 (6.1825)  time: 7.6291 (7.6291 -- 7.6291)  data: 5.9514 (5.9514 -- 5.9514)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:02:40  lr: 0.000023  min_lr: 0.000006  loss: 1.8423 (1.7466)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7109 (6.9806)  time: 0.8238 (0.5337 -- 2.9843)  data: 0.1813 (0.0002 -- 2.2073)  max mem: 16413
[2023-09-04 04:08:32,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16508
[2023-09-04 04:08:32,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16508
[2023-09-04 04:08:32,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:08:32,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:08:32,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [103]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000006  loss: 1.9101 (1.7513)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0104 (7.1280)  time: 0.8895 (0.5261 -- 3.0664)  data: 0.1355 (0.0004 -- 2.5320)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000006  loss: 1.7597 (1.7227)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3069 (7.0621)  time: 0.8752 (0.5251 -- 2.3843)  data: 0.1874 (0.0008 -- 1.5532)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000006  loss: 1.5535 (1.6977)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3004 (7.0224)  time: 0.7931 (0.5237 -- 3.1028)  data: 0.0845 (0.0003 -- 1.3505)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000006  loss: 1.7293 (1.7066)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1119 (6.8809)  time: 0.9401 (0.5298 -- 3.2616)  data: 0.2954 (0.0006 -- 2.7033)  max mem: 16413
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000006  loss: 1.7262 (1.6962)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7348 (6.9117)  time: 0.7958 (0.5378 -- 3.5482)  data: 0.1957 (0.0010 -- 2.9981)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.6806 (1.6932)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9981 (6.8457)  time: 0.8884 (0.5449 -- 2.7633)  data: 0.2678 (0.0008 -- 2.2415)  max mem: 16413
[2023-09-04 04:10:22,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:10:22,002] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:10:22,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:10:22,002] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.7233 (1.6894)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3876 (6.7949)  time: 0.7499 (0.4960 -- 1.9148)  data: 0.1404 (0.0002 -- 1.3902)  max mem: 16413
Epoch: [103] Total time: 0:02:21 (0.8875 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.7233 (1.6686)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3876 (6.7949)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1775 (0.1775)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3427 (2.3427 -- 2.3427)  data: 2.1102 (2.1102 -- 2.1102)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4482 (0.5829)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4305 (0.2020 -- 2.3427)  data: 0.2146 (0.0008 -- 2.1102)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4482 (0.5495)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2231 (0.1697 -- 0.4765)  data: 0.0189 (0.0001 -- 0.2339)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4963 (0.6009)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2079 (0.1338 -- 0.4765)  data: 0.0185 (0.0001 -- 0.2339)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.600
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 86.10%
Epoch: [104]  [  0/160]  eta: 0:22:56  lr: 0.000023  min_lr: 0.000006  loss: 1.9775 (1.9775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5378 (7.5378)  time: 8.6031 (8.6031 -- 8.6031)  data: 6.1401 (6.1401 -- 6.1401)  max mem: 16413
Epoch: [104]  [ 20/160]  eta: 0:03:00  lr: 0.000023  min_lr: 0.000006  loss: 1.5655 (1.6230)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3457 (6.6201)  time: 0.9255 (0.5177 -- 5.0109)  data: 0.0164 (0.0004 -- 0.3044)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:16  lr: 0.000023  min_lr: 0.000006  loss: 1.5895 (1.6210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6605 (6.9321)  time: 0.9726 (0.5218 -- 4.2907)  data: 0.0015 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:43  lr: 0.000023  min_lr: 0.000006  loss: 1.7961 (1.6659)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6251 (7.0179)  time: 0.8236 (0.5193 -- 3.1315)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
[2023-09-04 04:11:39,021] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16707
[2023-09-04 04:11:39,021] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16707
[2023-09-04 04:11:39,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:11:39,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:11:39,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000006  loss: 1.7010 (1.6471)  loss_scale: 16384.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7780 (6.8052)  time: 0.7807 (0.5284 -- 2.6039)  data: 0.0014 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.5316 (1.6438)  loss_scale: 16384.0000 (27252.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6897 (6.8393)  time: 0.8523 (0.5176 -- 3.8111)  data: 0.0023 (0.0002 -- 0.0134)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.8850 (1.6736)  loss_scale: 16384.0000 (25456.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9784 (6.9407)  time: 0.8902 (0.5262 -- 3.5152)  data: 0.0021 (0.0002 -- 0.0088)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.6016 (1.6752)  loss_scale: 16384.0000 (24169.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1219 (7.0284)  time: 0.8709 (0.5224 -- 3.9915)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.5837 (1.6673)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3093 (7.1059)  time: 0.6922 (0.4955 -- 3.0990)  data: 0.0008 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [104] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.5837 (1.6675)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3093 (7.1059)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1575 (0.1575)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2989 (2.2989 -- 2.2989)  data: 2.0900 (2.0900 -- 2.0900)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3977 (0.5739)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4028 (0.1986 -- 2.2989)  data: 0.1955 (0.0008 -- 2.0900)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3977 (0.5236)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2207 (0.1704 -- 0.4896)  data: 0.0190 (0.0001 -- 0.3153)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4223 (0.5742)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.2072 (0.1332 -- 0.4896)  data: 0.0188 (0.0001 -- 0.3153)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 85.062 Acc@5 96.888 loss 0.584
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.10%
Epoch: [105]  [  0/160]  eta: 0:22:06  lr: 0.000023  min_lr: 0.000006  loss: 1.4491 (1.4491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7976 (7.7976)  time: 8.2882 (8.2882 -- 8.2882)  data: 7.7590 (7.7590 -- 7.7590)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:44  lr: 0.000022  min_lr: 0.000006  loss: 1.5566 (1.4705)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1076 (6.4410)  time: 0.8209 (0.5130 -- 3.4495)  data: 0.2801 (0.0002 -- 2.9274)  max mem: 16413
[2023-09-04 04:13:43,515] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:13:43,515] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:13:43,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:13:43,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [105]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000006  loss: 1.8119 (1.6110)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8548 (6.9154)  time: 0.9584 (0.5314 -- 4.6469)  data: 0.0939 (0.0004 -- 1.8492)  max mem: 16413
[2023-09-04 04:13:48,839] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16844
[2023-09-04 04:13:48,839] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16844
[2023-09-04 04:13:48,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:13:48,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:13:48,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [105]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000006  loss: 1.6977 (1.6362)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7744 (7.2568)  time: 0.7998 (0.5327 -- 3.4903)  data: 0.0023 (0.0006 -- 0.0141)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000006  loss: 1.8051 (1.6619)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0980 (7.4573)  time: 0.9000 (0.5254 -- 3.6321)  data: 0.0027 (0.0004 -- 0.0133)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000006  loss: 1.5476 (1.6377)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0126 (7.1885)  time: 0.8314 (0.5181 -- 2.7687)  data: 0.0019 (0.0008 -- 0.0044)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000006  loss: 1.5901 (1.6406)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6337 (7.0830)  time: 0.9642 (0.5093 -- 3.2027)  data: 0.0599 (0.0002 -- 0.8656)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.6389 (1.6449)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5670 (7.0939)  time: 0.8326 (0.5132 -- 2.9583)  data: 0.0652 (0.0003 -- 0.7471)  max mem: 16413
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.7900 (1.6496)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5424 (7.1279)  time: 0.6421 (0.4957 -- 1.9832)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [105] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.7900 (1.6558)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5424 (7.1279)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1455 (0.1455)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2929 (2.2929 -- 2.2929)  data: 2.0886 (2.0886 -- 2.0886)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4350 (0.6037)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4123 (0.2011 -- 2.2929)  data: 0.1995 (0.0005 -- 2.0886)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4350 (0.5568)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2234 (0.1746 -- 0.3013)  data: 0.0133 (0.0001 -- 0.1193)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5003 (0.6070)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2101 (0.1327 -- 0.3013)  data: 0.0132 (0.0001 -- 0.1193)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.596
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.10%
Epoch: [106]  [  0/160]  eta: 0:15:50  lr: 0.000022  min_lr: 0.000006  loss: 1.1575 (1.1575)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8224 (8.8224)  time: 5.9416 (5.9416 -- 5.9416)  data: 5.3076 (5.3076 -- 5.3076)  max mem: 16413
[2023-09-04 04:15:50,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:15:50,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:15:50,864] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:15:50,864] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [106]  [ 20/160]  eta: 0:02:39  lr: 0.000022  min_lr: 0.000006  loss: 1.6534 (1.6547)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1191 (6.6344)  time: 0.8999 (0.5215 -- 2.3129)  data: 0.0943 (0.0005 -- 0.8901)  max mem: 16413
[2023-09-04 04:16:13,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=95, lr=[5.604708639631972e-06, 5.604708639631972e-06, 6.227454044035524e-06, 6.227454044035524e-06, 6.919393382261692e-06, 6.919393382261692e-06, 7.688214869179659e-06, 7.688214869179659e-06, 8.542460965755176e-06, 8.542460965755176e-06, 9.49162329528353e-06, 9.49162329528353e-06, 1.0546248105870587e-05, 1.0546248105870587e-05, 1.1718053450967318e-05, 1.1718053450967318e-05, 1.3020059389963686e-05, 1.3020059389963686e-05, 1.4466732655515206e-05, 1.4466732655515206e-05, 1.6074147395016896e-05, 1.6074147395016896e-05, 1.7860163772240996e-05, 1.7860163772240996e-05, 1.9844626413601107e-05, 1.9844626413601107e-05, 2.2049584904001228e-05, 2.2049584904001228e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 04:16:13,017] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=18.0673738267407, CurrSamplesPerSec=21.930131746410055, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:57  lr: 0.000022  min_lr: 0.000006  loss: 1.8247 (1.7143)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5431 (6.9681)  time: 0.8165 (0.5295 -- 3.7824)  data: 0.1114 (0.0004 -- 1.4270)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000006  loss: 1.7217 (1.7005)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4087 (7.0364)  time: 0.9262 (0.5194 -- 2.9264)  data: 0.2233 (0.0005 -- 2.1663)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000006  loss: 1.5258 (1.6637)  loss_scale: 32768.0000 (30138.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5629 (7.0398)  time: 0.8080 (0.5364 -- 2.3847)  data: 0.0935 (0.0003 -- 1.5287)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000006  loss: 1.6843 (1.6407)  loss_scale: 32768.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0361 (6.9203)  time: 0.9216 (0.5282 -- 3.7026)  data: 0.1346 (0.0007 -- 1.1216)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000006  loss: 1.5760 (1.6342)  loss_scale: 32768.0000 (31007.7355)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8423 (6.8069)  time: 0.9180 (0.5185 -- 2.9674)  data: 0.2264 (0.0004 -- 2.0510)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.7197 (1.6517)  loss_scale: 32768.0000 (31257.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0639 (6.8588)  time: 1.0049 (0.5205 -- 5.2843)  data: 0.4584 (0.0004 -- 4.7633)  max mem: 16413
[2023-09-04 04:17:45,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:17:45,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:17:45,719] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:17:45,719] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:17:55,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17117
[2023-09-04 04:17:55,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 04:17:55,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17117
[2023-09-04 04:17:55,369] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 04:17:55,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.7359 (1.6611)  loss_scale: 65536.0000 (34713.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5006 (6.8555)  time: 0.5844 (0.4852 -- 1.2688)  data: 0.0737 (0.0002 -- 0.7569)  max mem: 16413
Epoch: [106] Total time: 0:02:23 (0.8940 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.7359 (1.6789)  loss_scale: 65536.0000 (34713.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5006 (6.8555)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1752 (0.1752)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4042 (2.4042 -- 2.4042)  data: 2.1673 (2.1673 -- 2.1673)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4307 (0.5806)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4122 (0.1944 -- 2.4042)  data: 0.1990 (0.0007 -- 2.1673)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4915 (0.5561)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2266 (0.1712 -- 0.5652)  data: 0.0208 (0.0001 -- 0.3908)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5624 (0.6181)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2124 (0.1326 -- 0.5652)  data: 0.0201 (0.0001 -- 0.3908)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 85.062 Acc@5 96.888 loss 0.608
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.10%
Epoch: [107]  [  0/160]  eta: 0:17:23  lr: 0.000022  min_lr: 0.000006  loss: 1.3928 (1.3928)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6607 (6.6607)  time: 6.5236 (6.5236 -- 6.5236)  data: 6.0090 (6.0090 -- 6.0090)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000006  loss: 1.5756 (1.6657)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4054 (7.8690)  time: 0.8767 (0.5459 -- 3.9660)  data: 0.2932 (0.0006 -- 3.4497)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000006  loss: 1.7065 (1.7136)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8782 (7.5210)  time: 0.9762 (0.5273 -- 4.5344)  data: 0.4278 (0.0003 -- 4.0097)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000005  loss: 1.7158 (1.7179)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1644 (7.3490)  time: 0.7614 (0.5178 -- 3.4435)  data: 0.2139 (0.0001 -- 2.9293)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000005  loss: 1.6349 (1.6834)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1670 (7.2586)  time: 0.8776 (0.5309 -- 2.5880)  data: 0.0966 (0.0003 -- 1.1527)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000005  loss: 1.6957 (1.7065)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9142 (7.1766)  time: 0.8628 (0.5204 -- 3.3397)  data: 0.1259 (0.0003 -- 1.2510)  max mem: 16413
[2023-09-04 04:19:42,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17224
[2023-09-04 04:19:42,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17224
[2023-09-04 04:19:42,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:19:42,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:19:42,884] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000005  loss: 1.5282 (1.6911)  loss_scale: 16384.0000 (30466.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0718 (7.1477)  time: 0.8803 (0.5181 -- 3.3569)  data: 0.0721 (0.0003 -- 1.1005)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000005  loss: 1.7203 (1.6997)  loss_scale: 16384.0000 (28468.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6949 (7.1235)  time: 0.7653 (0.5268 -- 1.9489)  data: 0.0655 (0.0004 -- 0.9285)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5820 (1.6796)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4200 (7.0835)  time: 0.7259 (0.4970 -- 2.7162)  data: 0.0114 (0.0002 -- 0.2114)  max mem: 16413
Epoch: [107] Total time: 0:02:20 (0.8783 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5820 (1.6585)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4200 (7.0835)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1322 (0.1322)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3427 (2.3427 -- 2.3427)  data: 2.0891 (2.0891 -- 2.0891)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3394 (0.5814)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4303 (0.1971 -- 2.3427)  data: 0.2176 (0.0005 -- 2.0891)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3927 (0.5550)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2263 (0.1688 -- 0.5207)  data: 0.0259 (0.0001 -- 0.2834)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5866 (0.6044)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.6805)  time: 0.2126 (0.1326 -- 0.5207)  data: 0.0250 (0.0001 -- 0.2834)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 85.477 Acc@5 96.888 loss 0.595
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.10%
Epoch: [108]  [  0/160]  eta: 0:18:08  lr: 0.000021  min_lr: 0.000005  loss: 1.7305 (1.7305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7149 (6.7149)  time: 6.8015 (6.8015 -- 6.8015)  data: 5.1707 (5.1707 -- 5.1707)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000005  loss: 1.7234 (1.7143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4849 (7.4929)  time: 0.9340 (0.5140 -- 4.8458)  data: 0.2902 (0.0003 -- 4.3196)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000005  loss: 1.8193 (1.7212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9593 (7.3646)  time: 0.8103 (0.5186 -- 4.4982)  data: 0.2255 (0.0002 -- 3.9754)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000005  loss: 1.7765 (1.7185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2848 (7.1181)  time: 0.8953 (0.5291 -- 2.1182)  data: 0.1804 (0.0007 -- 1.5803)  max mem: 16413
[2023-09-04 04:21:42,893] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:21:42,894] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:21:42,894] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:21:42,894] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [108]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000005  loss: 1.6328 (1.7138)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3950 (7.2513)  time: 0.8443 (0.5271 -- 2.9845)  data: 0.0017 (0.0001 -- 0.0075)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000005  loss: 1.6906 (1.6917)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1513 (7.1436)  time: 0.9122 (0.5223 -- 2.7000)  data: 0.1267 (0.0004 -- 1.6123)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.6822 (1.6968)  loss_scale: 32768.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2333 (7.0948)  time: 0.9607 (0.5298 -- 3.7991)  data: 0.4135 (0.0005 -- 3.2673)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.7737 (1.7183)  loss_scale: 32768.0000 (24285.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5857 (7.1069)  time: 0.7160 (0.5223 -- 2.5958)  data: 0.1685 (0.0003 -- 2.0448)  max mem: 16413
[2023-09-04 04:22:44,962] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17424
[2023-09-04 04:22:44,962] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17424
[2023-09-04 04:22:44,962] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:22:44,962] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:22:44,962] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5710 (1.7082)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3993 (7.0913)  time: 0.6550 (0.4961 -- 2.3392)  data: 0.1302 (0.0002 -- 1.7707)  max mem: 16413
Epoch: [108] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5710 (1.6819)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3993 (7.0913)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1477 (0.1477)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3128 (2.3128 -- 2.3128)  data: 2.0804 (2.0804 -- 2.0804)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3162 (0.5318)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (97.9798)  time: 0.4343 (0.2006 -- 2.3128)  data: 0.2119 (0.0010 -- 2.0804)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4295 (0.5161)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2258 (0.1689 -- 0.4857)  data: 0.0180 (0.0001 -- 0.2352)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4534 (0.5657)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.0954)  time: 0.2091 (0.1332 -- 0.4857)  data: 0.0174 (0.0001 -- 0.2352)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 85.477 Acc@5 97.303 loss 0.557
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.10%
Epoch: [109]  [  0/160]  eta: 0:18:34  lr: 0.000021  min_lr: 0.000005  loss: 1.9691 (1.9691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7022 (5.7022)  time: 6.9682 (6.9682 -- 6.9682)  data: 5.1918 (5.1918 -- 5.1918)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:44  lr: 0.000021  min_lr: 0.000005  loss: 1.6375 (1.6680)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9089 (7.0628)  time: 0.8859 (0.5213 -- 2.7761)  data: 0.0252 (0.0007 -- 0.4632)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:01  lr: 0.000021  min_lr: 0.000005  loss: 1.8812 (1.7254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4108 (6.6564)  time: 0.8467 (0.5150 -- 3.0898)  data: 0.0159 (0.0003 -- 0.2942)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:35  lr: 0.000021  min_lr: 0.000005  loss: 1.7765 (1.7222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1031 (6.5300)  time: 0.8418 (0.5367 -- 2.6426)  data: 0.0069 (0.0006 -- 0.0473)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000005  loss: 1.8545 (1.7358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7713 (6.7012)  time: 0.9611 (0.5191 -- 3.9344)  data: 0.0036 (0.0004 -- 0.0394)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000005  loss: 1.7629 (1.7213)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2160 (6.6560)  time: 0.8187 (0.5148 -- 2.5794)  data: 0.0013 (0.0002 -- 0.0046)  max mem: 16413
[2023-09-04 04:24:48,512] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:24:48,512] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:24:48,514] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:24:48,514] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.6763 (1.7147)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0253 (6.6252)  time: 0.9771 (0.5273 -- 3.7397)  data: 0.0015 (0.0007 -- 0.0036)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.7480 (1.7202)  loss_scale: 32768.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5258 (6.6740)  time: 0.7496 (0.5378 -- 2.5392)  data: 0.0020 (0.0001 -- 0.0095)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.7052 (1.7180)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5935 (6.7709)  time: 0.7908 (0.4968 -- 2.5699)  data: 0.0008 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [109] Total time: 0:02:23 (0.8993 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.7052 (1.6735)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5935 (6.7709)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1377 (0.1377)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1925 (2.1925 -- 2.1925)  data: 1.9541 (1.9541 -- 1.9541)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3405 (0.5478)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4165 (0.1953 -- 2.1925)  data: 0.1989 (0.0008 -- 1.9541)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3524 (0.5215)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2271 (0.1719 -- 0.4403)  data: 0.0226 (0.0001 -- 0.2244)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4834 (0.5717)  acc1: 85.7143 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2119 (0.1331 -- 0.4403)  data: 0.0223 (0.0001 -- 0.2244)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 85.892 Acc@5 97.510 loss 0.563
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.10%
Epoch: [110]  [  0/160]  eta: 0:20:45  lr: 0.000021  min_lr: 0.000005  loss: 1.9286 (1.9286)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4021 (6.4021)  time: 7.7858 (7.7858 -- 7.7858)  data: 4.0407 (4.0407 -- 4.0407)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:55  lr: 0.000021  min_lr: 0.000005  loss: 1.7125 (1.7297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8635 (6.8942)  time: 0.9273 (0.5262 -- 4.1938)  data: 0.0089 (0.0003 -- 0.1500)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:12  lr: 0.000021  min_lr: 0.000005  loss: 1.6017 (1.7326)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7794 (6.9098)  time: 0.9521 (0.5220 -- 3.6346)  data: 0.0013 (0.0003 -- 0.0051)  max mem: 16413
[2023-09-04 04:26:22,142] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17647
[2023-09-04 04:26:22,143] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:26:22,143] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17647
[2023-09-04 04:26:22,143] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:26:22,143] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 60/160]  eta: 0:01:41  lr: 0.000020  min_lr: 0.000005  loss: 1.5896 (1.7218)  loss_scale: 16384.0000 (29007.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6398 (7.0213)  time: 0.8271 (0.5088 -- 3.5210)  data: 0.0014 (0.0001 -- 0.0050)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:19  lr: 0.000020  min_lr: 0.000005  loss: 1.6925 (1.7245)  loss_scale: 16384.0000 (25890.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4371 (7.1466)  time: 0.9535 (0.5240 -- 4.9054)  data: 0.0016 (0.0003 -- 0.0059)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:57  lr: 0.000020  min_lr: 0.000005  loss: 1.3493 (1.6952)  loss_scale: 16384.0000 (24008.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2858 (7.1658)  time: 0.7857 (0.5058 -- 3.1381)  data: 0.0014 (0.0003 -- 0.0056)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:38  lr: 0.000020  min_lr: 0.000005  loss: 1.5232 (1.6804)  loss_scale: 16384.0000 (22748.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0063 (7.0839)  time: 1.0141 (0.5245 -- 4.6355)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.6041 (1.6715)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9292 (7.0340)  time: 0.7869 (0.5136 -- 3.4943)  data: 0.0021 (0.0002 -- 0.0160)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.8677 (1.6869)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4822 (6.9819)  time: 0.6250 (0.4944 -- 2.7025)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [110] Total time: 0:02:24 (0.9043 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.8677 (1.6748)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4822 (6.9819)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1427 (0.1427)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3261 (2.3261 -- 2.3261)  data: 2.0782 (2.0782 -- 2.0782)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2727 (0.5511)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4058 (0.1995 -- 2.3261)  data: 0.1899 (0.0009 -- 2.0782)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3947 (0.5393)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2200 (0.1691 -- 0.5189)  data: 0.0160 (0.0001 -- 0.3056)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4507 (0.5742)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2058 (0.1329 -- 0.5189)  data: 0.0157 (0.0001 -- 0.3056)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 85.685 Acc@5 97.718 loss 0.564
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.10%
Epoch: [111]  [  0/160]  eta: 0:20:40  lr: 0.000020  min_lr: 0.000005  loss: 1.6265 (1.6265)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3350 (6.3350)  time: 7.7531 (7.7531 -- 7.7531)  data: 7.2055 (7.2055 -- 7.2055)  max mem: 16413
[2023-09-04 04:28:30,155] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:28:30,155] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:28:30,156] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:28:30,156] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [111]  [ 20/160]  eta: 0:02:58  lr: 0.000020  min_lr: 0.000005  loss: 1.6793 (1.6525)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3199 (7.0929)  time: 0.9523 (0.5165 -- 5.1177)  data: 0.4117 (0.0004 -- 4.6050)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:07  lr: 0.000020  min_lr: 0.000005  loss: 1.6592 (1.6084)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3858 (6.9394)  time: 0.8359 (0.5241 -- 4.0399)  data: 0.2922 (0.0002 -- 3.4925)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000005  loss: 1.5206 (1.5989)  loss_scale: 32768.0000 (28470.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5026 (6.9128)  time: 0.7712 (0.5317 -- 3.2169)  data: 0.2186 (0.0003 -- 2.6803)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000005  loss: 1.7990 (1.6293)  loss_scale: 32768.0000 (29531.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8993 (6.8189)  time: 0.7871 (0.5413 -- 2.9995)  data: 0.2322 (0.0003 -- 2.4610)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000005  loss: 1.6136 (1.6362)  loss_scale: 32768.0000 (30172.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5236 (6.8241)  time: 0.9191 (0.5219 -- 3.1169)  data: 0.3103 (0.0007 -- 2.5847)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000005  loss: 1.6260 (1.6334)  loss_scale: 32768.0000 (30601.5207)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2130 (7.0053)  time: 0.9245 (0.5382 -- 1.9986)  data: 0.2139 (0.0004 -- 1.4625)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.7808 (1.6393)  loss_scale: 32768.0000 (30908.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0894 (7.0778)  time: 0.8267 (0.5233 -- 2.0780)  data: 0.1013 (0.0003 -- 0.9106)  max mem: 16413
[2023-09-04 04:30:16,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:30:16,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:30:16,140] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:30:16,140] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:30:23,970] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17913
[2023-09-04 04:30:23,970] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 04:30:23,970] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17913
[2023-09-04 04:30:23,971] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 04:30:23,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6137 (1.6381)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6203 (7.0783)  time: 0.7289 (0.4815 -- 2.4814)  data: 0.0982 (0.0002 -- 1.9425)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8883 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6137 (1.6650)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6203 (7.0783)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1506 (0.1506)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4083 (2.4083 -- 2.4083)  data: 2.1601 (2.1601 -- 2.1601)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3312 (0.5204)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4492 (0.1968 -- 2.4083)  data: 0.2308 (0.0008 -- 2.1601)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4269 (0.5176)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2253 (0.1696 -- 0.6291)  data: 0.0216 (0.0001 -- 0.3666)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4667 (0.5670)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2111 (0.1329 -- 0.6291)  data: 0.0213 (0.0001 -- 0.3666)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 85.892 Acc@5 97.510 loss 0.566
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.10%
Epoch: [112]  [  0/160]  eta: 0:22:21  lr: 0.000020  min_lr: 0.000005  loss: 1.5389 (1.5389)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7242 (7.7242)  time: 8.3820 (8.3820 -- 8.3820)  data: 7.8375 (7.8375 -- 7.8375)  max mem: 16413
[2023-09-04 04:30:48,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17927
[2023-09-04 04:30:48,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17927
[2023-09-04 04:30:48,398] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:30:48,398] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:30:48,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [112]  [ 20/160]  eta: 0:02:44  lr: 0.000020  min_lr: 0.000005  loss: 1.6931 (1.6319)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4132 (7.3806)  time: 0.8135 (0.5178 -- 2.5342)  data: 0.0141 (0.0006 -- 0.2295)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:01:57  lr: 0.000020  min_lr: 0.000005  loss: 1.7070 (1.6344)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9485 (7.0048)  time: 0.7812 (0.5327 -- 2.0567)  data: 0.0280 (0.0005 -- 0.4879)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000005  loss: 1.7867 (1.6568)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3794 (6.8389)  time: 0.8971 (0.5172 -- 2.9594)  data: 0.0324 (0.0005 -- 0.3573)  max mem: 16413
[2023-09-04 04:31:49,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=101, lr=[5.0093528099075036e-06, 5.0093528099075036e-06, 5.565947566563894e-06, 5.565947566563894e-06, 6.184386185070991e-06, 6.184386185070991e-06, 6.8715402056344355e-06, 6.8715402056344355e-06, 7.63504467292715e-06, 7.63504467292715e-06, 8.483382969919056e-06, 8.483382969919056e-06, 9.42598107768784e-06, 9.42598107768784e-06, 1.0473312308542044e-05, 1.0473312308542044e-05, 1.1637013676157826e-05, 1.1637013676157826e-05, 1.2930015195730916e-05, 1.2930015195730916e-05, 1.436668355081213e-05, 1.436668355081213e-05, 1.5962981723124588e-05, 1.5962981723124588e-05, 1.773664635902732e-05, 1.773664635902732e-05, 1.970738484336369e-05, 1.970738484336369e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 04:31:49,896] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.984825149067646, CurrSamplesPerSec=16.462033898970315, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000005  loss: 1.6862 (1.6617)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4631 (6.8558)  time: 0.9286 (0.5241 -- 2.4413)  data: 0.0392 (0.0008 -- 0.7469)  max mem: 16413
[2023-09-04 04:32:10,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18020
[2023-09-04 04:32:10,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18020
[2023-09-04 04:32:10,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 04:32:10,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 04:32:10,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [112]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000005  loss: 1.7941 (1.6791)  loss_scale: 16384.0000 (17438.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8829 (6.8818)  time: 0.9106 (0.5200 -- 3.0245)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000005  loss: 1.4361 (1.6471)  loss_scale: 8192.0000 (15910.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2792 (6.9514)  time: 0.9069 (0.5288 -- 3.2461)  data: 0.0019 (0.0001 -- 0.0070)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.7023 (1.6374)  loss_scale: 8192.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0232 (7.0403)  time: 0.8270 (0.5241 -- 3.4685)  data: 0.0018 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.7134 (1.6459)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4087 (7.0125)  time: 0.6453 (0.4938 -- 3.0518)  data: 0.0015 (0.0002 -- 0.0167)  max mem: 16413
Epoch: [112] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.7134 (1.6413)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4087 (7.0125)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1528 (0.1528)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2227 (2.2227 -- 2.2227)  data: 1.9927 (1.9927 -- 1.9927)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3667 (0.5414)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4157 (0.1997 -- 2.2227)  data: 0.1991 (0.0006 -- 1.9927)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4886 (0.5494)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2265 (0.1693 -- 0.3937)  data: 0.0176 (0.0001 -- 0.1871)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5064 (0.5959)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.5104)  time: 0.2123 (0.1329 -- 0.3937)  data: 0.0173 (0.0001 -- 0.1871)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 85.685 Acc@5 97.303 loss 0.585
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.10%
Epoch: [113]  [  0/160]  eta: 0:19:59  lr: 0.000020  min_lr: 0.000005  loss: 0.8997 (0.8997)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3312 (6.3312)  time: 7.4963 (7.4963 -- 7.4963)  data: 6.5480 (6.5480 -- 6.5480)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:36  lr: 0.000019  min_lr: 0.000005  loss: 1.7917 (1.7555)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2458 (7.6023)  time: 0.7981 (0.5293 -- 3.2307)  data: 0.2459 (0.0003 -- 2.6795)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:02:03  lr: 0.000019  min_lr: 0.000005  loss: 1.6497 (1.6972)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7723 (7.4613)  time: 0.9398 (0.5285 -- 4.3488)  data: 0.3804 (0.0002 -- 3.8386)  max mem: 16413
Epoch: [113]  [ 60/160]  eta: 0:01:35  lr: 0.000019  min_lr: 0.000005  loss: 1.5786 (1.6930)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9112 (7.6075)  time: 0.7932 (0.5267 -- 1.7606)  data: 0.2157 (0.0008 -- 1.2191)  max mem: 16413
[2023-09-04 04:34:10,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:34:10,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 04:34:10,821] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:34:10,822] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [113]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000005  loss: 1.8326 (1.7167)  loss_scale: 16384.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7150 (7.6566)  time: 0.9590 (0.5288 -- 3.7378)  data: 0.0077 (0.0003 -- 0.1227)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:54  lr: 0.000019  min_lr: 0.000005  loss: 1.6605 (1.7156)  loss_scale: 16384.0000 (10787.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0050 (7.4943)  time: 0.7437 (0.5127 -- 2.7685)  data: 0.0018 (0.0003 -- 0.0059)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000005  loss: 1.5079 (1.6869)  loss_scale: 16384.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1066 (7.3337)  time: 0.8709 (0.5347 -- 1.9740)  data: 0.0051 (0.0004 -- 0.0702)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.6879 (1.6927)  loss_scale: 16384.0000 (12375.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3368 (7.3653)  time: 0.9334 (0.5223 -- 3.6845)  data: 0.0520 (0.0006 -- 0.4547)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.9209 (1.7111)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3368 (7.3510)  time: 0.7142 (0.4937 -- 2.6215)  data: 0.0356 (0.0002 -- 0.6898)  max mem: 16413
Epoch: [113] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.9209 (1.6790)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3368 (7.3510)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1767 (0.1767)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2783 (2.2783 -- 2.2783)  data: 2.0384 (2.0384 -- 2.0384)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3160 (0.5537)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4057 (0.2072 -- 2.2783)  data: 0.1877 (0.0006 -- 2.0384)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4515 (0.5321)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2256 (0.1695 -- 0.5821)  data: 0.0208 (0.0001 -- 0.3869)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5093 (0.5804)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2104 (0.1330 -- 0.5821)  data: 0.0205 (0.0001 -- 0.3869)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.591
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.10%
Epoch: [114]  [  0/160]  eta: 0:19:06  lr: 0.000019  min_lr: 0.000005  loss: 1.6957 (1.6957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5140 (7.5140)  time: 7.1679 (7.1679 -- 7.1679)  data: 4.2226 (4.2226 -- 4.2226)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:39  lr: 0.000019  min_lr: 0.000005  loss: 1.5981 (1.6615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1411 (7.8674)  time: 0.8352 (0.5429 -- 2.4400)  data: 0.1611 (0.0008 -- 1.9001)  max mem: 16413
[2023-09-04 04:36:14,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:36:14,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:36:14,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:36:14,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [114]  [ 40/160]  eta: 0:02:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7246 (1.7096)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4613 (7.3534)  time: 0.8652 (0.5300 -- 2.9533)  data: 0.0660 (0.0004 -- 0.6636)  max mem: 16413
[2023-09-04 04:36:24,606] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18289
[2023-09-04 04:36:24,606] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18289
[2023-09-04 04:36:24,606] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:36:24,606] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:36:24,606] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [114]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000005  loss: 1.5763 (1.6559)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4472 (7.1916)  time: 0.8887 (0.5145 -- 3.9986)  data: 0.0017 (0.0004 -- 0.0105)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000005  loss: 1.6731 (1.6523)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2400 (7.2336)  time: 0.9426 (0.5351 -- 3.1896)  data: 0.0019 (0.0005 -- 0.0090)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.5641 (1.6444)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7577 (7.2424)  time: 0.7688 (0.5230 -- 4.2950)  data: 0.0068 (0.0003 -- 0.1033)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000005  loss: 1.5199 (1.6380)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8548 (7.1942)  time: 0.9707 (0.5193 -- 4.1506)  data: 0.0015 (0.0005 -- 0.0052)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.7354 (1.6611)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4042 (7.2751)  time: 0.8545 (0.5191 -- 3.6526)  data: 0.0598 (0.0005 -- 1.1351)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7398 (1.6662)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3305 (7.2693)  time: 0.6710 (0.4945 -- 3.1212)  data: 0.0014 (0.0002 -- 0.0129)  max mem: 16413
Epoch: [114] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.7398 (1.6554)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3305 (7.2693)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1376 (0.1376)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3684 (2.3684 -- 2.3684)  data: 2.1521 (2.1521 -- 2.1521)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3286 (0.5394)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4130 (0.2016 -- 2.3684)  data: 0.1975 (0.0005 -- 2.1521)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4036 (0.5079)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2224 (0.1690 -- 0.5179)  data: 0.0173 (0.0001 -- 0.3240)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4401 (0.5557)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2070 (0.1330 -- 0.5179)  data: 0.0170 (0.0001 -- 0.3240)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 86.100 Acc@5 97.510 loss 0.570
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.10%
Epoch: [115]  [  0/160]  eta: 0:21:45  lr: 0.000019  min_lr: 0.000005  loss: 1.4699 (1.4699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2938 (5.2938)  time: 8.1568 (8.1568 -- 8.1568)  data: 5.6331 (5.6331 -- 5.6331)  max mem: 16413
[2023-09-04 04:38:30,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:38:30,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:38:30,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:38:30,158] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [ 20/160]  eta: 0:02:51  lr: 0.000019  min_lr: 0.000005  loss: 1.5679 (1.5963)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5011 (7.6774)  time: 0.8800 (0.5243 -- 2.9741)  data: 0.0273 (0.0002 -- 0.5125)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:07  lr: 0.000019  min_lr: 0.000005  loss: 1.4271 (1.5723)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8349 (7.3859)  time: 0.8879 (0.5234 -- 3.7855)  data: 0.0196 (0.0003 -- 0.3633)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000005  loss: 1.5107 (1.5715)  loss_scale: 32768.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1476 (7.2977)  time: 0.7974 (0.5359 -- 2.4074)  data: 0.0019 (0.0001 -- 0.0125)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000005  loss: 1.6622 (1.6068)  loss_scale: 32768.0000 (29127.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1305 (7.6518)  time: 0.8580 (0.5294 -- 3.1638)  data: 0.0718 (0.0002 -- 0.7714)  max mem: 16413
[2023-09-04 04:39:31,125] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18489
[2023-09-04 04:39:31,125] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18489
[2023-09-04 04:39:31,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:39:31,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:39:31,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.6508 (1.6159)  loss_scale: 16384.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8222 (7.5088)  time: 0.8304 (0.5256 -- 2.5350)  data: 0.0597 (0.0002 -- 1.0276)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000005  loss: 1.6374 (1.6294)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7923 (7.4073)  time: 0.9292 (0.5297 -- 3.7227)  data: 0.3514 (0.0001 -- 3.1817)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.6760 (1.6279)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8909 (7.3226)  time: 0.8052 (0.5230 -- 3.1651)  data: 0.2590 (0.0002 -- 2.6267)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.7431 (1.6365)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8208 (7.3068)  time: 0.7731 (0.4949 -- 4.7504)  data: 0.2588 (0.0002 -- 4.2368)  max mem: 16413
Epoch: [115] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.7431 (1.6387)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8208 (7.3068)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1357 (0.1357)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2802 (2.2802 -- 2.2802)  data: 2.0388 (2.0388 -- 2.0388)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3466 (0.5403)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4242 (0.1985 -- 2.2802)  data: 0.2087 (0.0005 -- 2.0388)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3466 (0.5001)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2282 (0.1703 -- 0.4904)  data: 0.0193 (0.0001 -- 0.2450)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4903 (0.5667)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2134 (0.1333 -- 0.4904)  data: 0.0189 (0.0001 -- 0.2450)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.580
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.10%
Epoch: [116]  [  0/160]  eta: 0:23:10  lr: 0.000018  min_lr: 0.000005  loss: 1.9290 (1.9290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0545 (6.0545)  time: 8.6937 (8.6937 -- 8.6937)  data: 8.1653 (8.1653 -- 8.1653)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000005  loss: 1.5879 (1.6323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4637 (7.1025)  time: 0.7998 (0.5234 -- 3.4857)  data: 0.2565 (0.0005 -- 2.9693)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:06  lr: 0.000018  min_lr: 0.000005  loss: 1.7614 (1.6688)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4666 (6.9237)  time: 0.9195 (0.5132 -- 3.6670)  data: 0.3841 (0.0003 -- 3.1456)  max mem: 16413
[2023-09-04 04:41:36,410] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:41:36,410] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:41:36,411] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:41:36,412] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [116]  [ 60/160]  eta: 0:01:40  lr: 0.000018  min_lr: 0.000005  loss: 1.6387 (1.6543)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4330 (6.9204)  time: 0.9117 (0.5127 -- 3.2849)  data: 0.3722 (0.0005 -- 2.7625)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:18  lr: 0.000018  min_lr: 0.000005  loss: 1.6151 (1.6395)  loss_scale: 32768.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9671 (6.9472)  time: 0.8850 (0.5261 -- 2.8813)  data: 0.2063 (0.0005 -- 2.3617)  max mem: 16413
[2023-09-04 04:42:04,853] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18654
[2023-09-04 04:42:04,853] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18654
[2023-09-04 04:42:04,853] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:42:04,853] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:42:04,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000005  loss: 1.6193 (1.6421)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0696 (6.9937)  time: 0.7634 (0.5320 -- 2.4666)  data: 0.1676 (0.0011 -- 1.9311)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000005  loss: 1.4370 (1.6207)  loss_scale: 16384.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5170 (6.9185)  time: 0.9056 (0.5333 -- 2.7030)  data: 0.2495 (0.0004 -- 1.6118)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.7327 (1.6321)  loss_scale: 16384.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5417 (6.8776)  time: 0.7507 (0.5295 -- 2.5438)  data: 0.0627 (0.0004 -- 0.9243)  max mem: 16413
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.5819 (1.6258)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3514 (6.8458)  time: 0.7576 (0.4967 -- 3.4439)  data: 0.2232 (0.0002 -- 2.9226)  max mem: 16413
Epoch: [116] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.5819 (1.6349)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3514 (6.8458)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1726 (0.1726)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5770 (2.5770 -- 2.5770)  data: 2.3560 (2.3560 -- 2.3560)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4118 (0.5321)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4364 (0.2088 -- 2.5770)  data: 0.2191 (0.0007 -- 2.3560)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4341 (0.5137)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2153 (0.1689 -- 0.3465)  data: 0.0103 (0.0001 -- 0.1496)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5170 (0.5703)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.1980 (0.1330 -- 0.3465)  data: 0.0100 (0.0001 -- 0.1496)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.572
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.10%
Epoch: [117]  [  0/160]  eta: 0:16:20  lr: 0.000018  min_lr: 0.000005  loss: 1.4296 (1.4296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1286 (7.1286)  time: 6.1254 (6.1254 -- 6.1254)  data: 5.5753 (5.5753 -- 5.5753)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:39  lr: 0.000018  min_lr: 0.000005  loss: 1.6746 (1.5924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6852 (7.0213)  time: 0.8908 (0.5404 -- 2.6840)  data: 0.0989 (0.0007 -- 1.0531)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000005  loss: 1.6552 (1.6209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6600 (7.4108)  time: 0.8926 (0.5330 -- 3.6703)  data: 0.2827 (0.0007 -- 3.1456)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000005  loss: 1.8405 (1.6878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6543 (7.5403)  time: 0.8579 (0.5176 -- 2.7687)  data: 0.2923 (0.0003 -- 2.2425)  max mem: 16413
[2023-09-04 04:44:06,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:44:06,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:44:06,968] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:44:06,968] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:44:14,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18792
[2023-09-04 04:44:14,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18792
[2023-09-04 04:44:14,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:44:14,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:44:14,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [117]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000005  loss: 1.7476 (1.6971)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2451 (7.2878)  time: 0.8670 (0.5243 -- 2.6084)  data: 0.2849 (0.0007 -- 2.0709)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000005  loss: 1.6104 (1.6834)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4437 (7.1996)  time: 0.9356 (0.5212 -- 3.4381)  data: 0.3962 (0.0004 -- 2.9028)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000005  loss: 1.6189 (1.6710)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3547 (7.1915)  time: 0.8853 (0.5190 -- 3.6376)  data: 0.3181 (0.0002 -- 2.7145)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.7411 (1.6744)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2075 (7.1960)  time: 0.8791 (0.5320 -- 2.4325)  data: 0.3368 (0.0003 -- 1.9165)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000004  loss: 1.7339 (1.6701)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8517 (7.1204)  time: 0.6925 (0.4962 -- 1.8425)  data: 0.1769 (0.0003 -- 1.3155)  max mem: 16413
Epoch: [117] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000004  loss: 1.7339 (1.6604)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8517 (7.1204)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1603 (0.1603)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3869 (2.3869 -- 2.3869)  data: 2.1667 (2.1667 -- 2.1667)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3331 (0.5460)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4120 (0.1919 -- 2.3869)  data: 0.1981 (0.0007 -- 2.1667)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4332 (0.5305)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2181 (0.1697 -- 0.4586)  data: 0.0149 (0.0001 -- 0.2768)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5636 (0.5942)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2044 (0.1334 -- 0.4586)  data: 0.0146 (0.0001 -- 0.2768)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 85.270 Acc@5 97.510 loss 0.574
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.10%
Epoch: [118]  [  0/160]  eta: 0:24:19  lr: 0.000018  min_lr: 0.000004  loss: 1.3573 (1.3573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1084 (6.1084)  time: 9.1210 (9.1210 -- 9.1210)  data: 8.5854 (8.5854 -- 8.5854)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:02:42  lr: 0.000018  min_lr: 0.000004  loss: 1.6123 (1.6759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3454 (6.7539)  time: 0.7608 (0.5255 -- 2.9046)  data: 0.2170 (0.0006 -- 2.3524)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000004  loss: 1.5501 (1.6556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6858 (7.0311)  time: 0.9105 (0.5301 -- 2.3669)  data: 0.1652 (0.0004 -- 1.6923)  max mem: 16413
[2023-09-04 04:46:21,202] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:46:21,202] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:46:21,204] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:46:21,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [ 60/160]  eta: 0:01:37  lr: 0.000018  min_lr: 0.000004  loss: 1.5634 (1.6483)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8360 (6.9664)  time: 0.8403 (0.5202 -- 3.2461)  data: 0.0761 (0.0002 -- 1.1955)  max mem: 16413
[2023-09-04 04:46:46,030] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18949
[2023-09-04 04:46:46,030] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18949
[2023-09-04 04:46:46,030] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:46:46,030] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:46:46,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [118]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.7801 (1.6733)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8725 (6.9563)  time: 0.8751 (0.5308 -- 2.1490)  data: 0.2076 (0.0002 -- 1.4793)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.6045 (1.6579)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1203 (7.0495)  time: 0.8976 (0.5341 -- 3.1759)  data: 0.3020 (0.0009 -- 2.6660)  max mem: 16413
[2023-09-04 04:47:28,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=107, lr=[4.423662355504158e-06, 4.423662355504158e-06, 4.91518039500462e-06, 4.91518039500462e-06, 5.4613115500051325e-06, 5.4613115500051325e-06, 6.068123944450148e-06, 6.068123944450148e-06, 6.742359938277941e-06, 6.742359938277941e-06, 7.491511042531047e-06, 7.491511042531047e-06, 8.323901158367829e-06, 8.323901158367829e-06, 9.248779064853141e-06, 9.248779064853141e-06, 1.0276421183170158e-05, 1.0276421183170158e-05, 1.1418245759077952e-05, 1.1418245759077952e-05, 1.2686939732308837e-05, 1.2686939732308837e-05, 1.4096599702565375e-05, 1.4096599702565375e-05, 1.566288855840597e-05, 1.566288855840597e-05, 1.7403209509339967e-05, 1.7403209509339967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 04:47:28,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.952998776965142, CurrSamplesPerSec=23.271167029539498, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000004  loss: 1.6187 (1.6379)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5952 (7.1929)  time: 0.8462 (0.5297 -- 2.4965)  data: 0.1579 (0.0002 -- 1.9494)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6440 (1.6398)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1170 (7.2679)  time: 0.9346 (0.5239 -- 3.6078)  data: 0.2788 (0.0005 -- 3.0623)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.6116 (1.6225)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6522 (7.2103)  time: 0.7099 (0.4960 -- 3.3486)  data: 0.0362 (0.0001 -- 0.7114)  max mem: 16413
Epoch: [118] Total time: 0:02:23 (0.8963 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.6116 (1.6414)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6522 (7.2103)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1424 (0.1424)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3539 (2.3539 -- 2.3539)  data: 2.1248 (2.1248 -- 2.1248)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2874 (0.5448)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4132 (0.1956 -- 2.3539)  data: 0.2058 (0.0006 -- 2.1248)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4715 (0.5410)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2190 (0.1704 -- 0.3370)  data: 0.0201 (0.0001 -- 0.1365)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5590 (0.5972)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2080 (0.1330 -- 0.3370)  data: 0.0198 (0.0001 -- 0.1365)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 85.270 Acc@5 97.718 loss 0.574
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.10%
Epoch: [119]  [  0/160]  eta: 0:22:05  lr: 0.000017  min_lr: 0.000004  loss: 1.6315 (1.6315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2966 (8.2966)  time: 8.2852 (8.2852 -- 8.2852)  data: 7.0861 (7.0861 -- 7.0861)  max mem: 16413
Epoch: [119]  [ 20/160]  eta: 0:02:38  lr: 0.000017  min_lr: 0.000004  loss: 1.6897 (1.6752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8964 (7.2475)  time: 0.7759 (0.5116 -- 3.1937)  data: 0.1857 (0.0003 -- 1.8421)  max mem: 16413
[2023-09-04 04:48:49,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:48:49,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:48:49,010] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:48:49,010] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [ 40/160]  eta: 0:02:05  lr: 0.000017  min_lr: 0.000004  loss: 1.7740 (1.7261)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1865 (7.5404)  time: 0.9501 (0.5167 -- 2.7432)  data: 0.2598 (0.0008 -- 1.6125)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000004  loss: 1.7601 (1.7066)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5926 (7.4197)  time: 0.8558 (0.5299 -- 2.9312)  data: 0.1902 (0.0004 -- 2.0202)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.7542 (1.7244)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7247 (7.3362)  time: 0.8428 (0.5196 -- 3.6889)  data: 0.2616 (0.0002 -- 2.7368)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.4352 (1.6880)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9862 (7.4561)  time: 0.9562 (0.5195 -- 3.7921)  data: 0.4207 (0.0003 -- 3.2661)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000004  loss: 1.5959 (1.6720)  loss_scale: 32768.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4852 (7.5846)  time: 0.7888 (0.5281 -- 2.7913)  data: 0.1500 (0.0007 -- 1.0708)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.7656 (1.6900)  loss_scale: 32768.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2097 (7.5481)  time: 0.9268 (0.5393 -- 2.9771)  data: 0.3759 (0.0004 -- 2.4521)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.6585 (1.6817)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0357 (7.4607)  time: 0.6235 (0.4949 -- 2.1909)  data: 0.1061 (0.0002 -- 1.6654)  max mem: 16413
Epoch: [119] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.6585 (1.6701)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0357 (7.4607)
[2023-09-04 04:50:30,951] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-09-04 04:50:30,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-09-04 04:50:30,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-09-04 04:50:30,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-09-04 04:50:31,901] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-09-04 04:50:31,901] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1655 (0.1655)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5448 (2.5448 -- 2.5448)  data: 2.2671 (2.2671 -- 2.2671)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2827 (0.4833)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (97.9798)  time: 0.4515 (0.2047 -- 2.5448)  data: 0.2212 (0.0006 -- 2.2671)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4007 (0.4841)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (96.8254)  time: 0.2242 (0.1693 -- 0.3889)  data: 0.0150 (0.0001 -- 0.1563)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4878 (0.5525)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.2045 (0.1326 -- 0.3889)  data: 0.0147 (0.0001 -- 0.1563)  max mem: 16413
Val: Total time: 0:00:08 (0.2968 s / it)
* Acc@1 86.100 Acc@5 97.095 loss 0.563
Accuracy of the network on the 482 val images: 86.10%
[2023-09-04 04:50:39,915] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 04:50:39,917] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 04:50:39,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 04:50:39,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 04:50:41,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 04:50:41,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.10%
Epoch: [120]  [  0/160]  eta: 0:21:03  lr: 0.000017  min_lr: 0.000004  loss: 1.7024 (1.7024)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9559 (8.9559)  time: 7.8950 (7.8950 -- 7.8950)  data: 6.1791 (6.1791 -- 6.1791)  max mem: 16413
[2023-09-04 04:50:53,538] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19205
[2023-09-04 04:50:53,538] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19205
[2023-09-04 04:50:53,538] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:50:53,538] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:50:53,539] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [120]  [ 20/160]  eta: 0:02:41  lr: 0.000017  min_lr: 0.000004  loss: 1.7343 (1.7696)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1139 (7.2062)  time: 0.8162 (0.5205 -- 3.3270)  data: 0.0020 (0.0005 -- 0.0077)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:07  lr: 0.000017  min_lr: 0.000004  loss: 1.7379 (1.7443)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5837 (7.0014)  time: 0.9695 (0.5123 -- 3.9570)  data: 0.4168 (0.0003 -- 3.4346)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000004  loss: 1.6378 (1.7140)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4304 (7.0506)  time: 0.8242 (0.5219 -- 4.5304)  data: 0.2786 (0.0003 -- 4.0161)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:14  lr: 0.000017  min_lr: 0.000004  loss: 1.4143 (1.6663)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5151 (7.1681)  time: 0.7603 (0.5298 -- 2.1908)  data: 0.0746 (0.0004 -- 0.4794)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000004  loss: 1.7501 (1.6961)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4606 (7.1930)  time: 0.8707 (0.5065 -- 2.6324)  data: 0.2007 (0.0004 -- 2.1002)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000004  loss: 1.5553 (1.6944)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8489 (7.1634)  time: 0.8973 (0.5247 -- 2.9173)  data: 0.0961 (0.0004 -- 0.9707)  max mem: 16413
[2023-09-04 04:52:44,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:52:44,909] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:52:44,910] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:52:44,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6700 (1.6775)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9896 (7.1326)  time: 0.8440 (0.5189 -- 3.0913)  data: 0.2819 (0.0004 -- 2.5459)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.7960 (1.6803)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4294 (7.1117)  time: 0.6968 (0.4951 -- 2.3622)  data: 0.1718 (0.0003 -- 1.8227)  max mem: 16413
Epoch: [120] Total time: 0:02:20 (0.8808 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.7960 (1.6618)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4294 (7.1117)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1581 (0.1581)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3751 (2.3751 -- 2.3751)  data: 2.1309 (2.1309 -- 2.1309)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3216 (0.4847)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (97.9798)  time: 0.4233 (0.2017 -- 2.3751)  data: 0.2070 (0.0007 -- 2.1309)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4512 (0.4846)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2194 (0.1685 -- 0.3649)  data: 0.0163 (0.0001 -- 0.1758)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5288 (0.5451)  acc1: 77.7778 (85.0622)  acc5: 100.0000 (97.0954)  time: 0.2045 (0.1331 -- 0.3649)  data: 0.0159 (0.0001 -- 0.1758)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 85.062 Acc@5 97.718 loss 0.567
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.10%
Epoch: [121]  [  0/160]  eta: 0:19:25  lr: 0.000017  min_lr: 0.000004  loss: 2.2425 (2.2425)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3312 (7.3312)  time: 7.2863 (7.2863 -- 7.2863)  data: 5.9400 (5.9400 -- 5.9400)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:50  lr: 0.000017  min_lr: 0.000004  loss: 1.5823 (1.6129)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4561 (6.8859)  time: 0.9133 (0.5281 -- 2.6996)  data: 0.0026 (0.0005 -- 0.0215)  max mem: 16413
Epoch: [121]  [ 40/160]  eta: 0:02:13  lr: 0.000016  min_lr: 0.000004  loss: 1.3619 (1.5441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6751 (6.7941)  time: 0.9947 (0.5380 -- 3.8785)  data: 0.0296 (0.0005 -- 0.5572)  max mem: 16413
Epoch: [121]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000004  loss: 1.7295 (1.5903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5615 (6.7691)  time: 0.6668 (0.5136 -- 1.5376)  data: 0.0243 (0.0001 -- 0.4616)  max mem: 16413
Epoch: [121]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000004  loss: 1.5851 (1.6128)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0616 (6.9553)  time: 0.9158 (0.5134 -- 3.6455)  data: 0.0181 (0.0004 -- 0.3400)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000004  loss: 1.5396 (1.6220)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5363 (7.0937)  time: 0.9180 (0.5326 -- 2.8123)  data: 0.2795 (0.0004 -- 2.2868)  max mem: 16413
[2023-09-04 04:54:46,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:54:46,609] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:54:46,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:54:46,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 04:54:49,973] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19467
[2023-09-04 04:54:49,974] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 04:54:49,974] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19467
[2023-09-04 04:54:49,974] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 04:54:49,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 04:54:58,654] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19476
[2023-09-04 04:54:58,654] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19476
[2023-09-04 04:54:58,655] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:54:58,655] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 04:54:58,655] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 1.7702 (1.6448)  loss_scale: 32768.0000 (33445.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1574 (7.1603)  time: 0.7691 (0.5168 -- 3.4353)  data: 0.2143 (0.0004 -- 2.9274)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.7427 (1.6515)  loss_scale: 16384.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8160 (7.1519)  time: 0.9017 (0.5323 -- 3.0384)  data: 0.3560 (0.0005 -- 2.4912)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.6008 (1.6462)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8632 (7.1572)  time: 0.6840 (0.4962 -- 3.4586)  data: 0.1667 (0.0001 -- 2.9423)  max mem: 16413
Epoch: [121] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.6008 (1.6443)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8632 (7.1572)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1584 (0.1584)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3089 (2.3089 -- 2.3089)  data: 2.0991 (2.0991 -- 2.0991)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4002 (0.5187)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4347 (0.1970 -- 2.3089)  data: 0.2132 (0.0008 -- 2.0991)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4175 (0.4959)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2258 (0.1684 -- 0.5119)  data: 0.0194 (0.0001 -- 0.2363)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4951 (0.5596)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2083 (0.1326 -- 0.5119)  data: 0.0191 (0.0001 -- 0.2363)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.10%
Epoch: [122]  [  0/160]  eta: 0:19:20  lr: 0.000016  min_lr: 0.000004  loss: 1.8923 (1.8923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8474 (7.8474)  time: 7.2534 (7.2534 -- 7.2534)  data: 6.6707 (6.6707 -- 6.6707)  max mem: 16413
[2023-09-04 04:56:03,155] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19538
[2023-09-04 04:56:03,155] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19538
[2023-09-04 04:56:03,155] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 04:56:03,156] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 04:56:03,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [122]  [ 20/160]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000004  loss: 1.6568 (1.6594)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5378 (7.8526)  time: 0.8580 (0.5226 -- 3.5612)  data: 0.3040 (0.0005 -- 3.0358)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:06  lr: 0.000016  min_lr: 0.000004  loss: 1.5652 (1.6874)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7182 (7.5170)  time: 0.9427 (0.5239 -- 3.0514)  data: 0.4004 (0.0002 -- 2.5165)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000004  loss: 1.7541 (1.7114)  loss_scale: 8192.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9538 (7.4050)  time: 0.7948 (0.5233 -- 2.1902)  data: 0.2009 (0.0003 -- 1.4026)  max mem: 16413
Epoch: [122]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000004  loss: 1.6327 (1.7069)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3266 (7.3856)  time: 0.9311 (0.5201 -- 3.3668)  data: 0.2070 (0.0002 -- 1.9066)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000004  loss: 1.6756 (1.6993)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1941 (7.2237)  time: 0.8601 (0.5307 -- 3.7566)  data: 0.3146 (0.0003 -- 3.2389)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000004  loss: 1.7308 (1.6994)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7932 (7.2693)  time: 0.8590 (0.5272 -- 3.1749)  data: 0.3139 (0.0006 -- 2.6243)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.4980 (1.6783)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3503 (7.1541)  time: 0.8762 (0.5128 -- 3.0174)  data: 0.3282 (0.0001 -- 2.4895)  max mem: 16413
[2023-09-04 04:57:55,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:57:55,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 04:57:55,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:57:55,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.4692 (1.6551)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8042 (7.0972)  time: 0.6757 (0.4948 -- 2.5232)  data: 0.1575 (0.0002 -- 1.9834)  max mem: 16413
Epoch: [122] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.4692 (1.6218)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8042 (7.0972)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1540 (0.1540)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5772 (2.5772 -- 2.5772)  data: 2.3281 (2.3281 -- 2.3281)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3661 (0.5119)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4314 (0.2057 -- 2.5772)  data: 0.2128 (0.0010 -- 2.3281)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4423 (0.4920)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (97.8836)  time: 0.2182 (0.1717 -- 0.4612)  data: 0.0146 (0.0001 -- 0.2752)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5000 (0.5482)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.5104)  time: 0.2011 (0.1320 -- 0.4612)  data: 0.0142 (0.0001 -- 0.2752)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 86.307 Acc@5 97.718 loss 0.547
Accuracy of the network on the 482 val images: 86.31%
[2023-09-04 04:58:10,426] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 04:58:10,428] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 04:58:10,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 04:58:10,428] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 04:58:11,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 04:58:11,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.31%
Epoch: [123]  [  0/160]  eta: 0:19:28  lr: 0.000016  min_lr: 0.000004  loss: 0.7941 (0.7941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7948 (6.7948)  time: 7.3062 (7.3062 -- 7.3062)  data: 6.7853 (6.7853 -- 6.7853)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:39  lr: 0.000016  min_lr: 0.000004  loss: 1.7188 (1.6510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6624 (7.9161)  time: 0.8307 (0.5407 -- 2.4499)  data: 0.1293 (0.0008 -- 1.1013)  max mem: 16413
Epoch: [123]  [ 40/160]  eta: 0:02:03  lr: 0.000016  min_lr: 0.000004  loss: 1.6001 (1.6567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2985 (7.3242)  time: 0.9146 (0.5317 -- 3.8610)  data: 0.3640 (0.0003 -- 3.3304)  max mem: 16413
Epoch: [123]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000004  loss: 1.8207 (1.7066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3109 (7.1550)  time: 0.8480 (0.5300 -- 4.0390)  data: 0.3026 (0.0005 -- 3.5095)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000004  loss: 1.7926 (1.6840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9596 (7.0465)  time: 0.8974 (0.5262 -- 4.7543)  data: 0.3503 (0.0003 -- 4.2148)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000004  loss: 1.7981 (1.7034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6120 (7.2289)  time: 0.7749 (0.5365 -- 2.8147)  data: 0.2144 (0.0006 -- 2.2943)  max mem: 16413
[2023-09-04 04:59:56,227] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:59:56,227] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 04:59:56,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 04:59:56,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [123]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 1.8657 (1.7086)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4916 (7.1338)  time: 0.8814 (0.5283 -- 3.6587)  data: 0.0962 (0.0006 -- 1.8494)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000004  loss: 1.5781 (1.6942)  loss_scale: 32768.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8140 (7.1759)  time: 0.8206 (0.5336 -- 2.9413)  data: 0.1121 (0.0006 -- 1.5514)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.6099 (1.6795)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7398 (7.1505)  time: 0.7163 (0.4951 -- 2.8849)  data: 0.0701 (0.0002 -- 0.7757)  max mem: 16413
Epoch: [123] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.6099 (1.6694)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7398 (7.1505)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1365 (0.1365)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3122 (2.3122 -- 2.3122)  data: 2.1057 (2.1057 -- 2.1057)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2844 (0.5227)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4050 (0.1985 -- 2.3122)  data: 0.1954 (0.0006 -- 2.1057)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4501 (0.5069)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2214 (0.1693 -- 0.4710)  data: 0.0161 (0.0001 -- 0.2741)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5070 (0.5682)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2083 (0.1334 -- 0.4710)  data: 0.0157 (0.0001 -- 0.2741)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 84.647 Acc@5 97.303 loss 0.562
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [124]  [  0/160]  eta: 0:22:21  lr: 0.000016  min_lr: 0.000004  loss: 2.1849 (2.1849)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7199 (5.7199)  time: 8.3849 (8.3849 -- 8.3849)  data: 6.5436 (6.5436 -- 6.5436)  max mem: 16413
[2023-09-04 05:01:05,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19859
[2023-09-04 05:01:05,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19859
[2023-09-04 05:01:05,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:01:05,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:01:05,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [124]  [ 20/160]  eta: 0:02:55  lr: 0.000015  min_lr: 0.000004  loss: 1.6412 (1.6684)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4947 (6.7535)  time: 0.8967 (0.5092 -- 4.4981)  data: 0.0349 (0.0003 -- 0.6670)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:11  lr: 0.000015  min_lr: 0.000004  loss: 1.6159 (1.6512)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7304 (6.7329)  time: 0.9368 (0.5173 -- 3.4459)  data: 0.0349 (0.0002 -- 0.6668)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000004  loss: 1.7543 (1.6756)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4410 (6.8908)  time: 0.8308 (0.5128 -- 3.7454)  data: 0.2247 (0.0002 -- 2.5651)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000004  loss: 1.6943 (1.6754)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9246 (7.0634)  time: 0.8382 (0.5304 -- 3.1120)  data: 0.2320 (0.0004 -- 2.5875)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 1.7600 (1.6880)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6076 (7.0438)  time: 0.8649 (0.5242 -- 4.4139)  data: 0.2896 (0.0003 -- 3.8977)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000004  loss: 1.7651 (1.6868)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7148 (6.9028)  time: 0.8090 (0.5265 -- 3.5111)  data: 0.2583 (0.0005 -- 2.9691)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.7187 (1.6835)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9782 (6.9286)  time: 0.8033 (0.5230 -- 3.8953)  data: 0.2564 (0.0004 -- 3.3441)  max mem: 16413
[2023-09-04 05:02:54,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:02:54,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:02:54,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:02:54,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:03:02,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=112, lr=[3.853570513929506e-06, 3.853570513929506e-06, 4.281745015477229e-06, 4.281745015477229e-06, 4.757494461641365e-06, 4.757494461641365e-06, 5.286104957379294e-06, 5.286104957379294e-06, 5.87344995264366e-06, 5.87344995264366e-06, 6.5260555029374e-06, 6.5260555029374e-06, 7.251172781041555e-06, 7.251172781041555e-06, 8.056858645601727e-06, 8.056858645601727e-06, 8.952065161779698e-06, 8.952065161779698e-06, 9.946739068644107e-06, 9.946739068644107e-06, 1.1051932298493453e-05, 1.1051932298493453e-05, 1.2279924776103836e-05, 1.2279924776103836e-05, 1.3644360862337596e-05, 1.3644360862337596e-05, 1.5160400958152884e-05, 1.5160400958152884e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 05:03:02,108] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=18.010597499209652, CurrSamplesPerSec=24.251225410481794, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.5086 (1.6664)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4178 (6.8916)  time: 0.7319 (0.4972 -- 2.6567)  data: 0.1990 (0.0002 -- 2.1371)  max mem: 16413
Epoch: [124] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.5086 (1.6269)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4178 (6.8916)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1347 (0.1347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2968 (2.2968 -- 2.2968)  data: 2.0640 (2.0640 -- 2.0640)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3904 (0.4942)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4119 (0.1979 -- 2.2968)  data: 0.1978 (0.0007 -- 2.0640)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4367 (0.4841)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2315 (0.1686 -- 0.6476)  data: 0.0280 (0.0001 -- 0.4453)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4764 (0.5535)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.0954)  time: 0.2150 (0.1330 -- 0.6476)  data: 0.0277 (0.0001 -- 0.4453)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 85.892 Acc@5 97.303 loss 0.550
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.31%
Epoch: [125]  [  0/160]  eta: 0:17:50  lr: 0.000015  min_lr: 0.000004  loss: 2.1671 (2.1671)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8566 (4.8566)  time: 6.6916 (6.6916 -- 6.6916)  data: 5.1129 (5.1129 -- 5.1129)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000004  loss: 1.7422 (1.7369)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0840 (7.6542)  time: 0.8926 (0.5233 -- 2.5576)  data: 0.0017 (0.0007 -- 0.0050)  max mem: 16413
[2023-09-04 05:03:35,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20022
[2023-09-04 05:03:35,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20022
[2023-09-04 05:03:35,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:03:35,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:03:35,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [125]  [ 40/160]  eta: 0:02:04  lr: 0.000015  min_lr: 0.000004  loss: 1.7553 (1.6902)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2957 (7.3731)  time: 0.9015 (0.5180 -- 3.5284)  data: 0.0017 (0.0003 -- 0.0076)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000004  loss: 1.6926 (1.6959)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4332 (7.1977)  time: 0.8876 (0.5243 -- 2.5718)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000004  loss: 1.7535 (1.7227)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5513 (7.0936)  time: 0.7688 (0.5291 -- 3.5736)  data: 0.0209 (0.0006 -- 0.3812)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000004  loss: 1.7264 (1.7225)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9992 (7.0152)  time: 0.8931 (0.5223 -- 3.8370)  data: 0.0266 (0.0004 -- 0.4890)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.4336 (1.6777)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7053 (7.1858)  time: 0.9933 (0.5208 -- 5.3249)  data: 0.0013 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.7125 (1.6767)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7711 (7.1763)  time: 0.8083 (0.5173 -- 3.6304)  data: 0.0017 (0.0005 -- 0.0092)  max mem: 16413
[2023-09-04 05:05:27,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:05:27,691] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:05:27,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:05:27,692] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.7082 (1.6768)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8171 (7.1795)  time: 0.6744 (0.4953 -- 2.7489)  data: 0.0005 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [125] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.7082 (1.6616)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8171 (7.1795)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1573 (0.1573)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2708 (2.2708 -- 2.2708)  data: 2.0427 (2.0427 -- 2.0427)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5030 (0.5138)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4229 (0.1956 -- 2.2708)  data: 0.2073 (0.0006 -- 2.0427)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4653 (0.4886)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (96.8254)  time: 0.2231 (0.1697 -- 0.4458)  data: 0.0160 (0.0001 -- 0.2141)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4826 (0.5580)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.2656)  time: 0.2077 (0.1335 -- 0.4458)  data: 0.0150 (0.0001 -- 0.2141)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 84.855 Acc@5 96.888 loss 0.566
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [126]  [  0/160]  eta: 0:24:01  lr: 0.000015  min_lr: 0.000004  loss: 1.5816 (1.5816)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9451 (5.9451)  time: 9.0102 (9.0102 -- 9.0102)  data: 8.4730 (8.4730 -- 8.4730)  max mem: 16413
[2023-09-04 05:05:58,806] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20170
[2023-09-04 05:05:58,806] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20170
[2023-09-04 05:05:58,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:05:58,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:05:58,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [ 20/160]  eta: 0:03:00  lr: 0.000015  min_lr: 0.000004  loss: 1.4906 (1.5506)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2529 (6.4730)  time: 0.9065 (0.5027 -- 4.5629)  data: 0.3733 (0.0004 -- 4.0458)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:12  lr: 0.000015  min_lr: 0.000004  loss: 1.6910 (1.5643)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8036 (6.7955)  time: 0.8989 (0.5213 -- 3.4729)  data: 0.3570 (0.0002 -- 2.9315)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:40  lr: 0.000015  min_lr: 0.000004  loss: 1.4905 (1.5728)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0360 (6.8076)  time: 0.8122 (0.5332 -- 3.2237)  data: 0.2625 (0.0007 -- 2.6781)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000004  loss: 1.7214 (1.6131)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7893 (6.7837)  time: 0.8482 (0.5263 -- 4.3279)  data: 0.0858 (0.0004 -- 1.1577)  max mem: 16413
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 1.6443 (1.6083)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5297 (6.7764)  time: 0.8794 (0.5212 -- 4.3310)  data: 0.0015 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.6742 (1.6293)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5151 (6.7253)  time: 0.8127 (0.5242 -- 4.2307)  data: 0.0015 (0.0003 -- 0.0040)  max mem: 16413
[2023-09-04 05:07:47,675] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:07:47,675] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:07:47,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:07:47,676] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:07:48,207] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20300
[2023-09-04 05:07:48,207] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20300
[2023-09-04 05:07:48,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:07:48,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:07:48,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.7431 (1.6435)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6254 (6.7393)  time: 0.7807 (0.5199 -- 2.3239)  data: 0.0248 (0.0007 -- 0.2682)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.6631 (1.6506)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2962 (6.8593)  time: 0.7429 (0.4956 -- 2.3340)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [126] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.6631 (1.6388)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2962 (6.8593)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1599 (0.1599)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1833 (2.1833 -- 2.1833)  data: 1.9722 (1.9722 -- 1.9722)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5095 (0.4969)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4168 (0.2017 -- 2.1833)  data: 0.1989 (0.0006 -- 1.9722)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4609 (0.4971)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2266 (0.1695 -- 0.4378)  data: 0.0181 (0.0001 -- 0.2016)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5777 (0.5607)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2105 (0.1327 -- 0.4378)  data: 0.0177 (0.0001 -- 0.2016)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 84.855 Acc@5 97.718 loss 0.568
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [127]  [  0/160]  eta: 0:20:29  lr: 0.000014  min_lr: 0.000004  loss: 2.2668 (2.2668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3465 (6.3465)  time: 7.6841 (7.6841 -- 7.6841)  data: 7.1576 (7.1576 -- 7.1576)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:36  lr: 0.000014  min_lr: 0.000004  loss: 1.8960 (1.7617)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0122 (7.2227)  time: 0.7904 (0.5350 -- 3.0986)  data: 0.2058 (0.0007 -- 2.5829)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:10  lr: 0.000014  min_lr: 0.000004  loss: 1.3514 (1.6035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7929 (7.1314)  time: 1.0512 (0.5280 -- 4.9866)  data: 0.4993 (0.0008 -- 4.4359)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000004  loss: 1.6629 (1.6303)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9845 (6.9680)  time: 0.7378 (0.5342 -- 3.4866)  data: 0.1839 (0.0006 -- 2.9477)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000004  loss: 1.6112 (1.6460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8635 (6.9784)  time: 0.8910 (0.5244 -- 3.2862)  data: 0.3446 (0.0003 -- 2.7498)  max mem: 16413
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000004  loss: 1.5267 (1.6370)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5212 (6.8883)  time: 0.7814 (0.5206 -- 4.0378)  data: 0.1240 (0.0001 -- 1.9805)  max mem: 16413
[2023-09-04 05:09:50,435] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:09:50,435] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:09:50,435] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:09:50,436] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [127]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000004  loss: 1.6912 (1.6324)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1226 (6.9608)  time: 0.9688 (0.5235 -- 5.3727)  data: 0.4100 (0.0002 -- 4.8605)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.6355 (1.6380)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0022 (6.9192)  time: 0.7697 (0.5326 -- 3.0789)  data: 0.2197 (0.0002 -- 2.5327)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.6651 (1.6458)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8205 (7.0246)  time: 0.6826 (0.4948 -- 2.5318)  data: 0.0862 (0.0002 -- 1.0309)  max mem: 16413
Epoch: [127] Total time: 0:02:20 (0.8788 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.6651 (1.6561)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8205 (7.0246)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1652 (0.1652)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3236 (2.3236 -- 2.3236)  data: 2.1110 (2.1110 -- 2.1110)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4395 (0.4959)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4104 (0.2043 -- 2.3236)  data: 0.1947 (0.0007 -- 2.1110)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4450 (0.4948)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2210 (0.1695 -- 0.4798)  data: 0.0159 (0.0001 -- 0.2848)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5199 (0.5492)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.5104)  time: 0.2068 (0.1334 -- 0.4798)  data: 0.0156 (0.0001 -- 0.2848)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 85.892 Acc@5 97.303 loss 0.551
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.31%
Epoch: [128]  [  0/160]  eta: 0:22:03  lr: 0.000014  min_lr: 0.000004  loss: 1.8591 (1.8591)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8972 (6.8972)  time: 8.2744 (8.2744 -- 8.2744)  data: 7.7347 (7.7347 -- 7.7347)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:52  lr: 0.000014  min_lr: 0.000004  loss: 1.5512 (1.6135)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7798 (7.0220)  time: 0.8810 (0.5165 -- 4.3294)  data: 0.3355 (0.0002 -- 3.7963)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:11  lr: 0.000014  min_lr: 0.000004  loss: 1.6972 (1.6754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8717 (7.8288)  time: 0.9567 (0.5165 -- 3.4004)  data: 0.4078 (0.0004 -- 2.8421)  max mem: 16413
Epoch: [128]  [ 60/160]  eta: 0:01:40  lr: 0.000014  min_lr: 0.000004  loss: 1.6746 (1.6771)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6161 (7.5100)  time: 0.8167 (0.5220 -- 3.5958)  data: 0.2757 (0.0004 -- 3.0719)  max mem: 16413
[2023-09-04 05:11:53,763] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20555
[2023-09-04 05:11:53,763] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20555
[2023-09-04 05:11:53,763] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:11:53,763] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:11:53,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [ 80/160]  eta: 0:01:19  lr: 0.000014  min_lr: 0.000004  loss: 1.6233 (1.6536)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5041 (7.4955)  time: 0.9533 (0.5178 -- 4.3716)  data: 0.4143 (0.0002 -- 3.8404)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000004  loss: 1.8261 (1.6741)  loss_scale: 16384.0000 (28550.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9189 (7.2451)  time: 0.7455 (0.5248 -- 2.7456)  data: 0.1985 (0.0006 -- 2.2225)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000004  loss: 1.7818 (1.6927)  loss_scale: 16384.0000 (26539.3719)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4345 (7.3736)  time: 0.9036 (0.5194 -- 3.6406)  data: 0.3590 (0.0003 -- 3.0918)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.6981 (1.6822)  loss_scale: 16384.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9875 (7.2546)  time: 0.8521 (0.5223 -- 2.9322)  data: 0.3049 (0.0005 -- 2.3826)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000003  loss: 1.5065 (1.6762)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0390 (7.2810)  time: 0.6936 (0.4952 -- 3.2200)  data: 0.1734 (0.0002 -- 2.7044)  max mem: 16413
Epoch: [128] Total time: 0:02:23 (0.8989 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000003  loss: 1.5065 (1.6584)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0390 (7.2810)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1674 (0.1674)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3541 (2.3541 -- 2.3541)  data: 2.1482 (2.1482 -- 2.1482)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4037 (0.5091)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4172 (0.1993 -- 2.3541)  data: 0.2061 (0.0006 -- 2.1482)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4033 (0.4899)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2257 (0.1685 -- 0.5045)  data: 0.0223 (0.0001 -- 0.3226)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4446 (0.5576)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2117 (0.1332 -- 0.5045)  data: 0.0220 (0.0001 -- 0.3226)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.556
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [129]  [  0/160]  eta: 0:19:26  lr: 0.000014  min_lr: 0.000003  loss: 0.9442 (0.9442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9419 (9.9419)  time: 7.2875 (7.2875 -- 7.2875)  data: 6.7321 (6.7321 -- 6.7321)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000003  loss: 1.7843 (1.7244)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6330 (7.2896)  time: 0.8684 (0.5198 -- 2.9417)  data: 0.3051 (0.0004 -- 2.3783)  max mem: 16413
Epoch: [129]  [ 40/160]  eta: 0:02:06  lr: 0.000014  min_lr: 0.000003  loss: 1.6823 (1.7296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2973 (7.4101)  time: 0.9218 (0.5197 -- 2.0899)  data: 0.3115 (0.0003 -- 1.5708)  max mem: 16413
[2023-09-04 05:13:57,226] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:13:57,226] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:13:57,227] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:13:57,227] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:14:12,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20699
[2023-09-04 05:14:12,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20699
[2023-09-04 05:14:12,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:14:12,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:14:12,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [129]  [ 60/160]  eta: 0:01:42  lr: 0.000014  min_lr: 0.000003  loss: 1.6367 (1.7157)  loss_scale: 32768.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4496 (7.3263)  time: 0.9587 (0.5121 -- 2.5027)  data: 0.4250 (0.0003 -- 1.9994)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000003  loss: 1.6570 (1.7048)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8892 (7.1748)  time: 0.7716 (0.5203 -- 2.7412)  data: 0.2257 (0.0006 -- 2.2274)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:57  lr: 0.000014  min_lr: 0.000003  loss: 1.5653 (1.6882)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8104 (7.2691)  time: 0.9544 (0.5163 -- 4.9894)  data: 0.4066 (0.0005 -- 4.4639)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000003  loss: 1.6760 (1.6960)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0244 (7.2894)  time: 0.7913 (0.5266 -- 3.0195)  data: 0.2466 (0.0004 -- 2.4982)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.6197 (1.6951)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3804 (7.4368)  time: 0.9646 (0.5142 -- 3.4196)  data: 0.4205 (0.0004 -- 2.8445)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.6184 (1.6841)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1648 (7.4908)  time: 0.6443 (0.4952 -- 2.0022)  data: 0.1293 (0.0002 -- 1.4963)  max mem: 16413
Epoch: [129] Total time: 0:02:24 (0.9015 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.6184 (1.6841)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1648 (7.4908)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1775 (0.1775)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2625 (2.2625 -- 2.2625)  data: 2.0484 (2.0484 -- 2.0484)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4251 (0.4820)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4149 (0.1869 -- 2.2625)  data: 0.2079 (0.0003 -- 2.0484)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3967 (0.4817)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2193 (0.1711 -- 0.4158)  data: 0.0150 (0.0001 -- 0.2302)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4767 (0.5312)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (97.5104)  time: 0.2067 (0.1335 -- 0.4158)  data: 0.0148 (0.0001 -- 0.2302)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 86.722 Acc@5 97.718 loss 0.549
Accuracy of the network on the 482 val images: 86.72%
[2023-09-04 05:15:42,364] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 05:15:42,365] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 05:15:42,366] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 05:15:42,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 05:15:43,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 05:15:43,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.72%
Epoch: [130]  [  0/160]  eta: 0:17:21  lr: 0.000013  min_lr: 0.000003  loss: 1.6950 (1.6950)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0077 (10.0077)  time: 6.5095 (6.5095 -- 6.5095)  data: 5.9625 (5.9625 -- 5.9625)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:43  lr: 0.000013  min_lr: 0.000003  loss: 1.7650 (1.6840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4102 (7.1486)  time: 0.8997 (0.5206 -- 3.5198)  data: 0.2045 (0.0004 -- 2.9907)  max mem: 16413
[2023-09-04 05:16:16,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:16:16,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:16:16,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:16:16,606] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [130]  [ 40/160]  eta: 0:02:12  lr: 0.000013  min_lr: 0.000003  loss: 1.7326 (1.7012)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9928 (7.4118)  time: 1.0355 (0.5096 -- 4.2964)  data: 0.4880 (0.0003 -- 3.7608)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000003  loss: 1.6311 (1.6487)  loss_scale: 32768.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0349 (7.4025)  time: 0.8418 (0.5090 -- 3.7352)  data: 0.2929 (0.0004 -- 3.2225)  max mem: 16413
[2023-09-04 05:16:47,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20862
[2023-09-04 05:16:47,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20862
[2023-09-04 05:16:47,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:16:47,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:16:47,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [130]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000003  loss: 1.5365 (1.6400)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6054 (7.4890)  time: 0.7814 (0.5310 -- 3.3899)  data: 0.1990 (0.0001 -- 2.8621)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000003  loss: 1.7506 (1.6484)  loss_scale: 16384.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1595 (7.5313)  time: 0.8420 (0.5279 -- 5.2864)  data: 0.2963 (0.0004 -- 4.7781)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000003  loss: 1.6357 (1.6604)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3046 (7.7075)  time: 1.0505 (0.5272 -- 4.3740)  data: 0.5071 (0.0003 -- 3.8475)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.4994 (1.6439)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4921 (7.6694)  time: 0.7802 (0.5169 -- 3.4438)  data: 0.2328 (0.0004 -- 2.9195)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.5676 (1.6296)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3769 (7.6655)  time: 0.6102 (0.4954 -- 2.3251)  data: 0.0923 (0.0002 -- 1.7871)  max mem: 16413
Epoch: [130] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.5676 (1.6690)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3769 (7.6655)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1589 (0.1589)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2840 (2.2840 -- 2.2840)  data: 2.0524 (2.0524 -- 2.0524)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4594 (0.4737)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4077 (0.1928 -- 2.2840)  data: 0.1961 (0.0007 -- 2.0524)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4567 (0.4822)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2255 (0.1687 -- 0.3841)  data: 0.0214 (0.0001 -- 0.1859)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4728 (0.5367)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2130 (0.1329 -- 0.3841)  data: 0.0211 (0.0001 -- 0.1859)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 86.722 Acc@5 97.925 loss 0.547
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 86.72%
Epoch: [131]  [  0/160]  eta: 0:20:30  lr: 0.000013  min_lr: 0.000003  loss: 1.0903 (1.0903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7865 (6.7865)  time: 7.6925 (7.6925 -- 7.6925)  data: 7.1153 (7.1153 -- 7.1153)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:49  lr: 0.000013  min_lr: 0.000003  loss: 1.7115 (1.6190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6596 (6.9382)  time: 0.8841 (0.5234 -- 3.0314)  data: 0.2311 (0.0003 -- 2.3903)  max mem: 16413
[2023-09-04 05:18:47,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:18:47,691] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:18:47,692] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:18:47,692] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:18:54,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=118, lr=[3.3048525035975797e-06, 3.3048525035975797e-06, 3.6720583373306447e-06, 3.6720583373306447e-06, 4.080064819256271e-06, 4.080064819256271e-06, 4.5334053547291906e-06, 4.5334053547291906e-06, 5.037117060810212e-06, 5.037117060810212e-06, 5.596796734233568e-06, 5.596796734233568e-06, 6.218663038037298e-06, 6.218663038037298e-06, 6.909625597819219e-06, 6.909625597819219e-06, 7.677361775354689e-06, 7.677361775354689e-06, 8.530401972616319e-06, 8.530401972616319e-06, 9.478224414018133e-06, 9.478224414018133e-06, 1.0531360460020147e-05, 1.0531360460020147e-05, 1.1701511622244608e-05, 1.1701511622244608e-05, 1.3001679580271786e-05, 1.3001679580271786e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 05:18:54,949] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=18.075099966222997, CurrSamplesPerSec=22.37871206347823, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:09  lr: 0.000013  min_lr: 0.000003  loss: 1.6706 (1.6626)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3216 (7.1774)  time: 0.9472 (0.5138 -- 3.8730)  data: 0.4017 (0.0003 -- 3.2905)  max mem: 16413
Epoch: [131]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000003  loss: 1.6119 (1.6584)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4891 (7.1021)  time: 0.7672 (0.5163 -- 3.0243)  data: 0.2243 (0.0003 -- 2.4994)  max mem: 16413
[2023-09-04 05:19:28,423] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21038
[2023-09-04 05:19:28,424] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21038
[2023-09-04 05:19:28,424] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:19:28,424] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:19:28,424] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [131]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000003  loss: 1.6102 (1.6481)  loss_scale: 32768.0000 (25890.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2355 (7.4224)  time: 0.9206 (0.5208 -- 3.6157)  data: 0.3796 (0.0005 -- 3.0930)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:54  lr: 0.000013  min_lr: 0.000003  loss: 1.6364 (1.6515)  loss_scale: 16384.0000 (24008.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9445 (7.4048)  time: 0.7189 (0.5278 -- 3.7960)  data: 0.1714 (0.0002 -- 3.2647)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000003  loss: 1.5597 (1.6336)  loss_scale: 16384.0000 (22748.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2282 (7.3096)  time: 0.9190 (0.5333 -- 3.5210)  data: 0.3644 (0.0002 -- 2.9876)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.4690 (1.6291)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0720 (7.2901)  time: 0.8279 (0.5245 -- 3.9020)  data: 0.2786 (0.0003 -- 3.3695)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.4898 (1.6081)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3943 (7.3060)  time: 0.7134 (0.4965 -- 3.4197)  data: 0.1421 (0.0003 -- 1.8152)  max mem: 16413
Epoch: [131] Total time: 0:02:21 (0.8824 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.4898 (1.6287)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3943 (7.3060)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1564 (0.1564)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3561 (2.3561 -- 2.3561)  data: 2.1404 (2.1404 -- 2.1404)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4241 (0.4541)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4176 (0.1991 -- 2.3561)  data: 0.2031 (0.0006 -- 2.1404)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3852 (0.4644)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2197 (0.1705 -- 0.3050)  data: 0.0127 (0.0001 -- 0.0982)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4434 (0.5243)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2031 (0.1331 -- 0.3050)  data: 0.0121 (0.0001 -- 0.0982)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 86.307 Acc@5 98.340 loss 0.534
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 86.72%
Epoch: [132]  [  0/160]  eta: 0:16:46  lr: 0.000013  min_lr: 0.000003  loss: 1.4610 (1.4610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1310 (7.1310)  time: 6.2898 (6.2898 -- 6.2898)  data: 4.8081 (4.8081 -- 4.8081)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:35  lr: 0.000013  min_lr: 0.000003  loss: 1.6252 (1.5881)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5840 (7.2219)  time: 0.8555 (0.5194 -- 2.5769)  data: 0.2232 (0.0004 -- 1.8832)  max mem: 16413
Epoch: [132]  [ 40/160]  eta: 0:02:03  lr: 0.000013  min_lr: 0.000003  loss: 1.6758 (1.6602)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5444 (6.9168)  time: 0.9372 (0.5246 -- 3.9714)  data: 0.3943 (0.0004 -- 3.4342)  max mem: 16413
[2023-09-04 05:21:32,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:21:32,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:21:32,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:21:32,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [132]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000003  loss: 1.6275 (1.6543)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7695 (6.8961)  time: 1.0020 (0.5182 -- 4.5276)  data: 0.4641 (0.0002 -- 3.9820)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000003  loss: 1.8207 (1.6804)  loss_scale: 32768.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9359 (6.9210)  time: 0.6920 (0.5158 -- 2.1156)  data: 0.1499 (0.0003 -- 1.6004)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000003  loss: 1.6694 (1.6660)  loss_scale: 32768.0000 (25143.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7097 (6.9300)  time: 0.8652 (0.5189 -- 4.2856)  data: 0.3241 (0.0002 -- 3.7483)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000003  loss: 1.6911 (1.6538)  loss_scale: 32768.0000 (26403.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3205 (6.9771)  time: 0.8521 (0.5200 -- 2.6043)  data: 0.2724 (0.0002 -- 2.0823)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:17  lr: 0.000012  min_lr: 0.000003  loss: 1.8037 (1.6700)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9166 (7.0320)  time: 0.8109 (0.5327 -- 2.1196)  data: 0.0716 (0.0005 -- 0.7000)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.6995 (1.6801)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5272 (7.0930)  time: 0.7913 (0.4982 -- 2.0746)  data: 0.1491 (0.0002 -- 1.5498)  max mem: 16413
Epoch: [132] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.6995 (1.6457)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5272 (7.0930)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1477 (0.1477)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3354 (2.3354 -- 2.3354)  data: 2.0995 (2.0995 -- 2.0995)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5130 (0.4909)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4162 (0.2009 -- 2.3354)  data: 0.1966 (0.0006 -- 2.0995)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3544 (0.4804)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2242 (0.1691 -- 0.3290)  data: 0.0162 (0.0001 -- 0.1524)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4605 (0.5351)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2056 (0.1336 -- 0.3290)  data: 0.0157 (0.0001 -- 0.1524)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 85.892 Acc@5 98.133 loss 0.545
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.72%
Epoch: [133]  [  0/160]  eta: 0:17:15  lr: 0.000012  min_lr: 0.000003  loss: 1.9155 (1.9155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9268 (8.9268)  time: 6.4737 (6.4737 -- 6.4737)  data: 5.4501 (5.4501 -- 5.4501)  max mem: 16413
[2023-09-04 05:23:33,308] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:23:33,309] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:23:33,310] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:23:33,311] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:23:38,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21300
[2023-09-04 05:23:38,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21300
[2023-09-04 05:23:38,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:23:38,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:23:38,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [133]  [ 20/160]  eta: 0:02:46  lr: 0.000012  min_lr: 0.000003  loss: 1.4779 (1.5297)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4174 (7.2614)  time: 0.9277 (0.5012 -- 2.7081)  data: 0.3590 (0.0009 -- 2.1762)  max mem: 16413
[2023-09-04 05:23:53,045] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21319
[2023-09-04 05:23:53,045] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21319
[2023-09-04 05:23:53,045] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:23:53,045] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:23:53,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [133]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000003  loss: 1.3702 (1.4797)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9805 (6.8206)  time: 0.9475 (0.5326 -- 4.0747)  data: 0.1816 (0.0002 -- 1.9282)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:39  lr: 0.000012  min_lr: 0.000003  loss: 1.8090 (1.5803)  loss_scale: 16384.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6143 (6.9031)  time: 0.8317 (0.5195 -- 2.1151)  data: 0.2205 (0.0004 -- 1.5817)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000003  loss: 1.8129 (1.6274)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0234 (7.1669)  time: 0.9283 (0.5115 -- 4.4335)  data: 0.0139 (0.0002 -- 0.2462)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000003  loss: 1.6295 (1.6272)  loss_scale: 16384.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5173 (7.2400)  time: 0.8114 (0.5236 -- 3.3693)  data: 0.0014 (0.0005 -- 0.0042)  max mem: 16413
Epoch: [133]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000003  loss: 1.6527 (1.6351)  loss_scale: 16384.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7133 (7.2914)  time: 0.7673 (0.5185 -- 2.3512)  data: 0.0268 (0.0002 -- 0.4872)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.5662 (1.6247)  loss_scale: 16384.0000 (22077.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0237 (7.1788)  time: 0.9417 (0.5269 -- 3.7036)  data: 0.1036 (0.0004 -- 1.8629)  max mem: 16413
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.5710 (1.6162)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4429 (7.1630)  time: 0.6281 (0.4960 -- 2.4466)  data: 0.0120 (0.0002 -- 0.2279)  max mem: 16413
Epoch: [133] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.5710 (1.6459)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4429 (7.1630)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1383 (0.1383)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2879 (2.2879 -- 2.2879)  data: 2.0611 (2.0611 -- 2.0611)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3476 (0.4607)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4140 (0.1958 -- 2.2879)  data: 0.2062 (0.0006 -- 2.0611)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3476 (0.4586)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2221 (0.1702 -- 0.4242)  data: 0.0219 (0.0001 -- 0.2280)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4319 (0.5136)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2095 (0.1348 -- 0.4242)  data: 0.0209 (0.0001 -- 0.2280)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 86.100 Acc@5 98.340 loss 0.525
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.72%
Epoch: [134]  [  0/160]  eta: 0:20:27  lr: 0.000012  min_lr: 0.000003  loss: 2.1677 (2.1677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2637 (7.2637)  time: 7.6704 (7.6704 -- 7.6704)  data: 5.1830 (5.1830 -- 5.1830)  max mem: 16413
[2023-09-04 05:25:55,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:25:55,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:25:56,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:25:56,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000003  loss: 1.5352 (1.6399)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9379 (7.1951)  time: 0.8360 (0.5243 -- 3.3604)  data: 0.1038 (0.0008 -- 0.7865)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:01  lr: 0.000012  min_lr: 0.000003  loss: 1.6859 (1.6306)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8148 (7.2302)  time: 0.8511 (0.5237 -- 1.8914)  data: 0.0988 (0.0003 -- 1.3440)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:36  lr: 0.000012  min_lr: 0.000003  loss: 1.5956 (1.6146)  loss_scale: 32768.0000 (30619.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2730 (7.0942)  time: 0.8829 (0.5342 -- 2.1026)  data: 0.1449 (0.0009 -- 1.5821)  max mem: 16413
[2023-09-04 05:26:57,825] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21519
[2023-09-04 05:26:57,825] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:26:57,825] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21519
[2023-09-04 05:26:57,825] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:26:57,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000003  loss: 1.6154 (1.6068)  loss_scale: 32768.0000 (30745.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4494 (7.0131)  time: 0.9621 (0.5194 -- 3.0177)  data: 0.0682 (0.0001 -- 1.3317)  max mem: 16413
Epoch: [134]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000003  loss: 1.5314 (1.5945)  loss_scale: 16384.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2050 (7.2767)  time: 0.8436 (0.5149 -- 3.9360)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000003  loss: 1.5917 (1.5838)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1532 (7.3944)  time: 0.7728 (0.5356 -- 2.8084)  data: 0.0015 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.8599 (1.6088)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2324 (7.3407)  time: 0.8379 (0.5247 -- 3.3256)  data: 0.0294 (0.0004 -- 0.5586)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.5017 (1.5970)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9539 (7.2128)  time: 0.7315 (0.4955 -- 3.0974)  data: 0.0009 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [134] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.5017 (1.6022)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9539 (7.2128)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1372 (0.1372)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2324 (2.2324 -- 2.2324)  data: 2.0310 (2.0310 -- 2.0310)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3963 (0.4696)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4155 (0.1861 -- 2.2324)  data: 0.2054 (0.0006 -- 2.0310)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3963 (0.4768)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2287 (0.1698 -- 0.5179)  data: 0.0279 (0.0001 -- 0.3269)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4563 (0.5318)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2149 (0.1323 -- 0.5179)  data: 0.0276 (0.0001 -- 0.3269)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 87.137 Acc@5 98.548 loss 0.533
Accuracy of the network on the 482 val images: 87.14%
[2023-09-04 05:28:11,839] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 05:28:11,840] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 05:28:11,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 05:28:11,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 05:28:13,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 05:28:13,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.14%
Epoch: [135]  [  0/160]  eta: 0:18:28  lr: 0.000012  min_lr: 0.000003  loss: 1.2844 (1.2844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2714 (6.2714)  time: 6.9255 (6.9255 -- 6.9255)  data: 6.1815 (6.1815 -- 6.1815)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:48  lr: 0.000012  min_lr: 0.000003  loss: 1.6162 (1.5783)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3296 (8.2355)  time: 0.9163 (0.5167 -- 2.4551)  data: 0.3701 (0.0005 -- 1.9386)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:03  lr: 0.000012  min_lr: 0.000003  loss: 1.5911 (1.6036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5948 (8.2399)  time: 0.8390 (0.5258 -- 2.8671)  data: 0.2964 (0.0005 -- 2.3332)  max mem: 16413
[2023-09-04 05:29:02,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:29:02,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:29:02,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:29:02,679] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [135]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000003  loss: 1.6051 (1.6116)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0803 (7.7561)  time: 0.8880 (0.5245 -- 3.2011)  data: 0.3453 (0.0003 -- 2.6678)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000003  loss: 1.7279 (1.6300)  loss_scale: 32768.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5050 (7.6339)  time: 0.8342 (0.5246 -- 2.8390)  data: 0.2870 (0.0004 -- 2.3067)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000003  loss: 1.6099 (1.6288)  loss_scale: 32768.0000 (24981.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1754 (7.4306)  time: 0.9468 (0.5183 -- 3.3844)  data: 0.3624 (0.0004 -- 2.8458)  max mem: 16413
[2023-09-04 05:30:00,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21714
[2023-09-04 05:30:00,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21714
[2023-09-04 05:30:00,260] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:30:00,260] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:30:00,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [135]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000003  loss: 1.7023 (1.6350)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9833 (7.3414)  time: 0.8489 (0.5245 -- 4.1076)  data: 0.2927 (0.0003 -- 3.5908)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.5328 (1.6205)  loss_scale: 16384.0000 (24053.1064)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1077 (7.3423)  time: 0.8927 (0.5138 -- 4.4669)  data: 0.3531 (0.0002 -- 3.9291)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.6768 (1.6271)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1285 (7.2725)  time: 0.6379 (0.4953 -- 1.7311)  data: 0.1146 (0.0002 -- 1.1921)  max mem: 16413
Epoch: [135] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.6768 (1.6369)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1285 (7.2725)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1346 (0.1346)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2787 (2.2787 -- 2.2787)  data: 2.0035 (2.0035 -- 2.0035)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3238 (0.4610)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4190 (0.2017 -- 2.2787)  data: 0.1998 (0.0004 -- 2.0035)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3688 (0.4718)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.8836)  time: 0.2337 (0.1688 -- 0.5759)  data: 0.0286 (0.0001 -- 0.3738)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4617 (0.5244)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.3402)  time: 0.2190 (0.1329 -- 0.5759)  data: 0.0284 (0.0001 -- 0.3738)  max mem: 16413
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 87.759 Acc@5 98.133 loss 0.521
Accuracy of the network on the 482 val images: 87.76%
[2023-09-04 05:30:42,600] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 05:30:42,601] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 05:30:42,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 05:30:42,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 05:30:44,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 05:30:44,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.76%
Epoch: [136]  [  0/160]  eta: 0:18:36  lr: 0.000011  min_lr: 0.000003  loss: 2.0173 (2.0173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7551 (7.7551)  time: 6.9784 (6.9784 -- 6.9784)  data: 6.4459 (6.4459 -- 6.4459)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000003  loss: 1.5721 (1.6350)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2627 (7.8010)  time: 0.8736 (0.5264 -- 2.2783)  data: 0.2530 (0.0004 -- 1.7495)  max mem: 16413
Epoch: [136]  [ 40/160]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000003  loss: 1.5732 (1.6526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9311 (7.3086)  time: 0.8796 (0.5247 -- 2.6898)  data: 0.3391 (0.0003 -- 2.1710)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:35  lr: 0.000011  min_lr: 0.000003  loss: 1.6880 (1.6938)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2149 (7.3344)  time: 0.8164 (0.5276 -- 4.3716)  data: 0.2717 (0.0006 -- 3.8457)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000003  loss: 1.5903 (1.6689)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9074 (7.3267)  time: 0.9633 (0.5099 -- 3.3864)  data: 0.0819 (0.0001 -- 1.0212)  max mem: 16413
[2023-09-04 05:32:03,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:32:03,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:32:03,610] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:32:03,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:32:15,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21856
[2023-09-04 05:32:15,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:32:15,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21856
[2023-09-04 05:32:15,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:32:15,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [136]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000003  loss: 1.5350 (1.6432)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6869 (7.3275)  time: 0.8183 (0.5402 -- 3.0581)  data: 0.2607 (0.0003 -- 2.5313)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000003  loss: 1.7139 (1.6659)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3789 (7.4019)  time: 0.9096 (0.5190 -- 3.7960)  data: 0.3626 (0.0004 -- 3.2645)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.4832 (1.6521)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5372 (7.4999)  time: 0.7486 (0.5214 -- 1.7111)  data: 0.1591 (0.0007 -- 1.1473)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.6572 (1.6570)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1039 (7.5860)  time: 0.7526 (0.4953 -- 2.9892)  data: 0.1113 (0.0002 -- 1.2733)  max mem: 16413
Epoch: [136] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.6572 (1.6508)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1039 (7.5860)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4528 (2.4528 -- 2.4528)  data: 2.2321 (2.2321 -- 2.2321)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2954 (0.4734)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4201 (0.1964 -- 2.4528)  data: 0.2056 (0.0006 -- 2.2321)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3947 (0.4781)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2183 (0.1751 -- 0.3209)  data: 0.0129 (0.0001 -- 0.1395)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4465 (0.5294)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2047 (0.1336 -- 0.3209)  data: 0.0126 (0.0001 -- 0.1395)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.524
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [137]  [  0/160]  eta: 0:20:10  lr: 0.000011  min_lr: 0.000003  loss: 2.0401 (2.0401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6544 (6.6544)  time: 7.5658 (7.5658 -- 7.5658)  data: 7.0085 (7.0085 -- 7.0085)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:41  lr: 0.000011  min_lr: 0.000003  loss: 1.6624 (1.7489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3405 (7.3443)  time: 0.8365 (0.5257 -- 3.3449)  data: 0.2884 (0.0003 -- 2.7971)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000003  loss: 1.5656 (1.7159)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9231 (7.2414)  time: 0.9286 (0.5198 -- 3.0400)  data: 0.3763 (0.0004 -- 2.4545)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000003  loss: 1.6053 (1.6876)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5167 (7.3203)  time: 0.8207 (0.5250 -- 3.1257)  data: 0.1550 (0.0002 -- 2.6038)  max mem: 16413
[2023-09-04 05:34:18,627] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:34:18,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:34:18,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:34:18,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:34:28,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=124, lr=[2.7830670189612398e-06, 2.7830670189612398e-06, 3.092296687734711e-06, 3.092296687734711e-06, 3.4358852085941226e-06, 3.4358852085941226e-06, 3.8176502317712475e-06, 3.8176502317712475e-06, 4.241833590856941e-06, 4.241833590856941e-06, 4.713148434285491e-06, 4.713148434285491e-06, 5.236831593650545e-06, 5.236831593650545e-06, 5.8187017707228276e-06, 5.8187017707228276e-06, 6.46522418969203e-06, 6.46522418969203e-06, 7.1835824329911445e-06, 7.1835824329911445e-06, 7.98175825887905e-06, 7.98175825887905e-06, 8.868620287643388e-06, 8.868620287643388e-06, 9.854022541825986e-06, 9.854022541825986e-06, 1.0948913935362207e-05, 1.0948913935362207e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 05:34:28,246] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=18.116813616416923, CurrSamplesPerSec=21.765846427153615, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000003  loss: 1.5562 (1.6759)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9685 (7.0155)  time: 0.9072 (0.5453 -- 2.7792)  data: 0.2075 (0.0004 -- 2.2358)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000003  loss: 1.7289 (1.6860)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2394 (7.1260)  time: 0.7855 (0.5307 -- 2.5818)  data: 0.1786 (0.0003 -- 2.0325)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000003  loss: 1.6815 (1.6811)  loss_scale: 32768.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0070 (7.1380)  time: 0.8675 (0.5383 -- 2.9726)  data: 0.1213 (0.0005 -- 2.1880)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.4635 (1.6642)  loss_scale: 32768.0000 (25215.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9588 (7.1394)  time: 0.9361 (0.5304 -- 4.0593)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.8214 (1.6797)  loss_scale: 32768.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3719 (7.2097)  time: 0.6613 (0.4947 -- 3.4642)  data: 0.0006 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [137] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.8214 (1.6438)  loss_scale: 32768.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3719 (7.2097)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1525 (0.1525)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2623 (2.2623 -- 2.2623)  data: 2.0386 (2.0386 -- 2.0386)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3082 (0.4938)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4064 (0.1949 -- 2.2623)  data: 0.1935 (0.0007 -- 2.0386)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3969 (0.4921)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2296 (0.1699 -- 0.6728)  data: 0.0289 (0.0001 -- 0.4844)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4360 (0.5458)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2151 (0.1325 -- 0.6728)  data: 0.0281 (0.0001 -- 0.4844)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 86.929 Acc@5 98.133 loss 0.538
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [138]  [  0/160]  eta: 0:23:18  lr: 0.000011  min_lr: 0.000003  loss: 1.8009 (1.8009)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2586 (4.2586)  time: 8.7428 (8.7428 -- 8.7428)  data: 8.2013 (8.2013 -- 8.2013)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000003  loss: 1.4842 (1.5836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1653 (6.9119)  time: 0.8321 (0.5302 -- 3.6286)  data: 0.2841 (0.0008 -- 3.0875)  max mem: 16413
[2023-09-04 05:36:22,345] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:36:22,346] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:36:22,352] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:36:22,353] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:36:25,077] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22118
[2023-09-04 05:36:25,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22118
[2023-09-04 05:36:25,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:36:25,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:36:25,078] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [138]  [ 40/160]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000003  loss: 1.6378 (1.5614)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6875 (6.8877)  time: 0.8734 (0.5224 -- 2.6584)  data: 0.3247 (0.0007 -- 2.1182)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000003  loss: 1.4632 (1.5403)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7509 (7.2395)  time: 0.9516 (0.5203 -- 4.7521)  data: 0.4125 (0.0004 -- 4.2395)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000003  loss: 1.5362 (1.5471)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3352 (7.0715)  time: 0.8410 (0.5218 -- 4.2512)  data: 0.3021 (0.0001 -- 3.7454)  max mem: 16413
[2023-09-04 05:37:05,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22165
[2023-09-04 05:37:05,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:37:05,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22165
[2023-09-04 05:37:05,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:37:05,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [138]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000003  loss: 1.6530 (1.5771)  loss_scale: 16384.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4449 (7.3639)  time: 0.8122 (0.5256 -- 3.5636)  data: 0.2657 (0.0008 -- 2.9983)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000003  loss: 1.4889 (1.5788)  loss_scale: 16384.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5076 (7.3766)  time: 0.8014 (0.5261 -- 3.5267)  data: 0.2530 (0.0004 -- 3.0088)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.4455 (1.5660)  loss_scale: 16384.0000 (27422.8652)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0586 (7.3971)  time: 0.9687 (0.5152 -- 3.6033)  data: 0.4285 (0.0004 -- 3.0826)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.5912 (1.5640)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6892 (7.5368)  time: 0.6246 (0.4962 -- 2.4877)  data: 0.1069 (0.0002 -- 1.9686)  max mem: 16413
Epoch: [138] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.5912 (1.6029)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6892 (7.5368)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1407 (0.1407)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2687 (2.2687 -- 2.2687)  data: 2.0284 (2.0284 -- 2.0284)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3527 (0.5145)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4151 (0.1985 -- 2.2687)  data: 0.1988 (0.0003 -- 2.0284)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3601 (0.4990)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2235 (0.1692 -- 0.4754)  data: 0.0216 (0.0001 -- 0.2717)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4440 (0.5527)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2094 (0.1328 -- 0.4754)  data: 0.0214 (0.0001 -- 0.2717)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 85.685 Acc@5 98.133 loss 0.542
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [139]  [  0/160]  eta: 0:21:01  lr: 0.000010  min_lr: 0.000003  loss: 1.2832 (1.2832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9688 (5.9688)  time: 7.8822 (7.8822 -- 7.8822)  data: 4.8998 (4.8998 -- 4.8998)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:02:46  lr: 0.000010  min_lr: 0.000003  loss: 1.6354 (1.5867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9660 (7.3306)  time: 0.8514 (0.5293 -- 4.5495)  data: 0.0016 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:08  lr: 0.000010  min_lr: 0.000003  loss: 1.7800 (1.6791)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5127 (7.6511)  time: 0.9536 (0.5171 -- 3.6330)  data: 0.0023 (0.0003 -- 0.0139)  max mem: 16413
[2023-09-04 05:39:06,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:39:06,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:39:06,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:39:06,138] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [139]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000003  loss: 1.5167 (1.6493)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0874 (7.4417)  time: 0.7263 (0.5177 -- 2.0694)  data: 0.0790 (0.0003 -- 1.5518)  max mem: 16413
[2023-09-04 05:39:26,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22318
[2023-09-04 05:39:26,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:39:26,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22318
[2023-09-04 05:39:26,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:39:26,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [139]  [ 80/160]  eta: 0:01:14  lr: 0.000010  min_lr: 0.000003  loss: 1.5153 (1.6296)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6621 (7.3463)  time: 0.8611 (0.5388 -- 2.7134)  data: 0.1999 (0.0002 -- 2.1610)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000003  loss: 1.7162 (1.6548)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1514 (7.4887)  time: 0.9504 (0.5250 -- 3.3658)  data: 0.2837 (0.0006 -- 2.8357)  max mem: 16413
Epoch: [139]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000003  loss: 1.5229 (1.6491)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7254 (7.5672)  time: 0.9523 (0.5429 -- 3.0158)  data: 0.0854 (0.0004 -- 1.6785)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000003  loss: 1.7217 (1.6506)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0216 (7.4872)  time: 0.7541 (0.5139 -- 3.5448)  data: 0.2001 (0.0005 -- 3.0231)  max mem: 16413
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.7855 (1.6598)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6783 (7.5225)  time: 0.6830 (0.4955 -- 3.5831)  data: 0.1695 (0.0002 -- 3.0814)  max mem: 16413
Epoch: [139] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.7855 (1.6743)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6783 (7.5225)
[2023-09-04 05:40:35,517] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-09-04 05:40:35,519] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-09-04 05:40:35,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-09-04 05:40:35,519] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-09-04 05:40:36,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-09-04 05:40:36,398] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1579 (0.1579)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3779 (2.3779 -- 2.3779)  data: 2.1542 (2.1542 -- 2.1542)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3736 (0.4907)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4315 (0.2073 -- 2.3779)  data: 0.2070 (0.0011 -- 2.1542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3980 (0.4919)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2217 (0.1712 -- 0.3437)  data: 0.0133 (0.0001 -- 0.1397)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4508 (0.5423)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2016 (0.1333 -- 0.3437)  data: 0.0129 (0.0001 -- 0.1397)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 86.515 Acc@5 97.718 loss 0.542
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [140]  [  0/160]  eta: 0:17:17  lr: 0.000010  min_lr: 0.000003  loss: 1.6337 (1.6337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3828 (10.3828)  time: 6.4821 (6.4821 -- 6.4821)  data: 5.9378 (5.9378 -- 5.9378)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:51  lr: 0.000010  min_lr: 0.000003  loss: 1.7459 (1.7397)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0842 (7.1998)  time: 0.9587 (0.5195 -- 3.1610)  data: 0.1171 (0.0004 -- 1.6586)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000003  loss: 1.6789 (1.7097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4496 (7.2288)  time: 0.8631 (0.5217 -- 4.2675)  data: 0.0020 (0.0004 -- 0.0173)  max mem: 16413
[2023-09-04 05:41:32,491] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:41:32,491] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:41:32,491] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:41:32,491] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:41:39,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22453
[2023-09-04 05:41:39,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22453
[2023-09-04 05:41:39,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:41:39,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:41:39,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000003  loss: 1.8847 (1.7521)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3296 (7.2162)  time: 0.9111 (0.5180 -- 3.8809)  data: 0.0022 (0.0006 -- 0.0165)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000003  loss: 1.5546 (1.7090)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2263 (7.1616)  time: 0.7613 (0.5248 -- 2.6609)  data: 0.0015 (0.0005 -- 0.0037)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000003  loss: 1.5679 (1.6928)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5909 (7.3321)  time: 0.8612 (0.5168 -- 4.1009)  data: 0.0018 (0.0002 -- 0.0145)  max mem: 16413
Epoch: [140]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000003  loss: 1.8026 (1.7216)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1979 (7.2950)  time: 0.8231 (0.5318 -- 3.2597)  data: 0.0018 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000003  loss: 1.7354 (1.7140)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2945 (7.3113)  time: 0.8617 (0.5306 -- 2.1326)  data: 0.1079 (0.0002 -- 1.2599)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.6524 (1.7069)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8796 (7.2409)  time: 0.7621 (0.4960 -- 4.4823)  data: 0.0412 (0.0002 -- 0.8115)  max mem: 16413
Epoch: [140] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.6524 (1.6850)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8796 (7.2409)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1560 (0.1560)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3235 (2.3235 -- 2.3235)  data: 2.0770 (2.0770 -- 2.0770)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2904 (0.4840)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4192 (0.2018 -- 2.3235)  data: 0.2089 (0.0008 -- 2.0770)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3355 (0.4776)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2256 (0.1692 -- 0.3768)  data: 0.0253 (0.0001 -- 0.1727)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4513 (0.5264)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2123 (0.1327 -- 0.3768)  data: 0.0245 (0.0001 -- 0.1727)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 87.552 Acc@5 97.718 loss 0.537
Accuracy of the network on the 482 val images: 87.55%
Max accuracy: 87.76%
Epoch: [141]  [  0/160]  eta: 0:25:38  lr: 0.000010  min_lr: 0.000003  loss: 1.1593 (1.1593)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7427 (5.7427)  time: 9.6187 (9.6187 -- 9.6187)  data: 6.7283 (6.7283 -- 6.7283)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:47  lr: 0.000010  min_lr: 0.000002  loss: 1.6895 (1.6909)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4941 (6.8605)  time: 0.7763 (0.5193 -- 3.5412)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
[2023-09-04 05:43:40,470] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:43:40,470] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:43:40,470] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:43:40,471] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [ 40/160]  eta: 0:02:06  lr: 0.000010  min_lr: 0.000002  loss: 1.5105 (1.6421)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0844 (6.7386)  time: 0.9053 (0.5307 -- 3.9626)  data: 0.0347 (0.0002 -- 0.6579)  max mem: 16413
[2023-09-04 05:44:03,384] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22606
[2023-09-04 05:44:03,384] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22606
[2023-09-04 05:44:03,384] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:44:03,384] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:44:03,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [ 60/160]  eta: 0:01:42  lr: 0.000010  min_lr: 0.000002  loss: 1.6362 (1.6350)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2655 (7.1030)  time: 0.9727 (0.5178 -- 3.0647)  data: 0.3090 (0.0003 -- 2.5303)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000002  loss: 1.6206 (1.6528)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8058 (7.1200)  time: 0.7311 (0.5247 -- 2.9759)  data: 0.1856 (0.0004 -- 2.4089)  max mem: 16413
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000002  loss: 1.6869 (1.6642)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2931 (7.1113)  time: 0.9997 (0.5240 -- 4.0849)  data: 0.4360 (0.0003 -- 3.5363)  max mem: 16413
Epoch: [141]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000002  loss: 1.5917 (1.6475)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9084 (7.1088)  time: 0.7021 (0.5307 -- 2.2352)  data: 0.1523 (0.0005 -- 1.7112)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000002  loss: 1.4402 (1.6165)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2271 (7.2732)  time: 0.9503 (0.5238 -- 4.2286)  data: 0.4034 (0.0005 -- 3.6997)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000002  loss: 1.5703 (1.6072)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7459 (7.2441)  time: 0.6697 (0.4949 -- 2.6249)  data: 0.1539 (0.0002 -- 2.1156)  max mem: 16413
Epoch: [141] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000002  loss: 1.5703 (1.6201)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7459 (7.2441)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1458 (0.1458)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3373 (2.3373 -- 2.3373)  data: 2.1103 (2.1103 -- 2.1103)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3282 (0.4943)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4283 (0.2067 -- 2.3373)  data: 0.2080 (0.0006 -- 2.1103)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3282 (0.4767)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.8836)  time: 0.2250 (0.1692 -- 0.4201)  data: 0.0208 (0.0001 -- 0.1681)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4453 (0.5417)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2074 (0.1327 -- 0.4201)  data: 0.0202 (0.0001 -- 0.1681)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 86.100 Acc@5 98.340 loss 0.541
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
Epoch: [142]  [  0/160]  eta: 0:19:18  lr: 0.000010  min_lr: 0.000002  loss: 1.8849 (1.8849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1750 (7.1750)  time: 7.2430 (7.2430 -- 7.2430)  data: 5.8123 (5.8123 -- 5.8123)  max mem: 16413
[2023-09-04 05:46:03,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:46:03,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:46:03,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:46:03,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [142]  [ 20/160]  eta: 0:02:43  lr: 0.000010  min_lr: 0.000002  loss: 1.6663 (1.6740)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7938 (6.7985)  time: 0.8613 (0.5295 -- 2.1475)  data: 0.0026 (0.0006 -- 0.0139)  max mem: 16413
[2023-09-04 05:46:10,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22741
[2023-09-04 05:46:10,308] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:46:10,308] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 05:46:10,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22741
[2023-09-04 05:46:10,309] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [142]  [ 40/160]  eta: 0:02:01  lr: 0.000009  min_lr: 0.000002  loss: 1.7191 (1.7011)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6341 (6.8081)  time: 0.8450 (0.5274 -- 3.1381)  data: 0.1508 (0.0006 -- 2.6078)  max mem: 16413
Epoch: [142]  [ 60/160]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000002  loss: 1.6396 (1.6853)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2450 (7.0372)  time: 0.9627 (0.5146 -- 3.1836)  data: 0.3162 (0.0004 -- 2.6698)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000002  loss: 1.8471 (1.7055)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2778 (7.2359)  time: 0.9364 (0.5176 -- 3.2326)  data: 0.1366 (0.0002 -- 2.7098)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000002  loss: 1.6821 (1.7015)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7966 (7.2221)  time: 0.7128 (0.5317 -- 2.3630)  data: 0.0295 (0.0003 -- 0.5559)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000002  loss: 1.4691 (1.6656)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5489 (7.3583)  time: 0.8249 (0.5313 -- 3.8661)  data: 0.0027 (0.0004 -- 0.0171)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.6781 (1.6653)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9518 (7.4492)  time: 0.9266 (0.5462 -- 2.9845)  data: 0.1182 (0.0005 -- 2.3226)  max mem: 16413
[2023-09-04 05:48:02,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:48:02,327] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:48:02,327] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:48:02,327] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.5394 (1.6450)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5891 (7.5309)  time: 0.8340 (0.4964 -- 2.8608)  data: 0.1948 (0.0002 -- 2.3226)  max mem: 16413
Epoch: [142] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.5394 (1.6472)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5891 (7.5309)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1481 (0.1481)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2600 (2.2600 -- 2.2600)  data: 2.0509 (2.0509 -- 2.0509)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4739 (0.5133)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4219 (0.2046 -- 2.2600)  data: 0.2069 (0.0007 -- 2.0509)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3712 (0.4914)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2274 (0.1718 -- 0.3593)  data: 0.0157 (0.0001 -- 0.1219)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4590 (0.5489)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2126 (0.1357 -- 0.3593)  data: 0.0149 (0.0001 -- 0.1219)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 86.722 Acc@5 98.548 loss 0.546
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [143]  [  0/160]  eta: 0:22:55  lr: 0.000009  min_lr: 0.000002  loss: 1.4810 (1.4810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2151 (7.2151)  time: 8.5949 (8.5949 -- 8.5949)  data: 6.5358 (6.5358 -- 6.5358)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:48  lr: 0.000009  min_lr: 0.000002  loss: 1.5534 (1.5950)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0012 (6.8569)  time: 0.8318 (0.5263 -- 3.4022)  data: 0.0700 (0.0003 -- 1.2834)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:06  lr: 0.000009  min_lr: 0.000002  loss: 1.7515 (1.6594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7779 (7.0529)  time: 0.8929 (0.5242 -- 3.1228)  data: 0.0024 (0.0002 -- 0.0163)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000002  loss: 1.4994 (1.5887)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7552 (7.0602)  time: 0.8393 (0.5210 -- 3.5621)  data: 0.0016 (0.0005 -- 0.0065)  max mem: 16413
Epoch: [143]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000002  loss: 1.6793 (1.6140)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8180 (7.0189)  time: 0.8813 (0.5174 -- 3.7726)  data: 0.0021 (0.0003 -- 0.0140)  max mem: 16413
[2023-09-04 05:49:43,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22972
[2023-09-04 05:49:43,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:49:43,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22972
[2023-09-04 05:49:43,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:49:43,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [143]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000002  loss: 1.6083 (1.6298)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5867 (7.2388)  time: 0.8515 (0.5099 -- 2.7479)  data: 0.1641 (0.0002 -- 2.2305)  max mem: 16413
[2023-09-04 05:50:05,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=131, lr=[2.293499919103678e-06, 2.293499919103678e-06, 2.5483332434485317e-06, 2.5483332434485317e-06, 2.8314813816094788e-06, 2.8314813816094788e-06, 3.1460904240105323e-06, 3.1460904240105323e-06, 3.495656026678369e-06, 3.495656026678369e-06, 3.884062251864855e-06, 3.884062251864855e-06, 4.315624724294282e-06, 4.315624724294282e-06, 4.795138582549202e-06, 4.795138582549202e-06, 5.3279317583880026e-06, 5.3279317583880026e-06, 5.91992417598667e-06, 5.91992417598667e-06, 6.577693528874078e-06, 6.577693528874078e-06, 7.308548365415641e-06, 7.308548365415641e-06, 8.120609294906268e-06, 8.120609294906268e-06, 9.02289921656252e-06, 9.02289921656252e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 05:50:05,528] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=18.076160359663636, CurrSamplesPerSec=22.227758836934058, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000002  loss: 1.7212 (1.6331)  loss_scale: 16384.0000 (28841.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8846 (7.3430)  time: 0.7958 (0.5331 -- 3.3350)  data: 0.2431 (0.0007 -- 2.8155)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.7856 (1.6517)  loss_scale: 16384.0000 (27074.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3421 (7.4069)  time: 0.8757 (0.5200 -- 2.8561)  data: 0.2693 (0.0004 -- 2.3128)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.6904 (1.6507)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6207 (7.3805)  time: 0.7303 (0.4955 -- 2.7991)  data: 0.1426 (0.0002 -- 2.2948)  max mem: 16413
Epoch: [143] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.6904 (1.6486)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6207 (7.3805)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1444 (0.1444)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3690 (2.3690 -- 2.3690)  data: 2.1229 (2.1229 -- 2.1229)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4469 (0.4924)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4138 (0.2063 -- 2.3690)  data: 0.1940 (0.0004 -- 2.1229)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3756 (0.4792)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2203 (0.1701 -- 0.3191)  data: 0.0112 (0.0001 -- 0.1388)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4477 (0.5341)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2039 (0.1331 -- 0.3191)  data: 0.0110 (0.0001 -- 0.1388)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.537
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [144]  [  0/160]  eta: 0:21:38  lr: 0.000009  min_lr: 0.000002  loss: 0.8065 (0.8065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9853 (10.9853)  time: 8.1181 (8.1181 -- 8.1181)  data: 6.5027 (6.5027 -- 6.5027)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:41  lr: 0.000009  min_lr: 0.000002  loss: 1.4995 (1.5750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5398 (7.5905)  time: 0.8076 (0.5304 -- 3.6681)  data: 0.0138 (0.0004 -- 0.2463)  max mem: 16413
Epoch: [144]  [ 40/160]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000002  loss: 1.6655 (1.5897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1260 (7.3164)  time: 0.9202 (0.5257 -- 2.9184)  data: 0.0687 (0.0006 -- 0.8740)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000002  loss: 1.7262 (1.6387)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3633 (7.4503)  time: 0.8339 (0.5277 -- 3.9050)  data: 0.0159 (0.0005 -- 0.2361)  max mem: 16413
[2023-09-04 05:51:45,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:51:45,332] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:51:45,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:51:45,333] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [144]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000002  loss: 1.6118 (1.6113)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1222 (7.4656)  time: 0.8780 (0.5303 -- 2.3866)  data: 0.1208 (0.0003 -- 1.6008)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000002  loss: 1.6781 (1.6315)  loss_scale: 32768.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1889 (7.4719)  time: 0.9458 (0.5319 -- 2.7223)  data: 0.2062 (0.0004 -- 2.0058)  max mem: 16413
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000002  loss: 1.6567 (1.6438)  loss_scale: 32768.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4895 (7.5295)  time: 0.8526 (0.5187 -- 2.5317)  data: 0.0434 (0.0003 -- 0.8401)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.5008 (1.6229)  loss_scale: 32768.0000 (25679.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8988 (7.4727)  time: 0.9540 (0.5085 -- 4.1014)  data: 0.3635 (0.0003 -- 3.5661)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.6656 (1.6149)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8443 (7.5161)  time: 0.5500 (0.4973 -- 1.1181)  data: 0.0300 (0.0002 -- 0.5873)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.6656 (1.6126)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8443 (7.5161)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1378 (0.1378)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3196 (2.3196 -- 2.3196)  data: 2.1234 (2.1234 -- 2.1234)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4810 (0.4985)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4158 (0.1961 -- 2.3196)  data: 0.2066 (0.0008 -- 2.1234)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3675 (0.4839)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2250 (0.1723 -- 0.3716)  data: 0.0215 (0.0001 -- 0.1770)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4571 (0.5337)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2125 (0.1338 -- 0.3716)  data: 0.0213 (0.0001 -- 0.1770)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 87.344 Acc@5 98.548 loss 0.536
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [145]  [  0/160]  eta: 0:19:04  lr: 0.000009  min_lr: 0.000002  loss: 1.3258 (1.3258)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9118 (8.9118)  time: 7.1552 (7.1552 -- 7.1552)  data: 5.7779 (5.7779 -- 5.7779)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:02:54  lr: 0.000009  min_lr: 0.000002  loss: 1.5692 (1.6335)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2236 (7.4454)  time: 0.9505 (0.5309 -- 5.2579)  data: 0.0827 (0.0005 -- 1.0690)  max mem: 16413
[2023-09-04 05:53:49,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:53:49,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:53:49,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:53:49,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 05:53:52,561] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23232
[2023-09-04 05:53:52,561] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23232
[2023-09-04 05:53:52,561] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:53:52,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 05:53:52,562] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [145]  [ 40/160]  eta: 0:02:11  lr: 0.000009  min_lr: 0.000002  loss: 1.7355 (1.6501)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3103 (7.0434)  time: 0.9408 (0.5187 -- 4.2372)  data: 0.0019 (0.0002 -- 0.0061)  max mem: 16413
[2023-09-04 05:54:07,836] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23249
[2023-09-04 05:54:07,836] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23249
[2023-09-04 05:54:07,836] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:54:07,836] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:54:07,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000002  loss: 1.4670 (1.6195)  loss_scale: 16384.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7515 (6.8633)  time: 0.7512 (0.5013 -- 2.7941)  data: 0.0014 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000002  loss: 1.7621 (1.6496)  loss_scale: 16384.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1434 (6.9161)  time: 0.9000 (0.5299 -- 3.5064)  data: 0.0015 (0.0007 -- 0.0029)  max mem: 16413
Epoch: [145]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000002  loss: 1.4180 (1.6210)  loss_scale: 16384.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5799 (7.1560)  time: 0.7963 (0.5209 -- 3.2743)  data: 0.0148 (0.0004 -- 0.2675)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.7899 (1.6419)  loss_scale: 16384.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6309 (7.4023)  time: 0.9590 (0.5201 -- 3.3083)  data: 0.0335 (0.0004 -- 0.3813)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5562 (1.6438)  loss_scale: 16384.0000 (22774.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5924 (7.3463)  time: 0.8015 (0.5142 -- 3.6878)  data: 0.0908 (0.0004 -- 1.7794)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.5528 (1.6316)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2920 (7.2406)  time: 0.7049 (0.4952 -- 2.8078)  data: 0.1859 (0.0002 -- 2.2686)  max mem: 16413
Epoch: [145] Total time: 0:02:22 (0.8919 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.5528 (1.6407)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2920 (7.2406)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1435 (0.1435)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6517 (2.6517 -- 2.6517)  data: 2.4374 (2.4374 -- 2.4374)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2906 (0.4629)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4448 (0.1969 -- 2.6517)  data: 0.2262 (0.0007 -- 2.4374)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3366 (0.4638)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2160 (0.1689 -- 0.3196)  data: 0.0093 (0.0001 -- 0.1329)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4536 (0.5194)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2000 (0.1322 -- 0.3196)  data: 0.0091 (0.0001 -- 0.1329)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 86.722 Acc@5 98.340 loss 0.527
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [146]  [  0/160]  eta: 0:19:24  lr: 0.000008  min_lr: 0.000002  loss: 1.8153 (1.8153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5263 (5.5263)  time: 7.2774 (7.2774 -- 7.2774)  data: 6.7491 (6.7491 -- 6.7491)  max mem: 16413
[2023-09-04 05:56:11,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:56:11,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 05:56:11,530] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 05:56:11,530] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [146]  [ 20/160]  eta: 0:02:54  lr: 0.000008  min_lr: 0.000002  loss: 1.5709 (1.5735)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7561 (6.6617)  time: 0.9477 (0.5290 -- 4.9211)  data: 0.3931 (0.0005 -- 4.3856)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:12  lr: 0.000008  min_lr: 0.000002  loss: 1.5735 (1.5549)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9416 (7.1318)  time: 0.9594 (0.5167 -- 5.0614)  data: 0.4251 (0.0003 -- 4.5524)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:39  lr: 0.000008  min_lr: 0.000002  loss: 1.5544 (1.5518)  loss_scale: 32768.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8767 (7.0666)  time: 0.7652 (0.5232 -- 3.5438)  data: 0.2195 (0.0003 -- 3.0152)  max mem: 16413
[2023-09-04 05:56:47,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23421
[2023-09-04 05:56:47,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23421
[2023-09-04 05:56:47,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:56:47,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 05:56:47,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [146]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000002  loss: 1.5939 (1.5613)  loss_scale: 16384.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9883 (7.2189)  time: 0.8923 (0.5262 -- 2.5737)  data: 0.3484 (0.0003 -- 2.0400)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.8192 (1.6117)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5778 (7.1521)  time: 0.8524 (0.5096 -- 2.5482)  data: 0.3161 (0.0003 -- 2.0121)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.7001 (1.6217)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5938 (7.1489)  time: 0.8211 (0.5318 -- 3.5008)  data: 0.2656 (0.0004 -- 2.9753)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5721 (1.6019)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2267 (7.2734)  time: 0.7890 (0.5311 -- 2.1534)  data: 0.2391 (0.0004 -- 1.6042)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.6245 (1.6142)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6032 (7.3577)  time: 0.7453 (0.4954 -- 4.2350)  data: 0.2276 (0.0002 -- 3.7245)  max mem: 16413
Epoch: [146] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.6245 (1.6095)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6032 (7.3577)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1396 (0.1396)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3508 (2.3508 -- 2.3508)  data: 2.1060 (2.1060 -- 2.1060)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4016 (0.4855)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4230 (0.2138 -- 2.3508)  data: 0.1974 (0.0008 -- 2.1060)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3322 (0.4672)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2173 (0.1710 -- 0.2558)  data: 0.0042 (0.0002 -- 0.0368)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4464 (0.5276)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.1991 (0.1335 -- 0.2558)  data: 0.0039 (0.0001 -- 0.0368)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.529
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [147]  [  0/160]  eta: 0:21:55  lr: 0.000008  min_lr: 0.000002  loss: 1.3309 (1.3309)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1896 (7.1896)  time: 8.2191 (8.2191 -- 8.2191)  data: 7.6997 (7.6997 -- 7.6997)  max mem: 16413
[2023-09-04 05:58:34,159] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23529
[2023-09-04 05:58:34,159] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23529
[2023-09-04 05:58:34,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 05:58:34,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 05:58:34,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [147]  [ 20/160]  eta: 0:02:48  lr: 0.000008  min_lr: 0.000002  loss: 1.5737 (1.5715)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6540 (7.1250)  time: 0.8519 (0.5249 -- 4.6701)  data: 0.3134 (0.0005 -- 4.1410)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:07  lr: 0.000008  min_lr: 0.000002  loss: 1.6236 (1.6249)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3646 (7.3026)  time: 0.9153 (0.5223 -- 2.8398)  data: 0.3678 (0.0004 -- 2.2794)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000002  loss: 1.6596 (1.6099)  loss_scale: 8192.0000 (9400.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4387 (7.1992)  time: 0.8289 (0.5139 -- 3.8047)  data: 0.2871 (0.0003 -- 3.3001)  max mem: 16413
Epoch: [147]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5996 (1.6258)  loss_scale: 8192.0000 (9102.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5743 (7.2302)  time: 0.9781 (0.5315 -- 4.2609)  data: 0.4352 (0.0003 -- 3.7505)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.8184 (1.6613)  loss_scale: 8192.0000 (8921.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2533 (7.3194)  time: 0.7850 (0.5259 -- 3.0050)  data: 0.2380 (0.0001 -- 2.4624)  max mem: 16413
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.6719 (1.6609)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2766 (7.2817)  time: 0.9470 (0.5181 -- 3.3068)  data: 0.3998 (0.0004 -- 2.7772)  max mem: 16413
[2023-09-04 06:00:26,407] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:00:26,407] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:00:26,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 06:00:26,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5314 (1.6524)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8821 (7.1964)  time: 0.8304 (0.5209 -- 3.9432)  data: 0.2846 (0.0002 -- 3.4018)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.7105 (1.6598)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4221 (7.2378)  time: 0.6395 (0.4942 -- 2.2326)  data: 0.1198 (0.0002 -- 1.6780)  max mem: 16413
Epoch: [147] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.7105 (1.6310)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4221 (7.2378)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1358 (0.1358)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2810 (2.2810 -- 2.2810)  data: 2.0748 (2.0748 -- 2.0748)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4037 (0.4857)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4364 (0.1995 -- 2.2810)  data: 0.2202 (0.0005 -- 2.0748)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3803 (0.4728)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2317 (0.1695 -- 0.5851)  data: 0.0272 (0.0001 -- 0.3391)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4591 (0.5321)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2179 (0.1342 -- 0.5851)  data: 0.0269 (0.0001 -- 0.3391)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 86.307 Acc@5 98.133 loss 0.533
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 87.76%
Epoch: [148]  [  0/160]  eta: 0:20:36  lr: 0.000008  min_lr: 0.000002  loss: 1.5086 (1.5086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.7062 (16.7062)  time: 7.7289 (7.7289 -- 7.7289)  data: 7.0403 (7.0403 -- 7.0403)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:51  lr: 0.000008  min_lr: 0.000002  loss: 1.6386 (1.6192)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2032 (7.6130)  time: 0.9003 (0.5194 -- 4.1091)  data: 0.3579 (0.0002 -- 3.5940)  max mem: 16413
Epoch: [148]  [ 40/160]  eta: 0:02:04  lr: 0.000008  min_lr: 0.000002  loss: 1.5783 (1.6122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3283 (7.4407)  time: 0.8411 (0.5321 -- 2.4917)  data: 0.2743 (0.0004 -- 1.9640)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000002  loss: 1.5404 (1.6103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7485 (7.2754)  time: 0.9331 (0.5361 -- 3.5656)  data: 0.3865 (0.0004 -- 3.0533)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000002  loss: 1.5264 (1.6094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4254 (7.3140)  time: 0.8196 (0.5230 -- 3.1144)  data: 0.2743 (0.0002 -- 2.5812)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.5930 (1.6126)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2050 (7.4319)  time: 0.8357 (0.5276 -- 2.7354)  data: 0.2910 (0.0002 -- 2.1828)  max mem: 16413
[2023-09-04 06:02:26,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:02:26,489] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:02:26,489] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:02:26,490] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [148]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000002  loss: 1.4946 (1.6020)  loss_scale: 32768.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2048 (7.3955)  time: 0.8762 (0.5111 -- 3.7222)  data: 0.2944 (0.0003 -- 3.1815)  max mem: 16413
[2023-09-04 06:02:54,782] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23814
[2023-09-04 06:02:54,782] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23814
[2023-09-04 06:02:54,782] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:02:54,782] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:02:54,782] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5473 (1.6096)  loss_scale: 32768.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6804 (7.4039)  time: 0.9243 (0.5053 -- 5.0605)  data: 0.3756 (0.0001 -- 4.5518)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.6743 (1.6189)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4239 (7.3957)  time: 0.6964 (0.4937 -- 1.8365)  data: 0.1763 (0.0002 -- 1.3450)  max mem: 16413
Epoch: [148] Total time: 0:02:23 (0.8984 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.6743 (1.6390)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4239 (7.3957)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3158 (2.3158 -- 2.3158)  data: 2.0990 (2.0990 -- 2.0990)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4490 (0.4886)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4166 (0.2005 -- 2.3158)  data: 0.2030 (0.0005 -- 2.0990)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3255 (0.4656)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2265 (0.1701 -- 0.4648)  data: 0.0207 (0.0001 -- 0.2750)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4591 (0.5291)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2097 (0.1319 -- 0.4648)  data: 0.0204 (0.0001 -- 0.2750)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.532
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
Epoch: [149]  [  0/160]  eta: 0:15:13  lr: 0.000008  min_lr: 0.000002  loss: 1.3758 (1.3758)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7686 (7.7686)  time: 5.7092 (5.7092 -- 5.7092)  data: 5.1882 (5.1882 -- 5.1882)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:36  lr: 0.000007  min_lr: 0.000002  loss: 1.4933 (1.5989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0900 (7.4295)  time: 0.8896 (0.5267 -- 2.9485)  data: 0.2443 (0.0007 -- 2.0738)  max mem: 16413
Epoch: [149]  [ 40/160]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000002  loss: 1.6436 (1.6329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1994 (7.2130)  time: 0.8728 (0.5231 -- 2.8732)  data: 0.1790 (0.0002 -- 2.3529)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000002  loss: 1.7402 (1.6588)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4198 (7.2921)  time: 0.8927 (0.5277 -- 2.8594)  data: 0.1666 (0.0003 -- 2.0907)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.4945 (1.6435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6143 (7.3151)  time: 0.9079 (0.5218 -- 2.7136)  data: 0.0013 (0.0002 -- 0.0033)  max mem: 16413
[2023-09-04 06:04:47,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23933
[2023-09-04 06:04:47,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23933
[2023-09-04 06:04:47,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:04:47,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:04:47,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [149]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.4974 (1.6378)  loss_scale: 16384.0000 (15735.1287)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2916 (7.2796)  time: 0.9398 (0.5251 -- 3.9763)  data: 0.0011 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.7310 (1.6418)  loss_scale: 8192.0000 (14488.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0156 (7.2618)  time: 0.7237 (0.5349 -- 2.7006)  data: 0.0446 (0.0004 -- 0.8597)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.3530 (1.6253)  loss_scale: 8192.0000 (13595.2340)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4411 (7.4393)  time: 0.8548 (0.5291 -- 4.7599)  data: 0.0023 (0.0004 -- 0.0111)  max mem: 16413
[2023-09-04 06:05:40,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=137, lr=[1.8411106802422644e-06, 1.8411106802422644e-06, 2.0456785336025163e-06, 2.0456785336025163e-06, 2.2729761484472397e-06, 2.2729761484472397e-06, 2.5255290538302664e-06, 2.5255290538302664e-06, 2.8061433931447404e-06, 2.8061433931447404e-06, 3.1179371034941563e-06, 3.1179371034941563e-06, 3.464374559437951e-06, 3.464374559437951e-06, 3.849305066042168e-06, 3.849305066042168e-06, 4.277005628935741e-06, 4.277005628935741e-06, 4.752228476595268e-06, 4.752228476595268e-06, 5.280253862883632e-06, 5.280253862883632e-06, 5.866948736537368e-06, 5.866948736537368e-06, 6.518831929485964e-06, 6.518831929485964e-06, 7.243146588317738e-06, 7.243146588317738e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 06:05:40,954] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=18.099114684375664, CurrSamplesPerSec=24.77906004134467, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.7319 (1.6344)  loss_scale: 8192.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2287 (7.4032)  time: 0.7469 (0.4941 -- 3.6033)  data: 0.0009 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [149] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.7319 (1.6558)  loss_scale: 8192.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2287 (7.4032)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1338 (0.1338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2921 (2.2921 -- 2.2921)  data: 2.0678 (2.0678 -- 2.0678)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4837 (0.5010)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4119 (0.1898 -- 2.2921)  data: 0.2017 (0.0005 -- 2.0678)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4313 (0.4843)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2209 (0.1698 -- 0.3730)  data: 0.0196 (0.0001 -- 0.1427)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4645 (0.5456)  acc1: 77.7778 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.2091 (0.1331 -- 0.3730)  data: 0.0193 (0.0001 -- 0.1427)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.541
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.76%
Epoch: [150]  [  0/160]  eta: 0:22:25  lr: 0.000007  min_lr: 0.000002  loss: 1.8959 (1.8959)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2391 (5.2391)  time: 8.4116 (8.4116 -- 8.4116)  data: 7.8386 (7.8386 -- 7.8386)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:02:46  lr: 0.000007  min_lr: 0.000002  loss: 1.6954 (1.7607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5508 (6.6976)  time: 0.8286 (0.5204 -- 2.8227)  data: 0.2847 (0.0004 -- 2.2983)  max mem: 16413
Epoch: [150]  [ 40/160]  eta: 0:02:05  lr: 0.000007  min_lr: 0.000002  loss: 1.4648 (1.6220)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2672 (7.1047)  time: 0.8923 (0.5164 -- 2.4594)  data: 0.3243 (0.0002 -- 1.9374)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:39  lr: 0.000007  min_lr: 0.000002  loss: 1.7155 (1.6281)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6737 (7.3273)  time: 0.9044 (0.5106 -- 2.4986)  data: 0.3602 (0.0003 -- 1.9539)  max mem: 16413
[2023-09-04 06:06:50,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:06:50,684] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 06:06:50,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:06:50,684] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [150]  [ 80/160]  eta: 0:01:17  lr: 0.000007  min_lr: 0.000002  loss: 1.5351 (1.6184)  loss_scale: 16384.0000 (10113.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0813 (7.3844)  time: 0.8626 (0.5234 -- 3.1803)  data: 0.2850 (0.0003 -- 2.6497)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000002  loss: 1.6767 (1.6387)  loss_scale: 16384.0000 (11355.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0704 (7.4086)  time: 0.7967 (0.5321 -- 3.1095)  data: 0.2293 (0.0004 -- 2.5951)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.5974 (1.6206)  loss_scale: 16384.0000 (12186.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2338 (7.2556)  time: 0.8723 (0.5255 -- 3.6971)  data: 0.3297 (0.0002 -- 3.1817)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.6633 (1.6338)  loss_scale: 16384.0000 (12781.8440)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2454 (7.2040)  time: 0.8753 (0.5169 -- 3.1173)  data: 0.3313 (0.0004 -- 2.5994)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.6071 (1.6349)  loss_scale: 16384.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0179 (7.2663)  time: 0.6536 (0.4945 -- 1.4853)  data: 0.1325 (0.0002 -- 0.9880)  max mem: 16413
Epoch: [150] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.6071 (1.6216)  loss_scale: 16384.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0179 (7.2663)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1344 (0.1344)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3479 (2.3479 -- 2.3479)  data: 2.1165 (2.1165 -- 2.1165)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3055 (0.4867)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4130 (0.2033 -- 2.3479)  data: 0.1946 (0.0009 -- 2.1165)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4100 (0.4818)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2169 (0.1705 -- 0.3813)  data: 0.0101 (0.0001 -- 0.1747)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4781 (0.5410)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.1987 (0.1330 -- 0.3813)  data: 0.0092 (0.0001 -- 0.1747)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.538
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
Epoch: [151]  [  0/160]  eta: 0:16:56  lr: 0.000007  min_lr: 0.000002  loss: 1.0604 (1.0604)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4830 (7.4830)  time: 6.3506 (6.3506 -- 6.3506)  data: 5.7637 (5.7637 -- 5.7637)  max mem: 16413
Epoch: [151]  [ 20/160]  eta: 0:02:42  lr: 0.000007  min_lr: 0.000002  loss: 1.6133 (1.5653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4703 (7.5154)  time: 0.8979 (0.5340 -- 2.7735)  data: 0.2251 (0.0005 -- 1.3702)  max mem: 16413
[2023-09-04 06:08:50,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:08:50,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:08:50,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:08:50,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [ 40/160]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000002  loss: 1.6746 (1.5876)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7728 (7.2040)  time: 0.8164 (0.5235 -- 1.8099)  data: 0.1771 (0.0006 -- 1.2694)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:34  lr: 0.000007  min_lr: 0.000002  loss: 1.5994 (1.5731)  loss_scale: 32768.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0170 (7.3224)  time: 0.8401 (0.5282 -- 3.0321)  data: 0.2563 (0.0003 -- 2.4895)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.6089 (1.5856)  loss_scale: 32768.0000 (26699.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1658 (7.4589)  time: 0.9845 (0.5106 -- 3.9628)  data: 0.3436 (0.0005 -- 3.4454)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000002  loss: 1.5635 (1.5819)  loss_scale: 32768.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1415 (7.4199)  time: 0.7895 (0.5297 -- 3.0617)  data: 0.2202 (0.0002 -- 2.5349)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.7498 (1.6080)  loss_scale: 32768.0000 (28705.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2114 (7.4034)  time: 0.9450 (0.5270 -- 4.1135)  data: 0.3926 (0.0005 -- 3.5972)  max mem: 16413
[2023-09-04 06:10:10,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24281
[2023-09-04 06:10:10,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24281
[2023-09-04 06:10:10,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:10:10,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:10:10,477] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [140/160]  eta: 0:00:17  lr: 0.000007  min_lr: 0.000002  loss: 1.4423 (1.5913)  loss_scale: 16384.0000 (26958.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2278 (7.3797)  time: 0.7100 (0.5208 -- 2.4570)  data: 0.1628 (0.0003 -- 1.9081)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.9127 (1.6195)  loss_scale: 16384.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8122 (7.4592)  time: 0.7341 (0.4998 -- 2.1605)  data: 0.2121 (0.0002 -- 1.6053)  max mem: 16413
Epoch: [151] Total time: 0:02:20 (0.8762 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.9127 (1.6309)  loss_scale: 16384.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8122 (7.4592)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1338 (0.1338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3992 (2.3992 -- 2.3992)  data: 2.1696 (2.1696 -- 2.1696)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4005 (0.5104)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4270 (0.2050 -- 2.3992)  data: 0.1988 (0.0009 -- 2.1696)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4079 (0.4875)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2161 (0.1718 -- 0.2486)  data: 0.0035 (0.0001 -- 0.0508)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4421 (0.5498)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.1972 (0.1337 -- 0.2420)  data: 0.0031 (0.0001 -- 0.0508)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 85.892 Acc@5 98.133 loss 0.541
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.76%
Epoch: [152]  [  0/160]  eta: 0:21:02  lr: 0.000007  min_lr: 0.000002  loss: 1.6796 (1.6796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7569 (6.7569)  time: 7.8917 (7.8917 -- 7.8917)  data: 5.9690 (5.9690 -- 5.9690)  max mem: 16413
[2023-09-04 06:11:03,963] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24334
[2023-09-04 06:11:03,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:11:03,963] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24334
[2023-09-04 06:11:03,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:11:03,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [152]  [ 20/160]  eta: 0:02:39  lr: 0.000007  min_lr: 0.000002  loss: 1.6152 (1.6219)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7102 (7.3113)  time: 0.8007 (0.5217 -- 3.2451)  data: 0.1778 (0.0005 -- 1.4613)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000002  loss: 1.7128 (1.6762)  loss_scale: 8192.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6495 (7.2776)  time: 0.8239 (0.5331 -- 2.2540)  data: 0.1298 (0.0009 -- 1.4864)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000002  loss: 1.6925 (1.6512)  loss_scale: 8192.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5537 (7.4444)  time: 1.0427 (0.5156 -- 3.2859)  data: 0.3272 (0.0002 -- 2.7440)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.6735 (1.6404)  loss_scale: 8192.0000 (9607.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6750 (7.4565)  time: 0.8099 (0.5166 -- 3.9662)  data: 0.2689 (0.0003 -- 3.4605)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.6915 (1.6486)  loss_scale: 8192.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1996 (7.3576)  time: 0.8889 (0.5240 -- 4.6448)  data: 0.3442 (0.0007 -- 4.1250)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.4337 (1.6351)  loss_scale: 8192.0000 (9139.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9291 (7.3693)  time: 0.7830 (0.5372 -- 2.7346)  data: 0.1936 (0.0005 -- 2.1977)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.5834 (1.6243)  loss_scale: 8192.0000 (9005.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1732 (7.3823)  time: 0.9013 (0.5259 -- 2.6902)  data: 0.3158 (0.0003 -- 2.1627)  max mem: 16413
[2023-09-04 06:12:57,779] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:12:57,779] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:12:57,779] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 06:12:57,779] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.6333 (1.6295)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0722 (7.3190)  time: 0.7021 (0.4965 -- 2.4696)  data: 0.1257 (0.0002 -- 1.9573)  max mem: 16413
Epoch: [152] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.6333 (1.6212)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0722 (7.3190)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1277 (0.1277)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3264 (2.3264 -- 2.3264)  data: 2.1112 (2.1112 -- 2.1112)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4114 (0.5030)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4098 (0.1906 -- 2.3264)  data: 0.1928 (0.0006 -- 2.1112)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4114 (0.4815)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2196 (0.1700 -- 0.4350)  data: 0.0140 (0.0002 -- 0.2302)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4465 (0.5450)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2008 (0.1331 -- 0.4350)  data: 0.0138 (0.0001 -- 0.2302)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.537
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [153]  [  0/160]  eta: 0:20:58  lr: 0.000006  min_lr: 0.000002  loss: 1.3666 (1.3666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2102 (7.2102)  time: 7.8675 (7.8675 -- 7.8675)  data: 6.9192 (6.9192 -- 6.9192)  max mem: 16413
Epoch: [153]  [ 20/160]  eta: 0:02:56  lr: 0.000006  min_lr: 0.000002  loss: 1.6530 (1.6065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6589 (8.2012)  time: 0.9267 (0.5183 -- 4.5178)  data: 0.3860 (0.0005 -- 3.9696)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:12  lr: 0.000006  min_lr: 0.000002  loss: 1.5780 (1.6331)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8111 (7.8048)  time: 0.9383 (0.5127 -- 4.9777)  data: 0.4073 (0.0002 -- 4.4645)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:42  lr: 0.000006  min_lr: 0.000002  loss: 1.6511 (1.6597)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0796 (7.4810)  time: 0.8791 (0.5139 -- 4.1074)  data: 0.3354 (0.0004 -- 3.5834)  max mem: 16413
Epoch: [153]  [ 80/160]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000002  loss: 1.5655 (1.6476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9566 (7.5028)  time: 0.8573 (0.5228 -- 3.9006)  data: 0.3150 (0.0003 -- 3.3682)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000002  loss: 1.8419 (1.6671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7714 (7.4297)  time: 0.7621 (0.5210 -- 2.7187)  data: 0.2102 (0.0004 -- 2.2085)  max mem: 16413
[2023-09-04 06:15:00,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:15:00,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:15:00,601] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:15:00,601] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:15:08,355] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24600
[2023-09-04 06:15:08,355] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24600
[2023-09-04 06:15:08,355] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:15:08,355] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:15:08,356] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000002  loss: 1.5318 (1.6550)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8439 (7.4112)  time: 0.8445 (0.5312 -- 3.1937)  data: 0.2923 (0.0003 -- 2.6632)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.6121 (1.6499)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7753 (7.4723)  time: 0.9396 (0.5256 -- 4.3181)  data: 0.3997 (0.0003 -- 3.8137)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.6211 (1.6435)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7982 (7.5576)  time: 0.6593 (0.4965 -- 2.6473)  data: 0.1356 (0.0002 -- 2.1260)  max mem: 16413
Epoch: [153] Total time: 0:02:23 (0.8968 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.6211 (1.6587)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7982 (7.5576)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1350 (0.1350)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3757 (2.3757 -- 2.3757)  data: 2.1751 (2.1751 -- 2.1751)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3627 (0.4664)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4518 (0.2008 -- 2.3757)  data: 0.2378 (0.0006 -- 2.1751)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3627 (0.4584)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2291 (0.1710 -- 0.6668)  data: 0.0273 (0.0001 -- 0.4193)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4375 (0.5170)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2138 (0.1329 -- 0.6668)  data: 0.0264 (0.0001 -- 0.4193)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 85.270 Acc@5 98.340 loss 0.533
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.76%
Epoch: [154]  [  0/160]  eta: 0:21:22  lr: 0.000006  min_lr: 0.000002  loss: 1.3125 (1.3125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9774 (5.9774)  time: 8.0166 (8.0166 -- 8.0166)  data: 4.6671 (4.6671 -- 4.6671)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:45  lr: 0.000006  min_lr: 0.000002  loss: 1.6800 (1.6528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3927 (7.8294)  time: 0.8376 (0.5353 -- 3.4149)  data: 0.0569 (0.0006 -- 0.7102)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000002  loss: 1.6471 (1.6727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4972 (7.4186)  time: 0.8422 (0.5214 -- 4.5254)  data: 0.0016 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [154]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000002  loss: 1.6163 (1.6739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3003 (7.5081)  time: 0.9017 (0.5257 -- 2.8232)  data: 0.3530 (0.0005 -- 2.2809)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000002  loss: 1.6235 (1.6671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2106 (7.4537)  time: 0.7873 (0.5168 -- 1.8200)  data: 0.1476 (0.0005 -- 1.2782)  max mem: 16413
[2023-09-04 06:17:13,977] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:17:13,977] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:17:13,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:17:13,978] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000002  loss: 1.6740 (1.6660)  loss_scale: 32768.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4440 (7.4627)  time: 0.9062 (0.5363 -- 3.5683)  data: 0.3554 (0.0006 -- 3.0367)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000002  loss: 1.8220 (1.6857)  loss_scale: 32768.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0220 (7.2866)  time: 0.8401 (0.5114 -- 4.3253)  data: 0.2822 (0.0003 -- 3.7914)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.7169 (1.6866)  loss_scale: 32768.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7425 (7.2803)  time: 0.9335 (0.5311 -- 3.4366)  data: 0.3420 (0.0004 -- 2.9156)  max mem: 16413
[2023-09-04 06:17:57,834] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24782
[2023-09-04 06:17:57,834] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:17:57,834] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24782
[2023-09-04 06:17:57,835] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:17:57,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.5165 (1.6655)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5886 (7.4062)  time: 0.6924 (0.4947 -- 2.9247)  data: 0.1750 (0.0002 -- 2.4045)  max mem: 16413
Epoch: [154] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.5165 (1.6361)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5886 (7.4062)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1340 (0.1340)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3132 (2.3132 -- 2.3132)  data: 2.0946 (2.0946 -- 2.0946)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3843 (0.4591)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4190 (0.1999 -- 2.3132)  data: 0.2013 (0.0009 -- 2.0946)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3843 (0.4546)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2288 (0.1699 -- 0.5472)  data: 0.0247 (0.0001 -- 0.3720)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4401 (0.5133)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2133 (0.1327 -- 0.5472)  data: 0.0242 (0.0001 -- 0.3720)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 86.722 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [155]  [  0/160]  eta: 0:22:37  lr: 0.000006  min_lr: 0.000002  loss: 1.3756 (1.3756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1797 (6.1797)  time: 8.4865 (8.4865 -- 8.4865)  data: 7.9417 (7.9417 -- 7.9417)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:02:55  lr: 0.000006  min_lr: 0.000002  loss: 1.7247 (1.6302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1186 (8.2836)  time: 0.8931 (0.5241 -- 4.3136)  data: 0.2384 (0.0003 -- 2.2908)  max mem: 16413
Epoch: [155]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000001  loss: 1.6223 (1.6134)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4802 (7.9976)  time: 0.8500 (0.5190 -- 3.1286)  data: 0.1015 (0.0002 -- 1.9903)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000001  loss: 1.7110 (1.6542)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7449 (7.7541)  time: 0.8838 (0.5258 -- 2.5779)  data: 0.2103 (0.0002 -- 2.0674)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000001  loss: 1.5897 (1.6359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4067 (7.5710)  time: 0.8349 (0.5294 -- 2.1566)  data: 0.1079 (0.0005 -- 1.6410)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 1.7013 (1.6381)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4137 (7.6580)  time: 0.9049 (0.5173 -- 3.0109)  data: 0.2994 (0.0003 -- 2.4640)  max mem: 16413
[2023-09-04 06:20:00,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:20:00,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:20:00,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:20:00,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [155]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000001  loss: 1.5836 (1.6152)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6437 (7.7194)  time: 0.8153 (0.5263 -- 3.1119)  data: 0.1121 (0.0002 -- 1.7440)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000001  loss: 1.5473 (1.6141)  loss_scale: 32768.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7031 (7.6084)  time: 0.8633 (0.5165 -- 2.9989)  data: 0.2261 (0.0003 -- 2.4818)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000001  loss: 1.8205 (1.6337)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8143 (7.5109)  time: 0.6738 (0.4941 -- 2.1200)  data: 0.0012 (0.0002 -- 0.0139)  max mem: 16413
Epoch: [155] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000001  loss: 1.8205 (1.6513)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8143 (7.5109)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1366 (0.1366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3458 (2.3458 -- 2.3458)  data: 2.1448 (2.1448 -- 2.1448)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3523 (0.4563)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4126 (0.2006 -- 2.3458)  data: 0.2045 (0.0005 -- 2.1448)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3659 (0.4545)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2251 (0.1720 -- 0.5720)  data: 0.0249 (0.0001 -- 0.3912)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4508 (0.5179)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2101 (0.1333 -- 0.5720)  data: 0.0246 (0.0001 -- 0.3912)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.528
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [156]  [  0/160]  eta: 0:21:38  lr: 0.000006  min_lr: 0.000001  loss: 1.9497 (1.9497)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4531 (8.4531)  time: 8.1128 (8.1128 -- 8.1128)  data: 5.2798 (5.2798 -- 5.2798)  max mem: 16413
Epoch: [156]  [ 20/160]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000001  loss: 1.6492 (1.7553)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4827 (7.2633)  time: 0.8478 (0.5200 -- 2.9765)  data: 0.2596 (0.0007 -- 2.4616)  max mem: 16413
[2023-09-04 06:21:17,766] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24987
[2023-09-04 06:21:17,766] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24987
[2023-09-04 06:21:17,767] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:21:17,767] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:21:17,767] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 06:21:27,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=142, lr=[1.4304821545984304e-06, 1.4304821545984304e-06, 1.5894246162204784e-06, 1.5894246162204784e-06, 1.7660273513560868e-06, 1.7660273513560868e-06, 1.9622526126178744e-06, 1.9622526126178744e-06, 2.180280680686527e-06, 2.180280680686527e-06, 2.4225340896516965e-06, 2.4225340896516965e-06, 2.6917045440574407e-06, 2.6917045440574407e-06, 2.9907828267304892e-06, 2.9907828267304892e-06, 3.323092029700544e-06, 3.323092029700544e-06, 3.6923244774450484e-06, 3.6923244774450484e-06, 4.102582752716721e-06, 4.102582752716721e-06, 4.558425280796356e-06, 4.558425280796356e-06, 5.064916978662618e-06, 5.064916978662618e-06, 5.627685531847353e-06, 5.627685531847353e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 06:21:27,206] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=18.174002504025186, CurrSamplesPerSec=20.804391389173638, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000001  loss: 1.6420 (1.6607)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3828 (6.8999)  time: 0.7730 (0.5171 -- 1.4388)  data: 0.0721 (0.0003 -- 0.7832)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000001  loss: 1.6442 (1.6475)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1620 (7.3721)  time: 0.8955 (0.5271 -- 3.0077)  data: 0.2532 (0.0004 -- 2.4599)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000001  loss: 1.5614 (1.6347)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7960 (7.3626)  time: 0.9552 (0.5091 -- 4.9347)  data: 0.0216 (0.0003 -- 0.3836)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 1.5273 (1.6197)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4488 (7.3908)  time: 0.8401 (0.5171 -- 2.9110)  data: 0.0016 (0.0004 -- 0.0090)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000001  loss: 1.5737 (1.6099)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4608 (7.2365)  time: 0.9485 (0.5156 -- 3.0565)  data: 0.2065 (0.0004 -- 2.0632)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.7329 (1.6275)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0424 (7.2524)  time: 0.8954 (0.5076 -- 4.2511)  data: 0.3635 (0.0002 -- 3.7351)  max mem: 16413
[2023-09-04 06:23:10,312] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:23:10,312] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:23:10,312] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:23:10,312] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.7397 (1.6211)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3365 (7.1907)  time: 0.6798 (0.4970 -- 2.6939)  data: 0.1599 (0.0002 -- 2.1851)  max mem: 16413
Epoch: [156] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.7397 (1.6101)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3365 (7.1907)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1374 (0.1374)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2730 (2.2730 -- 2.2730)  data: 2.0625 (2.0625 -- 2.0625)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3932 (0.4741)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4153 (0.1967 -- 2.2730)  data: 0.1903 (0.0006 -- 2.0625)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4007 (0.4667)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2251 (0.1688 -- 0.4279)  data: 0.0134 (0.0001 -- 0.2335)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4541 (0.5286)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2025 (0.1326 -- 0.4279)  data: 0.0125 (0.0001 -- 0.2335)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 86.515 Acc@5 98.548 loss 0.529
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [157]  [  0/160]  eta: 0:21:59  lr: 0.000005  min_lr: 0.000001  loss: 1.0498 (1.0498)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3549 (10.3549)  time: 8.2441 (8.2441 -- 8.2441)  data: 7.6846 (7.6846 -- 7.6846)  max mem: 16413
[2023-09-04 06:23:44,976] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25139
[2023-09-04 06:23:44,976] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25139
[2023-09-04 06:23:44,976] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:23:44,976] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:23:44,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [157]  [ 20/160]  eta: 0:02:52  lr: 0.000005  min_lr: 0.000001  loss: 1.7299 (1.5697)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2878 (7.5419)  time: 0.8850 (0.5265 -- 5.1840)  data: 0.2353 (0.0003 -- 2.6327)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:14  lr: 0.000005  min_lr: 0.000001  loss: 1.5566 (1.5803)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6654 (7.5450)  time: 1.0048 (0.5161 -- 3.8989)  data: 0.0016 (0.0003 -- 0.0125)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000001  loss: 1.5864 (1.5947)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1167 (7.3143)  time: 0.7936 (0.5244 -- 3.3899)  data: 0.0018 (0.0002 -- 0.0075)  max mem: 16413
[2023-09-04 06:24:24,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25184
[2023-09-04 06:24:24,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:24:24,702] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25184
[2023-09-04 06:24:24,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:24:24,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [157]  [ 80/160]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000001  loss: 1.6340 (1.6046)  loss_scale: 8192.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9917 (7.2284)  time: 0.8780 (0.5174 -- 4.2552)  data: 0.0015 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.8272 (1.6507)  loss_scale: 8192.0000 (16465.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5529 (7.3113)  time: 0.8065 (0.5238 -- 3.7519)  data: 0.0202 (0.0003 -- 0.3693)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.5851 (1.6554)  loss_scale: 8192.0000 (15097.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5863 (7.2014)  time: 0.8529 (0.5199 -- 3.1105)  data: 0.2101 (0.0002 -- 2.5622)  max mem: 16413
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.5482 (1.6410)  loss_scale: 8192.0000 (14118.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9136 (7.3526)  time: 0.8346 (0.5320 -- 2.7960)  data: 0.2510 (0.0009 -- 2.2539)  max mem: 16413
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.3918 (1.6157)  loss_scale: 8192.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3400 (7.3087)  time: 0.7059 (0.4944 -- 2.0779)  data: 0.0945 (0.0002 -- 1.0707)  max mem: 16413
Epoch: [157] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.3918 (1.6449)  loss_scale: 8192.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3400 (7.3087)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1356 (0.1356)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4966 (2.4966 -- 2.4966)  data: 2.2550 (2.2550 -- 2.2550)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4753 (0.4828)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4226 (0.1959 -- 2.4966)  data: 0.2061 (0.0008 -- 2.2550)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4083 (0.4680)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2153 (0.1696 -- 0.4074)  data: 0.0117 (0.0001 -- 0.2185)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4549 (0.5325)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.1984 (0.1331 -- 0.4074)  data: 0.0113 (0.0001 -- 0.2185)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 85.477 Acc@5 98.548 loss 0.533
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.76%
Epoch: [158]  [  0/160]  eta: 0:20:43  lr: 0.000005  min_lr: 0.000001  loss: 1.8950 (1.8950)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9103 (6.9103)  time: 7.7737 (7.7737 -- 7.7737)  data: 6.9582 (6.9582 -- 6.9582)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:46  lr: 0.000005  min_lr: 0.000001  loss: 1.6804 (1.7108)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7925 (6.9955)  time: 0.8637 (0.5343 -- 3.3294)  data: 0.2706 (0.0003 -- 2.4253)  max mem: 16413
[2023-09-04 06:26:27,846] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:26:27,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 06:26:27,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:26:27,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [158]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000001  loss: 1.5807 (1.6401)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2555 (7.2299)  time: 0.9031 (0.5159 -- 3.2717)  data: 0.0940 (0.0004 -- 1.1668)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000001  loss: 1.6108 (1.6531)  loss_scale: 16384.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8401 (6.9063)  time: 0.7859 (0.5265 -- 3.9285)  data: 0.0016 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000001  loss: 1.6881 (1.6512)  loss_scale: 16384.0000 (13046.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7051 (6.8903)  time: 1.0615 (0.5246 -- 5.1143)  data: 0.0122 (0.0004 -- 0.2181)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000001  loss: 1.3938 (1.6278)  loss_scale: 16384.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3001 (6.8888)  time: 0.6946 (0.5201 -- 2.4172)  data: 0.0015 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [158]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.5318 (1.6217)  loss_scale: 16384.0000 (14149.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2682 (7.0336)  time: 0.9147 (0.5292 -- 3.4905)  data: 0.2547 (0.0002 -- 2.9761)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.6224 (1.6186)  loss_scale: 16384.0000 (14466.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6909 (7.1041)  time: 0.9659 (0.5212 -- 4.5392)  data: 0.4207 (0.0004 -- 4.0174)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.5175 (1.6206)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5817 (7.0887)  time: 0.6646 (0.4945 -- 1.7394)  data: 0.1500 (0.0001 -- 1.2207)  max mem: 16413
Epoch: [158] Total time: 0:02:24 (0.9021 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.5175 (1.6040)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5817 (7.0887)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1366 (0.1366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2889 (2.2889 -- 2.2889)  data: 2.0686 (2.0686 -- 2.0686)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3788 (0.4761)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4138 (0.2177 -- 2.2889)  data: 0.1908 (0.0007 -- 2.0686)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3937 (0.4707)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.8836)  time: 0.2266 (0.1702 -- 0.5735)  data: 0.0216 (0.0001 -- 0.3987)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4648 (0.5275)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2078 (0.1334 -- 0.5735)  data: 0.0213 (0.0001 -- 0.3987)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.523
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [159]  [  0/160]  eta: 0:19:41  lr: 0.000005  min_lr: 0.000001  loss: 2.0923 (2.0923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0477 (9.0477)  time: 7.3844 (7.3844 -- 7.3844)  data: 6.6640 (6.6640 -- 6.6640)  max mem: 16413
[2023-09-04 06:28:30,399] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:28:30,399] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:28:30,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:28:30,400] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [ 20/160]  eta: 0:02:39  lr: 0.000005  min_lr: 0.000001  loss: 1.5442 (1.6346)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0761 (7.7936)  time: 0.8281 (0.5224 -- 4.0148)  data: 0.2467 (0.0004 -- 2.5674)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000001  loss: 1.5862 (1.6483)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2206 (7.4175)  time: 0.9651 (0.5214 -- 3.8130)  data: 0.4199 (0.0002 -- 3.2585)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:35  lr: 0.000005  min_lr: 0.000001  loss: 1.5359 (1.6047)  loss_scale: 32768.0000 (32499.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8084 (7.2627)  time: 0.7359 (0.5224 -- 2.0806)  data: 0.1056 (0.0003 -- 1.1534)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000001  loss: 1.3837 (1.5951)  loss_scale: 32768.0000 (32565.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8426 (7.3311)  time: 0.9332 (0.5084 -- 4.8812)  data: 0.3665 (0.0003 -- 4.3514)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.5376 (1.5925)  loss_scale: 32768.0000 (32605.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8680 (7.1324)  time: 0.9049 (0.5244 -- 4.8972)  data: 0.3605 (0.0002 -- 4.3737)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000001  loss: 1.7165 (1.6044)  loss_scale: 32768.0000 (32632.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6635 (7.1256)  time: 0.8148 (0.5342 -- 2.4376)  data: 0.0025 (0.0004 -- 0.0100)  max mem: 16413
[2023-09-04 06:30:20,761] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25567
[2023-09-04 06:30:20,761] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25567
[2023-09-04 06:30:20,761] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:30:20,761] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:30:20,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.6751 (1.6144)  loss_scale: 16384.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2185 (7.1665)  time: 0.8497 (0.5334 -- 2.3540)  data: 0.0166 (0.0003 -- 0.2984)  max mem: 16413
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.7720 (1.6194)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1556 (7.1860)  time: 0.6883 (0.4959 -- 2.4496)  data: 0.0010 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [159] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.7720 (1.6153)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1556 (7.1860)
[2023-09-04 06:30:43,751] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-09-04 06:30:43,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-09-04 06:30:43,752] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-09-04 06:30:43,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-09-04 06:30:44,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-09-04 06:30:44,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1357 (0.1357)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3216 (2.3216 -- 2.3216)  data: 2.0947 (2.0947 -- 2.0947)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4530 (0.4938)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4382 (0.2017 -- 2.3216)  data: 0.2127 (0.0007 -- 2.0947)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4456 (0.4766)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.8836)  time: 0.2360 (0.1696 -- 0.5290)  data: 0.0267 (0.0001 -- 0.2859)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4559 (0.5322)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2178 (0.1323 -- 0.5290)  data: 0.0264 (0.0001 -- 0.2859)  max mem: 16413
Val: Total time: 0:00:08 (0.2973 s / it)
* Acc@1 87.137 Acc@5 98.133 loss 0.527
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [160]  [  0/160]  eta: 0:22:54  lr: 0.000005  min_lr: 0.000001  loss: 1.9266 (1.9266)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7641 (5.7641)  time: 8.5884 (8.5884 -- 8.5884)  data: 6.7832 (6.7832 -- 6.7832)  max mem: 16413
[2023-09-04 06:31:10,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25613
[2023-09-04 06:31:10,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25613
[2023-09-04 06:31:10,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 06:31:10,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-04 06:31:10,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [160]  [ 20/160]  eta: 0:02:43  lr: 0.000005  min_lr: 0.000001  loss: 1.6599 (1.6420)  loss_scale: 16384.0000 (13263.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4797 (7.4229)  time: 0.7948 (0.5166 -- 3.3316)  data: 0.2352 (0.0003 -- 2.7667)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:11  lr: 0.000005  min_lr: 0.000001  loss: 1.5301 (1.6070)  loss_scale: 8192.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8842 (7.4809)  time: 1.0224 (0.5196 -- 5.2643)  data: 0.4567 (0.0006 -- 4.7560)  max mem: 16413
Epoch: [160]  [ 60/160]  eta: 0:01:38  lr: 0.000005  min_lr: 0.000001  loss: 1.7402 (1.6273)  loss_scale: 8192.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5462 (7.1864)  time: 0.7658 (0.5229 -- 3.4311)  data: 0.2195 (0.0002 -- 2.9007)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000001  loss: 1.5306 (1.5916)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2433 (7.3225)  time: 0.9091 (0.5079 -- 3.8038)  data: 0.3710 (0.0004 -- 3.2596)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.3992 (1.5871)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1830 (7.3575)  time: 0.7996 (0.5242 -- 3.1106)  data: 0.0616 (0.0004 -- 0.6878)  max mem: 16413
Epoch: [160]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000001  loss: 1.6823 (1.6057)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1261 (7.3677)  time: 0.8697 (0.5256 -- 3.3068)  data: 0.1856 (0.0003 -- 2.1608)  max mem: 16413
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.6834 (1.6143)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3752 (7.4724)  time: 0.7699 (0.5294 -- 2.9106)  data: 0.0698 (0.0003 -- 1.0837)  max mem: 16413
[2023-09-04 06:33:05,218] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:33:05,218] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 06:33:05,219] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:33:05,259] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.5328 (1.6061)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9205 (7.4383)  time: 0.7290 (0.4963 -- 4.6405)  data: 0.0008 (0.0001 -- 0.0049)  max mem: 16413
Epoch: [160] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.5328 (1.6150)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9205 (7.4383)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1364 (0.1364)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3843 (2.3843 -- 2.3843)  data: 2.1778 (2.1778 -- 2.1778)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3772 (0.4858)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4229 (0.2085 -- 2.3843)  data: 0.2018 (0.0009 -- 2.1778)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3772 (0.4706)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2259 (0.1707 -- 0.5008)  data: 0.0183 (0.0001 -- 0.3208)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4532 (0.5253)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2075 (0.1332 -- 0.5008)  data: 0.0179 (0.0001 -- 0.3208)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.525
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [161]  [  0/160]  eta: 0:18:44  lr: 0.000005  min_lr: 0.000001  loss: 2.4969 (2.4969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2099 (6.2099)  time: 7.0302 (7.0302 -- 7.0302)  data: 6.4839 (6.4839 -- 6.4839)  max mem: 16413
Epoch: [161]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000001  loss: 1.4115 (1.5001)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6741 (6.9533)  time: 0.8397 (0.5241 -- 1.8912)  data: 0.2916 (0.0004 -- 1.3521)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000001  loss: 1.4539 (1.5037)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2298 (6.8522)  time: 0.9174 (0.5253 -- 2.4860)  data: 0.3598 (0.0003 -- 1.9214)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:37  lr: 0.000004  min_lr: 0.000001  loss: 1.6397 (1.5598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8467 (7.0154)  time: 0.8723 (0.5290 -- 2.1346)  data: 0.3213 (0.0003 -- 1.5945)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000001  loss: 1.6699 (1.5856)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5268 (7.0454)  time: 0.9003 (0.5291 -- 3.0171)  data: 0.3534 (0.0005 -- 2.4846)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.6451 (1.5999)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5102 (7.0245)  time: 0.8990 (0.5270 -- 3.5950)  data: 0.3517 (0.0002 -- 3.0724)  max mem: 16413
[2023-09-04 06:35:05,043] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:35:05,044] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:35:05,050] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:35:05,050] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [161]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.5082 (1.5975)  loss_scale: 32768.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9650 (7.0262)  time: 0.8259 (0.5339 -- 4.0836)  data: 0.2676 (0.0005 -- 3.5617)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5390 (1.5803)  loss_scale: 32768.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4218 (7.0296)  time: 0.9108 (0.5213 -- 3.0988)  data: 0.2837 (0.0006 -- 2.5520)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5573 (1.5768)  loss_scale: 32768.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4584 (6.9748)  time: 0.7223 (0.4963 -- 2.7841)  data: 0.0014 (0.0002 -- 0.0129)  max mem: 16413
Epoch: [161] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5573 (1.6168)  loss_scale: 32768.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4584 (6.9748)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1339 (0.1339)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3900 (2.3900 -- 2.3900)  data: 2.1424 (2.1424 -- 2.1424)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4171 (0.4945)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4142 (0.2020 -- 2.3900)  data: 0.1967 (0.0006 -- 2.1424)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4171 (0.4707)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2270 (0.1690 -- 0.6193)  data: 0.0220 (0.0001 -- 0.4153)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4410 (0.5290)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.5104)  time: 0.2106 (0.1328 -- 0.6193)  data: 0.0216 (0.0001 -- 0.4153)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 86.722 Acc@5 97.718 loss 0.534
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [162]  [  0/160]  eta: 0:18:37  lr: 0.000004  min_lr: 0.000001  loss: 1.6697 (1.6697)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8493 (6.8493)  time: 6.9844 (6.9844 -- 6.9844)  data: 4.7915 (4.7915 -- 4.7915)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000001  loss: 1.6043 (1.6210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4880 (7.0054)  time: 0.8373 (0.5257 -- 2.2566)  data: 0.1467 (0.0007 -- 1.7199)  max mem: 16413
[2023-09-04 06:36:24,483] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25946
[2023-09-04 06:36:24,483] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25946
[2023-09-04 06:36:24,484] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:36:24,484] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:36:24,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [162]  [ 40/160]  eta: 0:02:06  lr: 0.000004  min_lr: 0.000001  loss: 1.4872 (1.5805)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4066 (6.7173)  time: 0.9698 (0.5174 -- 5.6042)  data: 0.3704 (0.0003 -- 5.0802)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000001  loss: 1.4929 (1.5664)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7551 (6.9907)  time: 0.7983 (0.5335 -- 3.0097)  data: 0.2425 (0.0009 -- 2.3269)  max mem: 16413
[2023-09-04 06:37:09,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=147, lr=[1.0657741445929144e-06, 1.0657741445929144e-06, 1.1841934939921272e-06, 1.1841934939921272e-06, 1.315770548880141e-06, 1.315770548880141e-06, 1.4619672765334902e-06, 1.4619672765334902e-06, 1.6244080850372113e-06, 1.6244080850372113e-06, 1.804897872263568e-06, 1.804897872263568e-06, 2.0054420802928532e-06, 2.0054420802928532e-06, 2.22826897810317e-06, 2.22826897810317e-06, 2.4758544201146333e-06, 2.4758544201146333e-06, 2.750949355682926e-06, 2.750949355682926e-06, 3.0566103952032513e-06, 3.0566103952032513e-06, 3.3962337724480566e-06, 3.3962337724480566e-06, 3.7735930804978407e-06, 3.7735930804978407e-06, 4.192881200553156e-06, 4.192881200553156e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 06:37:09,810] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=18.17610466926777, CurrSamplesPerSec=22.737914920607384, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000001  loss: 1.6089 (1.5828)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4628 (7.1548)  time: 0.8524 (0.5369 -- 3.1473)  data: 0.0825 (0.0003 -- 1.3636)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000001  loss: 1.6464 (1.5909)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7020 (7.0911)  time: 0.8884 (0.5292 -- 2.8512)  data: 0.1149 (0.0003 -- 1.3638)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000001  loss: 1.6095 (1.5891)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5688 (7.0605)  time: 0.7664 (0.5321 -- 2.7904)  data: 0.0287 (0.0005 -- 0.3842)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.4775 (1.5844)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9955 (7.1036)  time: 0.9427 (0.5350 -- 2.3323)  data: 0.3994 (0.0004 -- 1.8192)  max mem: 16413
[2023-09-04 06:38:13,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:38:13,007] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:38:13,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:38:13,007] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5226 (1.5827)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9452 (7.0894)  time: 0.6634 (0.4989 -- 1.7411)  data: 0.0875 (0.0001 -- 1.1838)  max mem: 16413
Epoch: [162] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5226 (1.6172)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9452 (7.0894)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1390 (0.1390)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6512 (2.6512 -- 2.6512)  data: 2.3606 (2.3606 -- 2.3606)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3173 (0.4734)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4457 (0.2057 -- 2.6512)  data: 0.2167 (0.0008 -- 2.3606)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3725 (0.4627)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2172 (0.1693 -- 0.3738)  data: 0.0109 (0.0001 -- 0.1928)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4408 (0.5177)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.3402)  time: 0.1968 (0.1365 -- 0.3738)  data: 0.0100 (0.0001 -- 0.1928)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 86.929 Acc@5 98.133 loss 0.526
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [163]  [  0/160]  eta: 0:15:34  lr: 0.000004  min_lr: 0.000001  loss: 0.9345 (0.9345)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1291 (7.1291)  time: 5.8378 (5.8378 -- 5.8378)  data: 4.5936 (4.5936 -- 4.5936)  max mem: 16413
[2023-09-04 06:38:33,022] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26084
[2023-09-04 06:38:33,022] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26084
[2023-09-04 06:38:33,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:38:33,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:38:33,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [163]  [ 20/160]  eta: 0:02:48  lr: 0.000004  min_lr: 0.000001  loss: 1.6820 (1.6278)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1969 (7.4282)  time: 0.9707 (0.5298 -- 3.3281)  data: 0.3631 (0.0009 -- 2.8056)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:04  lr: 0.000004  min_lr: 0.000001  loss: 1.6651 (1.6248)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2668 (7.3614)  time: 0.8689 (0.5175 -- 3.6966)  data: 0.3283 (0.0004 -- 3.1568)  max mem: 16413
Epoch: [163]  [ 60/160]  eta: 0:01:42  lr: 0.000004  min_lr: 0.000001  loss: 1.5763 (1.6130)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7849 (7.4155)  time: 0.9885 (0.5106 -- 3.9061)  data: 0.4511 (0.0003 -- 3.3833)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000001  loss: 1.5118 (1.6091)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3191 (7.4149)  time: 0.7587 (0.5131 -- 3.2113)  data: 0.2178 (0.0003 -- 2.6772)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.6328 (1.6088)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6397 (7.5977)  time: 0.8384 (0.5283 -- 2.1806)  data: 0.2962 (0.0005 -- 1.6537)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000001  loss: 1.6752 (1.6060)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6331 (7.3437)  time: 0.8279 (0.5287 -- 2.7199)  data: 0.1998 (0.0004 -- 2.1952)  max mem: 16413
[2023-09-04 06:40:25,901] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:40:25,901] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:40:25,901] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:40:25,901] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5180 (1.6048)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8299 (7.3012)  time: 0.8816 (0.5189 -- 2.2555)  data: 0.2357 (0.0003 -- 1.3993)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5695 (1.6038)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2606 (7.1925)  time: 0.7094 (0.4982 -- 2.0229)  data: 0.0458 (0.0002 -- 0.9025)  max mem: 16413
Epoch: [163] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5695 (1.6107)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2606 (7.1925)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1396 (0.1396)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3703 (2.3703 -- 2.3703)  data: 2.1207 (2.1207 -- 2.1207)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3622 (0.4823)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4165 (0.2106 -- 2.3703)  data: 0.1982 (0.0007 -- 2.1207)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3622 (0.4662)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2204 (0.1680 -- 0.3172)  data: 0.0144 (0.0001 -- 0.1131)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4480 (0.5204)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2045 (0.1327 -- 0.3172)  data: 0.0139 (0.0001 -- 0.1131)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 86.929 Acc@5 98.133 loss 0.531
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [164]  [  0/160]  eta: 0:19:36  lr: 0.000004  min_lr: 0.000001  loss: 1.9318 (1.9318)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5566 (6.5566)  time: 7.3528 (7.3528 -- 7.3528)  data: 6.7041 (6.7041 -- 6.7041)  max mem: 16413
Epoch: [164]  [ 20/160]  eta: 0:02:35  lr: 0.000004  min_lr: 0.000001  loss: 1.5710 (1.6283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4579 (7.3960)  time: 0.8003 (0.5256 -- 2.4640)  data: 0.0382 (0.0006 -- 0.5796)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:02:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5850 (1.6008)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0331 (7.4703)  time: 0.8965 (0.5157 -- 3.5635)  data: 0.2803 (0.0003 -- 3.0218)  max mem: 16413
[2023-09-04 06:41:44,340] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26291
[2023-09-04 06:41:44,340] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:41:44,340] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26291
[2023-09-04 06:41:44,340] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:41:44,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [164]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000001  loss: 1.6375 (1.5960)  loss_scale: 16384.0000 (30082.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7187 (7.6098)  time: 0.9367 (0.5187 -- 3.4534)  data: 0.3203 (0.0008 -- 2.9396)  max mem: 16413
Epoch: [164]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000001  loss: 1.6859 (1.6256)  loss_scale: 16384.0000 (26699.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9767 (7.7253)  time: 0.8109 (0.5205 -- 3.2660)  data: 0.2691 (0.0003 -- 2.7407)  max mem: 16413
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.5856 (1.6093)  loss_scale: 16384.0000 (24657.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5863 (7.6856)  time: 0.9626 (0.5259 -- 4.3004)  data: 0.4210 (0.0004 -- 3.7928)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.5128 (1.6111)  loss_scale: 16384.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.6344)  time: 0.8219 (0.5270 -- 3.2336)  data: 0.2703 (0.0003 -- 2.7156)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.7361 (1.6251)  loss_scale: 16384.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0959 (7.5389)  time: 0.9141 (0.5189 -- 2.6570)  data: 0.3750 (0.0004 -- 2.1252)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5426 (1.6084)  loss_scale: 16384.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1167 (7.5539)  time: 0.7163 (0.4961 -- 2.5018)  data: 0.1979 (0.0002 -- 1.9850)  max mem: 16413
Epoch: [164] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5426 (1.6222)  loss_scale: 16384.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1167 (7.5539)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1360 (0.1360)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2096 (2.2096 -- 2.2096)  data: 1.9773 (1.9773 -- 1.9773)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3333 (0.4814)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4007 (0.2033 -- 2.2096)  data: 0.1836 (0.0010 -- 1.9773)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3834 (0.4682)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2272 (0.1734 -- 0.5099)  data: 0.0196 (0.0002 -- 0.2966)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4449 (0.5213)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2090 (0.1331 -- 0.5099)  data: 0.0188 (0.0001 -- 0.2966)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 86.929 Acc@5 98.548 loss 0.526
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [165]  [  0/160]  eta: 0:19:29  lr: 0.000004  min_lr: 0.000001  loss: 2.3472 (2.3472)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (7.4353)  time: 7.3120 (7.3120 -- 7.3120)  data: 6.3291 (6.3291 -- 6.3291)  max mem: 16413
[2023-09-04 06:43:48,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:43:48,473] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:43:48,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:43:48,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [165]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000001  loss: 1.6613 (1.6838)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8694 (7.4632)  time: 0.8233 (0.5231 -- 3.3436)  data: 0.0108 (0.0002 -- 0.1822)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:02:01  lr: 0.000004  min_lr: 0.000001  loss: 1.5546 (1.6331)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3195 (6.9595)  time: 0.8805 (0.5365 -- 2.5068)  data: 0.0018 (0.0010 -- 0.0051)  max mem: 16413
Epoch: [165]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000001  loss: 1.5266 (1.6184)  loss_scale: 32768.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1044 (7.0387)  time: 1.0344 (0.5157 -- 6.5677)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [165]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000001  loss: 1.5154 (1.5964)  loss_scale: 32768.0000 (28722.5679)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1156 (7.1594)  time: 0.7828 (0.5289 -- 2.3344)  data: 0.0016 (0.0002 -- 0.0031)  max mem: 16413
[2023-09-04 06:44:58,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26498
[2023-09-04 06:44:58,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26498
[2023-09-04 06:44:58,365] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:44:58,365] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:44:58,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [165]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.7362 (1.6166)  loss_scale: 32768.0000 (29036.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7024 (7.3039)  time: 0.8490 (0.5175 -- 3.6447)  data: 0.0016 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.6023 (1.6174)  loss_scale: 16384.0000 (26945.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7171 (7.2250)  time: 0.9044 (0.5141 -- 3.5823)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5867 (1.6241)  loss_scale: 16384.0000 (25447.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7411 (7.1781)  time: 0.8609 (0.5259 -- 3.9090)  data: 0.0016 (0.0004 -- 0.0072)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.6115 (1.6128)  loss_scale: 16384.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2326 (7.1354)  time: 0.7209 (0.4960 -- 3.0644)  data: 0.0008 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [165] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.6115 (1.6470)  loss_scale: 16384.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2326 (7.1354)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1398 (0.1398)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3064 (2.3064 -- 2.3064)  data: 2.0609 (2.0609 -- 2.0609)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3215 (0.4758)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (98.9899)  time: 0.4224 (0.2052 -- 2.3064)  data: 0.2000 (0.0005 -- 2.0609)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3612 (0.4653)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (97.8836)  time: 0.2288 (0.1697 -- 0.4721)  data: 0.0211 (0.0001 -- 0.2791)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4462 (0.5190)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2122 (0.1330 -- 0.4721)  data: 0.0203 (0.0001 -- 0.2791)  max mem: 16413
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 86.929 Acc@5 97.925 loss 0.527
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [166]  [  0/160]  eta: 0:18:39  lr: 0.000003  min_lr: 0.000001  loss: 1.7278 (1.7278)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9536 (6.9536)  time: 6.9990 (6.9990 -- 6.9990)  data: 6.4472 (6.4472 -- 6.4472)  max mem: 16413
Epoch: [166]  [ 20/160]  eta: 0:02:39  lr: 0.000003  min_lr: 0.000001  loss: 1.7896 (1.7557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5789 (7.0481)  time: 0.8497 (0.5342 -- 4.1748)  data: 0.2906 (0.0002 -- 3.6483)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:01:54  lr: 0.000003  min_lr: 0.000001  loss: 1.6347 (1.7282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6999 (7.3280)  time: 0.7625 (0.5207 -- 2.7720)  data: 0.2154 (0.0009 -- 2.2516)  max mem: 16413
Epoch: [166]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000001  loss: 1.4750 (1.6463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1592 (7.0092)  time: 0.9806 (0.5219 -- 3.6362)  data: 0.2281 (0.0004 -- 3.1000)  max mem: 16413
[2023-09-04 06:47:01,877] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:47:01,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:47:01,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:47:01,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [166]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000001  loss: 1.4052 (1.6148)  loss_scale: 32768.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0803 (7.0517)  time: 0.8238 (0.5285 -- 2.9390)  data: 0.2570 (0.0007 -- 2.4149)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:54  lr: 0.000003  min_lr: 0.000001  loss: 1.6859 (1.6331)  loss_scale: 32768.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9008 (7.1419)  time: 0.7876 (0.5200 -- 1.9195)  data: 0.1770 (0.0011 -- 1.3586)  max mem: 16413
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.6560 (1.6358)  loss_scale: 32768.0000 (23695.8678)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8646 (7.3036)  time: 0.9453 (0.5312 -- 2.8215)  data: 0.3986 (0.0004 -- 2.3083)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.6095 (1.6318)  loss_scale: 32768.0000 (24982.6950)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3666 (7.3302)  time: 0.8466 (0.5349 -- 2.3015)  data: 0.2958 (0.0004 -- 1.7681)  max mem: 16413
[2023-09-04 06:48:12,959] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26710
[2023-09-04 06:48:12,959] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26710
[2023-09-04 06:48:12,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:48:12,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:48:12,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.6008 (1.6355)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3498 (7.3201)  time: 0.7329 (0.4963 -- 2.6771)  data: 0.2147 (0.0001 -- 2.1559)  max mem: 16413
Epoch: [166] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.6008 (1.6389)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3498 (7.3201)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1399 (0.1399)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4756 (2.4756 -- 2.4756)  data: 2.2350 (2.2350 -- 2.2350)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3439 (0.4791)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4267 (0.1981 -- 2.4756)  data: 0.2041 (0.0005 -- 2.2350)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3717 (0.4700)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2138 (0.1717 -- 0.2613)  data: 0.0077 (0.0001 -- 0.0514)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4457 (0.5246)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.1936 (0.1320 -- 0.2613)  data: 0.0074 (0.0001 -- 0.0514)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.529
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [167]  [  0/160]  eta: 0:20:22  lr: 0.000003  min_lr: 0.000001  loss: 2.0184 (2.0184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9474 (4.9474)  time: 7.6398 (7.6398 -- 7.6398)  data: 7.1350 (7.1350 -- 7.1350)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000001  loss: 1.5817 (1.6323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8797 (7.0142)  time: 0.8885 (0.5241 -- 3.3884)  data: 0.2888 (0.0002 -- 2.8469)  max mem: 16413
Epoch: [167]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5801 (1.5851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7447 (7.0508)  time: 0.7845 (0.5174 -- 3.4862)  data: 0.2422 (0.0004 -- 2.9510)  max mem: 16413
Epoch: [167]  [ 60/160]  eta: 0:01:39  lr: 0.000003  min_lr: 0.000001  loss: 1.6662 (1.5984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8862 (7.1001)  time: 0.9708 (0.5273 -- 4.1327)  data: 0.4251 (0.0003 -- 3.5768)  max mem: 16413
Epoch: [167]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000001  loss: 1.5963 (1.5989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1819 (7.1632)  time: 0.7343 (0.5184 -- 3.1993)  data: 0.1863 (0.0002 -- 2.6691)  max mem: 16413
Epoch: [167]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000001  loss: 1.7517 (1.6200)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9263 (7.2133)  time: 0.9360 (0.5188 -- 3.8868)  data: 0.1322 (0.0005 -- 1.2828)  max mem: 16413
[2023-09-04 06:50:18,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:50:18,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:50:18,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:50:18,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [167]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000001  loss: 1.8108 (1.6223)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1524 (7.2505)  time: 0.9844 (0.5286 -- 3.8167)  data: 0.0153 (0.0004 -- 0.2766)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.7192 (1.6378)  loss_scale: 32768.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5942 (7.2631)  time: 0.8013 (0.5278 -- 3.2449)  data: 0.2339 (0.0002 -- 2.7452)  max mem: 16413
[2023-09-04 06:50:46,796] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26875
[2023-09-04 06:50:46,796] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:50:46,796] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 06:50:46,796] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26875
[2023-09-04 06:50:46,796] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.6826 (1.6412)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3919 (7.1816)  time: 0.7212 (0.4775 -- 2.5914)  data: 0.1983 (0.0002 -- 2.0901)  max mem: 16413
Epoch: [167] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.6826 (1.6322)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3919 (7.1816)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1389 (0.1389)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2946 (2.2946 -- 2.2946)  data: 2.0408 (2.0408 -- 2.0408)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3849 (0.4830)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4096 (0.2016 -- 2.2946)  data: 0.1869 (0.0009 -- 2.0408)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3540 (0.4701)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2261 (0.1736 -- 0.3959)  data: 0.0151 (0.0001 -- 0.2213)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4521 (0.5256)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2074 (0.1331 -- 0.3959)  data: 0.0147 (0.0001 -- 0.2213)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 86.722 Acc@5 97.925 loss 0.532
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [168]  [  0/160]  eta: 0:20:03  lr: 0.000003  min_lr: 0.000001  loss: 2.1195 (2.1195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1969 (7.1969)  time: 7.5245 (7.5245 -- 7.5245)  data: 6.0767 (6.0767 -- 6.0767)  max mem: 16413
Epoch: [168]  [ 20/160]  eta: 0:02:58  lr: 0.000003  min_lr: 0.000001  loss: 1.6075 (1.6218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4051 (7.0764)  time: 0.9618 (0.5281 -- 5.4299)  data: 0.1734 (0.0004 -- 3.2644)  max mem: 16413
Epoch: [168]  [ 40/160]  eta: 0:02:02  lr: 0.000003  min_lr: 0.000001  loss: 1.5379 (1.6283)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1641 (7.2025)  time: 0.7590 (0.5196 -- 2.5598)  data: 0.0014 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [168]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000001  loss: 1.6719 (1.6415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3039 (7.1666)  time: 0.8577 (0.5218 -- 2.1514)  data: 0.1178 (0.0003 -- 1.3706)  max mem: 16413
Epoch: [168]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000001  loss: 1.7059 (1.6496)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5689 (7.1293)  time: 1.0230 (0.5276 -- 3.5812)  data: 0.0969 (0.0005 -- 1.4587)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.5142 (1.6374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2900 (7.0268)  time: 0.7955 (0.5233 -- 2.3051)  data: 0.0875 (0.0002 -- 1.3919)  max mem: 16413
[2023-09-04 06:52:47,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=152, lr=[7.506812626750167e-07, 7.506812626750167e-07, 8.340902918611298e-07, 8.340902918611298e-07, 9.267669909568107e-07, 9.267669909568107e-07, 1.029741101063123e-06, 1.029741101063123e-06, 1.1441567789590256e-06, 1.1441567789590256e-06, 1.271285309954473e-06, 1.271285309954473e-06, 1.4125392332827476e-06, 1.4125392332827476e-06, 1.5694880369808305e-06, 1.5694880369808305e-06, 1.7438755966453672e-06, 1.7438755966453672e-06, 1.9376395518281855e-06, 1.9376395518281855e-06, 2.152932835364651e-06, 2.152932835364651e-06, 2.392147594849612e-06, 2.392147594849612e-06, 2.6579417720551244e-06, 2.6579417720551244e-06, 2.9532686356168047e-06, 2.9532686356168047e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 06:52:47,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=18.18490973408946, CurrSamplesPerSec=21.93108731145351, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5957 (1.6334)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9159 (7.0625)  time: 0.7731 (0.5270 -- 3.0106)  data: 0.1949 (0.0002 -- 2.4877)  max mem: 16413
[2023-09-04 06:52:53,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:52:53,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:52:53,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:52:53,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:52:55,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27008
[2023-09-04 06:52:55,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27008
[2023-09-04 06:52:55,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:52:55,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:52:55,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.3719 (1.6113)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3958 (7.0511)  time: 0.9722 (0.5173 -- 4.7556)  data: 0.4264 (0.0003 -- 4.2529)  max mem: 16413
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.7542 (1.6182)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7527 (7.0083)  time: 0.6459 (0.4951 -- 2.9662)  data: 0.1239 (0.0002 -- 2.4400)  max mem: 16413
Epoch: [168] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.7542 (1.5979)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7527 (7.0083)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1334 (0.1334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2964 (2.2964 -- 2.2964)  data: 2.0983 (2.0983 -- 2.0983)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4184 (0.4876)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4164 (0.2047 -- 2.2964)  data: 0.2041 (0.0005 -- 2.0983)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3534 (0.4716)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2320 (0.1699 -- 0.6251)  data: 0.0289 (0.0001 -- 0.4281)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4566 (0.5288)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2152 (0.1330 -- 0.6251)  data: 0.0284 (0.0001 -- 0.4281)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 86.722 Acc@5 97.718 loss 0.533
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [169]  [  0/160]  eta: 0:21:56  lr: 0.000003  min_lr: 0.000001  loss: 1.4651 (1.4651)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0225 (6.0225)  time: 8.2295 (8.2295 -- 8.2295)  data: 7.6755 (7.6755 -- 7.6755)  max mem: 16413
Epoch: [169]  [ 20/160]  eta: 0:02:41  lr: 0.000003  min_lr: 0.000001  loss: 1.6959 (1.6982)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5957 (6.8520)  time: 0.8031 (0.5167 -- 3.6432)  data: 0.2516 (0.0004 -- 3.1189)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:02  lr: 0.000003  min_lr: 0.000001  loss: 1.4969 (1.5877)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0476 (6.6518)  time: 0.8863 (0.5301 -- 3.1812)  data: 0.3442 (0.0007 -- 2.6627)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000001  loss: 1.8575 (1.6528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7212 (6.7600)  time: 0.9628 (0.5222 -- 4.5328)  data: 0.4167 (0.0002 -- 4.0162)  max mem: 16413
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000001  loss: 1.5365 (1.6531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8877 (6.7712)  time: 0.8842 (0.5203 -- 4.1695)  data: 0.3466 (0.0002 -- 3.6485)  max mem: 16413
[2023-09-04 06:54:59,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:54:59,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:54:59,529] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:54:59,576] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000001  loss: 1.5951 (1.6515)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7440 (6.8141)  time: 0.8825 (0.5317 -- 3.5218)  data: 0.3338 (0.0003 -- 2.9959)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.6412 (1.6575)  loss_scale: 32768.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9301 (6.9048)  time: 0.6565 (0.5217 -- 1.8946)  data: 0.1024 (0.0005 -- 1.3416)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.4970 (1.6522)  loss_scale: 32768.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4629 (6.8451)  time: 1.0585 (0.5266 -- 5.1084)  data: 0.5153 (0.0005 -- 4.5773)  max mem: 16413
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.6280 (1.6496)  loss_scale: 32768.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8759 (6.8804)  time: 0.6075 (0.4972 -- 2.3478)  data: 0.0929 (0.0002 -- 1.8400)  max mem: 16413
Epoch: [169] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.6280 (1.6250)  loss_scale: 32768.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8759 (6.8804)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1323 (0.1323)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2913 (2.2913 -- 2.2913)  data: 2.0583 (2.0583 -- 2.0583)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4092 (0.4863)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4149 (0.2062 -- 2.2913)  data: 0.1993 (0.0006 -- 2.0583)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3571 (0.4713)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.8836)  time: 0.2235 (0.1706 -- 0.4494)  data: 0.0190 (0.0001 -- 0.2426)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4550 (0.5278)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2076 (0.1330 -- 0.4494)  data: 0.0186 (0.0001 -- 0.2426)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 86.515 Acc@5 97.925 loss 0.530
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.76%
Epoch: [170]  [  0/160]  eta: 0:17:15  lr: 0.000003  min_lr: 0.000001  loss: 1.7524 (1.7524)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4664 (9.4664)  time: 6.4719 (6.4719 -- 6.4719)  data: 5.5624 (5.5624 -- 5.5624)  max mem: 16413
[2023-09-04 06:56:08,269] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27207
[2023-09-04 06:56:08,269] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27207
[2023-09-04 06:56:08,269] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:56:08,269] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 06:56:08,269] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [170]  [ 20/160]  eta: 0:02:33  lr: 0.000003  min_lr: 0.000001  loss: 1.4619 (1.5182)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8485 (6.9084)  time: 0.8291 (0.5328 -- 3.3375)  data: 0.2636 (0.0002 -- 2.8132)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000001  loss: 1.2905 (1.4509)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8461 (6.8697)  time: 0.9223 (0.5262 -- 3.0224)  data: 0.1138 (0.0002 -- 1.9680)  max mem: 16413
Epoch: [170]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000001  loss: 1.7140 (1.5605)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6229 (6.8697)  time: 0.9138 (0.5085 -- 3.3763)  data: 0.1313 (0.0003 -- 1.3429)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000001  loss: 1.7057 (1.5943)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8386 (7.1018)  time: 0.7879 (0.5175 -- 3.0662)  data: 0.0339 (0.0004 -- 0.6455)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.5948 (1.5966)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5250 (7.1090)  time: 0.9944 (0.5104 -- 4.1548)  data: 0.0423 (0.0004 -- 0.8239)  max mem: 16413
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5597 (1.5994)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1890 (7.1191)  time: 0.7129 (0.5361 -- 2.9993)  data: 0.0017 (0.0005 -- 0.0039)  max mem: 16413
[2023-09-04 06:58:01,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:58:01,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 06:58:01,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 06:58:01,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5752 (1.6026)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3913 (7.1624)  time: 0.8925 (0.5274 -- 4.0803)  data: 0.0260 (0.0005 -- 0.4870)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5826 (1.6030)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7332 (7.1082)  time: 0.6746 (0.4987 -- 1.9329)  data: 0.1345 (0.0002 -- 1.2004)  max mem: 16413
Epoch: [170] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.5826 (1.6039)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7332 (7.1082)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1315 (0.1315)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2504 (2.2504 -- 2.2504)  data: 2.0353 (2.0353 -- 2.0353)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3802 (0.4872)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4232 (0.2064 -- 2.2504)  data: 0.2055 (0.0007 -- 2.0353)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3802 (0.4711)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2361 (0.1687 -- 0.5753)  data: 0.0309 (0.0001 -- 0.3895)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4538 (0.5295)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2201 (0.1358 -- 0.5753)  data: 0.0306 (0.0001 -- 0.3895)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 86.722 Acc@5 97.925 loss 0.528
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [171]  [  0/160]  eta: 0:23:18  lr: 0.000003  min_lr: 0.000001  loss: 1.6072 (1.6072)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5695 (8.5695)  time: 8.7400 (8.7400 -- 8.7400)  data: 8.2263 (8.2263 -- 8.2263)  max mem: 16413
Epoch: [171]  [ 20/160]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000001  loss: 1.5829 (1.5643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3466 (7.3487)  time: 0.7874 (0.5199 -- 3.6919)  data: 0.2265 (0.0002 -- 3.1566)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000001  loss: 1.8447 (1.6272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3537 (7.2249)  time: 0.8047 (0.5216 -- 2.3169)  data: 0.1011 (0.0004 -- 1.1451)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000001  loss: 1.4592 (1.5861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6494 (7.1193)  time: 0.9329 (0.5327 -- 3.0670)  data: 0.2643 (0.0004 -- 2.5320)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000001  loss: 1.5887 (1.5900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8711 (7.1047)  time: 0.8908 (0.5331 -- 3.3047)  data: 0.3450 (0.0003 -- 2.7870)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000001  loss: 1.4190 (1.5951)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7855 (7.1866)  time: 0.8538 (0.5252 -- 3.2401)  data: 0.3103 (0.0003 -- 2.7222)  max mem: 16413
[2023-09-04 07:00:02,565] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:00:02,565] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 07:00:02,566] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:00:02,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 07:00:06,222] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27468
[2023-09-04 07:00:06,222] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27468
[2023-09-04 07:00:06,222] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 07:00:06,222] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 07:00:06,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [171]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000001  loss: 1.7140 (1.6071)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3695 (7.1149)  time: 0.7612 (0.5259 -- 2.4120)  data: 0.2103 (0.0006 -- 1.8787)  max mem: 16413
[2023-09-04 07:00:27,282] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27491
[2023-09-04 07:00:27,282] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:00:27,283] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27491
[2023-09-04 07:00:27,284] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:00:27,284] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.5440 (1.5912)  loss_scale: 16384.0000 (32535.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4411 (7.1174)  time: 1.0182 (0.5275 -- 4.1038)  data: 0.4770 (0.0007 -- 3.5727)  max mem: 16413
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.6075 (1.5886)  loss_scale: 16384.0000 (30617.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6243 (7.1808)  time: 0.6699 (0.4954 -- 3.5505)  data: 0.1530 (0.0001 -- 3.0474)  max mem: 16413
Epoch: [171] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.6075 (1.5998)  loss_scale: 16384.0000 (30617.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6243 (7.1808)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3003 (2.3003 -- 2.3003)  data: 2.0531 (2.0531 -- 2.0531)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3868 (0.4797)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4147 (0.2005 -- 2.3003)  data: 0.2020 (0.0007 -- 2.0531)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3762 (0.4689)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2242 (0.1712 -- 0.4325)  data: 0.0193 (0.0001 -- 0.2143)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4496 (0.5263)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2100 (0.1331 -- 0.4325)  data: 0.0191 (0.0001 -- 0.2143)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 86.929 Acc@5 97.925 loss 0.526
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [172]  [  0/160]  eta: 0:21:45  lr: 0.000002  min_lr: 0.000001  loss: 1.9227 (1.9227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1401 (9.1401)  time: 8.1619 (8.1619 -- 8.1619)  data: 7.3484 (7.3484 -- 7.3484)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:54  lr: 0.000002  min_lr: 0.000001  loss: 1.7316 (1.6888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8461 (7.9345)  time: 0.8996 (0.5279 -- 4.6206)  data: 0.0878 (0.0003 -- 1.3779)  max mem: 16413
Epoch: [172]  [ 40/160]  eta: 0:02:14  lr: 0.000002  min_lr: 0.000001  loss: 1.6540 (1.6487)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8710 (7.4840)  time: 0.9882 (0.5071 -- 5.1531)  data: 0.0009 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [172]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000001  loss: 1.6801 (1.6425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6374 (7.2658)  time: 0.7705 (0.5368 -- 2.8712)  data: 0.0018 (0.0003 -- 0.0062)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000001  loss: 1.7892 (1.6582)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3616 (7.4805)  time: 0.7414 (0.5379 -- 2.4487)  data: 0.0984 (0.0009 -- 1.9225)  max mem: 16413
[2023-09-04 07:02:30,618] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:02:30,619] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:02:30,620] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:02:30,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [172]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000001  loss: 1.5602 (1.6628)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5370 (7.3731)  time: 0.8963 (0.5349 -- 2.9933)  data: 0.2828 (0.0002 -- 2.4401)  max mem: 16413
[2023-09-04 07:02:32,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27622
[2023-09-04 07:02:32,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27622
[2023-09-04 07:02:32,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:02:32,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:02:32,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000001  loss: 1.7820 (1.6900)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2360 (7.4117)  time: 0.8901 (0.5242 -- 3.1336)  data: 0.1933 (0.0002 -- 2.5976)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.2846 (1.6679)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9161 (7.4972)  time: 0.9687 (0.5209 -- 3.8385)  data: 0.4262 (0.0003 -- 3.2973)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.7240 (1.6627)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7768 (7.4815)  time: 0.6305 (0.4981 -- 1.8548)  data: 0.1143 (0.0002 -- 1.3346)  max mem: 16413
Epoch: [172] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.7240 (1.6258)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7768 (7.4815)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1333 (0.1333)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2856 (2.2856 -- 2.2856)  data: 2.0559 (2.0559 -- 2.0559)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3941 (0.4817)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4204 (0.2066 -- 2.2856)  data: 0.2009 (0.0007 -- 2.0559)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3689 (0.4689)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2278 (0.1709 -- 0.3798)  data: 0.0167 (0.0001 -- 0.1757)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4528 (0.5256)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2110 (0.1335 -- 0.3798)  data: 0.0163 (0.0001 -- 0.1757)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 86.307 Acc@5 98.133 loss 0.529
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 87.76%
Epoch: [173]  [  0/160]  eta: 0:20:18  lr: 0.000002  min_lr: 0.000001  loss: 1.9256 (1.9256)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8234 (6.8234)  time: 7.6174 (7.6174 -- 7.6174)  data: 5.5985 (5.5985 -- 5.5985)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000001  loss: 1.8758 (1.7390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7363 (6.9963)  time: 0.9348 (0.5123 -- 3.2958)  data: 0.0026 (0.0005 -- 0.0126)  max mem: 16413
Epoch: [173]  [ 40/160]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000001  loss: 1.6643 (1.6897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2441 (7.0551)  time: 0.8653 (0.5137 -- 3.4317)  data: 0.0012 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000001  loss: 1.3109 (1.6251)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2415 (6.9618)  time: 0.8107 (0.5201 -- 3.0590)  data: 0.0014 (0.0004 -- 0.0047)  max mem: 16413
[2023-09-04 07:04:35,162] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:04:35,162] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:04:35,163] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:04:35,163] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [173]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000001  loss: 1.4452 (1.6141)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7364 (6.7838)  time: 0.9161 (0.5292 -- 4.1338)  data: 0.0167 (0.0006 -- 0.3052)  max mem: 16413
[2023-09-04 07:04:57,314] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27774
[2023-09-04 07:04:57,314] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27774
[2023-09-04 07:04:57,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:04:57,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:04:57,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [173]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000001  loss: 1.6259 (1.6089)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3654 (6.9050)  time: 0.9375 (0.5089 -- 4.7481)  data: 0.0012 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000001  loss: 1.6268 (1.6080)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6404 (7.0158)  time: 0.7846 (0.5231 -- 3.4546)  data: 0.0016 (0.0002 -- 0.0066)  max mem: 16413
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.8665 (1.6281)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5551 (7.0094)  time: 0.8815 (0.5293 -- 4.2167)  data: 0.0019 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.5882 (1.6273)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8225 (7.0056)  time: 0.6474 (0.4952 -- 2.9259)  data: 0.0049 (0.0002 -- 0.0833)  max mem: 16413
Epoch: [173] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.5882 (1.6260)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8225 (7.0056)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1327 (0.1327)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3043 (2.3043 -- 2.3043)  data: 2.0641 (2.0641 -- 2.0641)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3802 (0.4746)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (98.9899)  time: 0.4230 (0.2087 -- 2.3043)  data: 0.2069 (0.0006 -- 2.0641)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3802 (0.4680)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (97.8836)  time: 0.2257 (0.1699 -- 0.3991)  data: 0.0217 (0.0001 -- 0.2173)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4554 (0.5241)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2088 (0.1331 -- 0.3991)  data: 0.0214 (0.0001 -- 0.2173)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 86.929 Acc@5 97.925 loss 0.525
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [174]  [  0/160]  eta: 0:17:18  lr: 0.000002  min_lr: 0.000001  loss: 1.5160 (1.5160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9476 (5.9476)  time: 6.4934 (6.4934 -- 6.4934)  data: 5.9466 (5.9466 -- 5.9466)  max mem: 16413
Epoch: [174]  [ 20/160]  eta: 0:02:34  lr: 0.000002  min_lr: 0.000001  loss: 1.5765 (1.5569)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4458 (6.7359)  time: 0.8376 (0.5326 -- 2.2501)  data: 0.0273 (0.0004 -- 0.3388)  max mem: 16413
Epoch: [174]  [ 40/160]  eta: 0:01:59  lr: 0.000002  min_lr: 0.000001  loss: 1.7356 (1.6184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8558 (7.0260)  time: 0.8834 (0.5258 -- 2.6577)  data: 0.0786 (0.0004 -- 0.9102)  max mem: 16413
Epoch: [174]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000001  loss: 1.6489 (1.6077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7925 (7.2145)  time: 0.9385 (0.5296 -- 3.7362)  data: 0.3847 (0.0004 -- 3.2092)  max mem: 16413
[2023-09-04 07:06:59,502] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:06:59,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:06:59,502] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:06:59,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:07:09,083] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27913
[2023-09-04 07:07:09,084] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:07:09,086] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27913
[2023-09-04 07:07:09,087] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:07:09,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [174]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000001  loss: 1.4818 (1.5864)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5888 (7.3985)  time: 0.9076 (0.5178 -- 4.6488)  data: 0.1378 (0.0004 -- 2.7325)  max mem: 16413
Epoch: [174]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000001  loss: 1.4616 (1.5679)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2978 (7.4265)  time: 0.9101 (0.5159 -- 3.8997)  data: 0.0011 (0.0004 -- 0.0035)  max mem: 16413
[2023-09-04 07:07:47,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27956
[2023-09-04 07:07:47,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:07:47,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27956
[2023-09-04 07:07:47,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:07:47,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [174]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.5768 (1.5770)  loss_scale: 16384.0000 (17399.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3941 (7.4140)  time: 0.7938 (0.5076 -- 3.2633)  data: 0.0800 (0.0005 -- 0.9629)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5562 (1.5922)  loss_scale: 8192.0000 (16093.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6762 (7.6724)  time: 0.9177 (0.5095 -- 4.6883)  data: 0.0472 (0.0003 -- 0.9245)  max mem: 16413
[2023-09-04 07:08:20,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=160, lr=[4.883955036797427e-07, 4.883955036797427e-07, 5.426616707552697e-07, 5.426616707552697e-07, 6.029574119502996e-07, 6.029574119502996e-07, 6.699526799447774e-07, 6.699526799447774e-07, 7.443918666053082e-07, 7.443918666053082e-07, 8.27102074005898e-07, 8.27102074005898e-07, 9.190023044509977e-07, 9.190023044509977e-07, 1.0211136716122195e-06, 1.0211136716122195e-06, 1.1345707462357997e-06, 1.1345707462357997e-06, 1.2606341624842215e-06, 1.2606341624842215e-06, 1.4007046249824687e-06, 1.4007046249824687e-06, 1.5563384722027429e-06, 1.5563384722027429e-06, 1.7292649691141586e-06, 1.7292649691141586e-06, 1.921405521237954e-06, 1.921405521237954e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 07:08:20,046] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=18.168120450317165, CurrSamplesPerSec=24.191976005883173, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5510 (1.5914)  loss_scale: 8192.0000 (15155.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3844 (7.5956)  time: 0.6065 (0.4956 -- 1.9982)  data: 0.0008 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [174] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5510 (1.5983)  loss_scale: 8192.0000 (15155.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3844 (7.5956)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1320 (0.1320)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3529 (2.3529 -- 2.3529)  data: 2.1054 (2.1054 -- 2.1054)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3444 (0.4766)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4271 (0.2041 -- 2.3529)  data: 0.2058 (0.0010 -- 2.1054)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4068 (0.4715)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2304 (0.1690 -- 0.4933)  data: 0.0229 (0.0001 -- 0.2963)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4498 (0.5283)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2147 (0.1327 -- 0.4933)  data: 0.0224 (0.0001 -- 0.2963)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.524
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [175]  [  0/160]  eta: 0:18:25  lr: 0.000002  min_lr: 0.000000  loss: 1.3929 (1.3929)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2115 (4.2115)  time: 6.9094 (6.9094 -- 6.9094)  data: 5.5397 (5.5397 -- 5.5397)  max mem: 16413
Epoch: [175]  [ 20/160]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 1.4788 (1.5298)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0416 (6.8357)  time: 0.8337 (0.5250 -- 2.8939)  data: 0.1243 (0.0004 -- 1.3983)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000000  loss: 1.5067 (1.5211)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2315 (7.2268)  time: 0.9593 (0.5244 -- 3.1972)  data: 0.2674 (0.0004 -- 2.6778)  max mem: 16413
Epoch: [175]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.5414 (1.5530)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7410 (7.2727)  time: 0.8812 (0.5216 -- 3.3798)  data: 0.3085 (0.0006 -- 2.8700)  max mem: 16413
Epoch: [175]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.6977 (1.5756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9060 (7.3188)  time: 0.8373 (0.5106 -- 2.9563)  data: 0.2220 (0.0004 -- 2.4506)  max mem: 16413
[2023-09-04 07:09:50,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:09:50,738] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 07:09:50,740] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:09:50,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [175]  [100/160]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000000  loss: 1.5483 (1.5716)  loss_scale: 16384.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7425 (7.2881)  time: 1.0781 (0.5198 -- 5.7227)  data: 0.5072 (0.0004 -- 5.2090)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5556 (1.5651)  loss_scale: 16384.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7492 (7.2695)  time: 0.6834 (0.5256 -- 2.4036)  data: 0.0386 (0.0003 -- 0.7420)  max mem: 16413
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.3760 (1.5350)  loss_scale: 16384.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9188 (7.1519)  time: 0.9324 (0.5369 -- 3.2356)  data: 0.2887 (0.0007 -- 2.7002)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5893 (1.5495)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6102 (7.3018)  time: 0.6897 (0.4933 -- 3.3353)  data: 0.1760 (0.0002 -- 2.8259)  max mem: 16413
Epoch: [175] Total time: 0:02:24 (0.9018 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5893 (1.5860)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6102 (7.3018)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1328 (0.1328)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2759 (2.2759 -- 2.2759)  data: 2.0784 (2.0784 -- 2.0784)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3400 (0.4704)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (98.9899)  time: 0.4208 (0.2019 -- 2.2759)  data: 0.2084 (0.0004 -- 2.0784)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3720 (0.4653)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (97.8836)  time: 0.2271 (0.1695 -- 0.4441)  data: 0.0235 (0.0001 -- 0.2041)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4492 (0.5225)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.3402)  time: 0.2119 (0.1324 -- 0.4441)  data: 0.0232 (0.0001 -- 0.2041)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 87.137 Acc@5 98.133 loss 0.522
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [176]  [  0/160]  eta: 0:16:21  lr: 0.000002  min_lr: 0.000000  loss: 2.1445 (2.1445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0585 (5.0585)  time: 6.1324 (6.1324 -- 6.1324)  data: 5.5993 (5.5993 -- 5.5993)  max mem: 16413
Epoch: [176]  [ 20/160]  eta: 0:02:47  lr: 0.000002  min_lr: 0.000000  loss: 1.6950 (1.7620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0809 (7.4328)  time: 0.9502 (0.5339 -- 3.5371)  data: 0.3981 (0.0004 -- 3.0172)  max mem: 16413
Epoch: [176]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.4913 (1.6518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9045 (7.4314)  time: 0.8644 (0.5311 -- 3.8170)  data: 0.3188 (0.0003 -- 3.3061)  max mem: 16413
[2023-09-04 07:11:53,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:11:53,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:11:53,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:11:53,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [176]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.5226 (1.6381)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5779 (7.6171)  time: 0.8351 (0.5225 -- 2.7052)  data: 0.2422 (0.0006 -- 2.1854)  max mem: 16413
Epoch: [176]  [ 80/160]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 1.7298 (1.6540)  loss_scale: 32768.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8635 (7.7280)  time: 0.7867 (0.5375 -- 1.8919)  data: 0.1815 (0.0004 -- 1.3521)  max mem: 16413
Epoch: [176]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.6533 (1.6662)  loss_scale: 32768.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2673 (7.6221)  time: 0.9363 (0.5103 -- 2.5866)  data: 0.1651 (0.0003 -- 1.4299)  max mem: 16413
[2023-09-04 07:12:44,018] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28270
[2023-09-04 07:12:44,019] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28270
[2023-09-04 07:12:44,019] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:12:44,019] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:12:44,020] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [176]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6139 (1.6664)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4927 (7.5115)  time: 0.9489 (0.5282 -- 3.8187)  data: 0.0015 (0.0002 -- 0.0074)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.4489 (1.6454)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3631 (7.5423)  time: 0.7888 (0.5158 -- 3.7173)  data: 0.0016 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7482 (1.6484)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4358 (7.5162)  time: 0.6766 (0.4946 -- 2.8455)  data: 0.0008 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [176] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7482 (1.6117)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4358 (7.5162)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1316 (0.1316)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3127 (2.3127 -- 2.3127)  data: 2.1002 (2.1002 -- 2.1002)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3513 (0.4725)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4119 (0.1941 -- 2.3127)  data: 0.1999 (0.0007 -- 2.1002)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3553 (0.4649)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2241 (0.1689 -- 0.4013)  data: 0.0206 (0.0001 -- 0.2111)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4467 (0.5225)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2084 (0.1334 -- 0.4013)  data: 0.0203 (0.0001 -- 0.2111)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.524
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [177]  [  0/160]  eta: 0:25:01  lr: 0.000002  min_lr: 0.000000  loss: 1.6754 (1.6754)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7395 (8.7395)  time: 9.3819 (9.3819 -- 9.3819)  data: 6.7148 (6.7148 -- 6.7148)  max mem: 16413
Epoch: [177]  [ 20/160]  eta: 0:02:45  lr: 0.000002  min_lr: 0.000000  loss: 1.6337 (1.5514)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0756 (7.1981)  time: 0.7741 (0.5238 -- 3.2268)  data: 0.2340 (0.0003 -- 2.6876)  max mem: 16413
Epoch: [177]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.7044 (1.6150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2210 (7.4375)  time: 0.8776 (0.5271 -- 3.6083)  data: 0.3177 (0.0004 -- 3.0854)  max mem: 16413
Epoch: [177]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.7105 (1.6493)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0866 (7.3934)  time: 0.9065 (0.5211 -- 3.2431)  data: 0.2945 (0.0003 -- 2.7341)  max mem: 16413
[2023-09-04 07:14:46,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:14:46,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:14:46,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:14:46,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [177]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.3284 (1.5922)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3074 (7.4044)  time: 0.8436 (0.5203 -- 3.3610)  data: 0.2113 (0.0005 -- 2.8187)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.7236 (1.6179)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1267 (7.4639)  time: 0.8749 (0.5236 -- 2.8777)  data: 0.2896 (0.0004 -- 2.3328)  max mem: 16413
[2023-09-04 07:15:20,234] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28439
[2023-09-04 07:15:20,234] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28439
[2023-09-04 07:15:20,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:15:20,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:15:20,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [177]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5288 (1.6113)  loss_scale: 32768.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1874 (7.3585)  time: 0.8708 (0.5241 -- 2.5061)  data: 0.1219 (0.0008 -- 1.4935)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6056 (1.6126)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7764 (7.2983)  time: 0.9009 (0.5255 -- 4.2750)  data: 0.3537 (0.0003 -- 3.7590)  max mem: 16413
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6748 (1.6133)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7148 (7.2807)  time: 0.6329 (0.4952 -- 2.3837)  data: 0.1121 (0.0002 -- 1.8404)  max mem: 16413
Epoch: [177] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6748 (1.6070)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7148 (7.2807)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1309 (0.1309)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4208 (2.4208 -- 2.4208)  data: 2.2070 (2.2070 -- 2.2070)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3471 (0.4720)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (98.9899)  time: 0.4150 (0.1949 -- 2.4208)  data: 0.2054 (0.0007 -- 2.2070)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3668 (0.4657)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (97.8836)  time: 0.2212 (0.1689 -- 0.5586)  data: 0.0217 (0.0001 -- 0.3775)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4471 (0.5218)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.3402)  time: 0.2083 (0.1330 -- 0.5586)  data: 0.0215 (0.0001 -- 0.3775)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 86.929 Acc@5 98.133 loss 0.523
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [178]  [  0/160]  eta: 0:17:47  lr: 0.000002  min_lr: 0.000000  loss: 2.2474 (2.2474)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0986 (7.0986)  time: 6.6715 (6.6715 -- 6.6715)  data: 6.1110 (6.1110 -- 6.1110)  max mem: 16413
Epoch: [178]  [ 20/160]  eta: 0:02:40  lr: 0.000001  min_lr: 0.000000  loss: 1.6747 (1.6585)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4787 (7.7514)  time: 0.8704 (0.5319 -- 3.2590)  data: 0.1073 (0.0005 -- 1.0318)  max mem: 16413
Epoch: [178]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.7234 (1.6498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6460 (7.3532)  time: 0.8717 (0.5147 -- 3.1209)  data: 0.1890 (0.0003 -- 2.5838)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6788 (1.6204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7549 (7.1680)  time: 0.8598 (0.5234 -- 2.3412)  data: 0.1181 (0.0009 -- 1.0262)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6407 (1.6223)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3418 (7.2589)  time: 0.8381 (0.5226 -- 2.5910)  data: 0.2904 (0.0005 -- 2.0367)  max mem: 16413
[2023-09-04 07:17:22,304] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:17:22,304] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:17:22,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:17:22,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [178]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.5294 (1.6168)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8953 (7.1907)  time: 0.9885 (0.5315 -- 2.9220)  data: 0.3757 (0.0004 -- 2.4052)  max mem: 16413
Epoch: [178]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7258 (1.6312)  loss_scale: 32768.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5887 (7.4143)  time: 0.8211 (0.5149 -- 3.9113)  data: 0.2613 (0.0005 -- 3.3774)  max mem: 16413
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7069 (1.6330)  loss_scale: 32768.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3891 (7.4419)  time: 0.8623 (0.5229 -- 2.3530)  data: 0.2820 (0.0005 -- 1.8316)  max mem: 16413
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7128 (1.6445)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4557 (7.5209)  time: 0.6752 (0.4961 -- 2.0288)  data: 0.1504 (0.0002 -- 1.4835)  max mem: 16413
Epoch: [178] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7128 (1.6248)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4557 (7.5209)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1324 (0.1324)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2842 (2.2842 -- 2.2842)  data: 2.0822 (2.0822 -- 2.0822)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3577 (0.4732)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4141 (0.1973 -- 2.2842)  data: 0.2004 (0.0006 -- 2.0822)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3577 (0.4655)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2273 (0.1693 -- 0.3900)  data: 0.0233 (0.0001 -- 0.1908)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4477 (0.5227)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.3402)  time: 0.2115 (0.1329 -- 0.3900)  data: 0.0230 (0.0001 -- 0.1908)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 86.722 Acc@5 98.133 loss 0.526
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [179]  [  0/160]  eta: 0:18:41  lr: 0.000001  min_lr: 0.000000  loss: 1.0354 (1.0354)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4667 (8.4667)  time: 7.0107 (7.0107 -- 7.0107)  data: 5.8926 (5.8926 -- 5.8926)  max mem: 16413
Epoch: [179]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.6791 (1.6535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7961 (7.1507)  time: 0.8939 (0.5300 -- 2.5576)  data: 0.1571 (0.0002 -- 2.0232)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.3764 (1.6108)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0239 (6.8265)  time: 0.9195 (0.5201 -- 4.0651)  data: 0.3686 (0.0003 -- 3.5196)  max mem: 16413
[2023-09-04 07:19:13,200] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28681
[2023-09-04 07:19:13,200] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28681
[2023-09-04 07:19:13,200] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:19:13,200] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:19:13,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [179]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5305 (1.6291)  loss_scale: 16384.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4090 (7.1653)  time: 0.8002 (0.5312 -- 2.3639)  data: 0.2447 (0.0002 -- 1.8410)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.6421 (1.6289)  loss_scale: 16384.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7590 (7.2527)  time: 0.9370 (0.5290 -- 3.8402)  data: 0.3943 (0.0004 -- 3.3064)  max mem: 16413
[2023-09-04 07:20:00,783] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28734
[2023-09-04 07:20:00,783] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28734
[2023-09-04 07:20:00,783] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:20:00,783] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:20:00,783] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.6167 (1.6292)  loss_scale: 16384.0000 (22467.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2180 (7.2949)  time: 0.9783 (0.5146 -- 3.5794)  data: 0.4435 (0.0003 -- 3.0529)  max mem: 16413
Epoch: [179]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7412 (1.6493)  loss_scale: 8192.0000 (20107.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1474 (7.4129)  time: 0.8457 (0.5232 -- 3.7004)  data: 0.3053 (0.0002 -- 3.1927)  max mem: 16413
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.3476 (1.6185)  loss_scale: 8192.0000 (18417.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3648 (7.3134)  time: 0.9568 (0.5219 -- 4.7319)  data: 0.4162 (0.0002 -- 4.2176)  max mem: 16413
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5428 (1.6073)  loss_scale: 8192.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1375 (7.2958)  time: 0.6331 (0.4930 -- 1.7470)  data: 0.1211 (0.0002 -- 1.2394)  max mem: 16413
Epoch: [179] Total time: 0:02:25 (0.9110 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5428 (1.6223)  loss_scale: 8192.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1375 (7.2958)
[2023-09-04 07:20:55,149] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-09-04 07:20:55,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-09-04 07:20:55,151] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-09-04 07:20:55,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-09-04 07:20:56,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-09-04 07:20:56,119] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1322 (0.1322)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2777 (2.2777 -- 2.2777)  data: 2.0540 (2.0540 -- 2.0540)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3566 (0.4692)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4123 (0.2011 -- 2.2777)  data: 0.1922 (0.0007 -- 2.0540)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3566 (0.4629)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2249 (0.1688 -- 0.4020)  data: 0.0129 (0.0001 -- 0.1922)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4485 (0.5199)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2055 (0.1326 -- 0.4020)  data: 0.0126 (0.0001 -- 0.1922)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 87.137 Acc@5 98.340 loss 0.522
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [180]  [  0/160]  eta: 0:23:30  lr: 0.000001  min_lr: 0.000000  loss: 1.4725 (1.4725)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7914 (5.7914)  time: 8.8176 (8.8176 -- 8.8176)  data: 5.6995 (5.6995 -- 5.6995)  max mem: 16413
Epoch: [180]  [ 20/160]  eta: 0:02:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7344 (1.7094)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7213 (7.3919)  time: 0.7534 (0.5181 -- 2.4329)  data: 0.1574 (0.0005 -- 1.3297)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.7497 (1.6614)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7078 (7.4300)  time: 0.9857 (0.5227 -- 4.1805)  data: 0.4189 (0.0003 -- 3.6544)  max mem: 16413
Epoch: [180]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6106 (1.6386)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3727 (7.1967)  time: 0.7770 (0.5207 -- 2.5876)  data: 0.2377 (0.0003 -- 2.0759)  max mem: 16413
[2023-09-04 07:22:04,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:22:04,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:22:04,649] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 07:22:04,649] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [180]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.5068 (1.6321)  loss_scale: 16384.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4783 (7.4168)  time: 0.9832 (0.5224 -- 3.0787)  data: 0.4357 (0.0003 -- 2.5375)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.4527 (1.6064)  loss_scale: 16384.0000 (11274.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2790 (7.2718)  time: 0.8612 (0.5198 -- 2.8361)  data: 0.3237 (0.0003 -- 2.3234)  max mem: 16413
Epoch: [180]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4245 (1.5881)  loss_scale: 16384.0000 (12118.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5978 (7.2856)  time: 0.7819 (0.5314 -- 3.3570)  data: 0.2336 (0.0003 -- 2.8204)  max mem: 16413
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4756 (1.5763)  loss_scale: 16384.0000 (12723.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0014 (7.2825)  time: 0.8568 (0.5309 -- 2.9465)  data: 0.3069 (0.0005 -- 2.4150)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5043 (1.5777)  loss_scale: 16384.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3017 (7.2315)  time: 0.6526 (0.4947 -- 2.2363)  data: 0.1297 (0.0002 -- 1.7071)  max mem: 16413
Epoch: [180] Total time: 0:02:21 (0.8832 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5043 (1.5972)  loss_scale: 16384.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3017 (7.2315)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1310 (0.1310)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2501 (2.2501 -- 2.2501)  data: 2.0353 (2.0353 -- 2.0353)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4100 (0.4737)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4358 (0.2090 -- 2.2501)  data: 0.2110 (0.0009 -- 2.0353)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3718 (0.4646)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2347 (0.1692 -- 0.5407)  data: 0.0241 (0.0001 -- 0.2727)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4547 (0.5226)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2174 (0.1331 -- 0.5407)  data: 0.0237 (0.0001 -- 0.2727)  max mem: 16413
Val: Total time: 0:00:07 (0.2938 s / it)
* Acc@1 87.137 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [181]  [  0/160]  eta: 0:21:43  lr: 0.000001  min_lr: 0.000000  loss: 2.0154 (2.0154)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0314 (8.0314)  time: 8.1468 (8.1468 -- 8.1468)  data: 5.7465 (5.7465 -- 5.7465)  max mem: 16413
Epoch: [181]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 1.6181 (1.6714)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7700 (7.1230)  time: 0.8164 (0.5203 -- 3.8675)  data: 0.1962 (0.0005 -- 3.3448)  max mem: 16413
[2023-09-04 07:24:06,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:24:06,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:24:06,931] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:24:06,931] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:24:12,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=164, lr=[2.8157390886715256e-07, 2.8157390886715256e-07, 3.128598987412807e-07, 3.128598987412807e-07, 3.47622109712534e-07, 3.47622109712534e-07, 3.862467885694823e-07, 3.862467885694823e-07, 4.2916309841053583e-07, 4.2916309841053583e-07, 4.768478871228176e-07, 4.768478871228176e-07, 5.298309856920195e-07, 5.298309856920195e-07, 5.887010952133549e-07, 5.887010952133549e-07, 6.541123280148388e-07, 6.541123280148388e-07, 7.267914755720431e-07, 7.267914755720431e-07, 8.075460839689369e-07, 8.075460839689369e-07, 8.97273426632152e-07, 8.97273426632152e-07, 9.969704740357244e-07, 9.969704740357244e-07, 1.1077449711508049e-06, 1.1077449711508049e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 07:24:12,548] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=18.224759542217562, CurrSamplesPerSec=22.05053538594183, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:01:58  lr: 0.000001  min_lr: 0.000000  loss: 1.5767 (1.6300)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2229 (6.8399)  time: 0.7950 (0.5304 -- 2.3870)  data: 0.2263 (0.0003 -- 1.8235)  max mem: 16413
Epoch: [181]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7337 (1.6626)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4598 (6.9568)  time: 0.9533 (0.5238 -- 3.4174)  data: 0.3914 (0.0003 -- 2.8680)  max mem: 16413
[2023-09-04 07:24:39,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29029
[2023-09-04 07:24:39,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29029
[2023-09-04 07:24:39,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:24:39,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:24:39,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6553 (1.6778)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1046 (6.9415)  time: 0.8593 (0.5170 -- 4.5014)  data: 0.3130 (0.0003 -- 3.9952)  max mem: 16413
Epoch: [181]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6405 (1.6647)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3806 (7.0790)  time: 0.8695 (0.5174 -- 2.6196)  data: 0.3196 (0.0007 -- 2.0840)  max mem: 16413
Epoch: [181]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6510 (1.6619)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8258 (7.1942)  time: 0.8368 (0.5284 -- 3.7518)  data: 0.2909 (0.0003 -- 3.2343)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5809 (1.6442)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3025 (7.1760)  time: 0.9334 (0.5325 -- 4.9457)  data: 0.3835 (0.0003 -- 4.4182)  max mem: 16413
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4251 (1.6292)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3723 (7.2185)  time: 0.6322 (0.4934 -- 2.8877)  data: 0.1199 (0.0002 -- 2.3822)  max mem: 16413
Epoch: [181] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4251 (1.6293)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3723 (7.2185)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1312 (0.1312)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3171 (2.3171 -- 2.3171)  data: 2.0498 (2.0498 -- 2.0498)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4262 (0.4765)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4027 (0.1971 -- 2.3171)  data: 0.1887 (0.0008 -- 2.0498)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3880 (0.4656)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2216 (0.1691 -- 0.5060)  data: 0.0196 (0.0001 -- 0.2900)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4547 (0.5227)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2069 (0.1332 -- 0.5060)  data: 0.0186 (0.0001 -- 0.2900)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 86.929 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [182]  [  0/160]  eta: 0:20:49  lr: 0.000001  min_lr: 0.000000  loss: 1.3925 (1.3925)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7979 (8.7979)  time: 7.8114 (7.8114 -- 7.8114)  data: 7.2896 (7.2896 -- 7.2896)  max mem: 16413
Epoch: [182]  [ 20/160]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7230 (1.6800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6255 (7.7158)  time: 0.9269 (0.5226 -- 4.7366)  data: 0.3842 (0.0003 -- 4.2134)  max mem: 16413
[2023-09-04 07:26:43,712] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:26:43,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:26:43,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:26:43,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [182]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.6434 (1.6721)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9350 (7.3909)  time: 0.7997 (0.5264 -- 3.2135)  data: 0.2487 (0.0003 -- 2.6812)  max mem: 16413
Epoch: [182]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.4943 (1.6055)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9908 (7.5867)  time: 0.9249 (0.5267 -- 3.3634)  data: 0.3777 (0.0009 -- 2.8356)  max mem: 16413
[2023-09-04 07:27:16,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29199
[2023-09-04 07:27:16,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29199
[2023-09-04 07:27:16,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:27:16,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:27:16,991] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [182]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7415 (1.6254)  loss_scale: 32768.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3374 (7.5131)  time: 0.7982 (0.5250 -- 2.2668)  data: 0.0843 (0.0002 -- 0.9100)  max mem: 16413
Epoch: [182]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6060 (1.6063)  loss_scale: 16384.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1685 (7.5612)  time: 0.9142 (0.5161 -- 2.8761)  data: 0.2675 (0.0005 -- 2.3609)  max mem: 16413
Epoch: [182]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5104 (1.6063)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6158 (7.5147)  time: 0.8430 (0.5254 -- 1.8988)  data: 0.2486 (0.0004 -- 1.3632)  max mem: 16413
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5111 (1.5944)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6679 (7.4303)  time: 0.8994 (0.5342 -- 2.6797)  data: 0.2032 (0.0004 -- 2.1544)  max mem: 16413
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6759 (1.6179)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7507 (7.4781)  time: 0.6250 (0.4978 -- 1.3772)  data: 0.0591 (0.0002 -- 0.8666)  max mem: 16413
Epoch: [182] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6759 (1.6119)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7507 (7.4781)
Val:  [ 0/27]  eta: 0:00:54  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0129 (2.0129 -- 2.0129)  data: 1.8012 (1.8012 -- 1.8012)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4207 (0.4771)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4256 (0.1974 -- 2.0129)  data: 0.2123 (0.0007 -- 1.8012)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3845 (0.4676)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2387 (0.1685 -- 0.5796)  data: 0.0346 (0.0001 -- 0.3590)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4541 (0.5240)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2251 (0.1325 -- 0.5796)  data: 0.0343 (0.0001 -- 0.3590)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 87.344 Acc@5 98.548 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [183]  [  0/160]  eta: 0:19:41  lr: 0.000001  min_lr: 0.000000  loss: 1.7339 (1.7339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9191 (7.9191)  time: 7.3834 (7.3834 -- 7.3834)  data: 6.8497 (6.8497 -- 6.8497)  max mem: 16413
Epoch: [183]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.6649 (1.6440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7555 (7.4759)  time: 0.8753 (0.5195 -- 4.2001)  data: 0.3371 (0.0004 -- 3.6783)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.4536 (1.6115)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0788 (7.4197)  time: 0.8240 (0.5344 -- 2.4102)  data: 0.2470 (0.0002 -- 1.8600)  max mem: 16413
[2023-09-04 07:29:19,976] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:29:19,977] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:29:19,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:29:19,979] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [183]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6185 (1.6336)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3621 (7.0684)  time: 0.9136 (0.5363 -- 4.0247)  data: 0.3578 (0.0004 -- 3.4999)  max mem: 16413
[2023-09-04 07:29:32,394] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29341
[2023-09-04 07:29:32,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:29:32,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 07:29:32,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29341
[2023-09-04 07:29:32,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [183]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.5096 (1.6076)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9869 (7.1539)  time: 0.8212 (0.5232 -- 3.0727)  data: 0.2745 (0.0004 -- 2.5393)  max mem: 16413
Epoch: [183]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6319 (1.6258)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6371 (7.2407)  time: 0.8866 (0.5207 -- 2.7584)  data: 0.1759 (0.0004 -- 1.5797)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5054 (1.6056)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4754 (7.1881)  time: 0.8562 (0.5335 -- 4.1504)  data: 0.3021 (0.0002 -- 3.6181)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5125 (1.5969)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6415 (7.1205)  time: 0.9764 (0.5142 -- 4.5134)  data: 0.4305 (0.0004 -- 3.9972)  max mem: 16413
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5554 (1.6008)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7518 (7.1629)  time: 0.6787 (0.4961 -- 2.9084)  data: 0.1646 (0.0003 -- 2.3973)  max mem: 16413
Epoch: [183] Total time: 0:02:23 (0.8970 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5554 (1.6023)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7518 (7.1629)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1299 (0.1299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3974 (2.3974 -- 2.3974)  data: 2.1742 (2.1742 -- 2.1742)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4216 (0.4765)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4138 (0.1987 -- 2.3974)  data: 0.2016 (0.0007 -- 2.1742)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3816 (0.4680)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2194 (0.1704 -- 0.4507)  data: 0.0152 (0.0001 -- 0.2566)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4523 (0.5232)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2053 (0.1328 -- 0.4507)  data: 0.0140 (0.0001 -- 0.2566)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 87.137 Acc@5 98.548 loss 0.524
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [184]  [  0/160]  eta: 0:20:12  lr: 0.000001  min_lr: 0.000000  loss: 1.5082 (1.5082)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4573 (7.4573)  time: 7.5796 (7.5796 -- 7.5796)  data: 7.0312 (7.0312 -- 7.0312)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.5178 (1.5410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5494 (7.3235)  time: 0.8635 (0.5285 -- 3.9039)  data: 0.0610 (0.0002 -- 0.9896)  max mem: 16413
[2023-09-04 07:31:36,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:31:36,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:31:36,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:31:36,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [184]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.6396 (1.5910)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6941 (7.0248)  time: 0.8556 (0.5236 -- 3.6567)  data: 0.0207 (0.0003 -- 0.3731)  max mem: 16413
Epoch: [184]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7004 (1.6173)  loss_scale: 32768.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3051 (7.2451)  time: 0.8719 (0.5215 -- 3.7411)  data: 0.0016 (0.0003 -- 0.0036)  max mem: 16413
[2023-09-04 07:32:12,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29512
[2023-09-04 07:32:12,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:32:12,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29512
[2023-09-04 07:32:12,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:32:12,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [184]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.5640 (1.5912)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7604 (7.2099)  time: 0.8062 (0.5257 -- 1.9305)  data: 0.0111 (0.0005 -- 0.1848)  max mem: 16413
Epoch: [184]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.5883 (1.5815)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2128 (7.0555)  time: 0.8831 (0.5217 -- 1.8880)  data: 0.0655 (0.0004 -- 1.2711)  max mem: 16413
Epoch: [184]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6576 (1.5673)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0480 (7.0521)  time: 0.9128 (0.5156 -- 3.9403)  data: 0.0015 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6000 (1.5954)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5460 (7.0465)  time: 0.8347 (0.5221 -- 4.1616)  data: 0.0020 (0.0004 -- 0.0141)  max mem: 16413
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4681 (1.5798)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2119 (6.9812)  time: 0.6839 (0.4947 -- 1.8426)  data: 0.0007 (0.0001 -- 0.0014)  max mem: 16413
Epoch: [184] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4681 (1.6060)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2119 (6.9812)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1305 (0.1305)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2638 (2.2638 -- 2.2638)  data: 2.0289 (2.0289 -- 2.0289)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4218 (0.4781)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4059 (0.2095 -- 2.2638)  data: 0.1869 (0.0008 -- 2.0289)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3989 (0.4687)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2238 (0.1709 -- 0.4071)  data: 0.0187 (0.0001 -- 0.1921)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4519 (0.5251)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2069 (0.1329 -- 0.4071)  data: 0.0183 (0.0001 -- 0.1921)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 87.344 Acc@5 98.548 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [185]  [  0/160]  eta: 0:19:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8537 (1.8537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6734 (9.6734)  time: 7.3606 (7.3606 -- 7.3606)  data: 6.7973 (6.7973 -- 6.7973)  max mem: 16413
Epoch: [185]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 1.3891 (1.5157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6678 (6.9927)  time: 0.8665 (0.5280 -- 2.4075)  data: 0.2348 (0.0005 -- 1.8347)  max mem: 16413
Epoch: [185]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.5883 (1.5397)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6266 (6.9947)  time: 0.8465 (0.5188 -- 2.6884)  data: 0.2481 (0.0004 -- 2.1556)  max mem: 16413
[2023-09-04 07:34:16,758] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:34:16,759] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:34:16,760] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:34:16,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [185]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4375 (1.5173)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9551 (7.0000)  time: 0.8729 (0.5269 -- 2.5539)  data: 0.3255 (0.0003 -- 1.9960)  max mem: 16413
Epoch: [185]  [ 80/160]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 1.7243 (1.5642)  loss_scale: 32768.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8302 (7.2139)  time: 0.7887 (0.5299 -- 2.0224)  data: 0.1923 (0.0005 -- 1.4726)  max mem: 16413
Epoch: [185]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6843 (1.5668)  loss_scale: 32768.0000 (26117.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8564 (7.1690)  time: 0.8947 (0.5308 -- 2.7881)  data: 0.3101 (0.0008 -- 2.2529)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7004 (1.5769)  loss_scale: 32768.0000 (27216.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4575 (7.2613)  time: 0.8850 (0.5270 -- 2.0954)  data: 0.3382 (0.0005 -- 1.5446)  max mem: 16413
[2023-09-04 07:35:38,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29739
[2023-09-04 07:35:38,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29739
[2023-09-04 07:35:38,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:35:38,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:35:38,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7290 (1.5797)  loss_scale: 32768.0000 (27771.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0932 (7.2637)  time: 0.9018 (0.5247 -- 2.3409)  data: 0.3623 (0.0003 -- 1.8076)  max mem: 16413
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6021 (1.5750)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6104 (7.3161)  time: 0.7172 (0.4951 -- 2.5440)  data: 0.1919 (0.0002 -- 2.0122)  max mem: 16413
Epoch: [185] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6021 (1.6034)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6104 (7.3161)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1297 (0.1297)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3784 (2.3784 -- 2.3784)  data: 2.1548 (2.1548 -- 2.1548)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4318 (0.4759)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4192 (0.1932 -- 2.3784)  data: 0.2074 (0.0005 -- 2.1548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4028 (0.4666)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2187 (0.1689 -- 0.3833)  data: 0.0156 (0.0001 -- 0.1823)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4524 (0.5227)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2038 (0.1333 -- 0.3833)  data: 0.0153 (0.0001 -- 0.1823)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 87.344 Acc@5 98.548 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [186]  [  0/160]  eta: 0:20:29  lr: 0.000001  min_lr: 0.000000  loss: 1.3058 (1.3058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2023 (5.2023)  time: 7.6858 (7.6858 -- 7.6858)  data: 7.1309 (7.1309 -- 7.1309)  max mem: 16413
Epoch: [186]  [ 20/160]  eta: 0:02:57  lr: 0.000001  min_lr: 0.000000  loss: 1.5692 (1.5277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6712 (7.2257)  time: 0.9443 (0.5298 -- 3.7644)  data: 0.1664 (0.0003 -- 1.3734)  max mem: 16413
Epoch: [186]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.7739 (1.6927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6842 (6.8703)  time: 0.7705 (0.5197 -- 2.3263)  data: 0.1113 (0.0002 -- 1.0517)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7288 (1.6954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3069 (7.0810)  time: 0.9333 (0.5210 -- 3.1396)  data: 0.3932 (0.0005 -- 2.5869)  max mem: 16413
Epoch: [186]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.6686 (1.6854)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9638 (7.1914)  time: 0.8946 (0.5204 -- 2.8465)  data: 0.3433 (0.0002 -- 2.2881)  max mem: 16413
Epoch: [186]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6646 (1.6655)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2057 (7.3039)  time: 0.7512 (0.5319 -- 3.0284)  data: 0.2065 (0.0004 -- 2.4954)  max mem: 16413
[2023-09-04 07:37:42,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:37:42,533] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:37:42,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:37:42,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:37:48,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29873
[2023-09-04 07:37:48,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29873
[2023-09-04 07:37:48,492] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:37:48,492] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:37:48,492] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5381 (1.6485)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4984 (7.3432)  time: 0.8954 (0.5319 -- 3.5311)  data: 0.3502 (0.0003 -- 2.9995)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5715 (1.6337)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2369 (7.4775)  time: 0.9929 (0.5257 -- 4.5460)  data: 0.4522 (0.0003 -- 4.0147)  max mem: 16413
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6991 (1.6299)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9144 (7.4269)  time: 0.6831 (0.4952 -- 3.6300)  data: 0.1688 (0.0002 -- 3.1181)  max mem: 16413
Epoch: [186] Total time: 0:02:24 (0.9028 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6991 (1.6405)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9144 (7.4269)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1300 (0.1300)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3259 (2.3259 -- 2.3259)  data: 2.0827 (2.0827 -- 2.0827)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4213 (0.4767)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4204 (0.2057 -- 2.3259)  data: 0.2020 (0.0006 -- 2.0827)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3897 (0.4666)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1693 -- 0.3692)  data: 0.0168 (0.0001 -- 0.1475)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4506 (0.5234)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2059 (0.1332 -- 0.3692)  data: 0.0165 (0.0001 -- 0.1475)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 87.137 Acc@5 98.548 loss 0.526
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.76%
Epoch: [187]  [  0/160]  eta: 0:21:30  lr: 0.000001  min_lr: 0.000000  loss: 2.2428 (2.2428)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2437 (7.2437)  time: 8.0627 (8.0627 -- 8.0627)  data: 7.4235 (7.4235 -- 7.4235)  max mem: 16413
Epoch: [187]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 1.4173 (1.6041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8054 (7.8663)  time: 0.8554 (0.5195 -- 4.0382)  data: 0.3083 (0.0002 -- 3.4889)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:09  lr: 0.000001  min_lr: 0.000000  loss: 1.8465 (1.6712)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7513 (7.2636)  time: 0.9603 (0.5226 -- 3.2245)  data: 0.4217 (0.0004 -- 2.7210)  max mem: 16413
Epoch: [187]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5224 (1.6513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4735 (6.9778)  time: 0.7597 (0.5282 -- 2.2102)  data: 0.2086 (0.0005 -- 1.6579)  max mem: 16413
[2023-09-04 07:39:49,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=170, lr=[1.3231164921790851e-07, 1.3231164921790851e-07, 1.4701294357545393e-07, 1.4701294357545393e-07, 1.6334771508383767e-07, 1.6334771508383767e-07, 1.814974612042641e-07, 1.814974612042641e-07, 2.0166384578251563e-07, 2.0166384578251563e-07, 2.240709397583507e-07, 2.240709397583507e-07, 2.489677108426119e-07, 2.489677108426119e-07, 2.766307898251243e-07, 2.766307898251243e-07, 3.073675442501381e-07, 3.073675442501381e-07, 3.415194936112645e-07, 3.415194936112645e-07, 3.794661040125162e-07, 3.794661040125162e-07, 4.216290044583513e-07, 4.216290044583513e-07, 4.684766716203903e-07, 4.684766716203903e-07, 5.20529635133767e-07, 5.20529635133767e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 07:39:49,698] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=18.27347844795523, CurrSamplesPerSec=22.649406940670666, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6467 (1.6721)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5613 (7.2841)  time: 0.8754 (0.5217 -- 3.4149)  data: 0.0690 (0.0004 -- 1.3486)  max mem: 16413
[2023-09-04 07:39:51,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:39:51,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:39:51,367] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:39:51,367] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [187]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.5880 (1.6638)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7635 (7.2924)  time: 0.8505 (0.5259 -- 3.1837)  data: 0.0846 (0.0004 -- 0.8185)  max mem: 16413
[2023-09-04 07:40:24,229] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30039
[2023-09-04 07:40:24,229] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30039
[2023-09-04 07:40:24,229] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:40:24,230] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:40:24,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [187]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5353 (1.6512)  loss_scale: 32768.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4519 (7.2405)  time: 0.8749 (0.5223 -- 3.7603)  data: 0.3007 (0.0003 -- 3.2485)  max mem: 16413
Epoch: [187]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7523 (1.6529)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1115 (7.2436)  time: 0.9449 (0.5183 -- 3.4568)  data: 0.2142 (0.0003 -- 2.9221)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4483 (1.6324)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8260 (7.2049)  time: 0.6833 (0.4947 -- 1.8237)  data: 0.0013 (0.0002 -- 0.0164)  max mem: 16413
Epoch: [187] Total time: 0:02:23 (0.8977 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4483 (1.6219)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8260 (7.2049)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4569 (2.4569 -- 2.4569)  data: 2.2411 (2.2411 -- 2.2411)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4188 (0.4765)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4154 (0.1931 -- 2.4569)  data: 0.2055 (0.0002 -- 2.2411)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3899 (0.4664)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2143 (0.1702 -- 0.3045)  data: 0.0118 (0.0001 -- 0.1128)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4500 (0.5237)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.1992 (0.1329 -- 0.3045)  data: 0.0115 (0.0001 -- 0.1128)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [188]  [  0/160]  eta: 0:18:57  lr: 0.000000  min_lr: 0.000000  loss: 1.1184 (1.1184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0185 (6.0185)  time: 7.1066 (7.1066 -- 7.1066)  data: 6.5596 (6.5596 -- 6.5596)  max mem: 16413
Epoch: [188]  [ 20/160]  eta: 0:02:29  lr: 0.000000  min_lr: 0.000000  loss: 1.8029 (1.6169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5082 (7.3129)  time: 0.7642 (0.5244 -- 3.2001)  data: 0.2103 (0.0005 -- 2.6672)  max mem: 16413
Epoch: [188]  [ 40/160]  eta: 0:02:01  lr: 0.000000  min_lr: 0.000000  loss: 1.7412 (1.6635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3192 (7.1374)  time: 0.9643 (0.5173 -- 4.0013)  data: 0.3303 (0.0005 -- 3.4645)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7804 (1.6936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5333 (7.0258)  time: 0.8542 (0.5276 -- 2.2300)  data: 0.2507 (0.0002 -- 1.7045)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.5120 (1.6633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9540 (7.0558)  time: 0.9044 (0.5179 -- 3.1795)  data: 0.1825 (0.0004 -- 2.6553)  max mem: 16413
[2023-09-04 07:42:29,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:42:29,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:42:29,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:42:29,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [188]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7097 (1.6579)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9517 (7.0417)  time: 0.8778 (0.5202 -- 2.7517)  data: 0.0128 (0.0002 -- 0.2329)  max mem: 16413
Epoch: [188]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5749 (1.6457)  loss_scale: 32768.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4154 (7.0825)  time: 0.8762 (0.5393 -- 2.4890)  data: 0.0635 (0.0006 -- 1.0968)  max mem: 16413
[2023-09-04 07:43:01,151] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30207
[2023-09-04 07:43:01,151] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30207
[2023-09-04 07:43:01,151] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:43:01,151] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:43:01,152] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5996 (1.6552)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3308 (7.0463)  time: 0.8886 (0.5167 -- 4.4403)  data: 0.0455 (0.0003 -- 0.8883)  max mem: 16413
[2023-09-04 07:43:27,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30239
[2023-09-04 07:43:27,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:43:27,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30239
[2023-09-04 07:43:27,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-04 07:43:27,266] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4071 (1.6316)  loss_scale: 16384.0000 (20326.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7915 (7.0642)  time: 0.6741 (0.4814 -- 2.8424)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [188] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4071 (1.6326)  loss_scale: 16384.0000 (20326.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7915 (7.0642)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2958 (2.2958 -- 2.2958)  data: 2.0744 (2.0744 -- 2.0744)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4182 (0.4782)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.3992 (0.1937 -- 2.2958)  data: 0.1896 (0.0006 -- 2.0744)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3803 (0.4673)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2259 (0.1693 -- 0.5799)  data: 0.0203 (0.0001 -- 0.3898)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4502 (0.5255)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2133 (0.1326 -- 0.5799)  data: 0.0200 (0.0001 -- 0.3898)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [189]  [  0/160]  eta: 0:22:53  lr: 0.000000  min_lr: 0.000000  loss: 0.9166 (0.9166)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9428 (7.9428)  time: 8.5867 (8.5867 -- 8.5867)  data: 8.0572 (8.0572 -- 8.0572)  max mem: 16413
Epoch: [189]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 1.6547 (1.6782)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4404 (8.1526)  time: 0.7395 (0.5344 -- 2.7060)  data: 0.1886 (0.0003 -- 2.1611)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.5273 (1.6129)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5698 (7.3776)  time: 0.9732 (0.5304 -- 4.7007)  data: 0.4286 (0.0005 -- 4.1662)  max mem: 16413
Epoch: [189]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6297 (1.6161)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2027 (7.4363)  time: 0.7941 (0.5132 -- 3.0371)  data: 0.2250 (0.0003 -- 2.5102)  max mem: 16413
Epoch: [189]  [ 80/160]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 1.5844 (1.6217)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2682 (7.5493)  time: 0.7959 (0.5069 -- 2.8396)  data: 0.2520 (0.0008 -- 2.3016)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7154 (1.6451)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4882 (7.3711)  time: 0.9529 (0.5159 -- 3.8021)  data: 0.4083 (0.0010 -- 3.2445)  max mem: 16413
Epoch: [189]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5965 (1.6379)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1848 (7.3736)  time: 0.7637 (0.5274 -- 2.1333)  data: 0.1809 (0.0003 -- 1.5808)  max mem: 16413
[2023-09-04 07:45:31,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:45:31,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 07:45:31,527] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:45:31,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6421 (1.6307)  loss_scale: 16384.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2892 (7.3094)  time: 0.9366 (0.5295 -- 2.8645)  data: 0.2968 (0.0005 -- 2.3442)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5560 (1.6159)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2187 (7.2577)  time: 0.7341 (0.4963 -- 3.0756)  data: 0.2150 (0.0002 -- 2.5570)  max mem: 16413
Epoch: [189] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5560 (1.6193)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2187 (7.2577)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1297 (0.1297)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2862 (2.2862 -- 2.2862)  data: 2.0637 (2.0637 -- 2.0637)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4135 (0.4773)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4060 (0.1969 -- 2.2862)  data: 0.1895 (0.0003 -- 2.0637)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3782 (0.4668)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2251 (0.1687 -- 0.4598)  data: 0.0193 (0.0001 -- 0.2717)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4499 (0.5250)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2083 (0.1330 -- 0.4598)  data: 0.0190 (0.0001 -- 0.2717)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [190]  [  0/160]  eta: 0:18:42  lr: 0.000000  min_lr: 0.000000  loss: 0.9466 (0.9466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3672 (8.3672)  time: 7.0171 (7.0171 -- 7.0171)  data: 6.4590 (6.4590 -- 6.4590)  max mem: 16413
[2023-09-04 07:46:12,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30402
[2023-09-04 07:46:12,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30402
[2023-09-04 07:46:12,867] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:46:12,867] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:46:12,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [190]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 1.3959 (1.4167)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2396 (7.0120)  time: 0.9140 (0.5340 -- 2.9061)  data: 0.3618 (0.0007 -- 2.3944)  max mem: 16413
Epoch: [190]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.7218 (1.5553)  loss_scale: 8192.0000 (8591.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0915 (7.6044)  time: 0.8814 (0.5205 -- 4.4115)  data: 0.2719 (0.0003 -- 3.8734)  max mem: 16413
Epoch: [190]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5587 (1.5512)  loss_scale: 8192.0000 (8460.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7477 (7.3360)  time: 0.8325 (0.5406 -- 3.2400)  data: 0.2720 (0.0005 -- 2.7327)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.6174 (1.5753)  loss_scale: 8192.0000 (8394.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4102 (7.4122)  time: 0.9699 (0.5240 -- 4.6466)  data: 0.4223 (0.0004 -- 4.1241)  max mem: 16413
Epoch: [190]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6485 (1.5965)  loss_scale: 8192.0000 (8354.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7372 (7.5153)  time: 0.7882 (0.5198 -- 3.4355)  data: 0.2392 (0.0003 -- 2.8875)  max mem: 16413
Epoch: [190]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5546 (1.5931)  loss_scale: 8192.0000 (8327.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1196 (7.4645)  time: 0.9044 (0.5152 -- 3.3149)  data: 0.3608 (0.0003 -- 2.7735)  max mem: 16413
[2023-09-04 07:48:04,662] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:48:04,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 07:48:04,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:48:04,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5157 (1.5849)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2243 (7.3411)  time: 0.7768 (0.5359 -- 2.6038)  data: 0.2251 (0.0004 -- 2.0368)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6589 (1.5976)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1721 (7.3460)  time: 0.7615 (0.4946 -- 1.7944)  data: 0.2388 (0.0002 -- 1.2672)  max mem: 16413
Epoch: [190] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6589 (1.6012)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1721 (7.3460)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1296 (0.1296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5596 (2.5596 -- 2.5596)  data: 2.3210 (2.3210 -- 2.3210)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4044 (0.4765)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4238 (0.1898 -- 2.5596)  data: 0.2121 (0.0005 -- 2.3210)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3750 (0.4659)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2101 (0.1696 -- 0.2525)  data: 0.0035 (0.0001 -- 0.0548)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4487 (0.5241)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.1962 (0.1330 -- 0.2525)  data: 0.0033 (0.0001 -- 0.0548)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [191]  [  0/160]  eta: 0:20:54  lr: 0.000000  min_lr: 0.000000  loss: 1.0384 (1.0384)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3384 (8.3384)  time: 7.8378 (7.8378 -- 7.8378)  data: 7.2849 (7.2849 -- 7.2849)  max mem: 16413
Epoch: [191]  [ 20/160]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 1.5443 (1.5635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4196 (7.6862)  time: 0.8021 (0.5294 -- 2.8087)  data: 0.2536 (0.0003 -- 2.2427)  max mem: 16413
Epoch: [191]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.8544 (1.6445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2461 (8.1825)  time: 0.9876 (0.5162 -- 3.5872)  data: 0.4481 (0.0003 -- 3.0630)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.5313 (1.6297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3014 (8.1384)  time: 0.8388 (0.5228 -- 2.3960)  data: 0.2817 (0.0005 -- 1.8790)  max mem: 16413
Epoch: [191]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6301 (1.6119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1346 (7.9239)  time: 0.7730 (0.5339 -- 2.2962)  data: 0.1839 (0.0003 -- 1.7413)  max mem: 16413
[2023-09-04 07:50:09,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:50:09,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:50:09,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:50:09,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [191]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7257 (1.6343)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9679 (7.6740)  time: 0.9495 (0.5314 -- 3.2195)  data: 0.4051 (0.0005 -- 2.6932)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5099 (1.6241)  loss_scale: 32768.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5516 (7.7510)  time: 0.8518 (0.5230 -- 3.2699)  data: 0.3111 (0.0003 -- 2.7492)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5055 (1.6013)  loss_scale: 32768.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3860 (7.5943)  time: 0.9234 (0.5125 -- 2.9168)  data: 0.3803 (0.0001 -- 2.3701)  max mem: 16413
[2023-09-04 07:50:57,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30717
[2023-09-04 07:50:57,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30717
[2023-09-04 07:50:57,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:50:57,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:50:57,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7012 (1.6028)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6347 (7.4716)  time: 0.6362 (0.4812 -- 2.2850)  data: 0.1200 (0.0002 -- 1.7839)  max mem: 16413
Epoch: [191] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7012 (1.5990)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6347 (7.4716)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1294 (0.1294)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3367 (2.3367 -- 2.3367)  data: 2.1358 (2.1358 -- 2.1358)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4073 (0.4757)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4267 (0.2197 -- 2.3367)  data: 0.1990 (0.0007 -- 2.1358)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3761 (0.4652)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2236 (0.1716 -- 0.3039)  data: 0.0046 (0.0001 -- 0.0348)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4483 (0.5232)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2055 (0.1333 -- 0.3039)  data: 0.0041 (0.0001 -- 0.0348)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [192]  [  0/160]  eta: 0:19:05  lr: 0.000000  min_lr: 0.000000  loss: 0.9781 (0.9781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0452 (4.0452)  time: 7.1600 (7.1600 -- 7.1600)  data: 6.6320 (6.6320 -- 6.6320)  max mem: 16413
Epoch: [192]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.6322 (1.5967)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4974 (6.6271)  time: 0.8805 (0.5316 -- 3.2955)  data: 0.2751 (0.0005 -- 2.7556)  max mem: 16413
Epoch: [192]  [ 40/160]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 1.6341 (1.5923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1241 (7.0473)  time: 1.0126 (0.5290 -- 4.0062)  data: 0.4680 (0.0005 -- 3.4864)  max mem: 16413
Epoch: [192]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 1.7739 (1.6153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4377 (6.9531)  time: 0.6192 (0.5193 -- 1.3761)  data: 0.0662 (0.0004 -- 0.8541)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.7718 (1.6452)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4710 (6.9976)  time: 1.0076 (0.5236 -- 3.3916)  data: 0.0097 (0.0005 -- 0.1620)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7221 (1.6667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8109 (7.0029)  time: 0.7803 (0.5312 -- 2.7195)  data: 0.1500 (0.0004 -- 2.1887)  max mem: 16413
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5792 (1.6538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5044 (7.1295)  time: 0.7899 (0.5375 -- 3.3854)  data: 0.1945 (0.0005 -- 2.8619)  max mem: 16413
[2023-09-04 07:53:00,569] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:53:00,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:53:00,570] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:53:00,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:53:02,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30849
[2023-09-04 07:53:02,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30849
[2023-09-04 07:53:02,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:53:02,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:53:02,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7485 (1.6621)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1733 (7.0638)  time: 0.9295 (0.5278 -- 3.0756)  data: 0.3528 (0.0006 -- 2.5423)  max mem: 16413
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6398 (1.6617)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6852 (7.1427)  time: 0.6752 (0.4965 -- 2.8464)  data: 0.1539 (0.0001 -- 2.3201)  max mem: 16413
Epoch: [192] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6398 (1.6570)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6852 (7.1427)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1294 (0.1294)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2590 (2.2590 -- 2.2590)  data: 2.0524 (2.0524 -- 2.0524)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4070 (0.4752)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4133 (0.2104 -- 2.2590)  data: 0.1884 (0.0009 -- 2.0524)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3789 (0.4654)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2391 (0.1693 -- 0.7924)  data: 0.0320 (0.0001 -- 0.6182)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4484 (0.5235)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2179 (0.1323 -- 0.7924)  data: 0.0316 (0.0001 -- 0.6182)  max mem: 16413
Val: Total time: 0:00:08 (0.2972 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.525
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [193]  [  0/160]  eta: 0:22:27  lr: 0.000000  min_lr: 0.000000  loss: 1.6954 (1.6954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7580 (4.7580)  time: 8.4244 (8.4244 -- 8.4244)  data: 5.5453 (5.5453 -- 5.5453)  max mem: 16413
Epoch: [193]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.6650 (1.6327)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3920 (7.1706)  time: 0.8244 (0.5292 -- 2.9616)  data: 0.1224 (0.0003 -- 1.6289)  max mem: 16413
Epoch: [193]  [ 40/160]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 1.6889 (1.6415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3098 (7.0893)  time: 0.7991 (0.5176 -- 2.7401)  data: 0.2054 (0.0002 -- 2.2068)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6805 (1.6424)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4790 (7.0424)  time: 0.9086 (0.5233 -- 2.5883)  data: 0.1575 (0.0006 -- 1.6247)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.5247 (1.6228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7320 (7.1980)  time: 0.8699 (0.5315 -- 2.7748)  data: 0.0740 (0.0004 -- 1.3590)  max mem: 16413
[2023-09-04 07:55:06,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:55:06,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:55:06,747] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:55:06,747] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [193]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6402 (1.6207)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4592 (7.3215)  time: 0.9361 (0.5233 -- 3.1615)  data: 0.3236 (0.0003 -- 2.6046)  max mem: 16413
[2023-09-04 07:55:25,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=176, lr=[4.212080066010446e-08, 4.212080066010446e-08, 4.68008896223383e-08, 4.68008896223383e-08, 5.200098846926476e-08, 5.200098846926476e-08, 5.777887607696085e-08, 5.777887607696085e-08, 6.419875119662316e-08, 6.419875119662316e-08, 7.133194577402574e-08, 7.133194577402574e-08, 7.925771752669526e-08, 7.925771752669526e-08, 8.806413058521695e-08, 8.806413058521695e-08, 9.784903398357439e-08, 9.784903398357439e-08, 1.087211488706382e-07, 1.087211488706382e-07, 1.2080127652293135e-07, 1.2080127652293135e-07, 1.342236405810348e-07, 1.342236405810348e-07, 1.4913737842337202e-07, 1.4913737842337202e-07, 1.6570819824819113e-07, 1.6570819824819113e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 07:55:25,084] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=18.319455040079703, CurrSamplesPerSec=22.558015855977633, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5949 (1.6210)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2196 (7.2104)  time: 0.7973 (0.5294 -- 4.2889)  data: 0.2459 (0.0003 -- 3.7486)  max mem: 16413
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6950 (1.6237)  loss_scale: 32768.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9712 (7.1326)  time: 0.8731 (0.5271 -- 3.1630)  data: 0.3267 (0.0006 -- 2.6400)  max mem: 16413
[2023-09-04 07:55:44,737] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31023
[2023-09-04 07:55:44,737] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31023
[2023-09-04 07:55:44,737] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:55:44,737] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:55:44,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6062 (1.6340)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1635 (7.2386)  time: 0.6786 (0.4980 -- 2.7615)  data: 0.1244 (0.0002 -- 2.2200)  max mem: 16413
Epoch: [193] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6062 (1.6226)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1635 (7.2386)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2743 (2.2743 -- 2.2743)  data: 2.0499 (2.0499 -- 2.0499)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4084 (0.4758)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4004 (0.1959 -- 2.2743)  data: 0.1877 (0.0006 -- 2.0499)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3801 (0.4658)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2223 (0.1701 -- 0.5219)  data: 0.0157 (0.0001 -- 0.2970)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4489 (0.5238)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2065 (0.1329 -- 0.5219)  data: 0.0153 (0.0001 -- 0.2970)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [194]  [  0/160]  eta: 0:24:11  lr: 0.000000  min_lr: 0.000000  loss: 1.8803 (1.8803)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8879 (8.8879)  time: 9.0740 (9.0740 -- 9.0740)  data: 6.1473 (6.1473 -- 6.1473)  max mem: 16413
Epoch: [194]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6355 (1.6910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2792 (8.2309)  time: 0.7505 (0.5220 -- 3.1202)  data: 0.0014 (0.0005 -- 0.0026)  max mem: 16413
Epoch: [194]  [ 40/160]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 1.6761 (1.6534)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8370 (7.6087)  time: 1.0270 (0.5267 -- 3.9382)  data: 0.0014 (0.0006 -- 0.0027)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6511 (1.6424)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7869 (7.5964)  time: 0.7800 (0.5143 -- 3.3572)  data: 0.0750 (0.0004 -- 1.4759)  max mem: 16413
Epoch: [194]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6937 (1.6483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4463 (7.4134)  time: 0.9379 (0.4989 -- 4.7554)  data: 0.4039 (0.0001 -- 4.2382)  max mem: 16413
Epoch: [194]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.6442 (1.6639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6528 (7.5325)  time: 0.8649 (0.5245 -- 3.0345)  data: 0.3230 (0.0005 -- 2.4884)  max mem: 16413
[2023-09-04 07:57:47,392] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:57:47,392] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:57:47,393] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 07:57:47,393] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 07:57:52,747] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31157
[2023-09-04 07:57:52,747] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31157
[2023-09-04 07:57:52,788] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:57:52,788] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 07:57:52,788] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [194]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6139 (1.6679)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7656 (7.5177)  time: 0.7114 (0.5299 -- 3.1520)  data: 0.1634 (0.0004 -- 2.6287)  max mem: 16413
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5606 (1.6535)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4567 (7.4828)  time: 1.0064 (0.5200 -- 4.0848)  data: 0.4609 (0.0004 -- 3.5384)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5454 (1.6457)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5853 (7.4869)  time: 0.5944 (0.4966 -- 1.7084)  data: 0.0714 (0.0001 -- 1.2039)  max mem: 16413
Epoch: [194] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5454 (1.6515)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5853 (7.4869)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2989 (2.2989 -- 2.2989)  data: 2.0780 (2.0780 -- 2.0780)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4073 (0.4756)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4034 (0.1999 -- 2.2989)  data: 0.1898 (0.0006 -- 2.0780)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3782 (0.4658)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2226 (0.1686 -- 0.3757)  data: 0.0197 (0.0001 -- 0.1954)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4487 (0.5239)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2076 (0.1339 -- 0.3757)  data: 0.0194 (0.0001 -- 0.1954)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [195]  [  0/160]  eta: 0:21:46  lr: 0.000000  min_lr: 0.000000  loss: 1.2409 (1.2409)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3177 (6.3177)  time: 8.1654 (8.1654 -- 8.1654)  data: 6.0906 (6.0906 -- 6.0906)  max mem: 16413
[2023-09-04 07:58:49,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31209
[2023-09-04 07:58:49,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31209
[2023-09-04 07:58:49,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:58:49,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 07:58:49,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [195]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.4256 (1.4739)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4779 (6.9016)  time: 0.8060 (0.5229 -- 3.3596)  data: 0.1772 (0.0004 -- 2.3631)  max mem: 16413
Epoch: [195]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.7668 (1.5983)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3629 (7.0585)  time: 0.9473 (0.5075 -- 4.5219)  data: 0.4069 (0.0004 -- 4.0100)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6590 (1.6178)  loss_scale: 8192.0000 (9400.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1646 (7.0050)  time: 0.7792 (0.5234 -- 3.3721)  data: 0.0642 (0.0004 -- 1.2577)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.6947 (1.6396)  loss_scale: 8192.0000 (9102.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5294 (6.9809)  time: 0.9554 (0.5245 -- 3.5122)  data: 0.0015 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [195]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5054 (1.6242)  loss_scale: 8192.0000 (8921.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8385 (8.0264)  time: 0.8502 (0.5216 -- 5.6751)  data: 0.0013 (0.0003 -- 0.0074)  max mem: 16413
Epoch: [195]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5829 (1.6302)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7347 (7.8742)  time: 0.8342 (0.5318 -- 2.4552)  data: 0.0017 (0.0002 -- 0.0037)  max mem: 16413
[2023-09-04 08:00:41,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:00:41,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:00:41,577] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 08:00:41,577] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8516 (1.6555)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0708 (7.7250)  time: 0.8628 (0.5240 -- 3.0467)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5263 (1.6388)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1000 (7.5685)  time: 0.6413 (0.4962 -- 1.8234)  data: 0.0011 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [195] Total time: 0:02:21 (0.8824 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5263 (1.6324)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1000 (7.5685)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2446 (2.2446 -- 2.2446)  data: 2.0119 (2.0119 -- 2.0119)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4057 (0.4753)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4053 (0.1987 -- 2.2446)  data: 0.1894 (0.0005 -- 2.0119)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3796 (0.4657)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2282 (0.1776 -- 0.5528)  data: 0.0203 (0.0001 -- 0.3320)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4488 (0.5237)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2141 (0.1331 -- 0.5528)  data: 0.0196 (0.0001 -- 0.3320)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [196]  [  0/160]  eta: 0:16:10  lr: 0.000000  min_lr: 0.000000  loss: 1.3465 (1.3465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9336 (9.9336)  time: 6.0684 (6.0684 -- 6.0684)  data: 5.5212 (5.5212 -- 5.5212)  max mem: 16413
Epoch: [196]  [ 20/160]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 1.5697 (1.5432)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8423 (8.7434)  time: 0.9952 (0.5271 -- 3.2203)  data: 0.4076 (0.0004 -- 2.6949)  max mem: 16413
Epoch: [196]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.6318 (1.5810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7925 (8.1353)  time: 0.8552 (0.5270 -- 2.4196)  data: 0.2885 (0.0003 -- 1.8971)  max mem: 16413
Epoch: [196]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7045 (1.6314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8990 (7.5863)  time: 0.9167 (0.5298 -- 3.3986)  data: 0.3730 (0.0004 -- 2.8716)  max mem: 16413
Epoch: [196]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6809 (1.6503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8900 (7.7955)  time: 0.9167 (0.5322 -- 4.1163)  data: 0.3687 (0.0005 -- 3.5772)  max mem: 16413
Epoch: [196]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5380 (1.6394)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6538 (7.8835)  time: 0.7932 (0.5166 -- 2.7034)  data: 0.2497 (0.0003 -- 2.1762)  max mem: 16413
[2023-09-04 08:02:44,460] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:02:44,460] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 08:02:44,462] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:02:44,462] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 08:02:51,430] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31474
[2023-09-04 08:02:51,430] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31474
[2023-09-04 08:02:51,431] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 08:02:51,431] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 08:02:51,431] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [196]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6701 (1.6370)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2150 (7.8019)  time: 0.8153 (0.5315 -- 2.8636)  data: 0.1896 (0.0004 -- 1.4410)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6765 (1.6375)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9202 (7.7291)  time: 0.9459 (0.5238 -- 4.3001)  data: 0.4033 (0.0003 -- 3.8031)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6124 (1.6352)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5353 (7.7713)  time: 0.7058 (0.4955 -- 2.6777)  data: 0.1804 (0.0002 -- 2.1384)  max mem: 16413
Epoch: [196] Total time: 0:02:24 (0.9027 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6124 (1.6303)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5353 (7.7713)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1302 (0.1302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2645 (2.2645 -- 2.2645)  data: 2.0515 (2.0515 -- 2.0515)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4071 (0.4753)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4015 (0.1897 -- 2.2645)  data: 0.1936 (0.0008 -- 2.0515)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3798 (0.4659)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2241 (0.1693 -- 0.5638)  data: 0.0235 (0.0001 -- 0.3766)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4489 (0.5239)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2092 (0.1319 -- 0.5638)  data: 0.0230 (0.0001 -- 0.3766)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [197]  [  0/160]  eta: 0:22:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6962 (1.6962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1116 (5.1116)  time: 8.3380 (8.3380 -- 8.3380)  data: 7.8224 (7.8224 -- 7.8224)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.6934 (1.6578)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9450 (7.0080)  time: 0.8297 (0.5238 -- 4.7137)  data: 0.0575 (0.0002 -- 0.5665)  max mem: 16413
Epoch: [197]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.4478 (1.6300)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2481 (7.1110)  time: 0.8711 (0.5338 -- 3.7214)  data: 0.0018 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [197]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7478 (1.6774)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9286 (7.2034)  time: 0.8180 (0.5277 -- 2.7016)  data: 0.0662 (0.0005 -- 0.6903)  max mem: 16413
Epoch: [197]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.5525 (1.6499)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2798 (7.2560)  time: 0.8552 (0.5310 -- 2.7460)  data: 0.1459 (0.0003 -- 0.9720)  max mem: 16413
[2023-09-04 08:04:53,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:04:53,589] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 08:04:53,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:04:53,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [197]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.3374 (1.6133)  loss_scale: 32768.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9128 (7.1894)  time: 0.9384 (0.5204 -- 4.2615)  data: 0.0642 (0.0003 -- 0.7690)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6855 (1.6319)  loss_scale: 32768.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2491 (7.1180)  time: 0.8248 (0.5161 -- 4.2457)  data: 0.0163 (0.0005 -- 0.3015)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5698 (1.6299)  loss_scale: 32768.0000 (23123.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2667 (7.1802)  time: 0.9198 (0.5063 -- 4.2096)  data: 0.0014 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7224 (1.6365)  loss_scale: 32768.0000 (24268.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8093 (7.2149)  time: 0.6380 (0.4939 -- 1.7300)  data: 0.0008 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [197] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7224 (1.6021)  loss_scale: 32768.0000 (24268.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8093 (7.2149)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2444 (2.2444 -- 2.2444)  data: 2.0257 (2.0257 -- 2.0257)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4093 (0.4757)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.3974 (0.1958 -- 2.2444)  data: 0.1853 (0.0005 -- 2.0257)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3808 (0.4660)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2198 (0.1698 -- 0.3942)  data: 0.0130 (0.0001 -- 0.1600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4486 (0.5240)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2055 (0.1332 -- 0.3942)  data: 0.0126 (0.0001 -- 0.1600)  max mem: 16413
Val: Total time: 0:00:07 (0.2825 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [198]  [  0/160]  eta: 0:17:50  lr: 0.000000  min_lr: 0.000000  loss: 1.4775 (1.4775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9335 (6.9335)  time: 6.6904 (6.6904 -- 6.6904)  data: 5.9741 (5.9741 -- 5.9741)  max mem: 16413
Epoch: [198]  [ 20/160]  eta: 0:02:57  lr: 0.000000  min_lr: 0.000000  loss: 1.7469 (1.6534)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9173 (7.8400)  time: 0.9947 (0.5183 -- 4.6405)  data: 0.0412 (0.0005 -- 0.4338)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.6774 (1.7083)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0845 (7.4543)  time: 0.8113 (0.5222 -- 2.6581)  data: 0.1624 (0.0001 -- 2.1183)  max mem: 16413
[2023-09-04 08:06:54,127] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31728
[2023-09-04 08:06:54,127] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31728
[2023-09-04 08:06:54,128] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 08:06:54,128] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 08:06:54,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [ 60/160]  eta: 0:01:42  lr: 0.000000  min_lr: 0.000000  loss: 1.5118 (1.6286)  loss_scale: 16384.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4339 (7.3155)  time: 0.9844 (0.5119 -- 4.7628)  data: 0.4518 (0.0004 -- 4.2577)  max mem: 16413
Epoch: [198]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7729 (1.6351)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6097 (7.3359)  time: 0.7207 (0.5072 -- 2.4667)  data: 0.1780 (0.0004 -- 1.9476)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.7581 (1.6476)  loss_scale: 16384.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9590 (7.2882)  time: 0.9901 (0.5120 -- 3.9652)  data: 0.4463 (0.0004 -- 3.4393)  max mem: 16413
Epoch: [198]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7206 (1.6636)  loss_scale: 16384.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3920 (7.3363)  time: 0.8285 (0.5203 -- 5.2193)  data: 0.2910 (0.0004 -- 4.7170)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5501 (1.6524)  loss_scale: 16384.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4711 (7.3414)  time: 1.0009 (0.5190 -- 3.9235)  data: 0.4592 (0.0003 -- 3.4062)  max mem: 16413
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6139 (1.6471)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6267 (7.4503)  time: 0.6574 (0.4961 -- 3.2818)  data: 0.1393 (0.0001 -- 2.7721)  max mem: 16413
Epoch: [198] Total time: 0:02:25 (0.9121 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6139 (1.6383)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6267 (7.4503)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1297 (0.1297)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3100 (2.3100 -- 2.3100)  data: 2.0420 (2.0420 -- 2.0420)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4098 (0.4755)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4051 (0.2009 -- 2.3100)  data: 0.1868 (0.0007 -- 2.0420)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3809 (0.4659)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2225 (0.1700 -- 0.5862)  data: 0.0205 (0.0001 -- 0.3937)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4490 (0.5239)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2077 (0.1333 -- 0.5862)  data: 0.0201 (0.0001 -- 0.3937)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Epoch: [199]  [  0/160]  eta: 0:17:48  lr: 0.000000  min_lr: 0.000000  loss: 1.2513 (1.2513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9909 (5.9909)  time: 6.6763 (6.6763 -- 6.6763)  data: 5.7176 (5.7176 -- 5.7176)  max mem: 16413
[2023-09-04 08:08:59,704] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:08:59,704] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 08:08:59,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 08:08:59,705] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [199]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 1.5662 (1.5526)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2190 (6.2051)  time: 0.8290 (0.5221 -- 3.8473)  data: 0.2411 (0.0003 -- 3.3158)  max mem: 16413
Epoch: [199]  [ 40/160]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 1.6974 (1.5976)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1614 (7.2535)  time: 0.8472 (0.5398 -- 2.7726)  data: 0.2237 (0.0004 -- 2.2442)  max mem: 16413
Epoch: [199]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 1.5808 (1.5814)  loss_scale: 32768.0000 (28201.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3403 (7.3398)  time: 0.8680 (0.5151 -- 3.4273)  data: 0.0549 (0.0002 -- 0.9338)  max mem: 16413
Epoch: [199]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8212 (1.6401)  loss_scale: 32768.0000 (29329.3827)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8190 (7.5232)  time: 0.9280 (0.5283 -- 2.1917)  data: 0.2460 (0.0003 -- 1.6362)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.5577 (1.6242)  loss_scale: 32768.0000 (30010.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7239 (7.3632)  time: 0.8844 (0.5199 -- 2.3912)  data: 0.1864 (0.0009 -- 1.8384)  max mem: 16413
[2023-09-04 08:10:28,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31959
[2023-09-04 08:10:28,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 08:10:28,683] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31959
[2023-09-04 08:10:28,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 08:10:28,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [199]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6133 (1.6298)  loss_scale: 32768.0000 (30195.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9770 (7.4571)  time: 0.8610 (0.5176 -- 1.9517)  data: 0.2485 (0.0001 -- 1.3676)  max mem: 16413
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.4906 (1.6134)  loss_scale: 16384.0000 (28236.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8097 (7.5259)  time: 0.9976 (0.5169 -- 4.3519)  data: 0.3789 (0.0004 -- 3.8460)  max mem: 16413
[2023-09-04 08:11:00,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=182, lr=[1.1915026241310033e-08, 1.1915026241310033e-08, 1.3238918045900038e-08, 1.3238918045900038e-08, 1.4709908939888929e-08, 1.4709908939888929e-08, 1.6344343266543253e-08, 1.6344343266543253e-08, 1.8160381407270283e-08, 1.8160381407270283e-08, 2.0178201563633648e-08, 2.0178201563633648e-08, 2.242022395959294e-08, 2.242022395959294e-08, 2.4911359955103263e-08, 2.4911359955103263e-08, 2.767928883900363e-08, 2.767928883900363e-08, 3.0754765376670695e-08, 3.0754765376670695e-08, 3.4171961529634105e-08, 3.4171961529634105e-08, 3.79688461440379e-08, 3.79688461440379e-08, 4.218760682670877e-08, 4.218760682670877e-08, 4.687511869634308e-08, 4.687511869634308e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 08:11:00,420] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=18.320733861945524, CurrSamplesPerSec=24.557378120985405, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5238 (1.6073)  loss_scale: 16384.0000 (26828.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7111 (7.5415)  time: 0.5873 (0.4972 -- 1.7259)  data: 0.0713 (0.0002 -- 1.2120)  max mem: 16413
Epoch: [199] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5238 (1.6121)  loss_scale: 16384.0000 (26828.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7111 (7.5415)
[2023-09-04 08:11:00,423] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-09-04 08:11:00,425] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-09-04 08:11:00,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-09-04 08:11:00,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-09-04 08:11:01,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-09-04 08:11:01,415] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1299 (0.1299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4853 (2.4853 -- 2.4853)  data: 2.2899 (2.2899 -- 2.2899)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4081 (0.4753)  acc1: 88.8889 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4452 (0.1918 -- 2.4853)  data: 0.2314 (0.0008 -- 2.2899)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3801 (0.4659)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2230 (0.1684 -- 0.4871)  data: 0.0208 (0.0001 -- 0.2361)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4489 (0.5238)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2063 (0.1332 -- 0.4871)  data: 0.0201 (0.0001 -- 0.2361)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 87.344 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.76%
Test:  [   0/8869]  eta: 5:24:37  loss: 1.5419 (1.5419)  acc1: 66.6667 (66.6667)  acc5: 83.3333 (83.3333)  time: 2.1961 (2.1961 -- 2.1961)  data: 1.8615 (1.8615 -- 1.8615)  max mem: 16413
Test:  [  10/8869]  eta: 1:02:59  loss: 4.0202 (3.3379)  acc1: 0.0000 (28.7879)  acc5: 66.6667 (62.1212)  time: 0.4266 (0.1424 -- 2.1961)  data: 0.2609 (0.0005 -- 1.8615)  max mem: 16413
Test:  [  20/8869]  eta: 0:44:15  loss: 3.5020 (2.8328)  acc1: 16.6667 (35.7143)  acc5: 66.6667 (68.2540)  time: 0.2053 (0.1424 -- 0.6652)  data: 0.0512 (0.0003 -- 0.5158)  max mem: 16413
Test:  [  30/8869]  eta: 0:36:49  loss: 1.4923 (2.4101)  acc1: 50.0000 (43.0108)  acc5: 100.0000 (73.1183)  time: 0.1528 (0.1361 -- 0.1741)  data: 0.0011 (0.0003 -- 0.0060)  max mem: 16413
Test:  [  40/8869]  eta: 0:33:11  loss: 1.5438 (2.5669)  acc1: 33.3333 (40.2439)  acc5: 66.6667 (68.6992)  time: 0.1472 (0.1361 -- 0.1656)  data: 0.0011 (0.0005 -- 0.0034)  max mem: 16413
Test:  [  50/8869]  eta: 0:30:59  loss: 3.2049 (2.6695)  acc1: 16.6667 (37.5817)  acc5: 50.0000 (65.6863)  time: 0.1503 (0.1414 -- 0.1656)  data: 0.0014 (0.0005 -- 0.0057)  max mem: 16413
Test:  [  60/8869]  eta: 0:29:40  loss: 3.1730 (2.7124)  acc1: 16.6667 (36.8852)  acc5: 66.6667 (66.1202)  time: 0.1542 (0.1425 -- 0.1730)  data: 0.0015 (0.0005 -- 0.0057)  max mem: 16413
Test:  [  70/8869]  eta: 0:28:39  loss: 3.3079 (2.8320)  acc1: 0.0000 (33.3333)  acc5: 66.6667 (67.8404)  time: 0.1560 (0.1437 -- 0.1730)  data: 0.0018 (0.0004 -- 0.0128)  max mem: 16413
Test:  [  80/8869]  eta: 0:27:52  loss: 3.4046 (2.8808)  acc1: 0.0000 (30.6584)  acc5: 83.3333 (69.1358)  time: 0.1543 (0.1437 -- 0.1779)  data: 0.0016 (0.0004 -- 0.0128)  max mem: 16413
Test:  [  90/8869]  eta: 0:27:14  loss: 2.9026 (2.8597)  acc1: 16.6667 (31.1355)  acc5: 66.6667 (68.8645)  time: 0.1535 (0.1437 -- 0.1779)  data: 0.0011 (0.0006 -- 0.0022)  max mem: 16413
Test:  [ 100/8869]  eta: 0:26:47  loss: 2.9026 (2.8771)  acc1: 16.6667 (30.6931)  acc5: 66.6667 (68.1518)  time: 0.1550 (0.1437 -- 0.1832)  data: 0.0011 (0.0006 -- 0.0020)  max mem: 16413
Test:  [ 110/8869]  eta: 0:26:21  loss: 3.0307 (2.8580)  acc1: 16.6667 (30.9309)  acc5: 66.6667 (68.0180)  time: 0.1551 (0.1435 -- 0.1832)  data: 0.0014 (0.0005 -- 0.0085)  max mem: 16413
Test:  [ 120/8869]  eta: 0:25:59  loss: 3.0221 (2.8393)  acc1: 16.6667 (30.9917)  acc5: 83.3333 (69.5592)  time: 0.1525 (0.1435 -- 0.1630)  data: 0.0015 (0.0005 -- 0.0085)  max mem: 16413
Test:  [ 130/8869]  eta: 0:25:40  loss: 2.9145 (2.8486)  acc1: 16.6667 (30.7888)  acc5: 83.3333 (70.8651)  time: 0.1525 (0.1444 -- 0.1630)  data: 0.0012 (0.0006 -- 0.0023)  max mem: 16413
Test:  [ 140/8869]  eta: 0:25:24  loss: 1.6218 (2.7502)  acc1: 33.3333 (32.5059)  acc5: 100.0000 (72.5768)  time: 0.1529 (0.1447 -- 0.1615)  data: 0.0012 (0.0006 -- 0.0021)  max mem: 16413
Test:  [ 150/8869]  eta: 0:25:09  loss: 1.3513 (2.7307)  acc1: 50.0000 (32.5607)  acc5: 100.0000 (73.8411)  time: 0.1527 (0.1443 -- 0.1599)  data: 0.0013 (0.0006 -- 0.0021)  max mem: 16413
Test:  [ 160/8869]  eta: 0:24:57  loss: 3.4235 (2.7224)  acc1: 16.6667 (32.9193)  acc5: 100.0000 (74.8447)  time: 0.1530 (0.1423 -- 0.1711)  data: 0.0011 (0.0003 -- 0.0018)  max mem: 16413
Test:  [ 170/8869]  eta: 0:24:45  loss: 2.5441 (2.7052)  acc1: 33.3333 (33.5283)  acc5: 83.3333 (75.2437)  time: 0.1528 (0.1423 -- 0.1711)  data: 0.0010 (0.0003 -- 0.0019)  max mem: 16413
Test:  [ 180/8869]  eta: 0:24:37  loss: 1.1184 (2.6152)  acc1: 66.6667 (35.6354)  acc5: 100.0000 (76.5193)  time: 0.1549 (0.1425 -- 0.1705)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [ 190/8869]  eta: 0:24:26  loss: 1.0400 (2.6070)  acc1: 66.6667 (35.9511)  acc5: 100.0000 (77.1379)  time: 0.1538 (0.1433 -- 0.1705)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [ 200/8869]  eta: 0:24:18  loss: 1.9160 (2.5738)  acc1: 33.3333 (36.2355)  acc5: 100.0000 (77.6949)  time: 0.1518 (0.1394 -- 0.1709)  data: 0.0010 (0.0004 -- 0.0023)  max mem: 16413
Test:  [ 210/8869]  eta: 0:24:09  loss: 1.9160 (2.5463)  acc1: 50.0000 (36.8088)  acc5: 100.0000 (78.5940)  time: 0.1518 (0.1394 -- 0.1709)  data: 0.0010 (0.0004 -- 0.0023)  max mem: 16413
Test:  [ 220/8869]  eta: 0:24:04  loss: 2.5441 (2.5577)  acc1: 33.3333 (36.1991)  acc5: 100.0000 (79.0347)  time: 0.1542 (0.1442 -- 0.1755)  data: 0.0010 (0.0004 -- 0.0043)  max mem: 16413
Test:  [ 230/8869]  eta: 0:23:58  loss: 1.9868 (2.5373)  acc1: 33.3333 (36.7244)  acc5: 83.3333 (79.3651)  time: 0.1573 (0.1444 -- 0.1755)  data: 0.0011 (0.0004 -- 0.0043)  max mem: 16413
Test:  [ 240/8869]  eta: 0:23:52  loss: 1.8719 (2.5235)  acc1: 50.0000 (36.9986)  acc5: 83.3333 (79.6681)  time: 0.1554 (0.1436 -- 0.1709)  data: 0.0010 (0.0005 -- 0.0027)  max mem: 16413
Test:  [ 250/8869]  eta: 0:23:48  loss: 2.4468 (2.5404)  acc1: 33.3333 (36.7198)  acc5: 83.3333 (78.8845)  time: 0.1574 (0.1436 -- 0.1844)  data: 0.0019 (0.0007 -- 0.0152)  max mem: 16413
Test:  [ 260/8869]  eta: 0:23:43  loss: 3.0109 (2.5525)  acc1: 16.6667 (36.6539)  acc5: 66.6667 (78.6079)  time: 0.1568 (0.1361 -- 0.1844)  data: 0.0018 (0.0006 -- 0.0152)  max mem: 16413
Test:  [ 270/8869]  eta: 0:23:35  loss: 1.4838 (2.5375)  acc1: 33.3333 (36.9619)  acc5: 83.3333 (78.7823)  time: 0.1504 (0.1361 -- 0.1671)  data: 0.0009 (0.0004 -- 0.0020)  max mem: 16413
Test:  [ 280/8869]  eta: 0:23:29  loss: 2.0669 (2.5448)  acc1: 33.3333 (36.7734)  acc5: 100.0000 (78.7663)  time: 0.1482 (0.1322 -- 0.1671)  data: 0.0008 (0.0004 -- 0.0012)  max mem: 16413
Test:  [ 290/8869]  eta: 0:23:22  loss: 2.8540 (2.5665)  acc1: 16.6667 (36.2543)  acc5: 83.3333 (78.6942)  time: 0.1467 (0.1322 -- 0.1625)  data: 0.0009 (0.0004 -- 0.0029)  max mem: 16413
Test:  [ 300/8869]  eta: 0:23:16  loss: 2.7118 (2.5470)  acc1: 33.3333 (36.7110)  acc5: 83.3333 (79.1805)  time: 0.1467 (0.1327 -- 0.1600)  data: 0.0009 (0.0004 -- 0.0029)  max mem: 16413
Test:  [ 310/8869]  eta: 0:23:10  loss: 1.9747 (2.5529)  acc1: 33.3333 (36.5488)  acc5: 83.3333 (79.0461)  time: 0.1491 (0.1429 -- 0.1600)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [ 320/8869]  eta: 0:23:06  loss: 3.0640 (2.5480)  acc1: 33.3333 (36.7082)  acc5: 66.6667 (78.9720)  time: 0.1516 (0.1417 -- 0.1652)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [ 330/8869]  eta: 0:23:02  loss: 1.6749 (2.5394)  acc1: 33.3333 (36.6566)  acc5: 100.0000 (79.0534)  time: 0.1530 (0.1417 -- 0.1693)  data: 0.0011 (0.0006 -- 0.0022)  max mem: 16413
Test:  [ 340/8869]  eta: 0:22:59  loss: 3.0500 (2.5704)  acc1: 0.0000 (35.8260)  acc5: 100.0000 (79.1789)  time: 0.1532 (0.1466 -- 0.1693)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
Test:  [ 350/8869]  eta: 0:22:54  loss: 3.1704 (2.5854)  acc1: 0.0000 (35.3751)  acc5: 83.3333 (79.2498)  time: 0.1523 (0.1433 -- 0.1669)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
Test:  [ 360/8869]  eta: 0:22:52  loss: 3.0162 (2.6118)  acc1: 0.0000 (34.8107)  acc5: 66.6667 (78.8089)  time: 0.1542 (0.1433 -- 0.1694)  data: 0.0011 (0.0006 -- 0.0020)  max mem: 16413
Test:  [ 370/8869]  eta: 0:22:49  loss: 3.4694 (2.6329)  acc1: 0.0000 (34.4564)  acc5: 50.0000 (78.3019)  time: 0.1562 (0.1408 -- 0.1694)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [ 380/8869]  eta: 0:22:46  loss: 3.7215 (2.6475)  acc1: 0.0000 (34.3395)  acc5: 66.6667 (78.0840)  time: 0.1553 (0.1408 -- 0.1715)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [ 390/8869]  eta: 0:22:42  loss: 4.3639 (2.6941)  acc1: 0.0000 (33.5465)  acc5: 50.0000 (76.9821)  time: 0.1536 (0.1428 -- 0.1715)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [ 400/8869]  eta: 0:22:40  loss: 4.5459 (2.7150)  acc1: 0.0000 (33.1255)  acc5: 33.3333 (76.4339)  time: 0.1533 (0.1428 -- 0.1670)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [ 410/8869]  eta: 0:22:36  loss: 3.1431 (2.7239)  acc1: 0.0000 (33.0089)  acc5: 50.0000 (75.9124)  time: 0.1545 (0.1417 -- 0.1734)  data: 0.0013 (0.0003 -- 0.0044)  max mem: 16413
Test:  [ 420/8869]  eta: 0:22:33  loss: 2.8261 (2.7258)  acc1: 16.6667 (32.9375)  acc5: 66.6667 (75.9699)  time: 0.1533 (0.1417 -- 0.1734)  data: 0.0014 (0.0006 -- 0.0044)  max mem: 16413
Test:  [ 430/8869]  eta: 0:22:30  loss: 3.1663 (2.7453)  acc1: 0.0000 (32.3279)  acc5: 83.3333 (76.1794)  time: 0.1533 (0.1399 -- 0.1706)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [ 440/8869]  eta: 0:22:27  loss: 3.8913 (2.7736)  acc1: 0.0000 (31.7082)  acc5: 50.0000 (75.5480)  time: 0.1517 (0.1399 -- 0.1706)  data: 0.0011 (0.0004 -- 0.0020)  max mem: 16413
Test:  [ 450/8869]  eta: 0:22:24  loss: 4.1683 (2.8046)  acc1: 0.0000 (31.1899)  acc5: 33.3333 (74.6489)  time: 0.1521 (0.1417 -- 0.1627)  data: 0.0012 (0.0007 -- 0.0020)  max mem: 16413
Test:  [ 460/8869]  eta: 0:22:22  loss: 4.0065 (2.8062)  acc1: 0.0000 (31.0195)  acc5: 33.3333 (74.5481)  time: 0.1538 (0.1452 -- 0.1627)  data: 0.0011 (0.0006 -- 0.0018)  max mem: 16413
Test:  [ 470/8869]  eta: 0:22:19  loss: 2.7384 (2.8001)  acc1: 33.3333 (31.2102)  acc5: 83.3333 (74.4515)  time: 0.1532 (0.1470 -- 0.1613)  data: 0.0012 (0.0004 -- 0.0022)  max mem: 16413
Test:  [ 480/8869]  eta: 0:22:16  loss: 4.1312 (2.8356)  acc1: 0.0000 (30.5613)  acc5: 50.0000 (73.5967)  time: 0.1542 (0.1448 -- 0.1657)  data: 0.0012 (0.0004 -- 0.0022)  max mem: 16413
Test:  [ 490/8869]  eta: 0:22:14  loss: 4.5465 (2.8619)  acc1: 0.0000 (30.0068)  acc5: 33.3333 (72.8106)  time: 0.1549 (0.1443 -- 0.1745)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [ 500/8869]  eta: 0:22:11  loss: 4.4069 (2.8899)  acc1: 0.0000 (29.5077)  acc5: 33.3333 (71.8896)  time: 0.1513 (0.1409 -- 0.1745)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [ 510/8869]  eta: 0:22:08  loss: 4.3145 (2.9168)  acc1: 0.0000 (28.9302)  acc5: 16.6667 (71.0698)  time: 0.1498 (0.1372 -- 0.1663)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [ 520/8869]  eta: 0:22:05  loss: 4.3145 (2.9416)  acc1: 0.0000 (28.4389)  acc5: 16.6667 (70.2815)  time: 0.1527 (0.1372 -- 0.1733)  data: 0.0012 (0.0004 -- 0.0033)  max mem: 16413
Test:  [ 530/8869]  eta: 0:22:03  loss: 4.2920 (2.9534)  acc1: 0.0000 (28.2800)  acc5: 16.6667 (69.7740)  time: 0.1533 (0.1393 -- 0.1733)  data: 0.0012 (0.0004 -- 0.0033)  max mem: 16413
Test:  [ 540/8869]  eta: 0:22:00  loss: 4.2513 (2.9796)  acc1: 0.0000 (27.8189)  acc5: 33.3333 (68.9772)  time: 0.1519 (0.1426 -- 0.1702)  data: 0.0011 (0.0005 -- 0.0023)  max mem: 16413
Test:  [ 550/8869]  eta: 0:21:58  loss: 4.2620 (2.9997)  acc1: 0.0000 (27.3745)  acc5: 33.3333 (68.3908)  time: 0.1529 (0.1426 -- 0.1633)  data: 0.0011 (0.0005 -- 0.0021)  max mem: 16413
Test:  [ 560/8869]  eta: 0:21:55  loss: 4.2620 (3.0255)  acc1: 0.0000 (26.8865)  acc5: 16.6667 (67.4985)  time: 0.1537 (0.1433 -- 0.1771)  data: 0.0011 (0.0004 -- 0.0029)  max mem: 16413
Test:  [ 570/8869]  eta: 0:21:53  loss: 4.3682 (3.0485)  acc1: 0.0000 (26.4448)  acc5: 16.6667 (66.7542)  time: 0.1522 (0.1433 -- 0.1771)  data: 0.0011 (0.0004 -- 0.0029)  max mem: 16413
Test:  [ 580/8869]  eta: 0:21:51  loss: 4.3682 (3.0672)  acc1: 0.0000 (26.1044)  acc5: 16.6667 (66.0069)  time: 0.1534 (0.1438 -- 0.1740)  data: 0.0011 (0.0004 -- 0.0020)  max mem: 16413
Test:  [ 590/8869]  eta: 0:21:49  loss: 4.2275 (3.0793)  acc1: 0.0000 (25.8883)  acc5: 33.3333 (65.8488)  time: 0.1542 (0.1450 -- 0.1740)  data: 0.0011 (0.0003 -- 0.0021)  max mem: 16413
Test:  [ 600/8869]  eta: 0:21:46  loss: 3.6508 (3.0843)  acc1: 0.0000 (25.9013)  acc5: 50.0000 (65.6961)  time: 0.1505 (0.1412 -- 0.1663)  data: 0.0009 (0.0003 -- 0.0021)  max mem: 16413
Test:  [ 610/8869]  eta: 0:21:43  loss: 3.2511 (3.0751)  acc1: 16.6667 (26.1593)  acc5: 83.3333 (65.8483)  time: 0.1504 (0.1412 -- 0.1692)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Test:  [ 620/8869]  eta: 0:21:41  loss: 1.0547 (3.0461)  acc1: 50.0000 (26.8116)  acc5: 100.0000 (66.3178)  time: 0.1536 (0.1437 -- 0.1776)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Test:  [ 630/8869]  eta: 0:21:39  loss: 1.7551 (3.0427)  acc1: 50.0000 (26.9150)  acc5: 83.3333 (66.0856)  time: 0.1534 (0.1443 -- 0.1776)  data: 0.0012 (0.0006 -- 0.0027)  max mem: 16413
Test:  [ 640/8869]  eta: 0:21:37  loss: 3.6820 (3.0506)  acc1: 0.0000 (26.7811)  acc5: 33.3333 (65.6266)  time: 0.1532 (0.1443 -- 0.1700)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [ 650/8869]  eta: 0:21:34  loss: 2.9028 (3.0425)  acc1: 0.0000 (26.9073)  acc5: 66.6667 (65.8730)  time: 0.1522 (0.1444 -- 0.1700)  data: 0.0010 (0.0005 -- 0.0024)  max mem: 16413
Test:  [ 660/8869]  eta: 0:21:32  loss: 2.8057 (3.0510)  acc1: 16.6667 (26.7524)  acc5: 83.3333 (66.1120)  time: 0.1521 (0.1444 -- 0.1667)  data: 0.0011 (0.0006 -- 0.0024)  max mem: 16413
Test:  [ 670/8869]  eta: 0:21:30  loss: 2.8107 (3.0500)  acc1: 0.0000 (26.5524)  acc5: 83.3333 (66.3934)  time: 0.1525 (0.1433 -- 0.1667)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [ 680/8869]  eta: 0:21:28  loss: 2.8107 (3.0468)  acc1: 16.6667 (26.5786)  acc5: 83.3333 (66.3485)  time: 0.1535 (0.1433 -- 0.1844)  data: 0.0018 (0.0008 -- 0.0136)  max mem: 16413
Test:  [ 690/8869]  eta: 0:21:26  loss: 2.7120 (3.0432)  acc1: 16.6667 (26.6040)  acc5: 50.0000 (66.2325)  time: 0.1546 (0.1402 -- 0.1844)  data: 0.0017 (0.0004 -- 0.0136)  max mem: 16413
Test:  [ 700/8869]  eta: 0:21:24  loss: 2.7120 (3.0413)  acc1: 16.6667 (26.6762)  acc5: 50.0000 (66.2625)  time: 0.1542 (0.1402 -- 0.1676)  data: 0.0014 (0.0004 -- 0.0068)  max mem: 16413
Test:  [ 710/8869]  eta: 0:21:22  loss: 2.8642 (3.0362)  acc1: 16.6667 (26.6760)  acc5: 83.3333 (66.5260)  time: 0.1523 (0.1407 -- 0.1670)  data: 0.0015 (0.0007 -- 0.0068)  max mem: 16413
Test:  [ 720/8869]  eta: 0:21:20  loss: 3.1110 (3.0371)  acc1: 0.0000 (26.5603)  acc5: 83.3333 (66.8516)  time: 0.1516 (0.1407 -- 0.1653)  data: 0.0010 (0.0005 -- 0.0026)  max mem: 16413
Test:  [ 730/8869]  eta: 0:21:18  loss: 1.6068 (3.0143)  acc1: 33.3333 (26.9950)  acc5: 100.0000 (67.2367)  time: 0.1526 (0.1424 -- 0.1646)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [ 740/8869]  eta: 0:21:16  loss: 1.6068 (3.0085)  acc1: 50.0000 (27.0355)  acc5: 100.0000 (67.4539)  time: 0.1513 (0.1424 -- 0.1636)  data: 0.0011 (0.0006 -- 0.0014)  max mem: 16413
Test:  [ 750/8869]  eta: 0:21:13  loss: 2.8320 (3.0008)  acc1: 16.6667 (27.2082)  acc5: 100.0000 (67.7541)  time: 0.1517 (0.1423 -- 0.1678)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [ 760/8869]  eta: 0:21:12  loss: 2.1340 (2.9923)  acc1: 33.3333 (27.4201)  acc5: 83.3333 (67.9588)  time: 0.1556 (0.1423 -- 0.1800)  data: 0.0009 (0.0005 -- 0.0015)  max mem: 16413
Test:  [ 770/8869]  eta: 0:21:10  loss: 1.4267 (2.9688)  acc1: 50.0000 (27.9075)  acc5: 100.0000 (68.3528)  time: 0.1546 (0.1378 -- 0.1800)  data: 0.0011 (0.0005 -- 0.0021)  max mem: 16413
Test:  [ 780/8869]  eta: 0:21:08  loss: 1.3593 (2.9622)  acc1: 50.0000 (27.9770)  acc5: 100.0000 (68.6086)  time: 0.1506 (0.1378 -- 0.1660)  data: 0.0011 (0.0004 -- 0.0021)  max mem: 16413
Test:  [ 790/8869]  eta: 0:21:06  loss: 1.6669 (2.9465)  acc1: 50.0000 (28.2554)  acc5: 100.0000 (68.8580)  time: 0.1510 (0.1417 -- 0.1618)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [ 800/8869]  eta: 0:21:04  loss: 1.6669 (2.9381)  acc1: 50.0000 (28.3396)  acc5: 100.0000 (69.1635)  time: 0.1531 (0.1417 -- 0.1726)  data: 0.0008 (0.0003 -- 0.0014)  max mem: 16413
Test:  [ 810/8869]  eta: 0:21:02  loss: 2.4841 (2.9316)  acc1: 16.6667 (28.4423)  acc5: 100.0000 (69.4616)  time: 0.1533 (0.1431 -- 0.1726)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [ 820/8869]  eta: 0:21:00  loss: 2.4196 (2.9225)  acc1: 33.3333 (28.6845)  acc5: 100.0000 (69.7320)  time: 0.1524 (0.1444 -- 0.1652)  data: 0.0010 (0.0004 -- 0.0014)  max mem: 16413
Test:  [ 830/8869]  eta: 0:20:58  loss: 2.3234 (2.9123)  acc1: 50.0000 (28.9210)  acc5: 100.0000 (69.9358)  time: 0.1535 (0.1451 -- 0.1652)  data: 0.0012 (0.0006 -- 0.0031)  max mem: 16413
Test:  [ 840/8869]  eta: 0:20:56  loss: 2.5163 (2.9157)  acc1: 33.3333 (28.8347)  acc5: 83.3333 (69.7582)  time: 0.1530 (0.1428 -- 0.1722)  data: 0.0011 (0.0007 -- 0.0031)  max mem: 16413
Test:  [ 850/8869]  eta: 0:20:54  loss: 2.1831 (2.9108)  acc1: 33.3333 (29.0247)  acc5: 50.0000 (69.7611)  time: 0.1517 (0.1428 -- 0.1722)  data: 0.0010 (0.0007 -- 0.0018)  max mem: 16413
Test:  [ 860/8869]  eta: 0:20:52  loss: 1.6676 (2.9047)  acc1: 50.0000 (29.1715)  acc5: 100.0000 (69.9574)  time: 0.1503 (0.1405 -- 0.1606)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [ 870/8869]  eta: 0:20:50  loss: 1.5606 (2.9015)  acc1: 50.0000 (29.2576)  acc5: 100.0000 (70.1684)  time: 0.1538 (0.1405 -- 0.1636)  data: 0.0011 (0.0005 -- 0.0031)  max mem: 16413
Test:  [ 880/8869]  eta: 0:20:48  loss: 2.7977 (2.9024)  acc1: 16.6667 (29.2282)  acc5: 100.0000 (70.3178)  time: 0.1554 (0.1401 -- 0.1699)  data: 0.0011 (0.0005 -- 0.0031)  max mem: 16413
Test:  [ 890/8869]  eta: 0:20:46  loss: 2.6550 (2.8961)  acc1: 33.3333 (29.4426)  acc5: 83.3333 (70.4826)  time: 0.1509 (0.1401 -- 0.1699)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [ 900/8869]  eta: 0:20:44  loss: 2.6550 (2.8962)  acc1: 33.3333 (29.3748)  acc5: 83.3333 (70.5142)  time: 0.1498 (0.1435 -- 0.1613)  data: 0.0010 (0.0003 -- 0.0019)  max mem: 16413
Test:  [ 910/8869]  eta: 0:20:42  loss: 2.6964 (2.8903)  acc1: 33.3333 (29.5646)  acc5: 83.3333 (70.6550)  time: 0.1503 (0.1367 -- 0.1699)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Test:  [ 920/8869]  eta: 0:20:40  loss: 2.1396 (2.8838)  acc1: 33.3333 (29.7141)  acc5: 83.3333 (70.8107)  time: 0.1511 (0.1367 -- 0.1699)  data: 0.0015 (0.0007 -- 0.0066)  max mem: 16413
Test:  [ 930/8869]  eta: 0:20:39  loss: 1.9769 (2.8837)  acc1: 16.6667 (29.7172)  acc5: 100.0000 (70.9810)  time: 0.1541 (0.1411 -- 0.1843)  data: 0.0016 (0.0004 -- 0.0066)  max mem: 16413
Test:  [ 940/8869]  eta: 0:20:37  loss: 3.3122 (2.8874)  acc1: 0.0000 (29.6139)  acc5: 100.0000 (71.0769)  time: 0.1539 (0.1411 -- 0.1843)  data: 0.0013 (0.0004 -- 0.0046)  max mem: 16413
Test:  [ 950/8869]  eta: 0:20:34  loss: 3.1816 (2.8911)  acc1: 0.0000 (29.5478)  acc5: 83.3333 (71.0130)  time: 0.1503 (0.1413 -- 0.1671)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Test:  [ 960/8869]  eta: 0:20:33  loss: 3.2316 (2.8976)  acc1: 16.6667 (29.4485)  acc5: 66.6667 (70.8984)  time: 0.1524 (0.1440 -- 0.1692)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [ 970/8869]  eta: 0:20:31  loss: 4.0091 (2.9063)  acc1: 0.0000 (29.2825)  acc5: 50.0000 (70.7003)  time: 0.1551 (0.1478 -- 0.1701)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [ 980/8869]  eta: 0:20:29  loss: 4.2653 (2.9166)  acc1: 0.0000 (29.1539)  acc5: 33.3333 (70.3874)  time: 0.1520 (0.1394 -- 0.1701)  data: 0.0010 (0.0007 -- 0.0016)  max mem: 16413
Test:  [ 990/8869]  eta: 0:20:27  loss: 4.5592 (2.9252)  acc1: 0.0000 (29.0111)  acc5: 33.3333 (70.2321)  time: 0.1516 (0.1394 -- 0.1688)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [1000/8869]  eta: 0:20:26  loss: 3.6543 (2.9249)  acc1: 16.6667 (29.0709)  acc5: 50.0000 (70.1132)  time: 0.1530 (0.1409 -- 0.1688)  data: 0.0008 (0.0003 -- 0.0014)  max mem: 16413
Test:  [1010/8869]  eta: 0:20:24  loss: 2.6318 (2.9252)  acc1: 16.6667 (29.0801)  acc5: 66.6667 (70.1121)  time: 0.1528 (0.1401 -- 0.1660)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [1020/8869]  eta: 0:20:22  loss: 3.5130 (2.9342)  acc1: 16.6667 (28.8932)  acc5: 83.3333 (70.1110)  time: 0.1528 (0.1390 -- 0.1691)  data: 0.0011 (0.0004 -- 0.0036)  max mem: 16413
Test:  [1030/8869]  eta: 0:20:20  loss: 4.0573 (2.9423)  acc1: 0.0000 (28.7423)  acc5: 66.6667 (69.9806)  time: 0.1544 (0.1390 -- 0.1762)  data: 0.0015 (0.0005 -- 0.0084)  max mem: 16413
Test:  [1040/8869]  eta: 0:20:19  loss: 4.2064 (2.9538)  acc1: 0.0000 (28.5783)  acc5: 50.0000 (69.5965)  time: 0.1548 (0.1393 -- 0.1762)  data: 0.0014 (0.0005 -- 0.0084)  max mem: 16413
Test:  [1050/8869]  eta: 0:20:17  loss: 4.0648 (2.9576)  acc1: 0.0000 (28.4808)  acc5: 33.3333 (69.5528)  time: 0.1538 (0.1390 -- 0.1697)  data: 0.0011 (0.0006 -- 0.0015)  max mem: 16413
Test:  [1060/8869]  eta: 0:20:15  loss: 2.7542 (2.9505)  acc1: 33.3333 (28.6836)  acc5: 83.3333 (69.6984)  time: 0.1523 (0.1390 -- 0.1697)  data: 0.0010 (0.0006 -- 0.0015)  max mem: 16413
Test:  [1070/8869]  eta: 0:20:13  loss: 4.0446 (2.9657)  acc1: 0.0000 (28.4314)  acc5: 50.0000 (69.2810)  time: 0.1523 (0.1416 -- 0.1615)  data: 0.0010 (0.0006 -- 0.0015)  max mem: 16413
Test:  [1080/8869]  eta: 0:20:12  loss: 4.5658 (2.9781)  acc1: 0.0000 (28.1838)  acc5: 16.6667 (68.9023)  time: 0.1526 (0.1391 -- 0.1758)  data: 0.0013 (0.0005 -- 0.0073)  max mem: 16413
Test:  [1090/8869]  eta: 0:20:10  loss: 4.4183 (2.9906)  acc1: 0.0000 (27.9866)  acc5: 16.6667 (68.4998)  time: 0.1518 (0.1391 -- 0.1758)  data: 0.0015 (0.0005 -- 0.0073)  max mem: 16413
Test:  [1100/8869]  eta: 0:20:08  loss: 4.5960 (3.0028)  acc1: 0.0000 (27.7626)  acc5: 16.6667 (68.1653)  time: 0.1526 (0.1392 -- 0.1703)  data: 0.0011 (0.0005 -- 0.0055)  max mem: 16413
Test:  [1110/8869]  eta: 0:20:06  loss: 4.4167 (3.0121)  acc1: 0.0000 (27.5428)  acc5: 33.3333 (67.8818)  time: 0.1533 (0.1456 -- 0.1703)  data: 0.0011 (0.0005 -- 0.0034)  max mem: 16413
Test:  [1120/8869]  eta: 0:20:05  loss: 4.0488 (3.0151)  acc1: 0.0000 (27.4903)  acc5: 33.3333 (67.7074)  time: 0.1544 (0.1446 -- 0.1632)  data: 0.0012 (0.0005 -- 0.0034)  max mem: 16413
Test:  [1130/8869]  eta: 0:20:03  loss: 4.4974 (3.0295)  acc1: 0.0000 (27.2473)  acc5: 16.6667 (67.2709)  time: 0.1520 (0.1437 -- 0.1632)  data: 0.0010 (0.0005 -- 0.0020)  max mem: 16413
Test:  [1140/8869]  eta: 0:20:01  loss: 4.3289 (3.0375)  acc1: 0.0000 (27.0961)  acc5: 16.6667 (67.0318)  time: 0.1498 (0.1407 -- 0.1657)  data: 0.0010 (0.0004 -- 0.0020)  max mem: 16413
Test:  [1150/8869]  eta: 0:19:59  loss: 4.2028 (3.0490)  acc1: 0.0000 (26.8752)  acc5: 16.6667 (66.6667)  time: 0.1538 (0.1407 -- 0.1724)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [1160/8869]  eta: 0:19:58  loss: 4.4111 (3.0580)  acc1: 0.0000 (26.6724)  acc5: 16.6667 (66.3221)  time: 0.1559 (0.1416 -- 0.1761)  data: 0.0010 (0.0005 -- 0.0023)  max mem: 16413
Test:  [1170/8869]  eta: 0:19:56  loss: 4.3135 (3.0680)  acc1: 0.0000 (26.4873)  acc5: 33.3333 (66.0120)  time: 0.1521 (0.1416 -- 0.1761)  data: 0.0010 (0.0006 -- 0.0023)  max mem: 16413
Test:  [1180/8869]  eta: 0:19:54  loss: 4.3167 (3.0741)  acc1: 0.0000 (26.3477)  acc5: 33.3333 (65.9046)  time: 0.1520 (0.1419 -- 0.1619)  data: 0.0011 (0.0006 -- 0.0034)  max mem: 16413
Test:  [1190/8869]  eta: 0:19:52  loss: 4.3210 (3.0808)  acc1: 0.0000 (26.2944)  acc5: 50.0000 (65.7571)  time: 0.1530 (0.1436 -- 0.1619)  data: 0.0012 (0.0005 -- 0.0034)  max mem: 16413
Test:  [1200/8869]  eta: 0:19:51  loss: 2.7779 (3.0722)  acc1: 16.6667 (26.5196)  acc5: 50.0000 (65.8063)  time: 0.1511 (0.1415 -- 0.1606)  data: 0.0011 (0.0005 -- 0.0019)  max mem: 16413
Test:  [1210/8869]  eta: 0:19:49  loss: 1.8177 (3.0594)  acc1: 50.0000 (26.7272)  acc5: 100.0000 (66.0336)  time: 0.1523 (0.1415 -- 0.1723)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [1220/8869]  eta: 0:19:47  loss: 1.8398 (3.0568)  acc1: 33.3333 (26.8086)  acc5: 100.0000 (65.9842)  time: 0.1518 (0.1402 -- 0.1723)  data: 0.0009 (0.0003 -- 0.0013)  max mem: 16413
Test:  [1230/8869]  eta: 0:19:45  loss: 2.9648 (3.0600)  acc1: 33.3333 (26.7804)  acc5: 50.0000 (65.8408)  time: 0.1499 (0.1402 -- 0.1625)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Test:  [1240/8869]  eta: 0:19:43  loss: 2.7085 (3.0550)  acc1: 33.3333 (26.8869)  acc5: 50.0000 (65.8877)  time: 0.1495 (0.1406 -- 0.1610)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
Test:  [1250/8869]  eta: 0:19:41  loss: 3.9660 (3.0625)  acc1: 0.0000 (26.7386)  acc5: 66.6667 (65.9073)  time: 0.1498 (0.1406 -- 0.1633)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
Test:  [1260/8869]  eta: 0:19:40  loss: 3.3846 (3.0625)  acc1: 0.0000 (26.6455)  acc5: 83.3333 (66.0587)  time: 0.1509 (0.1440 -- 0.1639)  data: 0.0012 (0.0006 -- 0.0029)  max mem: 16413
Test:  [1270/8869]  eta: 0:19:38  loss: 2.8088 (3.0605)  acc1: 16.6667 (26.6719)  acc5: 83.3333 (66.0897)  time: 0.1530 (0.1440 -- 0.1713)  data: 0.0010 (0.0006 -- 0.0016)  max mem: 16413
Test:  [1280/8869]  eta: 0:19:36  loss: 2.9054 (3.0608)  acc1: 16.6667 (26.6589)  acc5: 66.6667 (66.0291)  time: 0.1543 (0.1444 -- 0.1713)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Test:  [1290/8869]  eta: 0:19:35  loss: 2.8326 (3.0593)  acc1: 16.6667 (26.6589)  acc5: 50.0000 (65.9954)  time: 0.1518 (0.1371 -- 0.1668)  data: 0.0009 (0.0003 -- 0.0013)  max mem: 16413
Test:  [1300/8869]  eta: 0:19:33  loss: 2.3515 (3.0552)  acc1: 33.3333 (26.7358)  acc5: 83.3333 (66.1414)  time: 0.1515 (0.1371 -- 0.1674)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [1310/8869]  eta: 0:19:31  loss: 2.7708 (3.0565)  acc1: 33.3333 (26.6972)  acc5: 83.3333 (66.2726)  time: 0.1524 (0.1387 -- 0.1690)  data: 0.0012 (0.0004 -- 0.0074)  max mem: 16413
Test:  [1320/8869]  eta: 0:19:29  loss: 1.5495 (3.0436)  acc1: 50.0000 (26.9493)  acc5: 100.0000 (66.5153)  time: 0.1516 (0.1386 -- 0.1703)  data: 0.0012 (0.0005 -- 0.0074)  max mem: 16413
Test:  [1330/8869]  eta: 0:19:28  loss: 1.7218 (3.0414)  acc1: 50.0000 (26.9221)  acc5: 100.0000 (66.6792)  time: 0.1512 (0.1386 -- 0.1703)  data: 0.0009 (0.0006 -- 0.0017)  max mem: 16413
Test:  [1340/8869]  eta: 0:19:26  loss: 2.4186 (3.0343)  acc1: 33.3333 (27.0942)  acc5: 100.0000 (66.8904)  time: 0.1516 (0.1387 -- 0.1640)  data: 0.0010 (0.0005 -- 0.0017)  max mem: 16413
Test:  [1350/8869]  eta: 0:19:24  loss: 2.2756 (3.0303)  acc1: 50.0000 (27.2021)  acc5: 83.3333 (67.0121)  time: 0.1532 (0.1387 -- 0.1684)  data: 0.0009 (0.0005 -- 0.0015)  max mem: 16413
Test:  [1360/8869]  eta: 0:19:23  loss: 1.2534 (3.0182)  acc1: 50.0000 (27.4920)  acc5: 100.0000 (67.1932)  time: 0.1524 (0.1441 -- 0.1684)  data: 0.0010 (0.0007 -- 0.0013)  max mem: 16413
Test:  [1370/8869]  eta: 0:19:21  loss: 1.1877 (3.0154)  acc1: 50.0000 (27.5468)  acc5: 100.0000 (67.3474)  time: 0.1522 (0.1421 -- 0.1715)  data: 0.0011 (0.0004 -- 0.0016)  max mem: 16413
Test:  [1380/8869]  eta: 0:19:19  loss: 1.0751 (3.0034)  acc1: 50.0000 (27.7818)  acc5: 100.0000 (67.5115)  time: 0.1539 (0.1421 -- 0.1715)  data: 0.0011 (0.0004 -- 0.0031)  max mem: 16413
Test:  [1390/8869]  eta: 0:19:18  loss: 1.2024 (2.9997)  acc1: 50.0000 (27.8217)  acc5: 100.0000 (67.7091)  time: 0.1527 (0.1406 -- 0.1731)  data: 0.0012 (0.0005 -- 0.0031)  max mem: 16413
Test:  [1400/8869]  eta: 0:19:16  loss: 2.5085 (2.9952)  acc1: 16.6667 (27.8729)  acc5: 100.0000 (67.9039)  time: 0.1527 (0.1406 -- 0.1731)  data: 0.0011 (0.0005 -- 0.0028)  max mem: 16413
Test:  [1410/8869]  eta: 0:19:14  loss: 2.5085 (2.9921)  acc1: 16.6667 (27.9235)  acc5: 83.3333 (67.9896)  time: 0.1532 (0.1347 -- 0.1667)  data: 0.0010 (0.0004 -- 0.0028)  max mem: 16413
Test:  [1420/8869]  eta: 0:19:13  loss: 2.3807 (2.9833)  acc1: 50.0000 (28.1375)  acc5: 83.3333 (68.1210)  time: 0.1529 (0.1347 -- 0.1718)  data: 0.0008 (0.0004 -- 0.0015)  max mem: 16413
Test:  [1430/8869]  eta: 0:19:11  loss: 2.2920 (2.9847)  acc1: 33.3333 (28.0690)  acc5: 83.3333 (68.0876)  time: 0.1520 (0.1394 -- 0.1718)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [1440/8869]  eta: 0:19:09  loss: 2.6983 (2.9822)  acc1: 33.3333 (28.1864)  acc5: 66.6667 (68.1240)  time: 0.1500 (0.1394 -- 0.1620)  data: 0.0012 (0.0004 -- 0.0066)  max mem: 16413
Test:  [1450/8869]  eta: 0:19:07  loss: 2.6309 (2.9790)  acc1: 33.3333 (28.2449)  acc5: 83.3333 (68.1944)  time: 0.1493 (0.1362 -- 0.1606)  data: 0.0012 (0.0004 -- 0.0066)  max mem: 16413
Test:  [1460/8869]  eta: 0:19:05  loss: 2.2514 (2.9739)  acc1: 33.3333 (28.3596)  acc5: 83.3333 (68.2980)  time: 0.1471 (0.1356 -- 0.1624)  data: 0.0009 (0.0004 -- 0.0029)  max mem: 16413
Test:  [1470/8869]  eta: 0:19:04  loss: 2.2107 (2.9755)  acc1: 16.6667 (28.2688)  acc5: 100.0000 (68.3775)  time: 0.1482 (0.1356 -- 0.1698)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [1480/8869]  eta: 0:19:02  loss: 2.7229 (2.9719)  acc1: 16.6667 (28.3705)  acc5: 83.3333 (68.4898)  time: 0.1520 (0.1421 -- 0.1698)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [1490/8869]  eta: 0:19:00  loss: 2.6845 (2.9707)  acc1: 33.3333 (28.3814)  acc5: 83.3333 (68.5334)  time: 0.1498 (0.1375 -- 0.1640)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [1500/8869]  eta: 0:18:58  loss: 2.6845 (2.9654)  acc1: 16.6667 (28.4699)  acc5: 83.3333 (68.6320)  time: 0.1476 (0.1368 -- 0.1640)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [1510/8869]  eta: 0:18:56  loss: 2.5343 (2.9627)  acc1: 33.3333 (28.5021)  acc5: 83.3333 (68.6742)  time: 0.1501 (0.1368 -- 0.1667)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [1520/8869]  eta: 0:18:55  loss: 2.5343 (2.9615)  acc1: 33.3333 (28.4900)  acc5: 83.3333 (68.7815)  time: 0.1507 (0.1377 -- 0.1667)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [1530/8869]  eta: 0:18:53  loss: 3.0552 (2.9624)  acc1: 0.0000 (28.4019)  acc5: 100.0000 (68.9419)  time: 0.1493 (0.1377 -- 0.1623)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [1540/8869]  eta: 0:18:51  loss: 3.1048 (2.9648)  acc1: 0.0000 (28.3474)  acc5: 83.3333 (68.9055)  time: 0.1467 (0.1350 -- 0.1615)  data: 0.0008 (0.0002 -- 0.0012)  max mem: 16413
Test:  [1550/8869]  eta: 0:18:49  loss: 4.0771 (2.9702)  acc1: 0.0000 (28.2721)  acc5: 50.0000 (68.8158)  time: 0.1459 (0.1350 -- 0.1696)  data: 0.0007 (0.0002 -- 0.0013)  max mem: 16413
Test:  [1560/8869]  eta: 0:18:47  loss: 4.0983 (2.9753)  acc1: 0.0000 (28.1764)  acc5: 50.0000 (68.7700)  time: 0.1501 (0.1391 -- 0.1696)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [1570/8869]  eta: 0:18:46  loss: 4.2907 (2.9784)  acc1: 0.0000 (28.1562)  acc5: 50.0000 (68.7036)  time: 0.1506 (0.1400 -- 0.1664)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [1580/8869]  eta: 0:18:44  loss: 4.5814 (2.9834)  acc1: 0.0000 (28.0624)  acc5: 50.0000 (68.5747)  time: 0.1510 (0.1444 -- 0.1656)  data: 0.0010 (0.0005 -- 0.0017)  max mem: 16413
Test:  [1590/8869]  eta: 0:18:42  loss: 3.9458 (2.9839)  acc1: 0.0000 (28.0641)  acc5: 33.3333 (68.4685)  time: 0.1530 (0.1446 -- 0.1666)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [1600/8869]  eta: 0:18:41  loss: 2.6024 (2.9818)  acc1: 16.6667 (28.0970)  acc5: 83.3333 (68.5405)  time: 0.1514 (0.1418 -- 0.1666)  data: 0.0009 (0.0005 -- 0.0019)  max mem: 16413
Test:  [1610/8869]  eta: 0:18:39  loss: 3.5542 (2.9883)  acc1: 0.0000 (27.9330)  acc5: 83.3333 (68.5806)  time: 0.1523 (0.1418 -- 0.1802)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [1620/8869]  eta: 0:18:38  loss: 3.9899 (2.9925)  acc1: 0.0000 (27.8121)  acc5: 66.6667 (68.5071)  time: 0.1550 (0.1401 -- 0.1802)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [1630/8869]  eta: 0:18:36  loss: 3.9899 (2.9990)  acc1: 0.0000 (27.7028)  acc5: 50.0000 (68.3017)  time: 0.1538 (0.1401 -- 0.1719)  data: 0.0011 (0.0005 -- 0.0017)  max mem: 16413
Test:  [1640/8869]  eta: 0:18:34  loss: 4.2041 (3.0010)  acc1: 0.0000 (27.6559)  acc5: 33.3333 (68.2511)  time: 0.1536 (0.1416 -- 0.1723)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [1650/8869]  eta: 0:18:33  loss: 3.2002 (2.9967)  acc1: 0.0000 (27.7307)  acc5: 66.6667 (68.3222)  time: 0.1539 (0.1416 -- 0.1723)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [1660/8869]  eta: 0:18:31  loss: 3.8249 (3.0049)  acc1: 0.0000 (27.5838)  acc5: 50.0000 (68.1316)  time: 0.1508 (0.1435 -- 0.1671)  data: 0.0008 (0.0005 -- 0.0018)  max mem: 16413
Test:  [1670/8869]  eta: 0:18:29  loss: 4.1471 (3.0112)  acc1: 0.0000 (27.4387)  acc5: 33.3333 (67.9633)  time: 0.1504 (0.1424 -- 0.1688)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [1680/8869]  eta: 0:18:28  loss: 4.1471 (3.0186)  acc1: 0.0000 (27.3052)  acc5: 33.3333 (67.7176)  time: 0.1515 (0.1424 -- 0.1688)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Test:  [1690/8869]  eta: 0:18:26  loss: 4.5602 (3.0269)  acc1: 0.0000 (27.1437)  acc5: 16.6667 (67.4453)  time: 0.1523 (0.1424 -- 0.1688)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Test:  [1700/8869]  eta: 0:18:24  loss: 4.2721 (3.0332)  acc1: 0.0000 (27.0037)  acc5: 16.6667 (67.2644)  time: 0.1519 (0.1420 -- 0.1692)  data: 0.0010 (0.0004 -- 0.0014)  max mem: 16413
Test:  [1710/8869]  eta: 0:18:23  loss: 4.1006 (3.0355)  acc1: 0.0000 (26.9628)  acc5: 33.3333 (67.1440)  time: 0.1531 (0.1420 -- 0.1692)  data: 0.0012 (0.0005 -- 0.0027)  max mem: 16413
Test:  [1720/8869]  eta: 0:18:21  loss: 4.4570 (3.0450)  acc1: 0.0000 (26.8255)  acc5: 16.6667 (66.8410)  time: 0.1530 (0.1453 -- 0.1668)  data: 0.0012 (0.0005 -- 0.0027)  max mem: 16413
Test:  [1730/8869]  eta: 0:18:20  loss: 4.4570 (3.0500)  acc1: 0.0000 (26.6898)  acc5: 16.6667 (66.7341)  time: 0.1519 (0.1419 -- 0.1765)  data: 0.0010 (0.0004 -- 0.0025)  max mem: 16413
Test:  [1740/8869]  eta: 0:18:18  loss: 4.2059 (3.0589)  acc1: 0.0000 (26.5365)  acc5: 16.6667 (66.4082)  time: 0.1497 (0.1359 -- 0.1765)  data: 0.0011 (0.0004 -- 0.0028)  max mem: 16413
Test:  [1750/8869]  eta: 0:18:16  loss: 4.3665 (3.0656)  acc1: 0.0000 (26.3944)  acc5: 16.6667 (66.1812)  time: 0.1476 (0.1359 -- 0.1609)  data: 0.0010 (0.0003 -- 0.0028)  max mem: 16413
Test:  [1760/8869]  eta: 0:18:15  loss: 4.2770 (3.0711)  acc1: 0.0000 (26.2824)  acc5: 16.6667 (65.9758)  time: 0.1521 (0.1425 -- 0.1740)  data: 0.0011 (0.0003 -- 0.0020)  max mem: 16413
Test:  [1770/8869]  eta: 0:18:13  loss: 4.0983 (3.0747)  acc1: 0.0000 (26.2093)  acc5: 33.3333 (65.8950)  time: 0.1515 (0.1346 -- 0.1740)  data: 0.0014 (0.0005 -- 0.0060)  max mem: 16413
Test:  [1780/8869]  eta: 0:18:11  loss: 4.2581 (3.0795)  acc1: 0.0000 (26.1651)  acc5: 33.3333 (65.7777)  time: 0.1526 (0.1346 -- 0.1738)  data: 0.0014 (0.0005 -- 0.0060)  max mem: 16413
Test:  [1790/8869]  eta: 0:18:10  loss: 4.0934 (3.0771)  acc1: 0.0000 (26.2423)  acc5: 50.0000 (65.7919)  time: 0.1567 (0.1443 -- 0.1845)  data: 0.0014 (0.0006 -- 0.0033)  max mem: 16413
Test:  [1800/8869]  eta: 0:18:08  loss: 1.4163 (3.0686)  acc1: 50.0000 (26.4205)  acc5: 100.0000 (65.9634)  time: 0.1523 (0.1432 -- 0.1845)  data: 0.0012 (0.0006 -- 0.0033)  max mem: 16413
Test:  [1810/8869]  eta: 0:18:07  loss: 1.4163 (3.0650)  acc1: 50.0000 (26.5323)  acc5: 100.0000 (65.9396)  time: 0.1512 (0.1421 -- 0.1673)  data: 0.0013 (0.0003 -- 0.0065)  max mem: 16413
Test:  [1820/8869]  eta: 0:18:05  loss: 2.8888 (3.0662)  acc1: 16.6667 (26.5056)  acc5: 50.0000 (65.8155)  time: 0.1498 (0.1421 -- 0.1673)  data: 0.0013 (0.0003 -- 0.0065)  max mem: 16413
Test:  [1830/8869]  eta: 0:18:03  loss: 2.3918 (3.0625)  acc1: 16.6667 (26.5611)  acc5: 66.6667 (65.8565)  time: 0.1518 (0.1370 -- 0.1749)  data: 0.0010 (0.0005 -- 0.0018)  max mem: 16413
Test:  [1840/8869]  eta: 0:18:02  loss: 3.0894 (3.0677)  acc1: 0.0000 (26.4711)  acc5: 66.6667 (65.9062)  time: 0.1539 (0.1370 -- 0.1749)  data: 0.0011 (0.0003 -- 0.0018)  max mem: 16413
Test:  [1850/8869]  eta: 0:18:00  loss: 3.6965 (3.0675)  acc1: 0.0000 (26.4272)  acc5: 66.6667 (65.9914)  time: 0.1507 (0.1399 -- 0.1712)  data: 0.0009 (0.0003 -- 0.0016)  max mem: 16413
Test:  [1860/8869]  eta: 0:17:58  loss: 3.2244 (3.0651)  acc1: 16.6667 (26.4284)  acc5: 83.3333 (66.0577)  time: 0.1498 (0.1415 -- 0.1712)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [1870/8869]  eta: 0:17:57  loss: 2.7422 (3.0646)  acc1: 16.6667 (26.4119)  acc5: 66.6667 (66.0075)  time: 0.1508 (0.1415 -- 0.1651)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [1880/8869]  eta: 0:17:55  loss: 2.9626 (3.0647)  acc1: 16.6667 (26.4044)  acc5: 66.6667 (65.9755)  time: 0.1546 (0.1446 -- 0.1678)  data: 0.0010 (0.0005 -- 0.0018)  max mem: 16413
Test:  [1890/8869]  eta: 0:17:54  loss: 2.6174 (3.0613)  acc1: 33.3333 (26.4587)  acc5: 66.6667 (66.0585)  time: 0.1544 (0.1401 -- 0.1678)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [1900/8869]  eta: 0:17:52  loss: 2.6174 (3.0621)  acc1: 33.3333 (26.4247)  acc5: 83.3333 (66.1757)  time: 0.1514 (0.1401 -- 0.1625)  data: 0.0011 (0.0006 -- 0.0018)  max mem: 16413
Test:  [1910/8869]  eta: 0:17:50  loss: 1.8512 (3.0544)  acc1: 33.3333 (26.5393)  acc5: 100.0000 (66.3440)  time: 0.1502 (0.1376 -- 0.1677)  data: 0.0011 (0.0006 -- 0.0018)  max mem: 16413
Test:  [1920/8869]  eta: 0:17:49  loss: 1.8987 (3.0535)  acc1: 33.3333 (26.5140)  acc5: 100.0000 (66.4151)  time: 0.1513 (0.1376 -- 0.1729)  data: 0.0012 (0.0006 -- 0.0028)  max mem: 16413
Test:  [1930/8869]  eta: 0:17:47  loss: 2.2044 (3.0464)  acc1: 33.3333 (26.6874)  acc5: 100.0000 (66.5458)  time: 0.1514 (0.1421 -- 0.1729)  data: 0.0012 (0.0005 -- 0.0028)  max mem: 16413
Test:  [1940/8869]  eta: 0:17:45  loss: 2.1437 (3.0447)  acc1: 50.0000 (26.7044)  acc5: 100.0000 (66.6581)  time: 0.1500 (0.1421 -- 0.1619)  data: 0.0011 (0.0005 -- 0.0017)  max mem: 16413
Test:  [1950/8869]  eta: 0:17:44  loss: 1.5526 (3.0357)  acc1: 50.0000 (26.9093)  acc5: 100.0000 (66.7863)  time: 0.1489 (0.1413 -- 0.1590)  data: 0.0016 (0.0007 -- 0.0091)  max mem: 16413
Test:  [1960/8869]  eta: 0:17:42  loss: 1.5526 (3.0332)  acc1: 50.0000 (26.9505)  acc5: 100.0000 (66.8961)  time: 0.1513 (0.1413 -- 0.1713)  data: 0.0017 (0.0007 -- 0.0091)  max mem: 16413
Test:  [1970/8869]  eta: 0:17:41  loss: 1.7950 (3.0252)  acc1: 33.3333 (27.0844)  acc5: 100.0000 (67.0303)  time: 0.1527 (0.1441 -- 0.1713)  data: 0.0012 (0.0007 -- 0.0023)  max mem: 16413
Test:  [1980/8869]  eta: 0:17:39  loss: 2.2169 (3.0232)  acc1: 33.3333 (27.0823)  acc5: 100.0000 (67.1378)  time: 0.1532 (0.1441 -- 0.1706)  data: 0.0011 (0.0008 -- 0.0016)  max mem: 16413
Test:  [1990/8869]  eta: 0:17:38  loss: 2.1055 (3.0172)  acc1: 33.3333 (27.1890)  acc5: 100.0000 (67.2945)  time: 0.1544 (0.1435 -- 0.1706)  data: 0.0011 (0.0007 -- 0.0018)  max mem: 16413
Test:  [2000/8869]  eta: 0:17:36  loss: 1.7003 (3.0152)  acc1: 33.3333 (27.2447)  acc5: 100.0000 (67.3996)  time: 0.1520 (0.1430 -- 0.1667)  data: 0.0011 (0.0004 -- 0.0020)  max mem: 16413
Test:  [2010/8869]  eta: 0:17:34  loss: 1.1798 (3.0074)  acc1: 50.0000 (27.4242)  acc5: 100.0000 (67.5120)  time: 0.1506 (0.1414 -- 0.1641)  data: 0.0009 (0.0004 -- 0.0020)  max mem: 16413
Test:  [2020/8869]  eta: 0:17:33  loss: 1.9150 (3.0095)  acc1: 33.3333 (27.3544)  acc5: 83.3333 (67.4666)  time: 0.1530 (0.1414 -- 0.1767)  data: 0.0008 (0.0004 -- 0.0012)  max mem: 16413
Test:  [2030/8869]  eta: 0:17:31  loss: 2.8154 (3.0065)  acc1: 16.6667 (27.4659)  acc5: 66.6667 (67.4955)  time: 0.1537 (0.1452 -- 0.1767)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [2040/8869]  eta: 0:17:30  loss: 2.5927 (3.0069)  acc1: 33.3333 (27.4539)  acc5: 66.6667 (67.5323)  time: 0.1519 (0.1397 -- 0.1725)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [2050/8869]  eta: 0:17:28  loss: 2.4317 (3.0009)  acc1: 33.3333 (27.5963)  acc5: 100.0000 (67.6337)  time: 0.1525 (0.1340 -- 0.1725)  data: 0.0009 (0.0004 -- 0.0013)  max mem: 16413
Test:  [2060/8869]  eta: 0:17:26  loss: 2.4317 (3.0041)  acc1: 16.6667 (27.5028)  acc5: 100.0000 (67.7099)  time: 0.1529 (0.1340 -- 0.1695)  data: 0.0009 (0.0005 -- 0.0015)  max mem: 16413
Test:  [2070/8869]  eta: 0:17:25  loss: 3.1878 (2.9994)  acc1: 0.0000 (27.6437)  acc5: 100.0000 (67.7933)  time: 0.1549 (0.1416 -- 0.1815)  data: 0.0014 (0.0006 -- 0.0077)  max mem: 16413
Test:  [2080/8869]  eta: 0:17:23  loss: 2.8328 (3.0000)  acc1: 16.6667 (27.6149)  acc5: 100.0000 (67.8280)  time: 0.1523 (0.1380 -- 0.1815)  data: 0.0014 (0.0006 -- 0.0077)  max mem: 16413
Test:  [2090/8869]  eta: 0:17:22  loss: 1.9314 (2.9949)  acc1: 33.3333 (27.7379)  acc5: 100.0000 (67.9260)  time: 0.1501 (0.1380 -- 0.1675)  data: 0.0010 (0.0006 -- 0.0016)  max mem: 16413
Test:  [2100/8869]  eta: 0:17:20  loss: 1.9618 (2.9947)  acc1: 33.3333 (27.7408)  acc5: 83.3333 (67.9756)  time: 0.1520 (0.1443 -- 0.1675)  data: 0.0011 (0.0005 -- 0.0026)  max mem: 16413
Test:  [2110/8869]  eta: 0:17:18  loss: 2.0208 (2.9908)  acc1: 33.3333 (27.8462)  acc5: 83.3333 (68.0641)  time: 0.1523 (0.1443 -- 0.1646)  data: 0.0011 (0.0004 -- 0.0026)  max mem: 16413
Test:  [2120/8869]  eta: 0:17:17  loss: 3.0608 (2.9926)  acc1: 16.6667 (27.7699)  acc5: 100.0000 (68.1440)  time: 0.1551 (0.1465 -- 0.1788)  data: 0.0021 (0.0004 -- 0.0215)  max mem: 16413
Test:  [2130/8869]  eta: 0:17:15  loss: 3.2026 (2.9928)  acc1: 0.0000 (27.7804)  acc5: 83.3333 (68.1527)  time: 0.1526 (0.1371 -- 0.1788)  data: 0.0021 (0.0007 -- 0.0215)  max mem: 16413
Test:  [2140/8869]  eta: 0:17:14  loss: 3.1637 (2.9951)  acc1: 16.6667 (27.7440)  acc5: 66.6667 (68.1302)  time: 0.1499 (0.1371 -- 0.1611)  data: 0.0010 (0.0007 -- 0.0022)  max mem: 16413
Test:  [2150/8869]  eta: 0:17:12  loss: 4.1055 (2.9996)  acc1: 0.0000 (27.6461)  acc5: 50.0000 (68.0226)  time: 0.1538 (0.1383 -- 0.1681)  data: 0.0015 (0.0006 -- 0.0128)  max mem: 16413
Test:  [2160/8869]  eta: 0:17:11  loss: 4.1834 (3.0018)  acc1: 0.0000 (27.6338)  acc5: 50.0000 (67.9469)  time: 0.1533 (0.1383 -- 0.1681)  data: 0.0016 (0.0006 -- 0.0128)  max mem: 16413
Test:  [2170/8869]  eta: 0:17:09  loss: 4.1439 (3.0054)  acc1: 0.0000 (27.5756)  acc5: 50.0000 (67.8873)  time: 0.1513 (0.1448 -- 0.1658)  data: 0.0010 (0.0007 -- 0.0013)  max mem: 16413
Test:  [2180/8869]  eta: 0:17:07  loss: 4.1439 (3.0057)  acc1: 0.0000 (27.5944)  acc5: 50.0000 (67.8206)  time: 0.1524 (0.1434 -- 0.1716)  data: 0.0011 (0.0007 -- 0.0014)  max mem: 16413
Test:  [2190/8869]  eta: 0:17:06  loss: 2.6348 (3.0062)  acc1: 33.3333 (27.6130)  acc5: 50.0000 (67.8153)  time: 0.1525 (0.1417 -- 0.1716)  data: 0.0014 (0.0005 -- 0.0050)  max mem: 16413
Test:  [2200/8869]  eta: 0:17:04  loss: 3.7515 (3.0094)  acc1: 0.0000 (27.5254)  acc5: 66.6667 (67.8328)  time: 0.1523 (0.1401 -- 0.1670)  data: 0.0014 (0.0005 -- 0.0050)  max mem: 16413
Test:  [2210/8869]  eta: 0:17:03  loss: 3.9440 (3.0125)  acc1: 0.0000 (27.4612)  acc5: 66.6667 (67.7898)  time: 0.1518 (0.1401 -- 0.1652)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [2220/8869]  eta: 0:17:01  loss: 4.1161 (3.0172)  acc1: 0.0000 (27.3976)  acc5: 50.0000 (67.6422)  time: 0.1530 (0.1401 -- 0.1652)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [2230/8869]  eta: 0:17:00  loss: 4.1161 (3.0203)  acc1: 0.0000 (27.3420)  acc5: 33.3333 (67.5631)  time: 0.1539 (0.1424 -- 0.1645)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [2240/8869]  eta: 0:16:58  loss: 3.1399 (3.0161)  acc1: 16.6667 (27.4505)  acc5: 83.3333 (67.6558)  time: 0.1487 (0.1287 -- 0.1632)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [2250/8869]  eta: 0:16:56  loss: 3.8017 (3.0221)  acc1: 0.0000 (27.3508)  acc5: 66.6667 (67.5181)  time: 0.1461 (0.1287 -- 0.1619)  data: 0.0011 (0.0004 -- 0.0045)  max mem: 16413
Test:  [2260/8869]  eta: 0:16:55  loss: 4.4606 (3.0270)  acc1: 0.0000 (27.2372)  acc5: 33.3333 (67.3743)  time: 0.1510 (0.1391 -- 0.1725)  data: 0.0012 (0.0004 -- 0.0045)  max mem: 16413
Test:  [2270/8869]  eta: 0:16:53  loss: 4.5156 (3.0340)  acc1: 0.0000 (27.1320)  acc5: 16.6667 (67.1584)  time: 0.1555 (0.1398 -- 0.1725)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [2280/8869]  eta: 0:16:52  loss: 4.5866 (3.0393)  acc1: 0.0000 (27.0422)  acc5: 16.6667 (67.0028)  time: 0.1535 (0.1398 -- 0.1724)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [2290/8869]  eta: 0:16:50  loss: 4.1535 (3.0443)  acc1: 0.0000 (26.9387)  acc5: 33.3333 (66.8558)  time: 0.1517 (0.1424 -- 0.1765)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [2300/8869]  eta: 0:16:49  loss: 3.9326 (3.0463)  acc1: 0.0000 (26.8869)  acc5: 33.3333 (66.7681)  time: 0.1546 (0.1386 -- 0.1765)  data: 0.0015 (0.0007 -- 0.0098)  max mem: 16413
Test:  [2310/8869]  eta: 0:16:47  loss: 4.0439 (3.0517)  acc1: 0.0000 (26.7994)  acc5: 33.3333 (66.6018)  time: 0.1520 (0.1363 -- 0.1704)  data: 0.0015 (0.0005 -- 0.0098)  max mem: 16413
Test:  [2320/8869]  eta: 0:16:45  loss: 4.0895 (3.0554)  acc1: 0.0000 (26.7270)  acc5: 33.3333 (66.4728)  time: 0.1483 (0.1363 -- 0.1631)  data: 0.0010 (0.0004 -- 0.0040)  max mem: 16413
Test:  [2330/8869]  eta: 0:16:44  loss: 4.1877 (3.0619)  acc1: 0.0000 (26.6195)  acc5: 33.3333 (66.2734)  time: 0.1491 (0.1388 -- 0.1620)  data: 0.0008 (0.0004 -- 0.0015)  max mem: 16413
Test:  [2340/8869]  eta: 0:16:42  loss: 4.1877 (3.0661)  acc1: 0.0000 (26.5200)  acc5: 16.6667 (66.1113)  time: 0.1477 (0.1388 -- 0.1620)  data: 0.0009 (0.0004 -- 0.0013)  max mem: 16413
Test:  [2350/8869]  eta: 0:16:40  loss: 4.1006 (3.0707)  acc1: 0.0000 (26.4285)  acc5: 33.3333 (65.9790)  time: 0.1472 (0.1370 -- 0.1644)  data: 0.0014 (0.0004 -- 0.0081)  max mem: 16413
Test:  [2360/8869]  eta: 0:16:38  loss: 3.9821 (3.0736)  acc1: 0.0000 (26.3589)  acc5: 33.3333 (65.9043)  time: 0.1498 (0.1370 -- 0.1644)  data: 0.0014 (0.0003 -- 0.0081)  max mem: 16413
Test:  [2370/8869]  eta: 0:16:37  loss: 3.9821 (3.0772)  acc1: 0.0000 (26.3321)  acc5: 50.0000 (65.7880)  time: 0.1500 (0.1427 -- 0.1611)  data: 0.0011 (0.0003 -- 0.0018)  max mem: 16413
Test:  [2380/8869]  eta: 0:16:35  loss: 3.9749 (3.0749)  acc1: 0.0000 (26.3965)  acc5: 50.0000 (65.8197)  time: 0.1509 (0.1423 -- 0.1781)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [2390/8869]  eta: 0:16:34  loss: 2.1064 (3.0699)  acc1: 33.3333 (26.4813)  acc5: 100.0000 (65.9138)  time: 0.1519 (0.1397 -- 0.1781)  data: 0.0013 (0.0004 -- 0.0100)  max mem: 16413
Test:  [2400/8869]  eta: 0:16:32  loss: 1.5438 (3.0667)  acc1: 50.0000 (26.5653)  acc5: 100.0000 (65.9170)  time: 0.1496 (0.1374 -- 0.1649)  data: 0.0015 (0.0005 -- 0.0100)  max mem: 16413
Test:  [2410/8869]  eta: 0:16:30  loss: 2.9687 (3.0665)  acc1: 33.3333 (26.5796)  acc5: 50.0000 (65.8786)  time: 0.1506 (0.1374 -- 0.1629)  data: 0.0012 (0.0005 -- 0.0041)  max mem: 16413
Test:  [2420/8869]  eta: 0:16:29  loss: 3.0463 (3.0656)  acc1: 33.3333 (26.6006)  acc5: 50.0000 (65.8543)  time: 0.1524 (0.1447 -- 0.1667)  data: 0.0012 (0.0004 -- 0.0041)  max mem: 16413
Test:  [2430/8869]  eta: 0:16:27  loss: 3.6760 (3.0685)  acc1: 0.0000 (26.5529)  acc5: 66.6667 (65.8782)  time: 0.1535 (0.1447 -- 0.1691)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [2440/8869]  eta: 0:16:26  loss: 3.9535 (3.0691)  acc1: 0.0000 (26.4919)  acc5: 83.3333 (65.9156)  time: 0.1547 (0.1463 -- 0.1711)  data: 0.0010 (0.0003 -- 0.0018)  max mem: 16413
Test:  [2450/8869]  eta: 0:16:24  loss: 2.9593 (3.0680)  acc1: 0.0000 (26.5062)  acc5: 83.3333 (65.9799)  time: 0.1518 (0.1419 -- 0.1711)  data: 0.0010 (0.0003 -- 0.0018)  max mem: 16413
Test:  [2460/8869]  eta: 0:16:23  loss: 2.9026 (3.0673)  acc1: 16.6667 (26.5068)  acc5: 66.6667 (65.9759)  time: 0.1485 (0.1392 -- 0.1604)  data: 0.0010 (0.0005 -- 0.0017)  max mem: 16413
Test:  [2470/8869]  eta: 0:16:21  loss: 3.0352 (3.0681)  acc1: 16.6667 (26.4738)  acc5: 66.6667 (65.9180)  time: 0.1502 (0.1392 -- 0.1649)  data: 0.0015 (0.0005 -- 0.0079)  max mem: 16413
Test:  [2480/8869]  eta: 0:16:19  loss: 3.0307 (3.0659)  acc1: 16.6667 (26.5283)  acc5: 66.6667 (65.9747)  time: 0.1527 (0.1415 -- 0.1649)  data: 0.0019 (0.0008 -- 0.0079)  max mem: 16413
Test:  [2490/8869]  eta: 0:16:18  loss: 3.0221 (3.0657)  acc1: 33.3333 (26.5288)  acc5: 83.3333 (66.0444)  time: 0.1536 (0.1415 -- 0.1634)  data: 0.0013 (0.0005 -- 0.0038)  max mem: 16413
Test:  [2500/8869]  eta: 0:16:16  loss: 2.4668 (3.0608)  acc1: 33.3333 (26.6160)  acc5: 100.0000 (66.1669)  time: 0.1505 (0.1387 -- 0.1611)  data: 0.0013 (0.0004 -- 0.0071)  max mem: 16413
Test:  [2510/8869]  eta: 0:16:14  loss: 1.9308 (3.0588)  acc1: 33.3333 (26.6229)  acc5: 100.0000 (66.2551)  time: 0.1458 (0.1366 -- 0.1619)  data: 0.0011 (0.0003 -- 0.0071)  max mem: 16413
Test:  [2520/8869]  eta: 0:16:13  loss: 1.9308 (3.0547)  acc1: 33.3333 (26.7090)  acc5: 100.0000 (66.3692)  time: 0.1493 (0.1366 -- 0.1627)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Test:  [2530/8869]  eta: 0:16:12  loss: 1.3513 (3.0532)  acc1: 50.0000 (26.7483)  acc5: 100.0000 (66.4362)  time: 0.1564 (0.1391 -- 0.1774)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [2540/8869]  eta: 0:16:10  loss: 1.5176 (3.0473)  acc1: 50.0000 (26.8857)  acc5: 83.3333 (66.5289)  time: 0.1548 (0.1335 -- 0.1774)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [2550/8869]  eta: 0:16:08  loss: 1.5176 (3.0445)  acc1: 66.6667 (26.9633)  acc5: 100.0000 (66.6144)  time: 0.1505 (0.1335 -- 0.1710)  data: 0.0009 (0.0006 -- 0.0016)  max mem: 16413
Test:  [2560/8869]  eta: 0:16:07  loss: 1.1511 (3.0376)  acc1: 66.6667 (27.0923)  acc5: 100.0000 (66.7448)  time: 0.1523 (0.1419 -- 0.1652)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Test:  [2570/8869]  eta: 0:16:05  loss: 1.9902 (3.0369)  acc1: 33.3333 (27.0841)  acc5: 100.0000 (66.8158)  time: 0.1501 (0.1330 -- 0.1670)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Test:  [2580/8869]  eta: 0:16:04  loss: 2.3635 (3.0316)  acc1: 33.3333 (27.1858)  acc5: 100.0000 (66.9379)  time: 0.1483 (0.1330 -- 0.1670)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [2590/8869]  eta: 0:16:02  loss: 1.6165 (3.0312)  acc1: 33.3333 (27.1774)  acc5: 100.0000 (66.9883)  time: 0.1513 (0.1412 -- 0.1623)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [2600/8869]  eta: 0:16:00  loss: 1.9698 (3.0258)  acc1: 50.0000 (27.3036)  acc5: 83.3333 (67.0640)  time: 0.1528 (0.1453 -- 0.1687)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [2610/8869]  eta: 0:15:59  loss: 1.9698 (3.0254)  acc1: 50.0000 (27.2884)  acc5: 83.3333 (67.0943)  time: 0.1524 (0.1442 -- 0.1687)  data: 0.0010 (0.0004 -- 0.0033)  max mem: 16413
Test:  [2620/8869]  eta: 0:15:57  loss: 2.4468 (3.0229)  acc1: 33.3333 (27.3687)  acc5: 83.3333 (67.1181)  time: 0.1507 (0.1431 -- 0.1635)  data: 0.0010 (0.0004 -- 0.0033)  max mem: 16413
Test:  [2630/8869]  eta: 0:15:56  loss: 2.4253 (3.0239)  acc1: 33.3333 (27.3470)  acc5: 66.6667 (67.1228)  time: 0.1521 (0.1431 -- 0.1758)  data: 0.0008 (0.0004 -- 0.0015)  max mem: 16413
Test:  [2640/8869]  eta: 0:15:54  loss: 2.0968 (3.0194)  acc1: 33.3333 (27.4517)  acc5: 66.6667 (67.1715)  time: 0.1531 (0.1433 -- 0.1758)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [2650/8869]  eta: 0:15:53  loss: 2.6968 (3.0216)  acc1: 16.6667 (27.3922)  acc5: 83.3333 (67.2199)  time: 0.1518 (0.1432 -- 0.1686)  data: 0.0010 (0.0006 -- 0.0017)  max mem: 16413
Test:  [2660/8869]  eta: 0:15:51  loss: 2.7118 (3.0172)  acc1: 16.6667 (27.4834)  acc5: 83.3333 (67.2867)  time: 0.1522 (0.1432 -- 0.1686)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [2670/8869]  eta: 0:15:50  loss: 2.7118 (3.0172)  acc1: 33.3333 (27.4679)  acc5: 83.3333 (67.3406)  time: 0.1540 (0.1441 -- 0.1748)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [2680/8869]  eta: 0:15:48  loss: 2.7207 (3.0138)  acc1: 33.3333 (27.5333)  acc5: 83.3333 (67.4002)  time: 0.1536 (0.1412 -- 0.1748)  data: 0.0011 (0.0006 -- 0.0048)  max mem: 16413
Test:  [2690/8869]  eta: 0:15:46  loss: 2.0864 (3.0141)  acc1: 33.3333 (27.5177)  acc5: 66.6667 (67.3975)  time: 0.1497 (0.1412 -- 0.1729)  data: 0.0012 (0.0006 -- 0.0048)  max mem: 16413
Test:  [2700/8869]  eta: 0:15:45  loss: 2.0864 (3.0108)  acc1: 16.6667 (27.5639)  acc5: 100.0000 (67.4750)  time: 0.1538 (0.1423 -- 0.1825)  data: 0.0012 (0.0003 -- 0.0019)  max mem: 16413
Test:  [2710/8869]  eta: 0:15:43  loss: 3.0500 (3.0134)  acc1: 0.0000 (27.4683)  acc5: 100.0000 (67.5581)  time: 0.1532 (0.1377 -- 0.1825)  data: 0.0010 (0.0003 -- 0.0019)  max mem: 16413
Test:  [2720/8869]  eta: 0:15:42  loss: 3.3132 (3.0135)  acc1: 0.0000 (27.4654)  acc5: 83.3333 (67.5487)  time: 0.1506 (0.1377 -- 0.1656)  data: 0.0010 (0.0003 -- 0.0020)  max mem: 16413
Test:  [2730/8869]  eta: 0:15:40  loss: 3.0162 (3.0146)  acc1: 16.6667 (27.4503)  acc5: 66.6667 (67.5638)  time: 0.1535 (0.1431 -- 0.1695)  data: 0.0011 (0.0004 -- 0.0020)  max mem: 16413
Test:  [2740/8869]  eta: 0:15:39  loss: 4.2438 (3.0183)  acc1: 0.0000 (27.3805)  acc5: 50.0000 (67.4997)  time: 0.1537 (0.1431 -- 0.1695)  data: 0.0012 (0.0004 -- 0.0033)  max mem: 16413
Test:  [2750/8869]  eta: 0:15:37  loss: 3.8298 (3.0188)  acc1: 0.0000 (27.3900)  acc5: 66.6667 (67.5027)  time: 0.1543 (0.1424 -- 0.1657)  data: 0.0011 (0.0004 -- 0.0033)  max mem: 16413
Test:  [2760/8869]  eta: 0:15:36  loss: 3.7215 (3.0223)  acc1: 0.0000 (27.3331)  acc5: 66.6667 (67.4333)  time: 0.1554 (0.1424 -- 0.1674)  data: 0.0010 (0.0004 -- 0.0022)  max mem: 16413
Test:  [2770/8869]  eta: 0:15:34  loss: 4.3141 (3.0219)  acc1: 0.0000 (27.3427)  acc5: 50.0000 (67.3944)  time: 0.1500 (0.1309 -- 0.1674)  data: 0.0010 (0.0004 -- 0.0022)  max mem: 16413
Test:  [2780/8869]  eta: 0:15:32  loss: 2.8773 (3.0230)  acc1: 16.6667 (27.3283)  acc5: 50.0000 (67.3619)  time: 0.1453 (0.1309 -- 0.1602)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [2790/8869]  eta: 0:15:31  loss: 3.8864 (3.0251)  acc1: 0.0000 (27.2722)  acc5: 66.6667 (67.3952)  time: 0.1475 (0.1398 -- 0.1637)  data: 0.0009 (0.0004 -- 0.0012)  max mem: 16413
Test:  [2800/8869]  eta: 0:15:29  loss: 3.8799 (3.0272)  acc1: 0.0000 (27.1986)  acc5: 83.3333 (67.3926)  time: 0.1498 (0.1398 -- 0.1684)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [2810/8869]  eta: 0:15:28  loss: 3.8799 (3.0301)  acc1: 0.0000 (27.1434)  acc5: 50.0000 (67.3189)  time: 0.1524 (0.1419 -- 0.1684)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [2820/8869]  eta: 0:15:26  loss: 4.1683 (3.0334)  acc1: 0.0000 (27.0885)  acc5: 33.3333 (67.2220)  time: 0.1532 (0.1429 -- 0.1663)  data: 0.0012 (0.0004 -- 0.0055)  max mem: 16413
Test:  [2830/8869]  eta: 0:15:25  loss: 2.7695 (3.0288)  acc1: 0.0000 (27.1635)  acc5: 66.6667 (67.3025)  time: 0.1528 (0.1429 -- 0.1663)  data: 0.0013 (0.0007 -- 0.0055)  max mem: 16413
Test:  [2840/8869]  eta: 0:15:23  loss: 3.3776 (3.0330)  acc1: 0.0000 (27.0797)  acc5: 66.6667 (67.2240)  time: 0.1534 (0.1455 -- 0.1671)  data: 0.0011 (0.0006 -- 0.0016)  max mem: 16413
Test:  [2850/8869]  eta: 0:15:21  loss: 4.4131 (3.0367)  acc1: 0.0000 (26.9964)  acc5: 33.3333 (67.1226)  time: 0.1531 (0.1408 -- 0.1671)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [2860/8869]  eta: 0:15:20  loss: 4.3691 (3.0415)  acc1: 0.0000 (26.9137)  acc5: 33.3333 (66.9754)  time: 0.1528 (0.1408 -- 0.1709)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Test:  [2870/8869]  eta: 0:15:18  loss: 4.4069 (3.0460)  acc1: 0.0000 (26.8257)  acc5: 16.6667 (66.8176)  time: 0.1502 (0.1349 -- 0.1709)  data: 0.0010 (0.0004 -- 0.0023)  max mem: 16413
Test:  [2880/8869]  eta: 0:15:17  loss: 4.4974 (3.0505)  acc1: 0.0000 (26.7326)  acc5: 16.6667 (66.6956)  time: 0.1464 (0.1349 -- 0.1558)  data: 0.0011 (0.0004 -- 0.0044)  max mem: 16413
Test:  [2890/8869]  eta: 0:15:15  loss: 4.2920 (3.0534)  acc1: 0.0000 (26.6747)  acc5: 33.3333 (66.5860)  time: 0.1492 (0.1381 -- 0.1659)  data: 0.0012 (0.0005 -- 0.0044)  max mem: 16413
Test:  [2900/8869]  eta: 0:15:14  loss: 4.2513 (3.0562)  acc1: 0.0000 (26.6402)  acc5: 33.3333 (66.4886)  time: 0.1521 (0.1381 -- 0.1659)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [2910/8869]  eta: 0:15:12  loss: 4.2240 (3.0595)  acc1: 0.0000 (26.5602)  acc5: 33.3333 (66.3861)  time: 0.1537 (0.1402 -- 0.1742)  data: 0.0014 (0.0008 -- 0.0046)  max mem: 16413
Test:  [2920/8869]  eta: 0:15:10  loss: 4.3548 (3.0648)  acc1: 0.0000 (26.4692)  acc5: 16.6667 (66.2159)  time: 0.1507 (0.1363 -- 0.1742)  data: 0.0012 (0.0007 -- 0.0046)  max mem: 16413
Test:  [2930/8869]  eta: 0:15:09  loss: 4.3584 (3.0686)  acc1: 0.0000 (26.3846)  acc5: 16.6667 (66.0810)  time: 0.1479 (0.1363 -- 0.1664)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [2940/8869]  eta: 0:15:07  loss: 4.2275 (3.0720)  acc1: 0.0000 (26.3176)  acc5: 16.6667 (65.9583)  time: 0.1487 (0.1363 -- 0.1664)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [2950/8869]  eta: 0:15:06  loss: 4.2275 (3.0755)  acc1: 0.0000 (26.2453)  acc5: 33.3333 (65.8760)  time: 0.1516 (0.1401 -- 0.1736)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [2960/8869]  eta: 0:15:04  loss: 4.2965 (3.0777)  acc1: 0.0000 (26.2299)  acc5: 33.3333 (65.7942)  time: 0.1494 (0.1201 -- 0.1736)  data: 0.0009 (0.0002 -- 0.0017)  max mem: 16413
Test:  [2970/8869]  eta: 0:15:02  loss: 3.3446 (3.0758)  acc1: 33.3333 (26.2931)  acc5: 50.0000 (65.8420)  time: 0.1379 (0.1146 -- 0.1615)  data: 0.0005 (0.0001 -- 0.0012)  max mem: 16413
Test:  [2980/8869]  eta: 0:15:00  loss: 2.5807 (3.0734)  acc1: 33.3333 (26.3334)  acc5: 100.0000 (65.8895)  time: 0.1399 (0.1146 -- 0.1636)  data: 0.0007 (0.0001 -- 0.0023)  max mem: 16413
Test:  [2990/8869]  eta: 0:14:59  loss: 2.0545 (3.0707)  acc1: 33.3333 (26.4070)  acc5: 83.3333 (65.8921)  time: 0.1515 (0.1368 -- 0.1750)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Test:  [3000/8869]  eta: 0:14:57  loss: 2.0545 (3.0699)  acc1: 33.3333 (26.4301)  acc5: 66.6667 (65.8503)  time: 0.1545 (0.1415 -- 0.1750)  data: 0.0014 (0.0009 -- 0.0028)  max mem: 16413
Test:  [3010/8869]  eta: 0:14:56  loss: 2.8142 (3.0689)  acc1: 16.6667 (26.4419)  acc5: 50.0000 (65.8308)  time: 0.1516 (0.1378 -- 0.1668)  data: 0.0011 (0.0004 -- 0.0018)  max mem: 16413
Test:  [3020/8869]  eta: 0:14:54  loss: 2.8142 (3.0702)  acc1: 16.6667 (26.4316)  acc5: 66.6667 (65.8722)  time: 0.1477 (0.1357 -- 0.1648)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [3030/8869]  eta: 0:14:52  loss: 2.9028 (3.0707)  acc1: 0.0000 (26.3939)  acc5: 83.3333 (65.9023)  time: 0.1478 (0.1357 -- 0.1685)  data: 0.0008 (0.0004 -- 0.0011)  max mem: 16413
Test:  [3040/8869]  eta: 0:14:51  loss: 2.8107 (3.0697)  acc1: 0.0000 (26.3784)  acc5: 83.3333 (65.9761)  time: 0.1489 (0.1383 -- 0.1685)  data: 0.0010 (0.0006 -- 0.0017)  max mem: 16413
Test:  [3050/8869]  eta: 0:14:49  loss: 2.7120 (3.0686)  acc1: 16.6667 (26.3903)  acc5: 66.6667 (65.9511)  time: 0.1511 (0.1391 -- 0.1660)  data: 0.0011 (0.0005 -- 0.0017)  max mem: 16413
Test:  [3060/8869]  eta: 0:14:48  loss: 3.2718 (3.0696)  acc1: 16.6667 (26.3639)  acc5: 50.0000 (65.9098)  time: 0.1530 (0.1435 -- 0.1660)  data: 0.0012 (0.0005 -- 0.0046)  max mem: 16413
Test:  [3070/8869]  eta: 0:14:46  loss: 2.7531 (3.0670)  acc1: 16.6667 (26.4192)  acc5: 66.6667 (65.9557)  time: 0.1519 (0.1398 -- 0.1614)  data: 0.0011 (0.0005 -- 0.0046)  max mem: 16413
Test:  [3080/8869]  eta: 0:14:45  loss: 2.8216 (3.0675)  acc1: 16.6667 (26.3929)  acc5: 83.3333 (66.0338)  time: 0.1497 (0.1395 -- 0.1614)  data: 0.0011 (0.0005 -- 0.0017)  max mem: 16413
Test:  [3090/8869]  eta: 0:14:43  loss: 2.7181 (3.0639)  acc1: 16.6667 (26.4424)  acc5: 100.0000 (66.1275)  time: 0.1542 (0.1395 -- 0.1686)  data: 0.0014 (0.0008 -- 0.0029)  max mem: 16413
Test:  [3100/8869]  eta: 0:14:42  loss: 1.8072 (3.0626)  acc1: 33.3333 (26.4431)  acc5: 100.0000 (66.1776)  time: 0.1560 (0.1420 -- 0.1700)  data: 0.0013 (0.0006 -- 0.0029)  max mem: 16413
Test:  [3110/8869]  eta: 0:14:40  loss: 1.8372 (3.0582)  acc1: 33.3333 (26.5402)  acc5: 83.3333 (66.2488)  time: 0.1514 (0.1361 -- 0.2089)  data: 0.0045 (0.0004 -- 0.0712)  max mem: 16413
Test:  [3120/8869]  eta: 0:14:38  loss: 1.8108 (3.0567)  acc1: 33.3333 (26.5567)  acc5: 100.0000 (66.3249)  time: 0.1473 (0.1361 -- 0.2089)  data: 0.0043 (0.0004 -- 0.0712)  max mem: 16413
Test:  [3130/8869]  eta: 0:14:37  loss: 1.8958 (3.0530)  acc1: 50.0000 (26.6422)  acc5: 100.0000 (66.4005)  time: 0.1470 (0.1361 -- 0.1610)  data: 0.0009 (0.0002 -- 0.0018)  max mem: 16413
Test:  [3140/8869]  eta: 0:14:36  loss: 1.6034 (3.0502)  acc1: 50.0000 (26.7059)  acc5: 100.0000 (66.4703)  time: 0.1642 (0.1423 -- 0.3903)  data: 0.0138 (0.0002 -- 0.2574)  max mem: 16413
Test:  [3150/8869]  eta: 0:14:34  loss: 1.1340 (3.0445)  acc1: 66.6667 (26.8010)  acc5: 100.0000 (66.5767)  time: 0.1578 (0.1232 -- 0.3903)  data: 0.0136 (0.0003 -- 0.2574)  max mem: 16413
Test:  [3160/8869]  eta: 0:14:32  loss: 1.8256 (3.0445)  acc1: 16.6667 (26.7637)  acc5: 100.0000 (66.6298)  time: 0.1406 (0.1232 -- 0.1555)  data: 0.0007 (0.0003 -- 0.0013)  max mem: 16413
Test:  [3170/8869]  eta: 0:14:31  loss: 2.4841 (3.0400)  acc1: 16.6667 (26.8475)  acc5: 100.0000 (66.7297)  time: 0.1485 (0.1340 -- 0.1652)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [3180/8869]  eta: 0:14:29  loss: 2.1432 (3.0386)  acc1: 33.3333 (26.8783)  acc5: 100.0000 (66.8081)  time: 0.1521 (0.1415 -- 0.1652)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [3190/8869]  eta: 0:14:28  loss: 2.3234 (3.0342)  acc1: 50.0000 (26.9821)  acc5: 100.0000 (66.8651)  time: 0.1502 (0.1397 -- 0.1608)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [3200/8869]  eta: 0:14:26  loss: 1.9954 (3.0333)  acc1: 50.0000 (26.9812)  acc5: 83.3333 (66.9062)  time: 0.1500 (0.1397 -- 0.1604)  data: 0.0010 (0.0004 -- 0.0026)  max mem: 16413
Test:  [3210/8869]  eta: 0:14:25  loss: 2.1831 (3.0317)  acc1: 33.3333 (27.0425)  acc5: 83.3333 (66.9002)  time: 0.1538 (0.1421 -- 0.1763)  data: 0.0015 (0.0004 -- 0.0052)  max mem: 16413
Test:  [3220/8869]  eta: 0:14:23  loss: 3.0566 (3.0326)  acc1: 33.3333 (27.0361)  acc5: 66.6667 (66.8995)  time: 0.1562 (0.1470 -- 0.1763)  data: 0.0014 (0.0005 -- 0.0052)  max mem: 16413
Test:  [3230/8869]  eta: 0:14:21  loss: 1.6668 (3.0285)  acc1: 50.0000 (27.1330)  acc5: 83.3333 (66.9555)  time: 0.1523 (0.1390 -- 0.1690)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [3240/8869]  eta: 0:14:20  loss: 1.7532 (3.0303)  acc1: 16.6667 (27.0801)  acc5: 100.0000 (67.0112)  time: 0.1483 (0.1390 -- 0.1674)  data: 0.0010 (0.0006 -- 0.0024)  max mem: 16413
Test:  [3250/8869]  eta: 0:14:18  loss: 2.7977 (3.0281)  acc1: 16.6667 (27.1404)  acc5: 83.3333 (67.0563)  time: 0.1513 (0.1416 -- 0.1733)  data: 0.0010 (0.0003 -- 0.0024)  max mem: 16413
Test:  [3260/8869]  eta: 0:14:17  loss: 2.5881 (3.0266)  acc1: 33.3333 (27.1645)  acc5: 100.0000 (67.1266)  time: 0.1519 (0.1393 -- 0.1733)  data: 0.0010 (0.0003 -- 0.0016)  max mem: 16413
Test:  [3270/8869]  eta: 0:14:15  loss: 2.1396 (3.0241)  acc1: 33.3333 (27.2241)  acc5: 100.0000 (67.1609)  time: 0.1495 (0.1363 -- 0.1685)  data: 0.0010 (0.0005 -- 0.0031)  max mem: 16413
Test:  [3280/8869]  eta: 0:14:14  loss: 2.6459 (3.0240)  acc1: 33.3333 (27.2326)  acc5: 83.3333 (67.2000)  time: 0.1492 (0.1363 -- 0.1667)  data: 0.0010 (0.0003 -- 0.0031)  max mem: 16413
Test:  [3290/8869]  eta: 0:14:12  loss: 2.6459 (3.0216)  acc1: 33.3333 (27.3017)  acc5: 83.3333 (67.2440)  time: 0.1478 (0.1380 -- 0.1652)  data: 0.0008 (0.0003 -- 0.0023)  max mem: 16413
Test:  [3300/8869]  eta: 0:14:10  loss: 3.3122 (3.0231)  acc1: 0.0000 (27.2392)  acc5: 100.0000 (67.2927)  time: 0.1501 (0.1380 -- 0.1638)  data: 0.0008 (0.0003 -- 0.0012)  max mem: 16413
Test:  [3310/8869]  eta: 0:14:09  loss: 3.3122 (3.0226)  acc1: 0.0000 (27.2727)  acc5: 83.3333 (67.3110)  time: 0.1447 (0.1223 -- 0.1638)  data: 0.0007 (0.0001 -- 0.0014)  max mem: 16413
Test:  [3320/8869]  eta: 0:14:07  loss: 3.0289 (3.0242)  acc1: 16.6667 (27.2308)  acc5: 66.6667 (67.3141)  time: 0.1427 (0.1223 -- 0.1638)  data: 0.0008 (0.0001 -- 0.0015)  max mem: 16413
Test:  [3330/8869]  eta: 0:14:06  loss: 3.9368 (3.0265)  acc1: 0.0000 (27.1840)  acc5: 50.0000 (67.2471)  time: 0.1524 (0.1400 -- 0.1688)  data: 0.0010 (0.0003 -- 0.0016)  max mem: 16413
Test:  [3340/8869]  eta: 0:14:04  loss: 4.0091 (3.0277)  acc1: 0.0000 (27.1725)  acc5: 50.0000 (67.2104)  time: 0.1536 (0.1409 -- 0.1688)  data: 0.0010 (0.0003 -- 0.0016)  max mem: 16413
Test:  [3350/8869]  eta: 0:14:02  loss: 4.3099 (3.0306)  acc1: 0.0000 (27.1312)  acc5: 50.0000 (67.1591)  time: 0.1517 (0.1413 -- 0.1642)  data: 0.0011 (0.0005 -- 0.0015)  max mem: 16413
Test:  [3360/8869]  eta: 0:14:01  loss: 3.6543 (3.0293)  acc1: 16.6667 (27.1645)  acc5: 50.0000 (67.1675)  time: 0.1523 (0.1441 -- 0.1626)  data: 0.0010 (0.0005 -- 0.0023)  max mem: 16413
Test:  [3370/8869]  eta: 0:13:59  loss: 3.5530 (3.0314)  acc1: 16.6667 (27.1482)  acc5: 50.0000 (67.1067)  time: 0.1532 (0.1441 -- 0.1610)  data: 0.0010 (0.0005 -- 0.0023)  max mem: 16413
Test:  [3380/8869]  eta: 0:13:58  loss: 4.1400 (3.0330)  acc1: 0.0000 (27.1123)  acc5: 66.6667 (67.1103)  time: 0.1521 (0.1430 -- 0.1703)  data: 0.0010 (0.0005 -- 0.0018)  max mem: 16413
Test:  [3390/8869]  eta: 0:13:56  loss: 3.5973 (3.0338)  acc1: 0.0000 (27.0815)  acc5: 83.3333 (67.1090)  time: 0.1523 (0.1430 -- 0.1717)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [3400/8869]  eta: 0:13:55  loss: 4.0573 (3.0369)  acc1: 0.0000 (27.0411)  acc5: 50.0000 (67.0391)  time: 0.1535 (0.1443 -- 0.1717)  data: 0.0011 (0.0004 -- 0.0016)  max mem: 16413
Test:  [3410/8869]  eta: 0:13:53  loss: 4.5000 (3.0407)  acc1: 0.0000 (26.9764)  acc5: 33.3333 (66.9452)  time: 0.1532 (0.1443 -- 0.1719)  data: 0.0011 (0.0006 -- 0.0016)  max mem: 16413
Test:  [3420/8869]  eta: 0:13:52  loss: 2.8832 (3.0367)  acc1: 16.6667 (27.0584)  acc5: 66.6667 (67.0272)  time: 0.1541 (0.1411 -- 0.1719)  data: 0.0011 (0.0005 -- 0.0013)  max mem: 16413
Test:  [3430/8869]  eta: 0:13:50  loss: 2.7542 (3.0396)  acc1: 16.6667 (27.0135)  acc5: 83.3333 (66.9727)  time: 0.1522 (0.1396 -- 0.1705)  data: 0.0010 (0.0005 -- 0.0013)  max mem: 16413
Test:  [3440/8869]  eta: 0:13:49  loss: 4.2855 (3.0433)  acc1: 0.0000 (26.9398)  acc5: 33.3333 (66.8653)  time: 0.1499 (0.1396 -- 0.1705)  data: 0.0011 (0.0006 -- 0.0047)  max mem: 16413
Test:  [3450/8869]  eta: 0:13:47  loss: 4.2855 (3.0472)  acc1: 0.0000 (26.8714)  acc5: 16.6667 (66.7295)  time: 0.1525 (0.1419 -- 0.1694)  data: 0.0012 (0.0006 -- 0.0047)  max mem: 16413
Test:  [3460/8869]  eta: 0:13:46  loss: 4.8099 (3.0510)  acc1: 0.0000 (26.8131)  acc5: 16.6667 (66.6281)  time: 0.1541 (0.1373 -- 0.1694)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [3470/8869]  eta: 0:13:44  loss: 4.5960 (3.0545)  acc1: 0.0000 (26.7406)  acc5: 16.6667 (66.5370)  time: 0.1491 (0.1373 -- 0.1654)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [3480/8869]  eta: 0:13:43  loss: 4.4167 (3.0575)  acc1: 0.0000 (26.6734)  acc5: 33.3333 (66.4368)  time: 0.1500 (0.1410 -- 0.1619)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [3490/8869]  eta: 0:13:41  loss: 4.1475 (3.0591)  acc1: 0.0000 (26.6543)  acc5: 33.3333 (66.3754)  time: 0.1500 (0.1410 -- 0.1619)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [3500/8869]  eta: 0:13:39  loss: 4.0143 (3.0619)  acc1: 0.0000 (26.6019)  acc5: 33.3333 (66.2858)  time: 0.1496 (0.1410 -- 0.1659)  data: 0.0012 (0.0004 -- 0.0061)  max mem: 16413
Test:  [3510/8869]  eta: 0:13:38  loss: 4.3289 (3.0658)  acc1: 0.0000 (26.5309)  acc5: 16.6667 (66.1587)  time: 0.1523 (0.1397 -- 0.1659)  data: 0.0013 (0.0005 -- 0.0061)  max mem: 16413
Test:  [3520/8869]  eta: 0:13:36  loss: 4.3289 (3.0685)  acc1: 0.0000 (26.4698)  acc5: 16.6667 (66.0608)  time: 0.1513 (0.1397 -- 0.1622)  data: 0.0010 (0.0005 -- 0.0018)  max mem: 16413
Test:  [3530/8869]  eta: 0:13:35  loss: 4.1006 (3.0714)  acc1: 0.0000 (26.4090)  acc5: 33.3333 (65.9587)  time: 0.1525 (0.1437 -- 0.1657)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [3540/8869]  eta: 0:13:33  loss: 4.3210 (3.0751)  acc1: 0.0000 (26.3344)  acc5: 33.3333 (65.8759)  time: 0.1503 (0.1352 -- 0.1657)  data: 0.0012 (0.0005 -- 0.0021)  max mem: 16413
Test:  [3550/8869]  eta: 0:13:32  loss: 4.4180 (3.0773)  acc1: 0.0000 (26.3071)  acc5: 33.3333 (65.7937)  time: 0.1491 (0.1352 -- 0.1699)  data: 0.0015 (0.0005 -- 0.0081)  max mem: 16413
Test:  [3560/8869]  eta: 0:13:30  loss: 2.7779 (3.0752)  acc1: 16.6667 (26.3737)  acc5: 66.6667 (65.8476)  time: 0.1511 (0.1376 -- 0.1699)  data: 0.0014 (0.0005 -- 0.0081)  max mem: 16413
Test:  [3570/8869]  eta: 0:13:29  loss: 2.5456 (3.0738)  acc1: 33.3333 (26.3838)  acc5: 83.3333 (65.8639)  time: 0.1528 (0.1403 -- 0.1656)  data: 0.0012 (0.0005 -- 0.0048)  max mem: 16413
Test:  [3580/8869]  eta: 0:13:27  loss: 2.5456 (3.0713)  acc1: 33.3333 (26.4498)  acc5: 66.6667 (65.8661)  time: 0.1509 (0.1372 -- 0.1656)  data: 0.0012 (0.0003 -- 0.0048)  max mem: 16413
Test:  [3590/8869]  eta: 0:13:26  loss: 2.2716 (3.0710)  acc1: 33.3333 (26.4643)  acc5: 66.6667 (65.8498)  time: 0.1639 (0.1208 -- 0.5271)  data: 0.0206 (0.0001 -- 0.3990)  max mem: 16413
Test:  [3600/8869]  eta: 0:13:24  loss: 2.4987 (3.0698)  acc1: 33.3333 (26.4926)  acc5: 50.0000 (65.8336)  time: 0.1651 (0.1208 -- 0.5271)  data: 0.0205 (0.0001 -- 0.3990)  max mem: 16413
Test:  [3610/8869]  eta: 0:13:23  loss: 2.7085 (3.0702)  acc1: 33.3333 (26.4931)  acc5: 66.6667 (65.8497)  time: 0.1495 (0.1367 -- 0.1699)  data: 0.0009 (0.0005 -- 0.0024)  max mem: 16413
Test:  [3620/8869]  eta: 0:13:21  loss: 2.9674 (3.0715)  acc1: 0.0000 (26.4476)  acc5: 66.6667 (65.8704)  time: 0.1519 (0.1370 -- 0.1713)  data: 0.0011 (0.0005 -- 0.0024)  max mem: 16413
Test:  [3630/8869]  eta: 0:13:20  loss: 3.3332 (3.0720)  acc1: 0.0000 (26.4069)  acc5: 83.3333 (65.9139)  time: 0.1521 (0.1376 -- 0.1713)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [3640/8869]  eta: 0:13:18  loss: 2.9054 (3.0701)  acc1: 16.6667 (26.4534)  acc5: 83.3333 (65.9251)  time: 0.1466 (0.1376 -- 0.1646)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Test:  [3650/8869]  eta: 0:13:16  loss: 2.9054 (3.0710)  acc1: 16.6667 (26.4220)  acc5: 50.0000 (65.8906)  time: 0.1467 (0.1396 -- 0.1687)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Test:  [3660/8869]  eta: 0:13:15  loss: 2.5207 (3.0694)  acc1: 33.3333 (26.4636)  acc5: 50.0000 (65.9110)  time: 0.1517 (0.1418 -- 0.1687)  data: 0.0009 (0.0004 -- 0.0013)  max mem: 16413
Test:  [3670/8869]  eta: 0:13:13  loss: 2.3515 (3.0685)  acc1: 33.3333 (26.4687)  acc5: 83.3333 (65.9720)  time: 0.1535 (0.1418 -- 0.1855)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [3680/8869]  eta: 0:13:12  loss: 2.3336 (3.0668)  acc1: 33.3333 (26.5010)  acc5: 100.0000 (66.0373)  time: 0.1526 (0.1421 -- 0.1855)  data: 0.0011 (0.0003 -- 0.0034)  max mem: 16413
Test:  [3690/8869]  eta: 0:13:10  loss: 1.7218 (3.0644)  acc1: 50.0000 (26.5375)  acc5: 100.0000 (66.1067)  time: 0.1527 (0.1421 -- 0.1724)  data: 0.0017 (0.0003 -- 0.0102)  max mem: 16413
Test:  [3700/8869]  eta: 0:13:09  loss: 1.9667 (3.0612)  acc1: 50.0000 (26.5964)  acc5: 100.0000 (66.1803)  time: 0.1508 (0.1397 -- 0.1724)  data: 0.0015 (0.0006 -- 0.0102)  max mem: 16413
Test:  [3710/8869]  eta: 0:13:07  loss: 2.1574 (3.0600)  acc1: 50.0000 (26.6191)  acc5: 100.0000 (66.2400)  time: 0.1471 (0.1397 -- 0.1648)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Test:  [3720/8869]  eta: 0:13:05  loss: 2.4438 (3.0581)  acc1: 33.3333 (26.6729)  acc5: 83.3333 (66.2859)  time: 0.1457 (0.1349 -- 0.1611)  data: 0.0009 (0.0003 -- 0.0024)  max mem: 16413
Test:  [3730/8869]  eta: 0:13:04  loss: 1.1877 (3.0546)  acc1: 50.0000 (26.7533)  acc5: 100.0000 (66.3495)  time: 0.1444 (0.1349 -- 0.1611)  data: 0.0009 (0.0004 -- 0.0024)  max mem: 16413
Test:  [3740/8869]  eta: 0:13:02  loss: 0.8534 (3.0508)  acc1: 66.6667 (26.8377)  acc5: 100.0000 (66.4350)  time: 0.1447 (0.1355 -- 0.1530)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [3750/8869]  eta: 0:13:01  loss: 2.0326 (3.0497)  acc1: 33.3333 (26.8373)  acc5: 100.0000 (66.4845)  time: 0.1498 (0.1383 -- 0.1673)  data: 0.0010 (0.0004 -- 0.0020)  max mem: 16413
Test:  [3760/8869]  eta: 0:12:59  loss: 2.4641 (3.0464)  acc1: 33.3333 (26.8989)  acc5: 100.0000 (66.5692)  time: 0.1528 (0.1410 -- 0.1673)  data: 0.0011 (0.0006 -- 0.0022)  max mem: 16413
Test:  [3770/8869]  eta: 0:12:58  loss: 1.7468 (3.0453)  acc1: 50.0000 (26.9071)  acc5: 100.0000 (66.6181)  time: 0.1575 (0.1410 -- 0.1761)  data: 0.0011 (0.0006 -- 0.0022)  max mem: 16413
Test:  [3780/8869]  eta: 0:12:56  loss: 2.3807 (3.0421)  acc1: 50.0000 (26.9858)  acc5: 83.3333 (66.6755)  time: 0.1561 (0.1321 -- 0.1761)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [3790/8869]  eta: 0:12:55  loss: 2.4195 (3.0415)  acc1: 33.3333 (26.9850)  acc5: 83.3333 (66.7062)  time: 0.1482 (0.1321 -- 0.1604)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [3800/8869]  eta: 0:12:53  loss: 2.4195 (3.0400)  acc1: 33.3333 (27.0280)  acc5: 83.3333 (66.7149)  time: 0.1460 (0.1368 -- 0.1602)  data: 0.0011 (0.0005 -- 0.0026)  max mem: 16413
Test:  [3810/8869]  eta: 0:12:51  loss: 2.4299 (3.0400)  acc1: 33.3333 (27.0270)  acc5: 66.6667 (66.7235)  time: 0.1468 (0.1368 -- 0.1605)  data: 0.0010 (0.0003 -- 0.0026)  max mem: 16413
Test:  [3820/8869]  eta: 0:12:50  loss: 2.2514 (3.0364)  acc1: 33.3333 (27.1090)  acc5: 83.3333 (66.7757)  time: 0.1490 (0.1392 -- 0.1605)  data: 0.0008 (0.0003 -- 0.0014)  max mem: 16413
Test:  [3830/8869]  eta: 0:12:48  loss: 2.8242 (3.0385)  acc1: 16.6667 (27.0643)  acc5: 83.3333 (66.7841)  time: 0.1485 (0.1392 -- 0.1599)  data: 0.0008 (0.0004 -- 0.0014)  max mem: 16413
Test:  [3840/8869]  eta: 0:12:47  loss: 3.5020 (3.0373)  acc1: 0.0000 (27.0763)  acc5: 83.3333 (66.8185)  time: 0.1472 (0.1347 -- 0.1718)  data: 0.0014 (0.0005 -- 0.0099)  max mem: 16413
Test:  [3850/8869]  eta: 0:12:45  loss: 1.8837 (3.0351)  acc1: 33.3333 (27.1185)  acc5: 100.0000 (66.8831)  time: 0.1480 (0.1347 -- 0.1718)  data: 0.0014 (0.0005 -- 0.0099)  max mem: 16413
Test:  [3860/8869]  eta: 0:12:44  loss: 1.8837 (3.0338)  acc1: 33.3333 (27.1432)  acc5: 83.3333 (66.9084)  time: 0.1508 (0.1400 -- 0.1705)  data: 0.0011 (0.0005 -- 0.0038)  max mem: 16413
Test:  [3870/8869]  eta: 0:12:42  loss: 2.6845 (3.0330)  acc1: 16.6667 (27.1592)  acc5: 66.6667 (66.9207)  time: 0.1517 (0.1421 -- 0.1667)  data: 0.0011 (0.0002 -- 0.0038)  max mem: 16413
Test:  [3880/8869]  eta: 0:12:40  loss: 1.9824 (3.0304)  acc1: 33.3333 (27.1923)  acc5: 83.3333 (66.9673)  time: 0.1499 (0.1393 -- 0.1667)  data: 0.0009 (0.0002 -- 0.0015)  max mem: 16413
Test:  [3890/8869]  eta: 0:12:39  loss: 3.0552 (3.0325)  acc1: 0.0000 (27.1310)  acc5: 83.3333 (67.0136)  time: 0.1493 (0.1329 -- 0.1778)  data: 0.0009 (0.0002 -- 0.0017)  max mem: 16413
Test:  [3900/8869]  eta: 0:12:37  loss: 3.3790 (3.0327)  acc1: 0.0000 (27.1213)  acc5: 83.3333 (67.0256)  time: 0.1464 (0.1329 -- 0.1778)  data: 0.0008 (0.0002 -- 0.0017)  max mem: 16413
Test:  [3910/8869]  eta: 0:12:36  loss: 3.3790 (3.0339)  acc1: 0.0000 (27.0860)  acc5: 66.6667 (67.0246)  time: 0.1442 (0.1344 -- 0.1583)  data: 0.0007 (0.0002 -- 0.0012)  max mem: 16413
Test:  [3920/8869]  eta: 0:12:34  loss: 3.4255 (3.0350)  acc1: 0.0000 (27.0764)  acc5: 66.6667 (67.0025)  time: 0.1458 (0.1321 -- 0.1593)  data: 0.0008 (0.0003 -- 0.0013)  max mem: 16413
Test:  [3930/8869]  eta: 0:12:32  loss: 3.4769 (3.0357)  acc1: 0.0000 (27.0754)  acc5: 66.6667 (67.0016)  time: 0.1479 (0.1321 -- 0.1637)  data: 0.0010 (0.0006 -- 0.0016)  max mem: 16413
Test:  [3940/8869]  eta: 0:12:31  loss: 4.6380 (3.0393)  acc1: 0.0000 (27.0151)  acc5: 50.0000 (66.9373)  time: 0.1500 (0.1309 -- 0.2068)  data: 0.0044 (0.0003 -- 0.0722)  max mem: 16413
Test:  [3950/8869]  eta: 0:12:29  loss: 4.1064 (3.0386)  acc1: 0.0000 (27.0227)  acc5: 33.3333 (66.9324)  time: 0.1474 (0.1309 -- 0.2068)  data: 0.0042 (0.0003 -- 0.0722)  max mem: 16413
Test:  [3960/8869]  eta: 0:12:28  loss: 3.3601 (3.0399)  acc1: 16.6667 (27.0050)  acc5: 50.0000 (66.8855)  time: 0.1499 (0.1372 -- 0.1827)  data: 0.0011 (0.0003 -- 0.0076)  max mem: 16413
Test:  [3970/8869]  eta: 0:12:26  loss: 3.3610 (3.0398)  acc1: 0.0000 (26.9999)  acc5: 50.0000 (66.9143)  time: 0.1552 (0.1409 -- 0.1827)  data: 0.0013 (0.0003 -- 0.0076)  max mem: 16413
Test:  [3980/8869]  eta: 0:12:25  loss: 3.7025 (3.0411)  acc1: 0.0000 (26.9530)  acc5: 83.3333 (66.9388)  time: 0.1519 (0.1405 -- 0.1703)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16413
Test:  [3990/8869]  eta: 0:12:23  loss: 3.9600 (3.0431)  acc1: 0.0000 (26.9105)  acc5: 66.6667 (66.8964)  time: 0.1489 (0.1405 -- 0.1639)  data: 0.0010 (0.0004 -- 0.0022)  max mem: 16413
Test:  [4000/8869]  eta: 0:12:22  loss: 4.2109 (3.0467)  acc1: 0.0000 (26.8475)  acc5: 33.3333 (66.8000)  time: 0.1535 (0.1405 -- 0.1717)  data: 0.0010 (0.0004 -- 0.0021)  max mem: 16413
Test:  [4010/8869]  eta: 0:12:20  loss: 3.2920 (3.0438)  acc1: 0.0000 (26.8886)  acc5: 66.6667 (66.8578)  time: 0.1528 (0.1357 -- 0.1717)  data: 0.0010 (0.0002 -- 0.0021)  max mem: 16413
Test:  [4020/8869]  eta: 0:12:19  loss: 3.0815 (3.0450)  acc1: 16.6667 (26.8714)  acc5: 66.6667 (66.8283)  time: 0.1496 (0.1357 -- 0.1697)  data: 0.0010 (0.0002 -- 0.0014)  max mem: 16413
Test:  [4030/8869]  eta: 0:12:17  loss: 4.1891 (3.0480)  acc1: 0.0000 (26.8089)  acc5: 33.3333 (66.7494)  time: 0.1502 (0.1378 -- 0.1688)  data: 0.0010 (0.0006 -- 0.0016)  max mem: 16413
Test:  [4040/8869]  eta: 0:12:16  loss: 4.1471 (3.0506)  acc1: 0.0000 (26.7549)  acc5: 33.3333 (66.6667)  time: 0.1527 (0.1397 -- 0.1750)  data: 0.0011 (0.0003 -- 0.0025)  max mem: 16413
Test:  [4050/8869]  eta: 0:12:14  loss: 4.3730 (3.0538)  acc1: 0.0000 (26.6930)  acc5: 33.3333 (66.5638)  time: 0.1542 (0.1397 -- 0.1750)  data: 0.0012 (0.0003 -- 0.0025)  max mem: 16413
Test:  [4060/8869]  eta: 0:12:12  loss: 4.5602 (3.0572)  acc1: 0.0000 (26.6273)  acc5: 16.6667 (66.4738)  time: 0.1510 (0.1365 -- 0.1667)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [4070/8869]  eta: 0:12:11  loss: 4.2721 (3.0601)  acc1: 0.0000 (26.5700)  acc5: 33.3333 (66.3760)  time: 0.1501 (0.1365 -- 0.1629)  data: 0.0010 (0.0006 -- 0.0014)  max mem: 16413
Test:  [4080/8869]  eta: 0:12:09  loss: 4.1807 (3.0611)  acc1: 0.0000 (26.5621)  acc5: 33.3333 (66.3236)  time: 0.1471 (0.1380 -- 0.1597)  data: 0.0008 (0.0004 -- 0.0012)  max mem: 16413
Test:  [4090/8869]  eta: 0:12:08  loss: 4.0316 (3.0636)  acc1: 0.0000 (26.5053)  acc5: 33.3333 (66.2511)  time: 0.1717 (0.1167 -- 0.7434)  data: 0.0317 (0.0001 -- 0.6203)  max mem: 16413
Test:  [4100/8869]  eta: 0:12:07  loss: 4.2458 (3.0669)  acc1: 0.0000 (26.4407)  acc5: 16.6667 (66.1465)  time: 0.1746 (0.1167 -- 0.7434)  data: 0.0318 (0.0001 -- 0.6203)  max mem: 16413
Test:  [4110/8869]  eta: 0:12:05  loss: 4.5029 (3.0702)  acc1: 0.0000 (26.3764)  acc5: 16.6667 (66.0383)  time: 0.1506 (0.1397 -- 0.1800)  data: 0.0010 (0.0004 -- 0.0031)  max mem: 16413
Test:  [4120/8869]  eta: 0:12:04  loss: 4.2770 (3.0728)  acc1: 0.0000 (26.3205)  acc5: 16.6667 (65.9427)  time: 0.1541 (0.1397 -- 0.1800)  data: 0.0012 (0.0004 -- 0.0046)  max mem: 16413
Test:  [4130/8869]  eta: 0:12:02  loss: 4.2946 (3.0756)  acc1: 0.0000 (26.2689)  acc5: 16.6667 (65.8719)  time: 0.1522 (0.1352 -- 0.1761)  data: 0.0012 (0.0004 -- 0.0046)  max mem: 16413
Test:  [4140/8869]  eta: 0:12:01  loss: 4.2946 (3.0769)  acc1: 0.0000 (26.2577)  acc5: 33.3333 (65.8174)  time: 0.1476 (0.1352 -- 0.1683)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [4150/8869]  eta: 0:11:59  loss: 4.0085 (3.0769)  acc1: 0.0000 (26.2708)  acc5: 50.0000 (65.8355)  time: 0.1496 (0.1355 -- 0.1683)  data: 0.0008 (0.0003 -- 0.0013)  max mem: 16413
Test:  [4160/8869]  eta: 0:11:58  loss: 2.5612 (3.0752)  acc1: 33.3333 (26.3078)  acc5: 100.0000 (65.8576)  time: 0.1551 (0.1408 -- 0.1754)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Test:  [4170/8869]  eta: 0:11:56  loss: 1.4163 (3.0719)  acc1: 50.0000 (26.3886)  acc5: 83.3333 (65.8955)  time: 0.1536 (0.1421 -- 0.1754)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Test:  [4180/8869]  eta: 0:11:54  loss: 2.1639 (3.0720)  acc1: 33.3333 (26.3892)  acc5: 66.6667 (65.8495)  time: 0.1476 (0.1367 -- 0.1642)  data: 0.0008 (0.0002 -- 0.0014)  max mem: 16413
Test:  [4190/8869]  eta: 0:11:53  loss: 3.1704 (3.0721)  acc1: 16.6667 (26.3859)  acc5: 50.0000 (65.8117)  time: 0.1466 (0.1337 -- 0.1642)  data: 0.0009 (0.0002 -- 0.0023)  max mem: 16413
Test:  [4200/8869]  eta: 0:11:51  loss: 2.9189 (3.0710)  acc1: 16.6667 (26.4143)  acc5: 66.6667 (65.8494)  time: 0.1505 (0.1337 -- 0.1695)  data: 0.0010 (0.0004 -- 0.0025)  max mem: 16413
Test:  [4210/8869]  eta: 0:11:50  loss: 3.4679 (3.0726)  acc1: 0.0000 (26.3714)  acc5: 83.3333 (65.8711)  time: 0.1487 (0.1330 -- 0.1695)  data: 0.0008 (0.0003 -- 0.0025)  max mem: 16413
Test:  [4220/8869]  eta: 0:11:48  loss: 3.5989 (3.0726)  acc1: 0.0000 (26.3366)  acc5: 83.3333 (65.9204)  time: 0.1529 (0.1330 -- 0.2921)  data: 0.0085 (0.0003 -- 0.1545)  max mem: 16413
Test:  [4230/8869]  eta: 0:11:47  loss: 2.7422 (3.0709)  acc1: 16.6667 (26.3649)  acc5: 83.3333 (65.9222)  time: 0.1629 (0.1205 -- 0.3375)  data: 0.0190 (0.0002 -- 0.2134)  max mem: 16413
Test:  [4240/8869]  eta: 0:11:45  loss: 2.6305 (3.0709)  acc1: 33.3333 (26.3617)  acc5: 66.6667 (65.9003)  time: 0.1515 (0.1205 -- 0.3375)  data: 0.0111 (0.0002 -- 0.2134)  max mem: 16413
Test:  [4250/8869]  eta: 0:11:44  loss: 2.6305 (3.0699)  acc1: 33.3333 (26.3859)  acc5: 66.6667 (65.9100)  time: 0.1450 (0.1246 -- 0.1771)  data: 0.0007 (0.0002 -- 0.0013)  max mem: 16413
Test:  [4260/8869]  eta: 0:11:42  loss: 2.4006 (3.0689)  acc1: 33.3333 (26.3866)  acc5: 83.3333 (65.9704)  time: 0.1488 (0.1298 -- 0.1771)  data: 0.0009 (0.0003 -- 0.0016)  max mem: 16413
Test:  [4270/8869]  eta: 0:11:40  loss: 2.4684 (3.0687)  acc1: 0.0000 (26.3756)  acc5: 100.0000 (66.0267)  time: 0.1472 (0.1298 -- 0.1645)  data: 0.0008 (0.0003 -- 0.0016)  max mem: 16413
Test:  [4280/8869]  eta: 0:11:39  loss: 2.0275 (3.0658)  acc1: 33.3333 (26.4230)  acc5: 100.0000 (66.0866)  time: 0.1506 (0.1414 -- 0.1677)  data: 0.0008 (0.0003 -- 0.0020)  max mem: 16413
Test:  [4290/8869]  eta: 0:11:37  loss: 1.3905 (3.0630)  acc1: 33.3333 (26.4779)  acc5: 100.0000 (66.1345)  time: 0.1525 (0.1334 -- 0.1688)  data: 0.0008 (0.0003 -- 0.0020)  max mem: 16413
Test:  [4300/8869]  eta: 0:11:36  loss: 2.2716 (3.0624)  acc1: 33.3333 (26.4861)  acc5: 100.0000 (66.1862)  time: 0.1498 (0.1334 -- 0.1688)  data: 0.0010 (0.0003 -- 0.0016)  max mem: 16413
Test:  [4310/8869]  eta: 0:11:34  loss: 2.2716 (3.0608)  acc1: 33.3333 (26.5174)  acc5: 83.3333 (66.2259)  time: 0.1494 (0.1377 -- 0.1627)  data: 0.0011 (0.0004 -- 0.0017)  max mem: 16413
Test:  [4320/8869]  eta: 0:11:33  loss: 1.4318 (3.0564)  acc1: 50.0000 (26.6142)  acc5: 100.0000 (66.3002)  time: 0.1510 (0.1377 -- 0.1642)  data: 0.0011 (0.0004 -- 0.0018)  max mem: 16413
Test:  [4330/8869]  eta: 0:11:31  loss: 0.9966 (3.0543)  acc1: 66.6667 (26.6451)  acc5: 100.0000 (66.3550)  time: 0.1517 (0.1408 -- 0.1642)  data: 0.0011 (0.0006 -- 0.0018)  max mem: 16413
Test:  [4340/8869]  eta: 0:11:30  loss: 2.3670 (3.0526)  acc1: 33.3333 (26.6567)  acc5: 100.0000 (66.4018)  time: 0.1533 (0.1416 -- 0.1702)  data: 0.0012 (0.0006 -- 0.0019)  max mem: 16413
Test:  [4350/8869]  eta: 0:11:29  loss: 1.9854 (3.0497)  acc1: 33.3333 (26.7180)  acc5: 100.0000 (66.4675)  time: 0.1749 (0.1169 -- 0.6384)  data: 0.0269 (0.0001 -- 0.5211)  max mem: 16413
Test:  [4360/8869]  eta: 0:11:27  loss: 2.1055 (3.0488)  acc1: 33.3333 (26.7217)  acc5: 100.0000 (66.5253)  time: 0.1686 (0.1169 -- 0.6384)  data: 0.0267 (0.0001 -- 0.5211)  max mem: 16413
Test:  [4370/8869]  eta: 0:11:26  loss: 2.1055 (3.0459)  acc1: 33.3333 (26.7940)  acc5: 100.0000 (66.5866)  time: 0.1459 (0.1305 -- 0.1593)  data: 0.0010 (0.0004 -- 0.0027)  max mem: 16413
Test:  [4380/8869]  eta: 0:11:24  loss: 1.2670 (3.0448)  acc1: 33.3333 (26.8089)  acc5: 100.0000 (66.6210)  time: 0.1515 (0.1384 -- 0.1700)  data: 0.0011 (0.0004 -- 0.0027)  max mem: 16413
Test:  [4390/8869]  eta: 0:11:22  loss: 2.4557 (3.0442)  acc1: 16.6667 (26.8314)  acc5: 83.3333 (66.6059)  time: 0.1526 (0.1424 -- 0.1700)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [4400/8869]  eta: 0:11:21  loss: 2.5927 (3.0438)  acc1: 33.3333 (26.8537)  acc5: 66.6667 (66.6061)  time: 0.1523 (0.1440 -- 0.1594)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [4410/8869]  eta: 0:11:19  loss: 2.2204 (3.0415)  acc1: 50.0000 (26.9024)  acc5: 83.3333 (66.6591)  time: 0.1524 (0.1398 -- 0.1741)  data: 0.0009 (0.0005 -- 0.0017)  max mem: 16413
Test:  [4420/8869]  eta: 0:11:18  loss: 2.7131 (3.0424)  acc1: 16.6667 (26.8831)  acc5: 100.0000 (66.6855)  time: 0.1532 (0.1398 -- 0.1741)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [4430/8869]  eta: 0:11:16  loss: 3.1878 (3.0417)  acc1: 0.0000 (26.8976)  acc5: 100.0000 (66.7306)  time: 0.1506 (0.1416 -- 0.1738)  data: 0.0009 (0.0004 -- 0.0013)  max mem: 16413
Test:  [4440/8869]  eta: 0:11:15  loss: 2.2815 (3.0400)  acc1: 16.6667 (26.9421)  acc5: 100.0000 (66.7793)  time: 0.1507 (0.1413 -- 0.1753)  data: 0.0009 (0.0005 -- 0.0012)  max mem: 16413
Test:  [4450/8869]  eta: 0:11:13  loss: 1.9176 (3.0386)  acc1: 33.3333 (26.9640)  acc5: 100.0000 (66.8015)  time: 0.1489 (0.1172 -- 0.1753)  data: 0.0008 (0.0001 -- 0.0012)  max mem: 16413
Test:  [4460/8869]  eta: 0:11:11  loss: 1.9618 (3.0375)  acc1: 33.3333 (26.9969)  acc5: 83.3333 (66.8385)  time: 0.1368 (0.1149 -- 0.1537)  data: 0.0006 (0.0001 -- 0.0011)  max mem: 16413
Test:  [4470/8869]  eta: 0:11:10  loss: 1.9618 (3.0354)  acc1: 33.3333 (27.0484)  acc5: 83.3333 (66.8791)  time: 0.1400 (0.1149 -- 0.1623)  data: 0.0006 (0.0001 -- 0.0016)  max mem: 16413
Test:  [4480/8869]  eta: 0:11:08  loss: 3.6048 (3.0371)  acc1: 0.0000 (27.0066)  acc5: 83.3333 (66.8936)  time: 0.1565 (0.1378 -- 0.1795)  data: 0.0010 (0.0003 -- 0.0035)  max mem: 16413
Test:  [4490/8869]  eta: 0:11:07  loss: 3.4111 (3.0370)  acc1: 0.0000 (27.0059)  acc5: 83.3333 (66.9227)  time: 0.1536 (0.1341 -- 0.1795)  data: 0.0010 (0.0003 -- 0.0035)  max mem: 16413
Test:  [4500/8869]  eta: 0:11:05  loss: 3.1623 (3.0377)  acc1: 16.6667 (26.9903)  acc5: 83.3333 (66.9185)  time: 0.1447 (0.1292 -- 0.1669)  data: 0.0007 (0.0003 -- 0.0015)  max mem: 16413
Test:  [4510/8869]  eta: 0:11:04  loss: 4.0373 (3.0388)  acc1: 0.0000 (26.9674)  acc5: 66.6667 (66.8994)  time: 0.1468 (0.1292 -- 0.1669)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Test:  [4520/8869]  eta: 0:11:02  loss: 4.0570 (3.0395)  acc1: 0.0000 (26.9631)  acc5: 66.6667 (66.8731)  time: 0.1503 (0.1348 -- 0.1654)  data: 0.0010 (0.0003 -- 0.0018)  max mem: 16413
Test:  [4530/8869]  eta: 0:11:01  loss: 4.2679 (3.0427)  acc1: 0.0000 (26.9146)  acc5: 33.3333 (66.8101)  time: 0.1548 (0.1463 -- 0.1715)  data: 0.0010 (0.0005 -- 0.0019)  max mem: 16413
Test:  [4540/8869]  eta: 0:10:59  loss: 4.5417 (3.0427)  acc1: 0.0000 (26.9214)  acc5: 50.0000 (66.8025)  time: 0.1604 (0.1266 -- 0.3676)  data: 0.0124 (0.0003 -- 0.2333)  max mem: 16413
Test:  [4550/8869]  eta: 0:10:58  loss: 2.9137 (3.0432)  acc1: 0.0000 (26.9208)  acc5: 50.0000 (66.7729)  time: 0.1522 (0.1266 -- 0.3676)  data: 0.0122 (0.0003 -- 0.2333)  max mem: 16413
Test:  [4560/8869]  eta: 0:10:56  loss: 2.9137 (3.0435)  acc1: 16.6667 (26.9166)  acc5: 50.0000 (66.7690)  time: 0.1447 (0.1325 -- 0.1663)  data: 0.0008 (0.0003 -- 0.0015)  max mem: 16413
Test:  [4570/8869]  eta: 0:10:55  loss: 3.7515 (3.0445)  acc1: 16.6667 (26.8978)  acc5: 66.6667 (66.7833)  time: 0.1498 (0.1387 -- 0.1663)  data: 0.0017 (0.0005 -- 0.0113)  max mem: 16413
Test:  [4580/8869]  eta: 0:10:53  loss: 4.0117 (3.0461)  acc1: 0.0000 (26.8682)  acc5: 66.6667 (66.7613)  time: 0.1511 (0.1422 -- 0.1595)  data: 0.0020 (0.0006 -- 0.0113)  max mem: 16413
Test:  [4590/8869]  eta: 0:10:52  loss: 4.2210 (3.0495)  acc1: 0.0000 (26.8133)  acc5: 50.0000 (66.6630)  time: 0.1529 (0.1422 -- 0.1691)  data: 0.0012 (0.0004 -- 0.0046)  max mem: 16413
Test:  [4600/8869]  eta: 0:10:50  loss: 4.0000 (3.0483)  acc1: 0.0000 (26.8347)  acc5: 50.0000 (66.7029)  time: 0.1531 (0.1395 -- 0.1701)  data: 0.0011 (0.0004 -- 0.0034)  max mem: 16413
Test:  [4610/8869]  eta: 0:10:48  loss: 2.4187 (3.0481)  acc1: 16.6667 (26.8488)  acc5: 83.3333 (66.7064)  time: 0.1494 (0.1384 -- 0.1701)  data: 0.0011 (0.0004 -- 0.0034)  max mem: 16413
Test:  [4620/8869]  eta: 0:10:47  loss: 4.3464 (3.0514)  acc1: 0.0000 (26.7907)  acc5: 33.3333 (66.6054)  time: 0.1504 (0.1294 -- 0.1725)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [4630/8869]  eta: 0:10:45  loss: 4.5137 (3.0539)  acc1: 0.0000 (26.7401)  acc5: 16.6667 (66.5299)  time: 0.1486 (0.1228 -- 0.2271)  data: 0.0057 (0.0001 -- 0.1009)  max mem: 16413
Test:  [4640/8869]  eta: 0:10:44  loss: 4.3177 (3.0564)  acc1: 0.0000 (26.7004)  acc5: 33.3333 (66.4512)  time: 0.1456 (0.1228 -- 0.2271)  data: 0.0056 (0.0001 -- 0.1009)  max mem: 16413
Test:  [4650/8869]  eta: 0:10:42  loss: 4.5163 (3.0595)  acc1: 0.0000 (26.6430)  acc5: 16.6667 (66.3728)  time: 0.1491 (0.1276 -- 0.1654)  data: 0.0008 (0.0005 -- 0.0014)  max mem: 16413
Test:  [4660/8869]  eta: 0:10:41  loss: 4.2409 (3.0617)  acc1: 0.0000 (26.5930)  acc5: 33.3333 (66.3091)  time: 0.1550 (0.1431 -- 0.1726)  data: 0.0010 (0.0007 -- 0.0014)  max mem: 16413
Test:  [4670/8869]  eta: 0:10:39  loss: 3.8949 (3.0624)  acc1: 0.0000 (26.5825)  acc5: 33.3333 (66.2599)  time: 0.1566 (0.1457 -- 0.1726)  data: 0.0014 (0.0006 -- 0.0070)  max mem: 16413
Test:  [4680/8869]  eta: 0:10:38  loss: 4.0895 (3.0652)  acc1: 0.0000 (26.5328)  acc5: 33.3333 (66.1824)  time: 0.1538 (0.1446 -- 0.1764)  data: 0.0014 (0.0005 -- 0.0070)  max mem: 16413
Test:  [4690/8869]  eta: 0:10:36  loss: 4.1877 (3.0676)  acc1: 0.0000 (26.4904)  acc5: 33.3333 (66.1018)  time: 0.1521 (0.1408 -- 0.1764)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [4700/8869]  eta: 0:10:35  loss: 4.0104 (3.0698)  acc1: 0.0000 (26.4447)  acc5: 33.3333 (66.0285)  time: 0.1518 (0.1408 -- 0.1750)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [4710/8869]  eta: 0:10:33  loss: 4.1077 (3.0723)  acc1: 0.0000 (26.3921)  acc5: 16.6667 (65.9343)  time: 0.1546 (0.1451 -- 0.1662)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [4720/8869]  eta: 0:10:32  loss: 4.4014 (3.0750)  acc1: 0.0000 (26.3433)  acc5: 16.6667 (65.8688)  time: 0.1533 (0.1417 -- 0.1662)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [4730/8869]  eta: 0:10:30  loss: 4.1006 (3.0760)  acc1: 0.0000 (26.3228)  acc5: 33.3333 (65.8388)  time: 0.1519 (0.1339 -- 0.1762)  data: 0.0011 (0.0005 -- 0.0017)  max mem: 16413
Test:  [4740/8869]  eta: 0:10:29  loss: 3.9749 (3.0769)  acc1: 0.0000 (26.3200)  acc5: 50.0000 (65.8265)  time: 0.1496 (0.1339 -- 0.1762)  data: 0.0009 (0.0005 -- 0.0019)  max mem: 16413
Test:  [4750/8869]  eta: 0:10:27  loss: 3.5020 (3.0752)  acc1: 16.6667 (26.3559)  acc5: 66.6667 (65.8458)  time: 0.1495 (0.1374 -- 0.1699)  data: 0.0014 (0.0006 -- 0.0109)  max mem: 16413
Test:  [4760/8869]  eta: 0:10:25  loss: 1.4923 (3.0720)  acc1: 50.0000 (26.4230)  acc5: 100.0000 (65.8825)  time: 0.1508 (0.1422 -- 0.1699)  data: 0.0014 (0.0005 -- 0.0109)  max mem: 16413
Test:  [4770/8869]  eta: 0:10:24  loss: 1.5438 (3.0719)  acc1: 33.3333 (26.4340)  acc5: 66.6667 (65.8597)  time: 0.1498 (0.1425 -- 0.1605)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [4780/8869]  eta: 0:10:23  loss: 3.2049 (3.0720)  acc1: 16.6667 (26.4345)  acc5: 50.0000 (65.8335)  time: 0.1687 (0.1176 -- 0.6391)  data: 0.0268 (0.0001 -- 0.5182)  max mem: 16413
Test:  [4790/8869]  eta: 0:10:21  loss: 3.1730 (3.0717)  acc1: 16.6667 (26.4489)  acc5: 66.6667 (65.8387)  time: 0.1681 (0.1176 -- 0.6391)  data: 0.0266 (0.0001 -- 0.5182)  max mem: 16413
Test:  [4800/8869]  eta: 0:10:20  loss: 3.3079 (3.0727)  acc1: 0.0000 (26.4181)  acc5: 66.6667 (65.8648)  time: 0.1513 (0.1382 -- 0.1708)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Test:  [4810/8869]  eta: 0:10:18  loss: 3.4046 (3.0730)  acc1: 0.0000 (26.3874)  acc5: 83.3333 (65.8907)  time: 0.1533 (0.1435 -- 0.1817)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [4820/8869]  eta: 0:10:17  loss: 2.9026 (3.0722)  acc1: 16.6667 (26.4053)  acc5: 66.6667 (65.8923)  time: 0.1511 (0.1435 -- 0.1817)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [4830/8869]  eta: 0:10:15  loss: 2.9026 (3.0721)  acc1: 16.6667 (26.4059)  acc5: 66.6667 (65.8835)  time: 0.1502 (0.1425 -- 0.1609)  data: 0.0015 (0.0004 -- 0.0118)  max mem: 16413
Test:  [4840/8869]  eta: 0:10:14  loss: 3.0307 (3.0713)  acc1: 16.6667 (26.4202)  acc5: 66.6667 (65.8851)  time: 0.1519 (0.1378 -- 0.1713)  data: 0.0016 (0.0005 -- 0.0118)  max mem: 16413
Test:  [4850/8869]  eta: 0:10:12  loss: 3.0221 (3.0704)  acc1: 16.6667 (26.4310)  acc5: 83.3333 (65.9280)  time: 0.1524 (0.1378 -- 0.1713)  data: 0.0011 (0.0007 -- 0.0027)  max mem: 16413
Test:  [4860/8869]  eta: 0:10:10  loss: 2.9145 (3.0702)  acc1: 16.6667 (26.4349)  acc5: 83.3333 (65.9707)  time: 0.1526 (0.1426 -- 0.1713)  data: 0.0011 (0.0007 -- 0.0027)  max mem: 16413
Test:  [4870/8869]  eta: 0:10:09  loss: 1.6218 (3.0669)  acc1: 33.3333 (26.4935)  acc5: 100.0000 (66.0302)  time: 0.1515 (0.1430 -- 0.1690)  data: 0.0011 (0.0007 -- 0.0027)  max mem: 16413
Test:  [4880/8869]  eta: 0:10:07  loss: 1.3513 (3.0656)  acc1: 50.0000 (26.5075)  acc5: 100.0000 (66.0828)  time: 0.1497 (0.1410 -- 0.1690)  data: 0.0011 (0.0007 -- 0.0016)  max mem: 16413
Test:  [4890/8869]  eta: 0:10:06  loss: 3.4235 (3.0647)  acc1: 16.6667 (26.5317)  acc5: 100.0000 (66.1317)  time: 0.1529 (0.1410 -- 0.1849)  data: 0.0017 (0.0004 -- 0.0106)  max mem: 16413
Test:  [4900/8869]  eta: 0:10:04  loss: 2.5441 (3.0634)  acc1: 33.3333 (26.5660)  acc5: 83.3333 (66.1634)  time: 0.1551 (0.1459 -- 0.1849)  data: 0.0017 (0.0004 -- 0.0106)  max mem: 16413
Test:  [4910/8869]  eta: 0:10:03  loss: 1.1184 (3.0593)  acc1: 66.6667 (26.6578)  acc5: 100.0000 (66.2289)  time: 0.1519 (0.1383 -- 0.1695)  data: 0.0012 (0.0006 -- 0.0018)  max mem: 16413
Test:  [4920/8869]  eta: 0:10:01  loss: 1.0400 (3.0581)  acc1: 66.6667 (26.6883)  acc5: 100.0000 (66.2738)  time: 0.1512 (0.1383 -- 0.1695)  data: 0.0010 (0.0005 -- 0.0018)  max mem: 16413
Test:  [4930/8869]  eta: 0:10:00  loss: 1.9160 (3.0558)  acc1: 33.3333 (26.7187)  acc5: 100.0000 (66.3185)  time: 0.1543 (0.1431 -- 0.1739)  data: 0.0011 (0.0005 -- 0.0015)  max mem: 16413
Test:  [4940/8869]  eta: 0:09:58  loss: 1.9160 (3.0537)  acc1: 50.0000 (26.7625)  acc5: 100.0000 (66.3800)  time: 0.1550 (0.1412 -- 0.1739)  data: 0.0012 (0.0007 -- 0.0015)  max mem: 16413
Test:  [4950/8869]  eta: 0:09:57  loss: 2.5441 (3.0532)  acc1: 33.3333 (26.7555)  acc5: 100.0000 (66.4243)  time: 0.1533 (0.1412 -- 0.1706)  data: 0.0010 (0.0002 -- 0.0014)  max mem: 16413
Test:  [4960/8869]  eta: 0:09:55  loss: 1.9868 (3.0512)  acc1: 33.3333 (26.7990)  acc5: 83.3333 (66.4651)  time: 0.1486 (0.1329 -- 0.1706)  data: 0.0008 (0.0002 -- 0.0014)  max mem: 16413
Test:  [4970/8869]  eta: 0:09:54  loss: 1.8719 (3.0495)  acc1: 50.0000 (26.8323)  acc5: 83.3333 (66.5057)  time: 0.1461 (0.1329 -- 0.1658)  data: 0.0007 (0.0004 -- 0.0012)  max mem: 16413
Test:  [4980/8869]  eta: 0:09:52  loss: 2.4468 (3.0493)  acc1: 33.3333 (26.8387)  acc5: 83.3333 (66.4927)  time: 0.1478 (0.1375 -- 0.1588)  data: 0.0014 (0.0004 -- 0.0113)  max mem: 16413
Test:  [4990/8869]  eta: 0:09:51  loss: 3.0109 (3.0489)  acc1: 16.6667 (26.8550)  acc5: 66.6667 (66.5030)  time: 0.1492 (0.1375 -- 0.1582)  data: 0.0015 (0.0004 -- 0.0113)  max mem: 16413
Test:  [5000/8869]  eta: 0:09:49  loss: 1.4838 (3.0471)  acc1: 33.3333 (26.8913)  acc5: 83.3333 (66.5367)  time: 0.1490 (0.1392 -- 0.1619)  data: 0.0010 (0.0005 -- 0.0022)  max mem: 16413
Test:  [5010/8869]  eta: 0:09:47  loss: 2.0669 (3.0465)  acc1: 33.3333 (26.9008)  acc5: 100.0000 (66.5602)  time: 0.1500 (0.1392 -- 0.1781)  data: 0.0011 (0.0005 -- 0.0027)  max mem: 16413
Test:  [5020/8869]  eta: 0:09:46  loss: 2.8540 (3.0468)  acc1: 16.6667 (26.8904)  acc5: 83.3333 (66.5804)  time: 0.1511 (0.1418 -- 0.1781)  data: 0.0011 (0.0004 -- 0.0027)  max mem: 16413
Test:  [5030/8869]  eta: 0:09:44  loss: 2.7118 (3.0446)  acc1: 33.3333 (26.9363)  acc5: 83.3333 (66.6335)  time: 0.1518 (0.1418 -- 0.1734)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [5040/8869]  eta: 0:09:43  loss: 1.9747 (3.0440)  acc1: 33.3333 (26.9457)  acc5: 83.3333 (66.6501)  time: 0.1532 (0.1420 -- 0.1734)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5050/8869]  eta: 0:09:41  loss: 3.0640 (3.0427)  acc1: 33.3333 (26.9749)  acc5: 66.6667 (66.6700)  time: 0.1503 (0.1420 -- 0.1615)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5060/8869]  eta: 0:09:40  loss: 1.6749 (3.0412)  acc1: 33.3333 (26.9907)  acc5: 100.0000 (66.6996)  time: 0.1534 (0.1384 -- 0.1741)  data: 0.0010 (0.0005 -- 0.0020)  max mem: 16413
Test:  [5070/8869]  eta: 0:09:38  loss: 3.0500 (3.0423)  acc1: 0.0000 (26.9539)  acc5: 100.0000 (66.7324)  time: 0.1561 (0.1312 -- 0.2759)  data: 0.0085 (0.0003 -- 0.1523)  max mem: 16413
Test:  [5080/8869]  eta: 0:09:37  loss: 3.1704 (3.0424)  acc1: 0.0000 (26.9402)  acc5: 83.3333 (66.7618)  time: 0.1510 (0.1312 -- 0.2759)  data: 0.0084 (0.0003 -- 0.1523)  max mem: 16413
Test:  [5090/8869]  eta: 0:09:35  loss: 3.0162 (3.0434)  acc1: 0.0000 (26.9168)  acc5: 66.6667 (66.7551)  time: 0.1517 (0.1394 -- 0.1640)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [5100/8869]  eta: 0:09:34  loss: 3.4694 (3.0441)  acc1: 0.0000 (26.9065)  acc5: 50.0000 (66.7418)  time: 0.1557 (0.1446 -- 0.1743)  data: 0.0011 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5110/8869]  eta: 0:09:32  loss: 3.7215 (3.0443)  acc1: 0.0000 (26.9125)  acc5: 66.6667 (66.7482)  time: 0.1529 (0.1414 -- 0.1743)  data: 0.0013 (0.0004 -- 0.0042)  max mem: 16413
Test:  [5120/8869]  eta: 0:09:31  loss: 4.3639 (3.0471)  acc1: 0.0000 (26.8665)  acc5: 50.0000 (66.6862)  time: 0.1519 (0.1414 -- 0.1667)  data: 0.0013 (0.0004 -- 0.0042)  max mem: 16413
Test:  [5130/8869]  eta: 0:09:29  loss: 4.5459 (3.0481)  acc1: 0.0000 (26.8466)  acc5: 33.3333 (66.6634)  time: 0.1528 (0.1449 -- 0.1723)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5140/8869]  eta: 0:09:28  loss: 3.1431 (3.0481)  acc1: 0.0000 (26.8495)  acc5: 50.0000 (66.6407)  time: 0.1531 (0.1449 -- 0.1723)  data: 0.0008 (0.0004 -- 0.0014)  max mem: 16413
Test:  [5150/8869]  eta: 0:09:26  loss: 2.8261 (3.0477)  acc1: 16.6667 (26.8556)  acc5: 66.6667 (66.6634)  time: 0.1535 (0.1473 -- 0.1680)  data: 0.0008 (0.0004 -- 0.0014)  max mem: 16413
Test:  [5160/8869]  eta: 0:09:25  loss: 3.1663 (3.0487)  acc1: 0.0000 (26.8165)  acc5: 83.3333 (66.6990)  time: 0.1513 (0.1418 -- 0.1729)  data: 0.0008 (0.0004 -- 0.0012)  max mem: 16413
Test:  [5170/8869]  eta: 0:09:23  loss: 3.8913 (3.0505)  acc1: 0.0000 (26.7743)  acc5: 50.0000 (66.6634)  time: 0.1513 (0.1418 -- 0.1730)  data: 0.0010 (0.0004 -- 0.0022)  max mem: 16413
Test:  [5180/8869]  eta: 0:09:22  loss: 4.1683 (3.0527)  acc1: 0.0000 (26.7387)  acc5: 33.3333 (66.6023)  time: 0.1543 (0.1437 -- 0.1730)  data: 0.0012 (0.0005 -- 0.0022)  max mem: 16413
Test:  [5190/8869]  eta: 0:09:20  loss: 4.0065 (3.0523)  acc1: 0.0000 (26.7322)  acc5: 33.3333 (66.6089)  time: 0.1533 (0.1368 -- 0.1730)  data: 0.0011 (0.0005 -- 0.0025)  max mem: 16413
Test:  [5200/8869]  eta: 0:09:19  loss: 2.7384 (3.0513)  acc1: 33.3333 (26.7577)  acc5: 83.3333 (66.6154)  time: 0.1499 (0.1368 -- 0.1674)  data: 0.0011 (0.0003 -- 0.0025)  max mem: 16413
Test:  [5210/8869]  eta: 0:09:17  loss: 4.1312 (3.0541)  acc1: 0.0000 (26.7063)  acc5: 50.0000 (66.5515)  time: 0.1486 (0.1384 -- 0.1622)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Test:  [5220/8869]  eta: 0:09:15  loss: 4.5465 (3.0561)  acc1: 0.0000 (26.6616)  acc5: 33.3333 (66.4911)  time: 0.1496 (0.1388 -- 0.1608)  data: 0.0010 (0.0005 -- 0.0049)  max mem: 16413
Test:  [5230/8869]  eta: 0:09:14  loss: 4.4069 (3.0585)  acc1: 0.0000 (26.6202)  acc5: 33.3333 (66.4150)  time: 0.1504 (0.1367 -- 0.1608)  data: 0.0010 (0.0005 -- 0.0049)  max mem: 16413
Test:  [5240/8869]  eta: 0:09:12  loss: 4.3145 (3.0608)  acc1: 0.0000 (26.5694)  acc5: 16.6667 (66.3455)  time: 0.1494 (0.1367 -- 0.1675)  data: 0.0010 (0.0006 -- 0.0017)  max mem: 16413
Test:  [5250/8869]  eta: 0:09:11  loss: 4.3145 (3.0629)  acc1: 0.0000 (26.5251)  acc5: 16.6667 (66.2763)  time: 0.1519 (0.1397 -- 0.1693)  data: 0.0016 (0.0006 -- 0.0101)  max mem: 16413
Test:  [5260/8869]  eta: 0:09:09  loss: 4.2920 (3.0639)  acc1: 0.0000 (26.5127)  acc5: 16.6667 (66.2327)  time: 0.1532 (0.1421 -- 0.1719)  data: 0.0016 (0.0004 -- 0.0101)  max mem: 16413
Test:  [5270/8869]  eta: 0:09:08  loss: 4.2513 (3.0664)  acc1: 0.0000 (26.4687)  acc5: 33.3333 (66.1576)  time: 0.1516 (0.1406 -- 0.1719)  data: 0.0010 (0.0004 -- 0.0021)  max mem: 16413
Test:  [5280/8869]  eta: 0:09:06  loss: 4.2620 (3.0683)  acc1: 0.0000 (26.4249)  acc5: 33.3333 (66.1017)  time: 0.1524 (0.1406 -- 0.1674)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [5290/8869]  eta: 0:09:05  loss: 4.2620 (3.0709)  acc1: 0.0000 (26.3750)  acc5: 16.6667 (66.0115)  time: 0.1818 (0.1157 -- 0.8805)  data: 0.0388 (0.0001 -- 0.7619)  max mem: 16413
Test:  [5300/8869]  eta: 0:09:04  loss: 4.3682 (3.0733)  acc1: 0.0000 (26.3284)  acc5: 16.6667 (65.9341)  time: 0.1766 (0.1157 -- 0.8805)  data: 0.0389 (0.0001 -- 0.7619)  max mem: 16413
Test:  [5310/8869]  eta: 0:09:02  loss: 4.3682 (3.0753)  acc1: 0.0000 (26.2913)  acc5: 16.6667 (65.8539)  time: 0.1462 (0.1356 -- 0.1563)  data: 0.0011 (0.0003 -- 0.0047)  max mem: 16413
Test:  [5320/8869]  eta: 0:09:00  loss: 4.2275 (3.0766)  acc1: 0.0000 (26.2670)  acc5: 33.3333 (65.8366)  time: 0.1523 (0.1415 -- 0.1648)  data: 0.0012 (0.0006 -- 0.0027)  max mem: 16413
Test:  [5330/8869]  eta: 0:08:59  loss: 3.6508 (3.0772)  acc1: 0.0000 (26.2677)  acc5: 50.0000 (65.8194)  time: 0.1554 (0.1450 -- 0.1757)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Test:  [5340/8869]  eta: 0:08:57  loss: 3.2511 (3.0762)  acc1: 16.6667 (26.2966)  acc5: 83.3333 (65.8366)  time: 0.1541 (0.1438 -- 0.1757)  data: 0.0011 (0.0004 -- 0.0017)  max mem: 16413
Test:  [5350/8869]  eta: 0:08:56  loss: 1.0547 (3.0728)  acc1: 50.0000 (26.3720)  acc5: 100.0000 (65.8911)  time: 0.1510 (0.1365 -- 0.1647)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [5360/8869]  eta: 0:08:54  loss: 1.7551 (3.0724)  acc1: 50.0000 (26.3850)  acc5: 83.3333 (65.8646)  time: 0.1513 (0.1365 -- 0.1584)  data: 0.0015 (0.0007 -- 0.0107)  max mem: 16413
Test:  [5370/8869]  eta: 0:08:53  loss: 3.6820 (3.0732)  acc1: 0.0000 (26.3700)  acc5: 33.3333 (65.8102)  time: 0.1526 (0.1436 -- 0.1650)  data: 0.0016 (0.0004 -- 0.0107)  max mem: 16413
Test:  [5380/8869]  eta: 0:08:51  loss: 2.9028 (3.0722)  acc1: 0.0000 (26.3861)  acc5: 66.6667 (65.8397)  time: 0.1514 (0.1436 -- 0.1650)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Test:  [5390/8869]  eta: 0:08:50  loss: 2.8057 (3.0732)  acc1: 16.6667 (26.3680)  acc5: 83.3333 (65.8690)  time: 0.1514 (0.1406 -- 0.1780)  data: 0.0012 (0.0004 -- 0.0040)  max mem: 16413
Test:  [5400/8869]  eta: 0:08:48  loss: 2.8107 (3.0730)  acc1: 0.0000 (26.3439)  acc5: 83.3333 (65.9045)  time: 0.1522 (0.1406 -- 0.1780)  data: 0.0012 (0.0004 -- 0.0040)  max mem: 16413
Test:  [5410/8869]  eta: 0:08:47  loss: 2.8107 (3.0726)  acc1: 16.6667 (26.3476)  acc5: 83.3333 (65.8997)  time: 0.1522 (0.1444 -- 0.1632)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [5420/8869]  eta: 0:08:45  loss: 2.7120 (3.0721)  acc1: 16.6667 (26.3512)  acc5: 50.0000 (65.8858)  time: 0.1551 (0.1444 -- 0.1769)  data: 0.0010 (0.0004 -- 0.0030)  max mem: 16413
Test:  [5430/8869]  eta: 0:08:44  loss: 2.7120 (3.0718)  acc1: 16.6667 (26.3610)  acc5: 50.0000 (65.8903)  time: 0.1572 (0.1451 -- 0.1769)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
Test:  [5440/8869]  eta: 0:08:42  loss: 2.8642 (3.0711)  acc1: 16.6667 (26.3616)  acc5: 83.3333 (65.9254)  time: 0.1527 (0.1342 -- 0.1695)  data: 0.0009 (0.0003 -- 0.0019)  max mem: 16413
Test:  [5450/8869]  eta: 0:08:41  loss: 3.1110 (3.0711)  acc1: 0.0000 (26.3468)  acc5: 83.3333 (65.9695)  time: 0.1464 (0.1342 -- 0.1665)  data: 0.0009 (0.0004 -- 0.0012)  max mem: 16413
Test:  [5460/8869]  eta: 0:08:39  loss: 1.6068 (3.0680)  acc1: 33.3333 (26.4054)  acc5: 100.0000 (66.0227)  time: 0.1443 (0.1381 -- 0.1536)  data: 0.0010 (0.0004 -- 0.0022)  max mem: 16413
Test:  [5470/8869]  eta: 0:08:38  loss: 1.6068 (3.0671)  acc1: 50.0000 (26.4120)  acc5: 100.0000 (66.0543)  time: 0.1487 (0.1402 -- 0.1671)  data: 0.0016 (0.0005 -- 0.0084)  max mem: 16413
Test:  [5480/8869]  eta: 0:08:36  loss: 2.8320 (3.0660)  acc1: 16.6667 (26.4368)  acc5: 100.0000 (66.0980)  time: 0.1520 (0.1413 -- 0.1767)  data: 0.0016 (0.0006 -- 0.0084)  max mem: 16413
Test:  [5490/8869]  eta: 0:08:34  loss: 2.1340 (3.0647)  acc1: 33.3333 (26.4676)  acc5: 83.3333 (66.1294)  time: 0.1516 (0.1413 -- 0.1767)  data: 0.0011 (0.0006 -- 0.0028)  max mem: 16413
Test:  [5500/8869]  eta: 0:08:33  loss: 1.4267 (3.0612)  acc1: 50.0000 (26.5376)  acc5: 100.0000 (66.1880)  time: 0.1524 (0.1423 -- 0.1715)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [5510/8869]  eta: 0:08:31  loss: 1.3593 (3.0601)  acc1: 50.0000 (26.5499)  acc5: 100.0000 (66.2282)  time: 0.1534 (0.1423 -- 0.1727)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [5520/8869]  eta: 0:08:30  loss: 1.6669 (3.0577)  acc1: 50.0000 (26.5924)  acc5: 100.0000 (66.2682)  time: 0.1535 (0.1410 -- 0.1727)  data: 0.0010 (0.0004 -- 0.0025)  max mem: 16413
Test:  [5530/8869]  eta: 0:08:28  loss: 1.6669 (3.0563)  acc1: 50.0000 (26.6076)  acc5: 100.0000 (66.3171)  time: 0.1506 (0.1410 -- 0.1683)  data: 0.0011 (0.0004 -- 0.0029)  max mem: 16413
Test:  [5540/8869]  eta: 0:08:27  loss: 2.4841 (3.0551)  acc1: 16.6667 (26.6258)  acc5: 100.0000 (66.3659)  time: 0.1503 (0.1428 -- 0.1629)  data: 0.0012 (0.0005 -- 0.0029)  max mem: 16413
Test:  [5550/8869]  eta: 0:08:25  loss: 2.4196 (3.0536)  acc1: 33.3333 (26.6649)  acc5: 100.0000 (66.4115)  time: 0.1522 (0.1451 -- 0.1629)  data: 0.0010 (0.0005 -- 0.0028)  max mem: 16413
Test:  [5560/8869]  eta: 0:08:24  loss: 2.3234 (3.0518)  acc1: 50.0000 (26.7038)  acc5: 100.0000 (66.4479)  time: 0.1522 (0.1458 -- 0.1628)  data: 0.0010 (0.0005 -- 0.0022)  max mem: 16413
Test:  [5570/8869]  eta: 0:08:22  loss: 2.5163 (3.0521)  acc1: 33.3333 (26.6948)  acc5: 83.3333 (66.4273)  time: 0.1536 (0.1437 -- 0.1735)  data: 0.0015 (0.0005 -- 0.0064)  max mem: 16413
Test:  [5580/8869]  eta: 0:08:21  loss: 2.1831 (3.0511)  acc1: 33.3333 (26.7276)  acc5: 50.0000 (66.4337)  time: 0.1521 (0.1437 -- 0.1735)  data: 0.0015 (0.0007 -- 0.0064)  max mem: 16413
Test:  [5590/8869]  eta: 0:08:19  loss: 1.6676 (3.0499)  acc1: 50.0000 (26.7543)  acc5: 100.0000 (66.4699)  time: 0.1512 (0.1440 -- 0.1697)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [5600/8869]  eta: 0:08:18  loss: 1.5606 (3.0491)  acc1: 50.0000 (26.7720)  acc5: 100.0000 (66.5090)  time: 0.1533 (0.1455 -- 0.1697)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [5610/8869]  eta: 0:08:16  loss: 2.7977 (3.0490)  acc1: 16.6667 (26.7718)  acc5: 100.0000 (66.5389)  time: 0.1543 (0.1472 -- 0.1671)  data: 0.0013 (0.0007 -- 0.0020)  max mem: 16413
Test:  [5620/8869]  eta: 0:08:15  loss: 2.6550 (3.0477)  acc1: 33.3333 (26.8102)  acc5: 83.3333 (66.5718)  time: 0.1511 (0.1254 -- 0.1671)  data: 0.0011 (0.0005 -- 0.0018)  max mem: 16413
Test:  [5630/8869]  eta: 0:08:13  loss: 2.6550 (3.0475)  acc1: 33.3333 (26.8040)  acc5: 83.3333 (66.5838)  time: 0.1358 (0.1147 -- 0.1662)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Test:  [5640/8869]  eta: 0:08:11  loss: 2.6964 (3.0463)  acc1: 33.3333 (26.8392)  acc5: 83.3333 (66.6135)  time: 0.1357 (0.1147 -- 0.1555)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Test:  [5650/8869]  eta: 0:08:10  loss: 2.1396 (3.0449)  acc1: 33.3333 (26.8684)  acc5: 83.3333 (66.6460)  time: 0.1479 (0.1391 -- 0.1603)  data: 0.0011 (0.0005 -- 0.0028)  max mem: 16413
Test:  [5660/8869]  eta: 0:08:08  loss: 1.9769 (3.0446)  acc1: 16.6667 (26.8739)  acc5: 100.0000 (66.6814)  time: 0.1516 (0.1389 -- 0.1747)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Test:  [5670/8869]  eta: 0:08:07  loss: 3.3122 (3.0450)  acc1: 0.0000 (26.8618)  acc5: 100.0000 (66.7049)  time: 0.1551 (0.1389 -- 0.1747)  data: 0.0011 (0.0002 -- 0.0024)  max mem: 16413
Test:  [5680/8869]  eta: 0:08:05  loss: 3.1816 (3.0453)  acc1: 0.0000 (26.8556)  acc5: 83.3333 (66.7019)  time: 0.1496 (0.1356 -- 0.1722)  data: 0.0009 (0.0002 -- 0.0019)  max mem: 16413
Test:  [5690/8869]  eta: 0:08:04  loss: 3.2316 (3.0461)  acc1: 16.6667 (26.8436)  acc5: 66.6667 (66.6901)  time: 0.1334 (0.1148 -- 0.1578)  data: 0.0006 (0.0001 -- 0.0013)  max mem: 16413
Test:  [5700/8869]  eta: 0:08:02  loss: 4.0091 (3.0474)  acc1: 0.0000 (26.8199)  acc5: 50.0000 (66.6637)  time: 0.1324 (0.1148 -- 0.1531)  data: 0.0006 (0.0001 -- 0.0013)  max mem: 16413
Test:  [5710/8869]  eta: 0:08:00  loss: 4.2653 (3.0489)  acc1: 0.0000 (26.8021)  acc5: 33.3333 (66.6171)  time: 0.1436 (0.1192 -- 0.1658)  data: 0.0008 (0.0002 -- 0.0014)  max mem: 16413
Test:  [5720/8869]  eta: 0:07:59  loss: 4.5592 (3.0501)  acc1: 0.0000 (26.7814)  acc5: 33.3333 (66.5968)  time: 0.1479 (0.1369 -- 0.1658)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [5730/8869]  eta: 0:07:57  loss: 3.6543 (3.0499)  acc1: 16.6667 (26.7958)  acc5: 50.0000 (66.5823)  time: 0.1496 (0.1418 -- 0.1643)  data: 0.0010 (0.0006 -- 0.0019)  max mem: 16413
Test:  [5740/8869]  eta: 0:07:56  loss: 2.6318 (3.0497)  acc1: 16.6667 (26.8014)  acc5: 66.6667 (66.5883)  time: 0.1497 (0.1365 -- 0.1658)  data: 0.0009 (0.0006 -- 0.0019)  max mem: 16413
Test:  [5750/8869]  eta: 0:07:54  loss: 3.5130 (3.0511)  acc1: 16.6667 (26.7722)  acc5: 83.3333 (66.5942)  time: 0.1509 (0.1365 -- 0.1694)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5760/8869]  eta: 0:07:53  loss: 4.0573 (3.0523)  acc1: 0.0000 (26.7488)  acc5: 66.6667 (66.5770)  time: 0.1519 (0.1422 -- 0.1694)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [5770/8869]  eta: 0:07:51  loss: 4.2064 (3.0542)  acc1: 0.0000 (26.7227)  acc5: 50.0000 (66.5136)  time: 0.1537 (0.1451 -- 0.1772)  data: 0.0010 (0.0004 -- 0.0014)  max mem: 16413
Test:  [5780/8869]  eta: 0:07:50  loss: 4.0648 (3.0547)  acc1: 0.0000 (26.7082)  acc5: 33.3333 (66.5110)  time: 0.1525 (0.1379 -- 0.1772)  data: 0.0010 (0.0007 -- 0.0014)  max mem: 16413
Test:  [5790/8869]  eta: 0:07:48  loss: 2.7542 (3.0533)  acc1: 33.3333 (26.7484)  acc5: 83.3333 (66.5429)  time: 0.1487 (0.1296 -- 0.1794)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Test:  [5800/8869]  eta: 0:07:47  loss: 4.0446 (3.0559)  acc1: 0.0000 (26.7052)  acc5: 50.0000 (66.4713)  time: 0.1395 (0.1181 -- 0.1794)  data: 0.0008 (0.0001 -- 0.0028)  max mem: 16413
Test:  [5810/8869]  eta: 0:07:45  loss: 4.5658 (3.0580)  acc1: 0.0000 (26.6621)  acc5: 16.6667 (66.4057)  time: 0.1280 (0.1149 -- 0.1574)  data: 0.0005 (0.0001 -- 0.0028)  max mem: 16413
Test:  [5820/8869]  eta: 0:07:43  loss: 4.4183 (3.0602)  acc1: 0.0000 (26.6277)  acc5: 16.6667 (66.3345)  time: 0.1364 (0.1149 -- 0.1730)  data: 0.0007 (0.0001 -- 0.0038)  max mem: 16413
Test:  [5830/8869]  eta: 0:07:42  loss: 4.5960 (3.0624)  acc1: 0.0000 (26.5878)  acc5: 16.6667 (66.2751)  time: 0.1503 (0.1355 -- 0.1730)  data: 0.0010 (0.0003 -- 0.0038)  max mem: 16413
Test:  [5840/8869]  eta: 0:07:40  loss: 4.4167 (3.0641)  acc1: 0.0000 (26.5480)  acc5: 33.3333 (66.2244)  time: 0.1501 (0.1405 -- 0.1649)  data: 0.0010 (0.0004 -- 0.0013)  max mem: 16413
Test:  [5850/8869]  eta: 0:07:39  loss: 4.0488 (3.0646)  acc1: 0.0000 (26.5396)  acc5: 33.3333 (66.1938)  time: 0.1505 (0.1426 -- 0.1693)  data: 0.0012 (0.0004 -- 0.0034)  max mem: 16413
Test:  [5860/8869]  eta: 0:07:37  loss: 4.4974 (3.0673)  acc1: 0.0000 (26.4943)  acc5: 16.6667 (66.1122)  time: 0.1531 (0.1426 -- 0.1694)  data: 0.0013 (0.0004 -- 0.0035)  max mem: 16413
Test:  [5870/8869]  eta: 0:07:36  loss: 4.3289 (3.0688)  acc1: 0.0000 (26.4662)  acc5: 16.6667 (66.0677)  time: 0.1549 (0.1442 -- 0.1858)  data: 0.0015 (0.0004 -- 0.0087)  max mem: 16413
Test:  [5880/8869]  eta: 0:07:34  loss: 4.2028 (3.0710)  acc1: 0.0000 (26.4241)  acc5: 16.6667 (65.9978)  time: 0.1554 (0.1407 -- 0.1858)  data: 0.0017 (0.0006 -- 0.0087)  max mem: 16413
Test:  [5890/8869]  eta: 0:07:33  loss: 4.4111 (3.0727)  acc1: 0.0000 (26.3849)  acc5: 16.6667 (65.9311)  time: 0.1545 (0.1407 -- 0.1723)  data: 0.0014 (0.0005 -- 0.0050)  max mem: 16413
Test:  [5900/8869]  eta: 0:07:31  loss: 4.3135 (3.0747)  acc1: 0.0000 (26.3486)  acc5: 33.3333 (65.8702)  time: 0.1574 (0.1431 -- 0.1723)  data: 0.0012 (0.0005 -- 0.0050)  max mem: 16413
Test:  [5910/8869]  eta: 0:07:30  loss: 4.3167 (3.0759)  acc1: 0.0000 (26.3210)  acc5: 33.3333 (65.8490)  time: 0.1590 (0.1431 -- 0.1778)  data: 0.0017 (0.0005 -- 0.0097)  max mem: 16413
Test:  [5920/8869]  eta: 0:07:28  loss: 4.3210 (3.0772)  acc1: 0.0000 (26.3103)  acc5: 50.0000 (65.8194)  time: 0.1567 (0.1448 -- 0.1778)  data: 0.0017 (0.0006 -- 0.0097)  max mem: 16413
Test:  [5930/8869]  eta: 0:07:27  loss: 2.7779 (3.0755)  acc1: 16.6667 (26.3559)  acc5: 50.0000 (65.8293)  time: 0.1540 (0.1326 -- 0.1772)  data: 0.0009 (0.0006 -- 0.0015)  max mem: 16413
Test:  [5940/8869]  eta: 0:07:25  loss: 1.8177 (3.0729)  acc1: 50.0000 (26.3985)  acc5: 100.0000 (65.8756)  time: 0.1466 (0.1324 -- 0.1772)  data: 0.0008 (0.0002 -- 0.0013)  max mem: 16413
Test:  [5950/8869]  eta: 0:07:24  loss: 1.8398 (3.0723)  acc1: 33.3333 (26.4157)  acc5: 100.0000 (65.8657)  time: 0.1438 (0.1324 -- 0.1528)  data: 0.0010 (0.0002 -- 0.0050)  max mem: 16413
Test:  [5960/8869]  eta: 0:07:22  loss: 2.9648 (3.0729)  acc1: 33.3333 (26.4106)  acc5: 50.0000 (65.8363)  time: 0.1493 (0.1347 -- 0.1630)  data: 0.0012 (0.0005 -- 0.0050)  max mem: 16413
Test:  [5970/8869]  eta: 0:07:21  loss: 2.7085 (3.0719)  acc1: 33.3333 (26.4333)  acc5: 50.0000 (65.8460)  time: 0.1505 (0.1430 -- 0.1630)  data: 0.0009 (0.0006 -- 0.0019)  max mem: 16413
Test:  [5980/8869]  eta: 0:07:19  loss: 3.9660 (3.0734)  acc1: 0.0000 (26.4031)  acc5: 66.6667 (65.8502)  time: 0.1482 (0.1405 -- 0.1583)  data: 0.0010 (0.0006 -- 0.0025)  max mem: 16413
Test:  [5990/8869]  eta: 0:07:18  loss: 3.3846 (3.0734)  acc1: 0.0000 (26.3840)  acc5: 83.3333 (65.8822)  time: 0.1492 (0.1405 -- 0.1688)  data: 0.0011 (0.0006 -- 0.0025)  max mem: 16413
Test:  [6000/8869]  eta: 0:07:16  loss: 2.8088 (3.0730)  acc1: 16.6667 (26.3900)  acc5: 83.3333 (65.8890)  time: 0.1520 (0.1427 -- 0.1688)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6010/8869]  eta: 0:07:14  loss: 2.9054 (3.0730)  acc1: 16.6667 (26.3877)  acc5: 66.6667 (65.8765)  time: 0.1515 (0.1431 -- 0.1683)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6020/8869]  eta: 0:07:13  loss: 2.8326 (3.0727)  acc1: 16.6667 (26.3882)  acc5: 50.0000 (65.8695)  time: 0.1522 (0.1402 -- 0.1669)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6030/8869]  eta: 0:07:11  loss: 2.3515 (3.0718)  acc1: 33.3333 (26.4052)  acc5: 83.3333 (65.9012)  time: 0.1530 (0.1402 -- 0.1669)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Test:  [6040/8869]  eta: 0:07:10  loss: 2.7708 (3.0720)  acc1: 33.3333 (26.3974)  acc5: 83.3333 (65.9300)  time: 0.1505 (0.1396 -- 0.1638)  data: 0.0011 (0.0005 -- 0.0023)  max mem: 16413
Test:  [6050/8869]  eta: 0:07:08  loss: 1.5495 (3.0692)  acc1: 50.0000 (26.4529)  acc5: 100.0000 (65.9836)  time: 0.1515 (0.1396 -- 0.1675)  data: 0.0011 (0.0007 -- 0.0018)  max mem: 16413
Test:  [6060/8869]  eta: 0:07:07  loss: 1.7218 (3.0687)  acc1: 50.0000 (26.4478)  acc5: 100.0000 (66.0205)  time: 0.1553 (0.1421 -- 0.1675)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [6070/8869]  eta: 0:07:05  loss: 2.4186 (3.0670)  acc1: 33.3333 (26.4866)  acc5: 100.0000 (66.0682)  time: 0.1548 (0.1429 -- 0.1691)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6080/8869]  eta: 0:07:04  loss: 2.2756 (3.0661)  acc1: 50.0000 (26.5115)  acc5: 83.3333 (66.0966)  time: 0.1520 (0.1421 -- 0.1691)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [6090/8869]  eta: 0:07:02  loss: 1.2534 (3.0633)  acc1: 50.0000 (26.5775)  acc5: 100.0000 (66.1386)  time: 0.1510 (0.1421 -- 0.1684)  data: 0.0009 (0.0003 -- 0.0013)  max mem: 16413
Test:  [6100/8869]  eta: 0:07:01  loss: 1.1877 (3.0626)  acc1: 50.0000 (26.5913)  acc5: 100.0000 (66.1749)  time: 0.1529 (0.1410 -- 0.1703)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Test:  [6110/8869]  eta: 0:06:59  loss: 1.0751 (3.0598)  acc1: 50.0000 (26.6459)  acc5: 100.0000 (66.2139)  time: 0.1527 (0.1410 -- 0.1703)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
Test:  [6120/8869]  eta: 0:06:58  loss: 1.2024 (3.0589)  acc1: 50.0000 (26.6569)  acc5: 100.0000 (66.2610)  time: 0.1507 (0.1419 -- 0.1612)  data: 0.0015 (0.0007 -- 0.0063)  max mem: 16413
Test:  [6130/8869]  eta: 0:06:56  loss: 2.5085 (3.0578)  acc1: 16.6667 (26.6705)  acc5: 100.0000 (66.3078)  time: 0.1524 (0.1419 -- 0.1644)  data: 0.0014 (0.0005 -- 0.0063)  max mem: 16413
Test:  [6140/8869]  eta: 0:06:55  loss: 2.5085 (3.0570)  acc1: 16.6667 (26.6840)  acc5: 83.3333 (66.3301)  time: 0.1931 (0.1174 -- 0.9830)  data: 0.0444 (0.0001 -- 0.8670)  max mem: 16413
Test:  [6150/8869]  eta: 0:06:53  loss: 2.3807 (3.0548)  acc1: 50.0000 (26.7355)  acc5: 83.3333 (66.3632)  time: 0.1876 (0.1174 -- 0.9830)  data: 0.0440 (0.0001 -- 0.8670)  max mem: 16413
Test:  [6160/8869]  eta: 0:06:52  loss: 2.2920 (3.0550)  acc1: 33.3333 (26.7219)  acc5: 83.3333 (66.3583)  time: 0.1478 (0.1243 -- 0.1667)  data: 0.0007 (0.0003 -- 0.0013)  max mem: 16413
Test:  [6170/8869]  eta: 0:06:50  loss: 2.6983 (3.0543)  acc1: 33.3333 (26.7515)  acc5: 66.6667 (66.3696)  time: 0.1495 (0.1393 -- 0.1667)  data: 0.0008 (0.0003 -- 0.0015)  max mem: 16413
Test:  [6180/8869]  eta: 0:06:49  loss: 2.6309 (3.0535)  acc1: 33.3333 (26.7675)  acc5: 83.3333 (66.3889)  time: 0.1467 (0.1366 -- 0.1623)  data: 0.0006 (0.0003 -- 0.0015)  max mem: 16413
Test:  [6190/8869]  eta: 0:06:47  loss: 2.2514 (3.0521)  acc1: 33.3333 (26.7970)  acc5: 83.3333 (66.4163)  time: 0.1457 (0.1359 -- 0.1623)  data: 0.0007 (0.0003 -- 0.0013)  max mem: 16413
Test:  [6200/8869]  eta: 0:06:46  loss: 2.2107 (3.0524)  acc1: 16.6667 (26.7779)  acc5: 100.0000 (66.4382)  time: 0.1451 (0.1359 -- 0.1535)  data: 0.0008 (0.0004 -- 0.0014)  max mem: 16413
Test:  [6210/8869]  eta: 0:06:44  loss: 2.7229 (3.0514)  acc1: 16.6667 (26.8046)  acc5: 83.3333 (66.4681)  time: 0.1502 (0.1407 -- 0.1843)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [6220/8869]  eta: 0:06:43  loss: 2.6845 (3.0510)  acc1: 33.3333 (26.8097)  acc5: 83.3333 (66.4818)  time: 0.1521 (0.1387 -- 0.1843)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [6230/8869]  eta: 0:06:41  loss: 2.6845 (3.0496)  acc1: 16.6667 (26.8336)  acc5: 83.3333 (66.5089)  time: 0.1473 (0.1387 -- 0.1613)  data: 0.0010 (0.0004 -- 0.0024)  max mem: 16413
Test:  [6240/8869]  eta: 0:06:40  loss: 2.5343 (3.0488)  acc1: 33.3333 (26.8440)  acc5: 83.3333 (66.5225)  time: 0.1499 (0.1387 -- 0.1635)  data: 0.0011 (0.0004 -- 0.0024)  max mem: 16413
Test:  [6250/8869]  eta: 0:06:38  loss: 2.5343 (3.0484)  acc1: 33.3333 (26.8437)  acc5: 83.3333 (66.5520)  time: 0.1844 (0.1175 -- 0.9372)  data: 0.0416 (0.0002 -- 0.8172)  max mem: 16413
Test:  [6260/8869]  eta: 0:06:37  loss: 3.0552 (3.0485)  acc1: 0.0000 (26.8248)  acc5: 100.0000 (66.5948)  time: 0.1808 (0.1175 -- 0.9372)  data: 0.0415 (0.0002 -- 0.8172)  max mem: 16413
Test:  [6270/8869]  eta: 0:06:35  loss: 3.1048 (3.0489)  acc1: 0.0000 (26.8139)  acc5: 83.3333 (66.5896)  time: 0.1461 (0.1342 -- 0.1651)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Test:  [6280/8869]  eta: 0:06:34  loss: 4.0771 (3.0501)  acc1: 0.0000 (26.7978)  acc5: 50.0000 (66.5711)  time: 0.1476 (0.1342 -- 0.1651)  data: 0.0010 (0.0004 -- 0.0014)  max mem: 16413
Test:  [6290/8869]  eta: 0:06:32  loss: 4.0983 (3.0513)  acc1: 0.0000 (26.7763)  acc5: 50.0000 (66.5633)  time: 0.1489 (0.1410 -- 0.1632)  data: 0.0010 (0.0006 -- 0.0015)  max mem: 16413
Test:  [6300/8869]  eta: 0:06:31  loss: 4.2907 (3.0519)  acc1: 0.0000 (26.7735)  acc5: 50.0000 (66.5503)  time: 0.1508 (0.1430 -- 0.1601)  data: 0.0011 (0.0006 -- 0.0023)  max mem: 16413
Test:  [6310/8869]  eta: 0:06:29  loss: 4.5814 (3.0530)  acc1: 0.0000 (26.7522)  acc5: 50.0000 (66.5214)  time: 0.1523 (0.1449 -- 0.1601)  data: 0.0011 (0.0006 -- 0.0023)  max mem: 16413
Test:  [6320/8869]  eta: 0:06:28  loss: 3.9458 (3.0531)  acc1: 0.0000 (26.7547)  acc5: 33.3333 (66.4979)  time: 0.1523 (0.1455 -- 0.1617)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6330/8869]  eta: 0:06:26  loss: 2.6024 (3.0524)  acc1: 16.6667 (26.7651)  acc5: 83.3333 (66.5192)  time: 0.1528 (0.1447 -- 0.1617)  data: 0.0011 (0.0004 -- 0.0038)  max mem: 16413
Test:  [6340/8869]  eta: 0:06:25  loss: 3.5542 (3.0540)  acc1: 0.0000 (26.7255)  acc5: 83.3333 (66.5326)  time: 0.1530 (0.1433 -- 0.1678)  data: 0.0012 (0.0005 -- 0.0038)  max mem: 16413
Test:  [6350/8869]  eta: 0:06:23  loss: 3.9899 (3.0549)  acc1: 0.0000 (26.6966)  acc5: 66.6667 (66.5171)  time: 0.1529 (0.1433 -- 0.1678)  data: 0.0011 (0.0005 -- 0.0019)  max mem: 16413
Test:  [6360/8869]  eta: 0:06:22  loss: 3.9899 (3.0565)  acc1: 0.0000 (26.6703)  acc5: 50.0000 (66.4675)  time: 0.1533 (0.1444 -- 0.1629)  data: 0.0012 (0.0006 -- 0.0019)  max mem: 16413
Test:  [6370/8869]  eta: 0:06:20  loss: 4.2041 (3.0569)  acc1: 0.0000 (26.6599)  acc5: 33.3333 (66.4574)  time: 0.1513 (0.1410 -- 0.1610)  data: 0.0012 (0.0003 -- 0.0030)  max mem: 16413
Test:  [6380/8869]  eta: 0:06:19  loss: 3.2002 (3.0557)  acc1: 0.0000 (26.6808)  acc5: 66.6667 (66.4786)  time: 0.1519 (0.1410 -- 0.1650)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
Test:  [6390/8869]  eta: 0:06:17  loss: 3.8249 (3.0578)  acc1: 0.0000 (26.6442)  acc5: 50.0000 (66.4320)  time: 0.1575 (0.1444 -- 0.1795)  data: 0.0018 (0.0007 -- 0.0127)  max mem: 16413
Test:  [6400/8869]  eta: 0:06:16  loss: 4.1471 (3.0593)  acc1: 0.0000 (26.6078)  acc5: 33.3333 (66.3907)  time: 0.1574 (0.1438 -- 0.1795)  data: 0.0017 (0.0006 -- 0.0127)  max mem: 16413
Test:  [6410/8869]  eta: 0:06:14  loss: 4.1471 (3.0612)  acc1: 0.0000 (26.5741)  acc5: 33.3333 (66.3287)  time: 0.1521 (0.1328 -- 0.1771)  data: 0.0010 (0.0004 -- 0.0020)  max mem: 16413
Test:  [6420/8869]  eta: 0:06:12  loss: 4.5602 (3.0633)  acc1: 0.0000 (26.5327)  acc5: 16.6667 (66.2592)  time: 0.1487 (0.1328 -- 0.1632)  data: 0.0008 (0.0002 -- 0.0020)  max mem: 16413
Test:  [6430/8869]  eta: 0:06:11  loss: 4.2721 (3.0649)  acc1: 0.0000 (26.4967)  acc5: 16.6667 (66.2131)  time: 0.1457 (0.1363 -- 0.1632)  data: 0.0007 (0.0002 -- 0.0015)  max mem: 16413
Test:  [6440/8869]  eta: 0:06:09  loss: 4.1006 (3.0655)  acc1: 0.0000 (26.4866)  acc5: 33.3333 (66.1828)  time: 0.1451 (0.1384 -- 0.1657)  data: 0.0010 (0.0002 -- 0.0031)  max mem: 16413
Test:  [6450/8869]  eta: 0:06:08  loss: 4.4570 (3.0680)  acc1: 0.0000 (26.4507)  acc5: 16.6667 (66.1034)  time: 0.1470 (0.1381 -- 0.1657)  data: 0.0014 (0.0002 -- 0.0085)  max mem: 16413
Test:  [6460/8869]  eta: 0:06:06  loss: 4.4570 (3.0693)  acc1: 0.0000 (26.4149)  acc5: 16.6667 (66.0759)  time: 0.1463 (0.1348 -- 0.1620)  data: 0.0013 (0.0002 -- 0.0085)  max mem: 16413
Test:  [6470/8869]  eta: 0:06:05  loss: 4.2059 (3.0716)  acc1: 0.0000 (26.3741)  acc5: 16.6667 (65.9893)  time: 0.1498 (0.1348 -- 0.1637)  data: 0.0011 (0.0005 -- 0.0014)  max mem: 16413
Test:  [6480/8869]  eta: 0:06:03  loss: 4.3665 (3.0734)  acc1: 0.0000 (26.3360)  acc5: 16.6667 (65.9286)  time: 0.1547 (0.1449 -- 0.1754)  data: 0.0012 (0.0005 -- 0.0031)  max mem: 16413
Test:  [6490/8869]  eta: 0:06:02  loss: 4.2770 (3.0749)  acc1: 0.0000 (26.3057)  acc5: 16.6667 (65.8733)  time: 0.1875 (0.1178 -- 0.9153)  data: 0.0408 (0.0001 -- 0.7970)  max mem: 16413
Test:  [6500/8869]  eta: 0:06:00  loss: 4.0983 (3.0759)  acc1: 0.0000 (26.2857)  acc5: 33.3333 (65.8514)  time: 0.1800 (0.1178 -- 0.9153)  data: 0.0405 (0.0001 -- 0.7970)  max mem: 16413
Test:  [6510/8869]  eta: 0:05:59  loss: 4.2581 (3.0772)  acc1: 0.0000 (26.2735)  acc5: 33.3333 (65.8194)  time: 0.1464 (0.1317 -- 0.1630)  data: 0.0008 (0.0002 -- 0.0014)  max mem: 16413
Test:  [6520/8869]  eta: 0:05:57  loss: 4.0934 (3.0765)  acc1: 0.0000 (26.2945)  acc5: 50.0000 (65.8232)  time: 0.1534 (0.1429 -- 0.1748)  data: 0.0011 (0.0003 -- 0.0034)  max mem: 16413
Test:  [6530/8869]  eta: 0:05:56  loss: 1.4163 (3.0742)  acc1: 50.0000 (26.3436)  acc5: 100.0000 (65.8705)  time: 0.1526 (0.1406 -- 0.1748)  data: 0.0011 (0.0005 -- 0.0034)  max mem: 16413
Test:  [6540/8869]  eta: 0:05:54  loss: 1.4163 (3.0732)  acc1: 50.0000 (26.3747)  acc5: 100.0000 (65.8640)  time: 0.1530 (0.1406 -- 0.1647)  data: 0.0010 (0.0005 -- 0.0019)  max mem: 16413
Test:  [6550/8869]  eta: 0:05:53  loss: 2.8888 (3.0735)  acc1: 16.6667 (26.3675)  acc5: 50.0000 (65.8296)  time: 0.1534 (0.1465 -- 0.1647)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [6560/8869]  eta: 0:05:51  loss: 2.3918 (3.0725)  acc1: 16.6667 (26.3832)  acc5: 66.6667 (65.8411)  time: 0.1533 (0.1418 -- 0.1702)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [6570/8869]  eta: 0:05:50  loss: 3.0894 (3.0739)  acc1: 0.0000 (26.3582)  acc5: 66.6667 (65.8550)  time: 0.1534 (0.1418 -- 0.1702)  data: 0.0010 (0.0004 -- 0.0020)  max mem: 16413
Test:  [6580/8869]  eta: 0:05:48  loss: 3.6965 (3.0738)  acc1: 0.0000 (26.3460)  acc5: 66.6667 (65.8790)  time: 0.1517 (0.1422 -- 0.1737)  data: 0.0011 (0.0004 -- 0.0027)  max mem: 16413
Test:  [6590/8869]  eta: 0:05:47  loss: 3.2244 (3.0732)  acc1: 16.6667 (26.3465)  acc5: 83.3333 (65.8979)  time: 0.1522 (0.1424 -- 0.1737)  data: 0.0011 (0.0004 -- 0.0027)  max mem: 16413
Test:  [6600/8869]  eta: 0:05:45  loss: 2.7422 (3.0730)  acc1: 16.6667 (26.3420)  acc5: 66.6667 (65.8840)  time: 0.1521 (0.1425 -- 0.1691)  data: 0.0012 (0.0007 -- 0.0019)  max mem: 16413
Test:  [6610/8869]  eta: 0:05:44  loss: 2.9626 (3.0730)  acc1: 16.6667 (26.3399)  acc5: 66.6667 (65.8751)  time: 0.1500 (0.1413 -- 0.1661)  data: 0.0010 (0.0005 -- 0.0019)  max mem: 16413
Test:  [6620/8869]  eta: 0:05:42  loss: 2.6174 (3.0720)  acc1: 33.3333 (26.3555)  acc5: 66.6667 (65.8989)  time: 0.1521 (0.1413 -- 0.1723)  data: 0.0009 (0.0004 -- 0.0013)  max mem: 16413
Test:  [6630/8869]  eta: 0:05:41  loss: 2.6174 (3.0722)  acc1: 33.3333 (26.3460)  acc5: 83.3333 (65.9327)  time: 0.1577 (0.1432 -- 0.1723)  data: 0.0013 (0.0004 -- 0.0032)  max mem: 16413
Test:  [6640/8869]  eta: 0:05:39  loss: 1.8512 (3.0700)  acc1: 33.3333 (26.3791)  acc5: 100.0000 (65.9815)  time: 0.1548 (0.1375 -- 0.1676)  data: 0.0013 (0.0005 -- 0.0032)  max mem: 16413
Test:  [6650/8869]  eta: 0:05:38  loss: 1.8987 (3.0697)  acc1: 33.3333 (26.3720)  acc5: 100.0000 (66.0026)  time: 0.1545 (0.1375 -- 0.1859)  data: 0.0012 (0.0005 -- 0.0024)  max mem: 16413
Test:  [6660/8869]  eta: 0:05:36  loss: 2.2044 (3.0676)  acc1: 33.3333 (26.4225)  acc5: 100.0000 (66.0411)  time: 0.1515 (0.1325 -- 0.1859)  data: 0.0010 (0.0004 -- 0.0024)  max mem: 16413
Test:  [6670/8869]  eta: 0:05:35  loss: 2.1437 (3.0671)  acc1: 50.0000 (26.4278)  acc5: 100.0000 (66.0746)  time: 0.1474 (0.1325 -- 0.1590)  data: 0.0007 (0.0004 -- 0.0014)  max mem: 16413
Test:  [6680/8869]  eta: 0:05:33  loss: 1.5526 (3.0645)  acc1: 50.0000 (26.4881)  acc5: 100.0000 (66.1129)  time: 0.1516 (0.1438 -- 0.1640)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [6690/8869]  eta: 0:05:31  loss: 1.5526 (3.0637)  acc1: 50.0000 (26.5008)  acc5: 100.0000 (66.1461)  time: 0.1542 (0.1450 -- 0.1687)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [6700/8869]  eta: 0:05:30  loss: 1.7950 (3.0613)  acc1: 33.3333 (26.5408)  acc5: 100.0000 (66.1866)  time: 0.1530 (0.1384 -- 0.1687)  data: 0.0012 (0.0004 -- 0.0019)  max mem: 16413
Test:  [6710/8869]  eta: 0:05:28  loss: 2.2169 (3.0606)  acc1: 33.3333 (26.5410)  acc5: 100.0000 (66.2196)  time: 0.1494 (0.1384 -- 0.1653)  data: 0.0009 (0.0003 -- 0.0019)  max mem: 16413
Test:  [6720/8869]  eta: 0:05:27  loss: 2.1055 (3.0588)  acc1: 33.3333 (26.5734)  acc5: 100.0000 (66.2674)  time: 0.1471 (0.1404 -- 0.1653)  data: 0.0007 (0.0003 -- 0.0012)  max mem: 16413
Test:  [6730/8869]  eta: 0:05:25  loss: 1.7003 (3.0581)  acc1: 33.3333 (26.5909)  acc5: 100.0000 (66.3002)  time: 0.1462 (0.1384 -- 0.1628)  data: 0.0008 (0.0003 -- 0.0026)  max mem: 16413
Test:  [6740/8869]  eta: 0:05:24  loss: 1.1798 (3.0557)  acc1: 50.0000 (26.6454)  acc5: 100.0000 (66.3354)  time: 0.1480 (0.1368 -- 0.1628)  data: 0.0011 (0.0004 -- 0.0026)  max mem: 16413
Test:  [6750/8869]  eta: 0:05:22  loss: 1.9150 (3.0563)  acc1: 33.3333 (26.6257)  acc5: 83.3333 (66.3235)  time: 0.1508 (0.1368 -- 0.1625)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [6760/8869]  eta: 0:05:21  loss: 2.8154 (3.0553)  acc1: 16.6667 (26.6603)  acc5: 66.6667 (66.3339)  time: 0.1524 (0.1437 -- 0.1679)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [6770/8869]  eta: 0:05:19  loss: 2.5927 (3.0554)  acc1: 33.3333 (26.6578)  acc5: 66.6667 (66.3467)  time: 0.1524 (0.1428 -- 0.1679)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [6780/8869]  eta: 0:05:18  loss: 2.4317 (3.0535)  acc1: 33.3333 (26.7021)  acc5: 100.0000 (66.3791)  time: 0.1539 (0.1428 -- 0.1704)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [6790/8869]  eta: 0:05:16  loss: 2.4317 (3.0544)  acc1: 16.6667 (26.6750)  acc5: 100.0000 (66.4041)  time: 0.1539 (0.1439 -- 0.1704)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [6800/8869]  eta: 0:05:15  loss: 3.1878 (3.0529)  acc1: 0.0000 (26.7191)  acc5: 100.0000 (66.4314)  time: 0.1515 (0.1417 -- 0.1646)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [6810/8869]  eta: 0:05:13  loss: 2.8328 (3.0530)  acc1: 16.6667 (26.7117)  acc5: 100.0000 (66.4440)  time: 0.1504 (0.1416 -- 0.1696)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [6820/8869]  eta: 0:05:12  loss: 1.9314 (3.0514)  acc1: 33.3333 (26.7507)  acc5: 100.0000 (66.4761)  time: 0.1517 (0.1416 -- 0.1696)  data: 0.0011 (0.0004 -- 0.0024)  max mem: 16413
Test:  [6830/8869]  eta: 0:05:10  loss: 1.9618 (3.0512)  acc1: 33.3333 (26.7530)  acc5: 83.3333 (66.4934)  time: 0.1508 (0.1411 -- 0.1640)  data: 0.0011 (0.0004 -- 0.0024)  max mem: 16413
Test:  [6840/8869]  eta: 0:05:09  loss: 2.0208 (3.0499)  acc1: 33.3333 (26.7870)  acc5: 83.3333 (66.5229)  time: 0.1523 (0.1411 -- 0.1718)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [6850/8869]  eta: 0:05:07  loss: 3.0608 (3.0504)  acc1: 16.6667 (26.7650)  acc5: 100.0000 (66.5499)  time: 0.1546 (0.1344 -- 0.1826)  data: 0.0008 (0.0005 -- 0.0014)  max mem: 16413
Test:  [6860/8869]  eta: 0:05:06  loss: 3.2026 (3.0504)  acc1: 0.0000 (26.7697)  acc5: 83.3333 (66.5549)  time: 0.1881 (0.1164 -- 1.0343)  data: 0.0464 (0.0002 -- 0.9142)  max mem: 16413
Test:  [6870/8869]  eta: 0:05:04  loss: 3.1637 (3.0510)  acc1: 16.6667 (26.7598)  acc5: 66.6667 (66.5502)  time: 0.1889 (0.1164 -- 1.0343)  data: 0.0465 (0.0002 -- 0.9142)  max mem: 16413
Test:  [6880/8869]  eta: 0:05:03  loss: 4.1055 (3.0523)  acc1: 0.0000 (26.7306)  acc5: 50.0000 (66.5189)  time: 0.1530 (0.1396 -- 0.1637)  data: 0.0010 (0.0004 -- 0.0021)  max mem: 16413
Test:  [6890/8869]  eta: 0:05:01  loss: 4.1834 (3.0529)  acc1: 0.0000 (26.7281)  acc5: 50.0000 (66.4974)  time: 0.1482 (0.1281 -- 0.1709)  data: 0.0008 (0.0004 -- 0.0021)  max mem: 16413
Test:  [6900/8869]  eta: 0:05:00  loss: 4.1439 (3.0540)  acc1: 0.0000 (26.7111)  acc5: 50.0000 (66.4807)  time: 0.1439 (0.1281 -- 0.1709)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [6910/8869]  eta: 0:04:58  loss: 4.1439 (3.0540)  acc1: 0.0000 (26.7183)  acc5: 50.0000 (66.4617)  time: 0.1473 (0.1386 -- 0.1646)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [6920/8869]  eta: 0:04:57  loss: 2.6348 (3.0541)  acc1: 33.3333 (26.7254)  acc5: 50.0000 (66.4620)  time: 0.1504 (0.1373 -- 0.1646)  data: 0.0011 (0.0005 -- 0.0030)  max mem: 16413
Test:  [6930/8869]  eta: 0:04:55  loss: 3.7515 (3.0551)  acc1: 0.0000 (26.6989)  acc5: 66.6667 (66.4695)  time: 0.1505 (0.1373 -- 0.1638)  data: 0.0013 (0.0005 -- 0.0030)  max mem: 16413
Test:  [6940/8869]  eta: 0:04:53  loss: 3.9440 (3.0560)  acc1: 0.0000 (26.6796)  acc5: 66.6667 (66.4578)  time: 0.1529 (0.1432 -- 0.1625)  data: 0.0017 (0.0004 -- 0.0115)  max mem: 16413
Test:  [6950/8869]  eta: 0:04:52  loss: 4.1161 (3.0574)  acc1: 0.0000 (26.6604)  acc5: 50.0000 (66.4125)  time: 0.1553 (0.1423 -- 0.1714)  data: 0.0015 (0.0003 -- 0.0115)  max mem: 16413
Test:  [6960/8869]  eta: 0:04:50  loss: 4.1161 (3.0584)  acc1: 0.0000 (26.6437)  acc5: 33.3333 (66.3889)  time: 0.1443 (0.1183 -- 0.1714)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Test:  [6970/8869]  eta: 0:04:49  loss: 3.1399 (3.0570)  acc1: 16.6667 (26.6796)  acc5: 83.3333 (66.4204)  time: 0.1278 (0.1149 -- 0.1426)  data: 0.0004 (0.0001 -- 0.0014)  max mem: 16413
Test:  [6980/8869]  eta: 0:04:47  loss: 3.8017 (3.0588)  acc1: 0.0000 (26.6485)  acc5: 66.6667 (66.3778)  time: 0.1350 (0.1149 -- 0.1531)  data: 0.0006 (0.0001 -- 0.0013)  max mem: 16413
Test:  [6990/8869]  eta: 0:04:46  loss: 4.4606 (3.0604)  acc1: 0.0000 (26.6128)  acc5: 33.3333 (66.3329)  time: 0.1483 (0.1400 -- 0.1574)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [7000/8869]  eta: 0:04:44  loss: 4.5156 (3.0626)  acc1: 0.0000 (26.5795)  acc5: 16.6667 (66.2643)  time: 0.1524 (0.1440 -- 0.1699)  data: 0.0012 (0.0006 -- 0.0020)  max mem: 16413
Test:  [7010/8869]  eta: 0:04:43  loss: 4.5866 (3.0643)  acc1: 0.0000 (26.5511)  acc5: 16.6667 (66.2150)  time: 0.1551 (0.1440 -- 0.1699)  data: 0.0017 (0.0006 -- 0.0105)  max mem: 16413
Test:  [7020/8869]  eta: 0:04:41  loss: 4.1535 (3.0659)  acc1: 0.0000 (26.5181)  acc5: 33.3333 (66.1682)  time: 0.1554 (0.1406 -- 0.1768)  data: 0.0015 (0.0004 -- 0.0105)  max mem: 16413
Test:  [7030/8869]  eta: 0:04:40  loss: 3.9326 (3.0665)  acc1: 0.0000 (26.5017)  acc5: 33.3333 (66.1404)  time: 0.1528 (0.1406 -- 0.1768)  data: 0.0010 (0.0004 -- 0.0028)  max mem: 16413
Test:  [7040/8869]  eta: 0:04:38  loss: 4.0439 (3.0682)  acc1: 0.0000 (26.4735)  acc5: 33.3333 (66.0867)  time: 0.1506 (0.1418 -- 0.1648)  data: 0.0012 (0.0005 -- 0.0028)  max mem: 16413
Test:  [7050/8869]  eta: 0:04:37  loss: 4.0895 (3.0694)  acc1: 0.0000 (26.4501)  acc5: 33.3333 (66.0450)  time: 0.1519 (0.1418 -- 0.1648)  data: 0.0011 (0.0005 -- 0.0026)  max mem: 16413
Test:  [7060/8869]  eta: 0:04:35  loss: 4.1877 (3.0716)  acc1: 0.0000 (26.4151)  acc5: 33.3333 (65.9798)  time: 0.1527 (0.1404 -- 0.1724)  data: 0.0009 (0.0005 -- 0.0020)  max mem: 16413
Test:  [7070/8869]  eta: 0:04:34  loss: 4.1877 (3.0729)  acc1: 0.0000 (26.3824)  acc5: 16.6667 (65.9266)  time: 0.1525 (0.1404 -- 0.1724)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [7080/8869]  eta: 0:04:32  loss: 4.1006 (3.0744)  acc1: 0.0000 (26.3522)  acc5: 33.3333 (65.8829)  time: 0.1526 (0.1445 -- 0.1656)  data: 0.0011 (0.0006 -- 0.0018)  max mem: 16413
Test:  [7090/8869]  eta: 0:04:31  loss: 3.9821 (3.0754)  acc1: 0.0000 (26.3292)  acc5: 33.3333 (65.8581)  time: 0.1531 (0.1437 -- 0.1656)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [7100/8869]  eta: 0:04:29  loss: 3.9821 (3.0766)  acc1: 0.0000 (26.3202)  acc5: 50.0000 (65.8194)  time: 0.1541 (0.1394 -- 0.1710)  data: 0.0011 (0.0004 -- 0.0019)  max mem: 16413
Test:  [7110/8869]  eta: 0:04:27  loss: 3.9749 (3.0759)  acc1: 0.0000 (26.3418)  acc5: 50.0000 (65.8299)  time: 0.1529 (0.1394 -- 0.1710)  data: 0.0012 (0.0005 -- 0.0028)  max mem: 16413
Test:  [7120/8869]  eta: 0:04:26  loss: 2.1064 (3.0742)  acc1: 33.3333 (26.3704)  acc5: 100.0000 (65.8615)  time: 0.1559 (0.1403 -- 0.1822)  data: 0.0016 (0.0005 -- 0.0090)  max mem: 16413
Test:  [7130/8869]  eta: 0:04:24  loss: 1.5438 (3.0731)  acc1: 50.0000 (26.3988)  acc5: 100.0000 (65.8627)  time: 0.1592 (0.1451 -- 0.1822)  data: 0.0017 (0.0005 -- 0.0090)  max mem: 16413
Test:  [7140/8869]  eta: 0:04:23  loss: 2.9687 (3.0730)  acc1: 33.3333 (26.4039)  acc5: 50.0000 (65.8498)  time: 0.1509 (0.1334 -- 0.1696)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Test:  [7150/8869]  eta: 0:04:21  loss: 3.0463 (3.0727)  acc1: 33.3333 (26.4112)  acc5: 50.0000 (65.8416)  time: 0.1462 (0.1334 -- 0.1614)  data: 0.0008 (0.0003 -- 0.0024)  max mem: 16413
Test:  [7160/8869]  eta: 0:04:20  loss: 3.6760 (3.0737)  acc1: 0.0000 (26.3953)  acc5: 66.6667 (65.8497)  time: 0.1505 (0.1391 -- 0.1782)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [7170/8869]  eta: 0:04:18  loss: 3.9535 (3.0739)  acc1: 0.0000 (26.3748)  acc5: 83.3333 (65.8625)  time: 0.1528 (0.1391 -- 0.1782)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
Test:  [7180/8869]  eta: 0:04:17  loss: 2.9593 (3.0735)  acc1: 0.0000 (26.3798)  acc5: 83.3333 (65.8845)  time: 0.1497 (0.1390 -- 0.1650)  data: 0.0012 (0.0004 -- 0.0056)  max mem: 16413
Test:  [7190/8869]  eta: 0:04:15  loss: 2.9026 (3.0732)  acc1: 16.6667 (26.3802)  acc5: 66.6667 (65.8833)  time: 0.1469 (0.1390 -- 0.1621)  data: 0.0012 (0.0004 -- 0.0056)  max mem: 16413
Test:  [7200/8869]  eta: 0:04:14  loss: 3.0352 (3.0735)  acc1: 16.6667 (26.3690)  acc5: 66.6667 (65.8635)  time: 0.1490 (0.1408 -- 0.1625)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [7210/8869]  eta: 0:04:12  loss: 3.0307 (3.0727)  acc1: 16.6667 (26.3879)  acc5: 66.6667 (65.8831)  time: 0.1525 (0.1418 -- 0.1746)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [7220/8869]  eta: 0:04:11  loss: 3.0221 (3.0727)  acc1: 33.3333 (26.3883)  acc5: 83.3333 (65.9073)  time: 0.1497 (0.1381 -- 0.1746)  data: 0.0009 (0.0005 -- 0.0016)  max mem: 16413
Test:  [7230/8869]  eta: 0:04:09  loss: 2.4668 (3.0710)  acc1: 33.3333 (26.4187)  acc5: 100.0000 (65.9498)  time: 0.1483 (0.1381 -- 0.1626)  data: 0.0010 (0.0007 -- 0.0025)  max mem: 16413
Test:  [7240/8869]  eta: 0:04:08  loss: 1.9308 (3.0702)  acc1: 33.3333 (26.4213)  acc5: 100.0000 (65.9808)  time: 0.1523 (0.1401 -- 0.1680)  data: 0.0015 (0.0006 -- 0.0079)  max mem: 16413
Test:  [7250/8869]  eta: 0:04:06  loss: 1.9308 (3.0688)  acc1: 33.3333 (26.4515)  acc5: 100.0000 (66.0208)  time: 0.1519 (0.1437 -- 0.1680)  data: 0.0014 (0.0006 -- 0.0079)  max mem: 16413
Test:  [7260/8869]  eta: 0:04:05  loss: 1.3513 (3.0683)  acc1: 50.0000 (26.4656)  acc5: 100.0000 (66.0446)  time: 0.1521 (0.1437 -- 0.1629)  data: 0.0017 (0.0006 -- 0.0126)  max mem: 16413
Test:  [7270/8869]  eta: 0:04:03  loss: 1.5176 (3.0662)  acc1: 50.0000 (26.5140)  acc5: 83.3333 (66.0776)  time: 0.1523 (0.1416 -- 0.1694)  data: 0.0017 (0.0006 -- 0.0126)  max mem: 16413
Test:  [7280/8869]  eta: 0:04:02  loss: 1.5176 (3.0652)  acc1: 66.6667 (26.5417)  acc5: 100.0000 (66.1081)  time: 0.1503 (0.1400 -- 0.1694)  data: 0.0011 (0.0006 -- 0.0017)  max mem: 16413
Test:  [7290/8869]  eta: 0:04:00  loss: 1.1511 (3.0627)  acc1: 66.6667 (26.5876)  acc5: 100.0000 (66.1546)  time: 0.1523 (0.1400 -- 0.1667)  data: 0.0011 (0.0006 -- 0.0021)  max mem: 16413
Test:  [7300/8869]  eta: 0:03:58  loss: 1.9902 (3.0624)  acc1: 33.3333 (26.5854)  acc5: 100.0000 (66.1804)  time: 0.1544 (0.1413 -- 0.1715)  data: 0.0011 (0.0007 -- 0.0021)  max mem: 16413
Test:  [7310/8869]  eta: 0:03:57  loss: 2.3635 (3.0605)  acc1: 33.3333 (26.6220)  acc5: 100.0000 (66.2244)  time: 0.1533 (0.1435 -- 0.1715)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [7320/8869]  eta: 0:03:55  loss: 1.6165 (3.0603)  acc1: 33.3333 (26.6198)  acc5: 100.0000 (66.2432)  time: 0.1523 (0.1382 -- 0.1676)  data: 0.0011 (0.0004 -- 0.0029)  max mem: 16413
Test:  [7330/8869]  eta: 0:03:54  loss: 1.9698 (3.0584)  acc1: 50.0000 (26.6653)  acc5: 83.3333 (66.2711)  time: 0.1518 (0.1382 -- 0.1676)  data: 0.0018 (0.0006 -- 0.0119)  max mem: 16413
Test:  [7340/8869]  eta: 0:03:52  loss: 1.9698 (3.0582)  acc1: 50.0000 (26.6608)  acc5: 83.3333 (66.2830)  time: 0.1522 (0.1407 -- 0.1692)  data: 0.0017 (0.0006 -- 0.0119)  max mem: 16413
Test:  [7350/8869]  eta: 0:03:51  loss: 2.4468 (3.0573)  acc1: 33.3333 (26.6902)  acc5: 83.3333 (66.2926)  time: 0.1923 (0.1273 -- 1.0367)  data: 0.0460 (0.0005 -- 0.9029)  max mem: 16413
Test:  [7360/8869]  eta: 0:03:50  loss: 2.4253 (3.0576)  acc1: 33.3333 (26.6834)  acc5: 66.6667 (66.2953)  time: 0.1905 (0.1273 -- 1.0367)  data: 0.0458 (0.0003 -- 0.9029)  max mem: 16413
Test:  [7370/8869]  eta: 0:03:48  loss: 2.0968 (3.0559)  acc1: 33.3333 (26.7218)  acc5: 66.6667 (66.3139)  time: 0.1486 (0.1372 -- 0.1693)  data: 0.0007 (0.0003 -- 0.0012)  max mem: 16413
Test:  [7380/8869]  eta: 0:03:46  loss: 2.6968 (3.0567)  acc1: 16.6667 (26.7014)  acc5: 83.3333 (66.3325)  time: 0.1486 (0.1372 -- 0.1693)  data: 0.0007 (0.0003 -- 0.0016)  max mem: 16413
Test:  [7390/8869]  eta: 0:03:45  loss: 2.7118 (3.0550)  acc1: 16.6667 (26.7352)  acc5: 83.3333 (66.3577)  time: 0.1498 (0.1385 -- 0.1695)  data: 0.0009 (0.0004 -- 0.0016)  max mem: 16413
Test:  [7400/8869]  eta: 0:03:43  loss: 2.7118 (3.0550)  acc1: 33.3333 (26.7306)  acc5: 83.3333 (66.3784)  time: 0.1521 (0.1385 -- 0.1729)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [7410/8869]  eta: 0:03:42  loss: 2.7207 (3.0537)  acc1: 33.3333 (26.7553)  acc5: 83.3333 (66.4013)  time: 0.1485 (0.1307 -- 0.1729)  data: 0.0011 (0.0003 -- 0.0045)  max mem: 16413
Test:  [7420/8869]  eta: 0:03:40  loss: 2.0864 (3.0537)  acc1: 33.3333 (26.7507)  acc5: 66.6667 (66.4017)  time: 0.1888 (0.1178 -- 1.0939)  data: 0.0494 (0.0001 -- 0.9712)  max mem: 16413
Test:  [7430/8869]  eta: 0:03:39  loss: 2.0864 (3.0525)  acc1: 16.6667 (26.7685)  acc5: 100.0000 (66.4312)  time: 0.1909 (0.1178 -- 1.0939)  data: 0.0492 (0.0001 -- 0.9712)  max mem: 16413
Test:  [7440/8869]  eta: 0:03:37  loss: 3.0500 (3.0534)  acc1: 0.0000 (26.7348)  acc5: 100.0000 (66.4628)  time: 0.1477 (0.1382 -- 0.1623)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Test:  [7450/8869]  eta: 0:03:36  loss: 3.3132 (3.0534)  acc1: 0.0000 (26.7347)  acc5: 83.3333 (66.4609)  time: 0.1425 (0.1193 -- 0.1546)  data: 0.0008 (0.0003 -- 0.0019)  max mem: 16413
Test:  [7460/8869]  eta: 0:03:34  loss: 3.0162 (3.0537)  acc1: 16.6667 (26.7301)  acc5: 66.6667 (66.4679)  time: 0.1293 (0.1161 -- 0.1541)  data: 0.0004 (0.0001 -- 0.0008)  max mem: 16413
Test:  [7470/8869]  eta: 0:03:33  loss: 4.2438 (3.0550)  acc1: 0.0000 (26.7055)  acc5: 50.0000 (66.4458)  time: 0.1317 (0.1161 -- 0.1481)  data: 0.0005 (0.0001 -- 0.0017)  max mem: 16413
Test:  [7480/8869]  eta: 0:03:31  loss: 3.8298 (3.0552)  acc1: 0.0000 (26.7099)  acc5: 66.6667 (66.4483)  time: 0.1449 (0.1359 -- 0.1675)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Test:  [7490/8869]  eta: 0:03:30  loss: 3.7215 (3.0564)  acc1: 0.0000 (26.6898)  acc5: 66.6667 (66.4242)  time: 0.1504 (0.1399 -- 0.1675)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [7500/8869]  eta: 0:03:28  loss: 4.3141 (3.0562)  acc1: 0.0000 (26.6942)  acc5: 50.0000 (66.4111)  time: 0.1524 (0.1426 -- 0.1680)  data: 0.0012 (0.0005 -- 0.0052)  max mem: 16413
Test:  [7510/8869]  eta: 0:03:27  loss: 2.8773 (3.0566)  acc1: 16.6667 (26.6897)  acc5: 50.0000 (66.4004)  time: 0.1533 (0.1426 -- 0.1691)  data: 0.0015 (0.0005 -- 0.0052)  max mem: 16413
Test:  [7520/8869]  eta: 0:03:25  loss: 3.8864 (3.0573)  acc1: 0.0000 (26.6698)  acc5: 66.6667 (66.4140)  time: 0.1535 (0.1426 -- 0.1691)  data: 0.0014 (0.0005 -- 0.0048)  max mem: 16413
Test:  [7530/8869]  eta: 0:03:24  loss: 3.8799 (3.0580)  acc1: 0.0000 (26.6432)  acc5: 83.3333 (66.4144)  time: 0.1510 (0.1418 -- 0.1684)  data: 0.0011 (0.0005 -- 0.0027)  max mem: 16413
Test:  [7540/8869]  eta: 0:03:22  loss: 3.8799 (3.0591)  acc1: 0.0000 (26.6233)  acc5: 50.0000 (66.3882)  time: 0.1527 (0.1418 -- 0.1684)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [7550/8869]  eta: 0:03:21  loss: 4.1683 (3.0603)  acc1: 0.0000 (26.6035)  acc5: 33.3333 (66.3532)  time: 0.1525 (0.1374 -- 0.1695)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [7560/8869]  eta: 0:03:19  loss: 2.7695 (3.0585)  acc1: 0.0000 (26.6323)  acc5: 66.6667 (66.3845)  time: 0.1506 (0.1374 -- 0.1695)  data: 0.0011 (0.0008 -- 0.0018)  max mem: 16413
Test:  [7570/8869]  eta: 0:03:17  loss: 3.3776 (3.0601)  acc1: 0.0000 (26.6015)  acc5: 66.6667 (66.3563)  time: 0.1520 (0.1396 -- 0.1692)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [7580/8869]  eta: 0:03:16  loss: 4.4131 (3.0614)  acc1: 0.0000 (26.5708)  acc5: 33.3333 (66.3193)  time: 0.1545 (0.1449 -- 0.1755)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [7590/8869]  eta: 0:03:14  loss: 4.3691 (3.0632)  acc1: 0.0000 (26.5402)  acc5: 33.3333 (66.2649)  time: 0.1535 (0.1451 -- 0.1755)  data: 0.0015 (0.0004 -- 0.0044)  max mem: 16413
Test:  [7600/8869]  eta: 0:03:13  loss: 4.4069 (3.0648)  acc1: 0.0000 (26.5075)  acc5: 16.6667 (66.2062)  time: 0.1520 (0.1336 -- 0.1682)  data: 0.0016 (0.0004 -- 0.0044)  max mem: 16413
Test:  [7610/8869]  eta: 0:03:11  loss: 4.4974 (3.0665)  acc1: 0.0000 (26.4727)  acc5: 16.6667 (66.1608)  time: 0.1518 (0.1305 -- 0.1682)  data: 0.0010 (0.0003 -- 0.0042)  max mem: 16413
Test:  [7620/8869]  eta: 0:03:10  loss: 4.2920 (3.0676)  acc1: 0.0000 (26.4510)  acc5: 33.3333 (66.1199)  time: 0.1477 (0.1305 -- 0.1659)  data: 0.0008 (0.0003 -- 0.0015)  max mem: 16413
Test:  [7630/8869]  eta: 0:03:08  loss: 4.2513 (3.0686)  acc1: 0.0000 (26.4382)  acc5: 33.3333 (66.0835)  time: 0.1879 (0.1158 -- 1.1115)  data: 0.0503 (0.0001 -- 0.9910)  max mem: 16413
Test:  [7640/8869]  eta: 0:03:07  loss: 4.2240 (3.0699)  acc1: 0.0000 (26.4080)  acc5: 33.3333 (66.0450)  time: 0.1885 (0.1158 -- 1.1115)  data: 0.0501 (0.0001 -- 0.9910)  max mem: 16413
Test:  [7650/8869]  eta: 0:03:05  loss: 4.3548 (3.0719)  acc1: 0.0000 (26.3735)  acc5: 16.6667 (65.9805)  time: 0.1467 (0.1328 -- 0.1625)  data: 0.0007 (0.0003 -- 0.0013)  max mem: 16413
Test:  [7660/8869]  eta: 0:03:04  loss: 4.3584 (3.0734)  acc1: 0.0000 (26.3412)  acc5: 16.6667 (65.9292)  time: 0.1500 (0.1400 -- 0.1632)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [7670/8869]  eta: 0:03:02  loss: 4.2275 (3.0747)  acc1: 0.0000 (26.3156)  acc5: 16.6667 (65.8823)  time: 0.1520 (0.1419 -- 0.1632)  data: 0.0010 (0.0005 -- 0.0019)  max mem: 16413
Test:  [7680/8869]  eta: 0:03:01  loss: 4.2275 (3.0760)  acc1: 0.0000 (26.2878)  acc5: 33.3333 (65.8508)  time: 0.1524 (0.1433 -- 0.1634)  data: 0.0014 (0.0006 -- 0.0084)  max mem: 16413
Test:  [7690/8869]  eta: 0:02:59  loss: 4.2965 (3.0769)  acc1: 0.0000 (26.2818)  acc5: 33.3333 (65.8194)  time: 0.1549 (0.1451 -- 0.1669)  data: 0.0016 (0.0006 -- 0.0084)  max mem: 16413
Test:  [7700/8869]  eta: 0:02:58  loss: 3.3446 (3.0761)  acc1: 33.3333 (26.3061)  acc5: 50.0000 (65.8378)  time: 0.1534 (0.1403 -- 0.1669)  data: 0.0014 (0.0006 -- 0.0054)  max mem: 16413
Test:  [7710/8869]  eta: 0:02:56  loss: 2.5807 (3.0752)  acc1: 33.3333 (26.3217)  acc5: 100.0000 (65.8561)  time: 0.1498 (0.1403 -- 0.1595)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Test:  [7720/8869]  eta: 0:02:55  loss: 2.0545 (3.0741)  acc1: 33.3333 (26.3502)  acc5: 83.3333 (65.8572)  time: 0.1502 (0.1417 -- 0.1698)  data: 0.0009 (0.0003 -- 0.0014)  max mem: 16413
Test:  [7730/8869]  eta: 0:02:53  loss: 2.0545 (3.0738)  acc1: 33.3333 (26.3592)  acc5: 66.6667 (65.8410)  time: 0.1520 (0.1425 -- 0.1698)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [7740/8869]  eta: 0:02:52  loss: 2.8142 (3.0734)  acc1: 16.6667 (26.3640)  acc5: 50.0000 (65.8334)  time: 0.1543 (0.1454 -- 0.1668)  data: 0.0011 (0.0005 -- 0.0027)  max mem: 16413
Test:  [7750/8869]  eta: 0:02:50  loss: 2.8142 (3.0739)  acc1: 16.6667 (26.3600)  acc5: 66.6667 (65.8496)  time: 0.1541 (0.1331 -- 0.1668)  data: 0.0013 (0.0005 -- 0.0057)  max mem: 16413
Test:  [7760/8869]  eta: 0:02:49  loss: 2.9028 (3.0741)  acc1: 0.0000 (26.3454)  acc5: 83.3333 (65.8614)  time: 0.1528 (0.1138 -- 0.3226)  data: 0.0110 (0.0001 -- 0.2007)  max mem: 16413
Test:  [7770/8869]  eta: 0:02:47  loss: 2.8107 (3.0737)  acc1: 0.0000 (26.3394)  acc5: 83.3333 (65.8903)  time: 0.1499 (0.1138 -- 0.3226)  data: 0.0107 (0.0001 -- 0.2007)  max mem: 16413
Test:  [7780/8869]  eta: 0:02:46  loss: 2.7120 (3.0733)  acc1: 16.6667 (26.3441)  acc5: 66.6667 (65.8806)  time: 0.1466 (0.1267 -- 0.1669)  data: 0.0010 (0.0002 -- 0.0037)  max mem: 16413
Test:  [7790/8869]  eta: 0:02:44  loss: 3.2718 (3.0737)  acc1: 16.6667 (26.3338)  acc5: 50.0000 (65.8645)  time: 0.1517 (0.1399 -- 0.1706)  data: 0.0012 (0.0004 -- 0.0037)  max mem: 16413
Test:  [7800/8869]  eta: 0:02:43  loss: 2.7531 (3.0727)  acc1: 16.6667 (26.3556)  acc5: 66.6667 (65.8826)  time: 0.1548 (0.1411 -- 0.1706)  data: 0.0011 (0.0004 -- 0.0036)  max mem: 16413
Test:  [7810/8869]  eta: 0:02:41  loss: 2.8216 (3.0728)  acc1: 16.6667 (26.3453)  acc5: 83.3333 (65.9135)  time: 0.1556 (0.1411 -- 0.1768)  data: 0.0012 (0.0004 -- 0.0040)  max mem: 16413
Test:  [7820/8869]  eta: 0:02:39  loss: 2.7181 (3.0714)  acc1: 16.6667 (26.3649)  acc5: 100.0000 (65.9506)  time: 0.1591 (0.1474 -- 0.1768)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
Test:  [7830/8869]  eta: 0:02:38  loss: 1.8072 (3.0709)  acc1: 33.3333 (26.3653)  acc5: 100.0000 (65.9707)  time: 0.1562 (0.1389 -- 0.1748)  data: 0.0011 (0.0003 -- 0.0021)  max mem: 16413
Test:  [7840/8869]  eta: 0:02:36  loss: 1.8372 (3.0691)  acc1: 33.3333 (26.4039)  acc5: 83.3333 (65.9992)  time: 0.1525 (0.1389 -- 0.1663)  data: 0.0010 (0.0003 -- 0.0016)  max mem: 16413
Test:  [7850/8869]  eta: 0:02:35  loss: 1.8108 (3.0685)  acc1: 33.3333 (26.4106)  acc5: 100.0000 (66.0298)  time: 0.1495 (0.1321 -- 0.1618)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Test:  [7860/8869]  eta: 0:02:33  loss: 1.8958 (3.0670)  acc1: 50.0000 (26.4449)  acc5: 100.0000 (66.0603)  time: 0.1404 (0.1224 -- 0.1596)  data: 0.0007 (0.0003 -- 0.0013)  max mem: 16413
Test:  [7870/8869]  eta: 0:02:32  loss: 1.6034 (3.0659)  acc1: 50.0000 (26.4706)  acc5: 100.0000 (66.0886)  time: 0.1421 (0.1224 -- 0.1690)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Test:  [7880/8869]  eta: 0:02:30  loss: 1.1340 (3.0636)  acc1: 66.6667 (26.5089)  acc5: 100.0000 (66.1316)  time: 0.1513 (0.1397 -- 0.1690)  data: 0.0009 (0.0005 -- 0.0017)  max mem: 16413
Test:  [7890/8869]  eta: 0:02:29  loss: 1.8256 (3.0636)  acc1: 16.6667 (26.4943)  acc5: 100.0000 (66.1534)  time: 0.1509 (0.1405 -- 0.1674)  data: 0.0011 (0.0005 -- 0.0043)  max mem: 16413
Test:  [7900/8869]  eta: 0:02:27  loss: 2.4841 (3.0617)  acc1: 16.6667 (26.5283)  acc5: 100.0000 (66.1942)  time: 0.1528 (0.1319 -- 0.1790)  data: 0.0012 (0.0005 -- 0.0043)  max mem: 16413
Test:  [7910/8869]  eta: 0:02:26  loss: 2.1432 (3.0612)  acc1: 33.3333 (26.5411)  acc5: 100.0000 (66.2264)  time: 0.1681 (0.1155 -- 0.6116)  data: 0.0253 (0.0001 -- 0.4916)  max mem: 16413
Test:  [7920/8869]  eta: 0:02:24  loss: 2.3234 (3.0593)  acc1: 50.0000 (26.5833)  acc5: 100.0000 (66.2501)  time: 0.1647 (0.1155 -- 0.6116)  data: 0.0253 (0.0001 -- 0.4916)  max mem: 16413
Test:  [7930/8869]  eta: 0:02:23  loss: 1.9954 (3.0590)  acc1: 50.0000 (26.5834)  acc5: 83.3333 (66.2674)  time: 0.1518 (0.1383 -- 0.1627)  data: 0.0011 (0.0005 -- 0.0031)  max mem: 16413
Test:  [7940/8869]  eta: 0:02:21  loss: 2.1831 (3.0583)  acc1: 33.3333 (26.6087)  acc5: 83.3333 (66.2658)  time: 0.1552 (0.1383 -- 0.1678)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Test:  [7950/8869]  eta: 0:02:20  loss: 3.0566 (3.0586)  acc1: 33.3333 (26.6067)  acc5: 66.6667 (66.2663)  time: 0.1574 (0.1454 -- 0.1715)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Test:  [7960/8869]  eta: 0:02:18  loss: 1.6668 (3.0569)  acc1: 50.0000 (26.6466)  acc5: 83.3333 (66.2898)  time: 0.1548 (0.1408 -- 0.1715)  data: 0.0010 (0.0004 -- 0.0018)  max mem: 16413
Test:  [7970/8869]  eta: 0:02:17  loss: 1.7532 (3.0576)  acc1: 16.6667 (26.6257)  acc5: 100.0000 (66.3133)  time: 0.1531 (0.1408 -- 0.1656)  data: 0.0011 (0.0004 -- 0.0017)  max mem: 16413
Test:  [7980/8869]  eta: 0:02:15  loss: 2.7977 (3.0567)  acc1: 16.6667 (26.6508)  acc5: 83.3333 (66.3325)  time: 0.1530 (0.1433 -- 0.1656)  data: 0.0010 (0.0005 -- 0.0017)  max mem: 16413
Test:  [7990/8869]  eta: 0:02:14  loss: 2.5881 (3.0560)  acc1: 33.3333 (26.6612)  acc5: 100.0000 (66.3622)  time: 0.1525 (0.1433 -- 0.1601)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [8000/8869]  eta: 0:02:12  loss: 2.1396 (3.0550)  acc1: 33.3333 (26.6862)  acc5: 100.0000 (66.3771)  time: 0.1518 (0.1448 -- 0.1611)  data: 0.0014 (0.0007 -- 0.0054)  max mem: 16413
Test:  [8010/8869]  eta: 0:02:11  loss: 2.6459 (3.0549)  acc1: 33.3333 (26.6904)  acc5: 83.3333 (66.3941)  time: 0.1531 (0.1448 -- 0.1700)  data: 0.0013 (0.0005 -- 0.0054)  max mem: 16413
Test:  [8020/8869]  eta: 0:02:09  loss: 2.6459 (3.0539)  acc1: 33.3333 (26.7194)  acc5: 83.3333 (66.4132)  time: 0.1541 (0.1446 -- 0.1700)  data: 0.0018 (0.0005 -- 0.0131)  max mem: 16413
Test:  [8030/8869]  eta: 0:02:07  loss: 3.3122 (3.0544)  acc1: 0.0000 (26.6945)  acc5: 100.0000 (66.4342)  time: 0.1519 (0.1438 -- 0.1693)  data: 0.0018 (0.0005 -- 0.0131)  max mem: 16413
Test:  [8040/8869]  eta: 0:02:06  loss: 3.3122 (3.0542)  acc1: 0.0000 (26.7090)  acc5: 83.3333 (66.4428)  time: 0.1516 (0.1438 -- 0.1704)  data: 0.0010 (0.0004 -- 0.0036)  max mem: 16413
Test:  [8050/8869]  eta: 0:02:04  loss: 3.0289 (3.0548)  acc1: 16.6667 (26.6923)  acc5: 66.6667 (66.4452)  time: 0.1527 (0.1438 -- 0.1704)  data: 0.0009 (0.0004 -- 0.0014)  max mem: 16413
Test:  [8060/8869]  eta: 0:02:03  loss: 3.9368 (3.0557)  acc1: 0.0000 (26.6737)  acc5: 50.0000 (66.4186)  time: 0.1535 (0.1434 -- 0.1730)  data: 0.0009 (0.0004 -- 0.0017)  max mem: 16413
Test:  [8070/8869]  eta: 0:02:01  loss: 4.0091 (3.0562)  acc1: 0.0000 (26.6696)  acc5: 50.0000 (66.4044)  time: 0.1539 (0.1434 -- 0.1730)  data: 0.0011 (0.0005 -- 0.0020)  max mem: 16413
Test:  [8080/8869]  eta: 0:02:00  loss: 4.3099 (3.0574)  acc1: 0.0000 (26.6531)  acc5: 50.0000 (66.3841)  time: 0.1555 (0.1465 -- 0.1679)  data: 0.0010 (0.0005 -- 0.0020)  max mem: 16413
Test:  [8090/8869]  eta: 0:01:58  loss: 3.6543 (3.0568)  acc1: 16.6667 (26.6675)  acc5: 50.0000 (66.3886)  time: 0.1520 (0.1402 -- 0.1679)  data: 0.0009 (0.0002 -- 0.0023)  max mem: 16413
Test:  [8100/8869]  eta: 0:01:57  loss: 3.5530 (3.0576)  acc1: 16.6667 (26.6613)  acc5: 50.0000 (66.3642)  time: 0.1481 (0.1402 -- 0.1560)  data: 0.0009 (0.0002 -- 0.0023)  max mem: 16413
Test:  [8110/8869]  eta: 0:01:55  loss: 4.1400 (3.0582)  acc1: 0.0000 (26.6469)  acc5: 66.6667 (66.3667)  time: 0.1472 (0.1368 -- 0.1560)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Test:  [8120/8869]  eta: 0:01:54  loss: 3.5973 (3.0586)  acc1: 0.0000 (26.6347)  acc5: 83.3333 (66.3670)  time: 0.1471 (0.1368 -- 0.1637)  data: 0.0010 (0.0003 -- 0.0013)  max mem: 16413
Test:  [8130/8869]  eta: 0:01:52  loss: 4.0573 (3.0598)  acc1: 0.0000 (26.6183)  acc5: 50.0000 (66.3387)  time: 0.1488 (0.1371 -- 0.1652)  data: 0.0011 (0.0006 -- 0.0026)  max mem: 16413
Test:  [8140/8869]  eta: 0:01:51  loss: 4.5000 (3.0614)  acc1: 0.0000 (26.5917)  acc5: 33.3333 (66.3002)  time: 0.1508 (0.1371 -- 0.1713)  data: 0.0010 (0.0003 -- 0.0026)  max mem: 16413
Test:  [8150/8869]  eta: 0:01:49  loss: 2.8832 (3.0597)  acc1: 16.6667 (26.6266)  acc5: 66.6667 (66.3354)  time: 0.1514 (0.1397 -- 0.1719)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Test:  [8160/8869]  eta: 0:01:48  loss: 2.7542 (3.0609)  acc1: 16.6667 (26.6083)  acc5: 83.3333 (66.3134)  time: 0.1507 (0.1403 -- 0.1719)  data: 0.0009 (0.0004 -- 0.0019)  max mem: 16413
Test:  [8170/8869]  eta: 0:01:46  loss: 4.2855 (3.0624)  acc1: 0.0000 (26.5777)  acc5: 33.3333 (66.2689)  time: 0.1514 (0.1424 -- 0.1616)  data: 0.0011 (0.0006 -- 0.0019)  max mem: 16413
Test:  [8180/8869]  eta: 0:01:45  loss: 4.2855 (3.0640)  acc1: 0.0000 (26.5493)  acc5: 16.6667 (66.2124)  time: 0.1514 (0.1424 -- 0.1733)  data: 0.0010 (0.0006 -- 0.0015)  max mem: 16413
Test:  [8190/8869]  eta: 0:01:43  loss: 4.8099 (3.0656)  acc1: 0.0000 (26.5250)  acc5: 16.6667 (66.1702)  time: 0.1528 (0.1426 -- 0.1733)  data: 0.0010 (0.0005 -- 0.0020)  max mem: 16413
Test:  [8200/8869]  eta: 0:01:42  loss: 4.5960 (3.0671)  acc1: 0.0000 (26.4947)  acc5: 16.6667 (66.1322)  time: 0.1540 (0.1405 -- 0.1693)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
Test:  [8210/8869]  eta: 0:01:40  loss: 4.4167 (3.0683)  acc1: 0.0000 (26.4665)  acc5: 33.3333 (66.0902)  time: 0.1530 (0.1393 -- 0.1696)  data: 0.0014 (0.0003 -- 0.0093)  max mem: 16413
Test:  [8220/8869]  eta: 0:01:38  loss: 4.1475 (3.0690)  acc1: 0.0000 (26.4587)  acc5: 33.3333 (66.0646)  time: 0.1511 (0.1383 -- 0.1696)  data: 0.0013 (0.0003 -- 0.0093)  max mem: 16413
Test:  [8230/8869]  eta: 0:01:37  loss: 4.0143 (3.0702)  acc1: 0.0000 (26.4366)  acc5: 33.3333 (66.0268)  time: 0.1528 (0.1383 -- 0.1716)  data: 0.0017 (0.0004 -- 0.0156)  max mem: 16413
Test:  [8240/8869]  eta: 0:01:35  loss: 4.3289 (3.0718)  acc1: 0.0000 (26.4066)  acc5: 16.6667 (65.9730)  time: 0.1535 (0.1441 -- 0.1716)  data: 0.0018 (0.0007 -- 0.0156)  max mem: 16413
Test:  [8250/8869]  eta: 0:01:34  loss: 4.3289 (3.0730)  acc1: 0.0000 (26.3806)  acc5: 16.6667 (65.9314)  time: 0.1512 (0.1430 -- 0.1738)  data: 0.0009 (0.0003 -- 0.0016)  max mem: 16413
Test:  [8260/8869]  eta: 0:01:32  loss: 4.1006 (3.0742)  acc1: 0.0000 (26.3548)  acc5: 33.3333 (65.8879)  time: 0.1528 (0.1430 -- 0.1738)  data: 0.0011 (0.0003 -- 0.0036)  max mem: 16413
Test:  [8270/8869]  eta: 0:01:31  loss: 4.3210 (3.0758)  acc1: 0.0000 (26.3229)  acc5: 33.3333 (65.8526)  time: 0.1541 (0.1399 -- 0.1736)  data: 0.0013 (0.0008 -- 0.0036)  max mem: 16413
Test:  [8280/8869]  eta: 0:01:29  loss: 4.4180 (3.0768)  acc1: 0.0000 (26.3112)  acc5: 33.3333 (65.8173)  time: 0.1497 (0.1387 -- 0.1678)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [8290/8869]  eta: 0:01:28  loss: 2.7779 (3.0758)  acc1: 16.6667 (26.3398)  acc5: 66.6667 (65.8405)  time: 0.1503 (0.1347 -- 0.1787)  data: 0.0008 (0.0004 -- 0.0015)  max mem: 16413
Test:  [8300/8869]  eta: 0:01:26  loss: 2.5456 (3.0752)  acc1: 33.3333 (26.3442)  acc5: 83.3333 (65.8475)  time: 0.1453 (0.1189 -- 0.1787)  data: 0.0007 (0.0001 -- 0.0015)  max mem: 16413
Test:  [8310/8869]  eta: 0:01:25  loss: 2.5456 (3.0741)  acc1: 33.3333 (26.3727)  acc5: 66.6667 (65.8485)  time: 0.1273 (0.1159 -- 0.1538)  data: 0.0004 (0.0001 -- 0.0010)  max mem: 16413
Test:  [8320/8869]  eta: 0:01:23  loss: 2.2716 (3.0740)  acc1: 33.3333 (26.3790)  acc5: 66.6667 (65.8414)  time: 0.1333 (0.1159 -- 0.1630)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Test:  [8330/8869]  eta: 0:01:22  loss: 2.4987 (3.0735)  acc1: 33.3333 (26.3914)  acc5: 50.0000 (65.8344)  time: 0.1527 (0.1366 -- 0.1647)  data: 0.0014 (0.0004 -- 0.0093)  max mem: 16413
Test:  [8340/8869]  eta: 0:01:20  loss: 2.7085 (3.0737)  acc1: 33.3333 (26.3917)  acc5: 66.6667 (65.8414)  time: 0.1546 (0.1388 -- 0.1647)  data: 0.0014 (0.0004 -- 0.0093)  max mem: 16413
Test:  [8350/8869]  eta: 0:01:19  loss: 2.9674 (3.0742)  acc1: 0.0000 (26.3721)  acc5: 66.6667 (65.8504)  time: 0.1502 (0.1314 -- 0.1704)  data: 0.0008 (0.0004 -- 0.0013)  max mem: 16413
Test:  [8360/8869]  eta: 0:01:17  loss: 3.3332 (3.0744)  acc1: 0.0000 (26.3545)  acc5: 83.3333 (65.8693)  time: 0.1444 (0.1293 -- 0.1704)  data: 0.0007 (0.0002 -- 0.0013)  max mem: 16413
Test:  [8370/8869]  eta: 0:01:16  loss: 2.9054 (3.0736)  acc1: 16.6667 (26.3748)  acc5: 83.3333 (65.8742)  time: 0.1425 (0.1293 -- 0.1570)  data: 0.0008 (0.0002 -- 0.0014)  max mem: 16413
Test:  [8380/8869]  eta: 0:01:14  loss: 2.9054 (3.0740)  acc1: 16.6667 (26.3612)  acc5: 50.0000 (65.8593)  time: 0.1469 (0.1389 -- 0.1659)  data: 0.0010 (0.0004 -- 0.0020)  max mem: 16413
Test:  [8390/8869]  eta: 0:01:12  loss: 2.5207 (3.0733)  acc1: 33.3333 (26.3795)  acc5: 50.0000 (65.8682)  time: 0.1506 (0.1379 -- 0.1682)  data: 0.0012 (0.0004 -- 0.0039)  max mem: 16413
Test:  [8400/8869]  eta: 0:01:11  loss: 2.3515 (3.0729)  acc1: 33.3333 (26.3818)  acc5: 83.3333 (65.8949)  time: 0.1517 (0.1379 -- 0.1682)  data: 0.0011 (0.0004 -- 0.0039)  max mem: 16413
Test:  [8410/8869]  eta: 0:01:09  loss: 2.3336 (3.0721)  acc1: 33.3333 (26.3960)  acc5: 100.0000 (65.9236)  time: 0.1522 (0.1415 -- 0.1734)  data: 0.0010 (0.0006 -- 0.0023)  max mem: 16413
Test:  [8420/8869]  eta: 0:01:08  loss: 1.7218 (3.0711)  acc1: 50.0000 (26.4121)  acc5: 100.0000 (65.9542)  time: 0.1542 (0.1397 -- 0.1774)  data: 0.0011 (0.0006 -- 0.0023)  max mem: 16413
Test:  [8430/8869]  eta: 0:01:06  loss: 1.9667 (3.0697)  acc1: 50.0000 (26.4381)  acc5: 100.0000 (65.9866)  time: 0.1649 (0.1146 -- 0.5350)  data: 0.0219 (0.0001 -- 0.4201)  max mem: 16413
Test:  [8440/8869]  eta: 0:01:05  loss: 2.1574 (3.0692)  acc1: 50.0000 (26.4483)  acc5: 100.0000 (66.0131)  time: 0.1585 (0.1146 -- 0.5350)  data: 0.0217 (0.0001 -- 0.4201)  max mem: 16413
Test:  [8450/8869]  eta: 0:01:03  loss: 2.4438 (3.0683)  acc1: 33.3333 (26.4722)  acc5: 83.3333 (66.0336)  time: 0.1476 (0.1247 -- 0.1700)  data: 0.0009 (0.0002 -- 0.0018)  max mem: 16413
Test:  [8460/8869]  eta: 0:01:02  loss: 1.1877 (3.0667)  acc1: 50.0000 (26.5079)  acc5: 100.0000 (66.0619)  time: 0.1510 (0.1428 -- 0.1700)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Test:  [8470/8869]  eta: 0:01:00  loss: 0.8534 (3.0650)  acc1: 66.6667 (26.5455)  acc5: 100.0000 (66.1000)  time: 0.1500 (0.1369 -- 0.1638)  data: 0.0014 (0.0003 -- 0.0088)  max mem: 16413
Test:  [8480/8869]  eta: 0:00:59  loss: 2.0326 (3.0646)  acc1: 33.3333 (26.5456)  acc5: 100.0000 (66.1223)  time: 0.1507 (0.1360 -- 0.1717)  data: 0.0015 (0.0005 -- 0.0088)  max mem: 16413
Test:  [8490/8869]  eta: 0:00:57  loss: 2.4641 (3.0631)  acc1: 33.3333 (26.5732)  acc5: 100.0000 (66.1602)  time: 0.1536 (0.1360 -- 0.1717)  data: 0.0011 (0.0005 -- 0.0015)  max mem: 16413
Test:  [8500/8869]  eta: 0:00:56  loss: 1.7468 (3.0625)  acc1: 50.0000 (26.5773)  acc5: 100.0000 (66.1824)  time: 0.1578 (0.1398 -- 0.1799)  data: 0.0010 (0.0005 -- 0.0015)  max mem: 16413
Test:  [8510/8869]  eta: 0:00:54  loss: 2.3807 (3.0611)  acc1: 50.0000 (26.6126)  acc5: 83.3333 (66.2084)  time: 0.1546 (0.1416 -- 0.1799)  data: 0.0009 (0.0005 -- 0.0014)  max mem: 16413
Test:  [8520/8869]  eta: 0:00:53  loss: 2.4195 (3.0608)  acc1: 33.3333 (26.6127)  acc5: 83.3333 (66.2227)  time: 0.1517 (0.1416 -- 0.1611)  data: 0.0010 (0.0006 -- 0.0016)  max mem: 16413
Test:  [8530/8869]  eta: 0:00:51  loss: 2.4195 (3.0601)  acc1: 33.3333 (26.6323)  acc5: 83.3333 (66.2271)  time: 0.1521 (0.1442 -- 0.1679)  data: 0.0010 (0.0005 -- 0.0016)  max mem: 16413
Test:  [8540/8869]  eta: 0:00:50  loss: 2.4299 (3.0601)  acc1: 33.3333 (26.6323)  acc5: 66.6667 (66.2315)  time: 0.1516 (0.1308 -- 0.1679)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [8550/8869]  eta: 0:00:48  loss: 2.2514 (3.0585)  acc1: 33.3333 (26.6694)  acc5: 83.3333 (66.2554)  time: 0.1622 (0.1166 -- 0.4948)  data: 0.0196 (0.0001 -- 0.3767)  max mem: 16413
Test:  [8560/8869]  eta: 0:00:47  loss: 2.8242 (3.0594)  acc1: 16.6667 (26.6499)  acc5: 83.3333 (66.2598)  time: 0.1593 (0.1166 -- 0.4948)  data: 0.0195 (0.0001 -- 0.3767)  max mem: 16413
Test:  [8570/8869]  eta: 0:00:45  loss: 3.5020 (3.0588)  acc1: 0.0000 (26.6558)  acc5: 83.3333 (66.2758)  time: 0.2097 (0.1249 -- 1.3852)  data: 0.0644 (0.0001 -- 1.2701)  max mem: 16413
Test:  [8580/8869]  eta: 0:00:44  loss: 1.8837 (3.0578)  acc1: 33.3333 (26.6752)  acc5: 100.0000 (66.3054)  time: 0.2093 (0.1249 -- 1.3852)  data: 0.0643 (0.0001 -- 1.2701)  max mem: 16413
Test:  [8590/8869]  eta: 0:00:42  loss: 1.8837 (3.0572)  acc1: 33.3333 (26.6868)  acc5: 83.3333 (66.3175)  time: 0.1453 (0.1258 -- 0.1626)  data: 0.0009 (0.0002 -- 0.0014)  max mem: 16413
Test:  [8600/8869]  eta: 0:00:41  loss: 2.6845 (3.0568)  acc1: 16.6667 (26.6946)  acc5: 66.6667 (66.3237)  time: 0.1505 (0.1303 -- 0.1816)  data: 0.0010 (0.0004 -- 0.0017)  max mem: 16413
Test:  [8610/8869]  eta: 0:00:39  loss: 1.9824 (3.0556)  acc1: 33.3333 (26.7100)  acc5: 83.3333 (66.3454)  time: 0.1491 (0.1366 -- 0.1816)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Test:  [8620/8869]  eta: 0:00:37  loss: 3.0552 (3.0565)  acc1: 0.0000 (26.6829)  acc5: 83.3333 (66.3670)  time: 0.1459 (0.1366 -- 0.1698)  data: 0.0009 (0.0003 -- 0.0016)  max mem: 16413
Test:  [8630/8869]  eta: 0:00:36  loss: 3.3790 (3.0566)  acc1: 0.0000 (26.6790)  acc5: 83.3333 (66.3732)  time: 0.1527 (0.1413 -- 0.1752)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [8640/8869]  eta: 0:00:34  loss: 3.3790 (3.0571)  acc1: 0.0000 (26.6636)  acc5: 66.6667 (66.3735)  time: 0.1543 (0.1418 -- 0.1769)  data: 0.0009 (0.0005 -- 0.0018)  max mem: 16413
Test:  [8650/8869]  eta: 0:00:33  loss: 3.4255 (3.0576)  acc1: 0.0000 (26.6597)  acc5: 66.6667 (66.3642)  time: 0.1537 (0.1445 -- 0.1769)  data: 0.0011 (0.0005 -- 0.0035)  max mem: 16413
Test:  [8660/8869]  eta: 0:00:31  loss: 3.4769 (3.0579)  acc1: 0.0000 (26.6597)  acc5: 66.6667 (66.3645)  time: 0.1556 (0.1458 -- 0.1679)  data: 0.0014 (0.0005 -- 0.0049)  max mem: 16413
Test:  [8670/8869]  eta: 0:00:30  loss: 4.6380 (3.0595)  acc1: 0.0000 (26.6328)  acc5: 50.0000 (66.3361)  time: 0.1524 (0.1411 -- 0.1670)  data: 0.0018 (0.0004 -- 0.0119)  max mem: 16413
Test:  [8680/8869]  eta: 0:00:28  loss: 4.1064 (3.0591)  acc1: 0.0000 (26.6367)  acc5: 33.3333 (66.3345)  time: 0.1507 (0.1411 -- 0.1661)  data: 0.0015 (0.0004 -- 0.0119)  max mem: 16413
Test:  [8690/8869]  eta: 0:00:27  loss: 3.3601 (3.0597)  acc1: 16.6667 (26.6291)  acc5: 50.0000 (66.3138)  time: 0.1542 (0.1456 -- 0.1661)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [8700/8869]  eta: 0:00:25  loss: 3.3610 (3.0596)  acc1: 0.0000 (26.6272)  acc5: 50.0000 (66.3276)  time: 0.1522 (0.1396 -- 0.1623)  data: 0.0010 (0.0004 -- 0.0015)  max mem: 16413
Test:  [8710/8869]  eta: 0:00:24  loss: 3.7025 (3.0602)  acc1: 0.0000 (26.6062)  acc5: 83.3333 (66.3395)  time: 0.1491 (0.1302 -- 0.1628)  data: 0.0010 (0.0003 -- 0.0018)  max mem: 16413
Test:  [8720/8869]  eta: 0:00:22  loss: 3.9600 (3.0611)  acc1: 0.0000 (26.5872)  acc5: 66.6667 (66.3208)  time: 0.1492 (0.1302 -- 0.1688)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Test:  [8730/8869]  eta: 0:00:21  loss: 4.2109 (3.0627)  acc1: 0.0000 (26.5586)  acc5: 33.3333 (66.2773)  time: 0.1492 (0.1362 -- 0.1688)  data: 0.0008 (0.0004 -- 0.0012)  max mem: 16413
Test:  [8740/8869]  eta: 0:00:19  loss: 3.2920 (3.0614)  acc1: 0.0000 (26.5778)  acc5: 66.6667 (66.3044)  time: 0.1522 (0.1426 -- 0.1787)  data: 0.0010 (0.0005 -- 0.0029)  max mem: 16413
Test:  [8750/8869]  eta: 0:00:18  loss: 3.0815 (3.0619)  acc1: 16.6667 (26.5703)  acc5: 66.6667 (66.2915)  time: 0.1525 (0.1390 -- 0.1787)  data: 0.0011 (0.0005 -- 0.0029)  max mem: 16413
Test:  [8760/8869]  eta: 0:00:16  loss: 4.1891 (3.0633)  acc1: 0.0000 (26.5419)  acc5: 33.3333 (66.2558)  time: 0.1494 (0.1390 -- 0.1625)  data: 0.0010 (0.0005 -- 0.0014)  max mem: 16413
Test:  [8770/8869]  eta: 0:00:15  loss: 4.1471 (3.0645)  acc1: 0.0000 (26.5173)  acc5: 33.3333 (66.2182)  time: 0.1521 (0.1391 -- 0.1625)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [8780/8869]  eta: 0:00:13  loss: 4.3730 (3.0659)  acc1: 0.0000 (26.4890)  acc5: 33.3333 (66.1713)  time: 0.1540 (0.1447 -- 0.1641)  data: 0.0009 (0.0005 -- 0.0013)  max mem: 16413
Test:  [8790/8869]  eta: 0:00:12  loss: 4.5602 (3.0675)  acc1: 0.0000 (26.4589)  acc5: 16.6667 (66.1301)  time: 0.1539 (0.1437 -- 0.1641)  data: 0.0010 (0.0005 -- 0.0019)  max mem: 16413
Test:  [8800/8869]  eta: 0:00:10  loss: 4.2721 (3.0688)  acc1: 0.0000 (26.4326)  acc5: 33.3333 (66.0853)  time: 0.1536 (0.1437 -- 0.1641)  data: 0.0012 (0.0006 -- 0.0019)  max mem: 16413
Test:  [8810/8869]  eta: 0:00:08  loss: 4.1807 (3.0693)  acc1: 0.0000 (26.4291)  acc5: 33.3333 (66.0614)  time: 0.1531 (0.1446 -- 0.1655)  data: 0.0011 (0.0007 -- 0.0018)  max mem: 16413
Test:  [8820/8869]  eta: 0:00:07  loss: 4.0316 (3.0704)  acc1: 0.0000 (26.4029)  acc5: 33.3333 (66.0280)  time: 0.1526 (0.1431 -- 0.1655)  data: 0.0009 (0.0004 -- 0.0015)  max mem: 16413
Test:  [8830/8869]  eta: 0:00:05  loss: 4.2458 (3.0719)  acc1: 0.0000 (26.3730)  acc5: 16.6667 (65.9797)  time: 0.1503 (0.1381 -- 0.1643)  data: 0.0009 (0.0004 -- 0.0018)  max mem: 16413
Test:  [8840/8869]  eta: 0:00:04  loss: 4.5029 (3.0735)  acc1: 0.0000 (26.3432)  acc5: 16.6667 (65.9296)  time: 0.1473 (0.1345 -- 0.1643)  data: 0.0008 (0.0003 -- 0.0018)  max mem: 16413
Test:  [8850/8869]  eta: 0:00:02  loss: 4.2770 (3.0747)  acc1: 0.0000 (26.3172)  acc5: 16.6667 (65.8852)  time: 0.1486 (0.1345 -- 0.1669)  data: 0.0009 (0.0003 -- 0.0016)  max mem: 16413
Test:  [8860/8869]  eta: 0:00:01  loss: 4.2946 (3.0760)  acc1: 0.0000 (26.2931)  acc5: 16.6667 (65.8522)  time: 0.1366 (0.1129 -- 0.1669)  data: 0.0008 (0.0001 -- 0.0039)  max mem: 16413
Test:  [8868/8869]  eta: 0:00:00  loss: 4.2946 (3.0766)  acc1: 0.0000 (26.2849)  acc5: 33.3333 (65.8335)  time: 0.1211 (0.1128 -- 0.1552)  data: 0.0005 (0.0001 -- 0.0039)  max mem: 16413
Test: Total time: 0:22:34 (0.1527 s / it)
* Acc@1 26.314 Acc@5 65.834 loss 3.076
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 106425 test videos: Top-1: 26.31%, Top-5: 65.84%
Training time 8:44:53
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
