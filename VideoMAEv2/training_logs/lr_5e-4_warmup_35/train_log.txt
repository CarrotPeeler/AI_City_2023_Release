[2023-08-30 01:46:05,669] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 01:46:05,729] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=0.4, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=160, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.0005, min_lr=1e-06, mixup=0.4, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=35, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f128e3d1b20>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
[2023-08-30 01:46:10,813] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 01:46:10,814] [INFO] [comm.py:631:init_distributed] cdb=None
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00002344
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-30 01:46:10,860] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 01:46:10,860] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-30 01:46:10,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.5051565170288086 seconds
[2023-08-30 01:46:12,104] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-30 01:46:12,113] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-30 01:46:12,114] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-30 01:46:12,144] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-30 01:46:12,144] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-30 01:46:12,145] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-30 01:46:12,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 01:46:12,146] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 01:46:12,146] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 01:46:12,146] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 01:46:12,146] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 01:46:12,146] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1242e3e5e0>
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 01:46:12,147] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 01:46:12,148] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-30 01:46:12,149] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 01:46:12,150] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 01:46:12,151] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 5600
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 160 epochs
Epoch: [0]  [  0/160]  eta: 0:30:18  lr: 0.000000  min_lr: 0.000000  loss: 2.7733 (2.7733)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.3643 (11.3643 -- 11.3643)  data: 6.9161 (6.9161 -- 6.9161)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 2.7732 (2.7732)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5654 (1.5769)  time: 0.5989 (0.5053 -- 1.6182)  data: 0.0018 (0.0004 -- 0.0105)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:01:58  lr: 0.000000  min_lr: 0.000000  loss: 2.7730 (2.7732)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4803 (1.5436)  time: 0.8600 (0.4992 -- 2.1711)  data: 0.1934 (0.0006 -- 1.6676)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 2.7732 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4377 (1.5322)  time: 0.9526 (0.5210 -- 4.0501)  data: 0.4152 (0.0004 -- 3.4946)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 2.7728 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4300 (1.5258)  time: 0.8873 (0.5155 -- 4.5199)  data: 0.3616 (0.0003 -- 3.9815)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 2.7728 (2.7730)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4526 (1.5183)  time: 0.8152 (0.5168 -- 2.0590)  data: 0.2804 (0.0003 -- 1.5269)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 2.7726 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5289 (1.5181)  time: 0.7984 (0.5257 -- 2.6307)  data: 0.2539 (0.0003 -- 2.1136)  max mem: 16413
[2023-08-30 01:48:08,504] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:48:08,505] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-30 01:48:08,506] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:48:08,507] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.7724 (2.7729)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4196 (1.5042)  time: 0.9011 (0.5355 -- 2.6555)  data: 0.3578 (0.0009 -- 2.1293)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 2.7724 (2.7728)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3668 (1.4898)  time: 0.6505 (0.4849 -- 1.8699)  data: 0.1334 (0.0002 -- 1.2982)  max mem: 16413
Epoch: [0] Total time: 0:02:20 (0.8759 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 2.7724 (2.7728)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3668 (1.4898)
Val:  [ 0/27]  eta: 0:01:25  loss: 2.7717 (2.7717)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 3.1714 (3.1714 -- 3.1714)  data: 2.8156 (2.8156 -- 2.8156)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.7719 (2.7718)  acc1: 22.2222 (27.2727)  acc5: 77.7778 (76.7677)  time: 0.4789 (0.1968 -- 3.1714)  data: 0.2570 (0.0007 -- 2.8156)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7719 (2.7719)  acc1: 22.2222 (28.5714)  acc5: 77.7778 (77.2487)  time: 0.2022 (0.1688 -- 0.2279)  data: 0.0007 (0.0001 -- 0.0015)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7719 (2.7719)  acc1: 28.5714 (29.0456)  acc5: 77.7778 (74.6888)  time: 0.1893 (0.1684 -- 0.2195)  data: 0.0004 (0.0001 -- 0.0014)  max mem: 16413
Val: Total time: 0:00:08 (0.3051 s / it)
* Acc@1 29.876 Acc@5 75.104 loss 2.772
Accuracy of the network on the 482 val images: 29.88%
[2023-08-30 01:48:40,647] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-30 01:48:40,650] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 01:48:40,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-30 01:48:40,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 01:48:41,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 01:48:41,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 29.88%
Epoch: [1]  [  0/160]  eta: 0:17:38  lr: 0.000001  min_lr: 0.000000  loss: 2.7723 (2.7723)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6972 (1.6972)  time: 6.6142 (6.6142 -- 6.6142)  data: 4.4014 (4.4014 -- 4.4014)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 2.7723 (2.7723)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4905 (1.5063)  time: 0.8977 (0.5189 -- 3.1222)  data: 0.1137 (0.0005 -- 1.5117)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 2.7724 (2.7724)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5100 (1.5067)  time: 0.9383 (0.5086 -- 3.4346)  data: 0.0017 (0.0006 -- 0.0064)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 2.7719 (2.7723)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4474 (1.5137)  time: 0.8767 (0.5183 -- 3.2642)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 2.7720 (2.7722)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4436 (1.4962)  time: 0.8068 (0.5186 -- 3.2809)  data: 0.0015 (0.0005 -- 0.0047)  max mem: 16413
[2023-08-30 01:50:12,943] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:50:12,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-30 01:50:12,944] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:50:12,944] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 2.7721 (2.7722)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4855 (1.4953)  time: 0.9481 (0.5015 -- 4.1700)  data: 0.0018 (0.0001 -- 0.0137)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 2.7721 (2.7722)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4215 (1.4808)  time: 0.8170 (0.5256 -- 2.0751)  data: 0.0014 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.7718 (2.7721)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3863 (1.4765)  time: 0.8357 (0.5120 -- 2.5759)  data: 0.0507 (0.0003 -- 0.7390)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 2.7717 (2.7721)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4395 (1.4776)  time: 0.6722 (0.4908 -- 1.5805)  data: 0.0897 (0.0002 -- 1.0768)  max mem: 16413
Epoch: [1] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 2.7717 (2.7720)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4395 (1.4776)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.7708 (2.7708)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.4076 (2.4076 -- 2.4076)  data: 2.1717 (2.1717 -- 2.1717)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7706 (2.7706)  acc1: 33.3333 (30.3030)  acc5: 88.8889 (82.8283)  time: 0.4163 (0.1935 -- 2.4076)  data: 0.1997 (0.0009 -- 2.1717)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7704 (2.7706)  acc1: 33.3333 (33.3333)  acc5: 77.7778 (80.4233)  time: 0.2133 (0.1699 -- 0.3066)  data: 0.0076 (0.0001 -- 0.1221)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7706 (2.7707)  acc1: 33.3333 (33.6100)  acc5: 71.4286 (79.2531)  time: 0.1975 (0.1335 -- 0.3066)  data: 0.0066 (0.0001 -- 0.1221)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 34.440 Acc@5 79.046 loss 2.771
Accuracy of the network on the 482 val images: 34.44%
[2023-08-30 01:51:10,954] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 01:51:10,955] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 01:51:10,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 01:51:10,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 01:51:12,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 01:51:12,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 34.44%
Epoch: [2]  [  0/160]  eta: 0:20:08  lr: 0.000001  min_lr: 0.000000  loss: 2.7723 (2.7723)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2498 (1.2498)  time: 7.5550 (7.5550 -- 7.5550)  data: 7.0497 (7.0497 -- 7.0497)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 2.7716 (2.7715)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5386 (1.5020)  time: 0.8919 (0.5317 -- 4.3106)  data: 0.3497 (0.0007 -- 3.7730)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 2.7713 (2.7714)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5018 (1.5174)  time: 0.8374 (0.5238 -- 2.7415)  data: 0.2832 (0.0003 -- 2.2189)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:34  lr: 0.000002  min_lr: 0.000000  loss: 2.7709 (2.7713)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4736 (1.5086)  time: 0.7779 (0.5389 -- 2.4247)  data: 0.1792 (0.0004 -- 1.8738)  max mem: 16413
[2023-08-30 01:52:14,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:52:14,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:52:14,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-30 01:52:14,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 2.7714 (2.7712)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3915 (1.4910)  time: 0.8616 (0.5243 -- 2.7076)  data: 0.1102 (0.0003 -- 1.2441)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 2.7707 (2.7711)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5075 (1.4949)  time: 0.9098 (0.5077 -- 2.3363)  data: 0.1396 (0.0003 -- 1.8002)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 2.7705 (2.7710)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4533 (1.4978)  time: 0.8705 (0.5185 -- 2.5960)  data: 0.1110 (0.0003 -- 2.0498)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:17  lr: 0.000002  min_lr: 0.000000  loss: 2.7701 (2.7709)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3874 (1.4917)  time: 0.8170 (0.5308 -- 2.3286)  data: 0.1465 (0.0005 -- 1.7711)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 2.7701 (2.7707)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4842 (1.4886)  time: 0.7235 (0.4920 -- 2.2768)  data: 0.0783 (0.0002 -- 1.5543)  max mem: 16413
Epoch: [2] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 2.7701 (2.7708)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4842 (1.4886)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.7665 (2.7665)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.4108 (2.4108 -- 2.4108)  data: 2.1500 (2.1500 -- 2.1500)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7665 (2.7669)  acc1: 33.3333 (33.3333)  acc5: 77.7778 (81.8182)  time: 0.4185 (0.2014 -- 2.4108)  data: 0.1967 (0.0006 -- 2.1500)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7665 (2.7667)  acc1: 33.3333 (36.5079)  acc5: 77.7778 (80.4233)  time: 0.2112 (0.1703 -- 0.2464)  data: 0.0009 (0.0001 -- 0.0030)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7667 (2.7669)  acc1: 42.8571 (38.1743)  acc5: 77.7778 (79.2531)  time: 0.1947 (0.1359 -- 0.2451)  data: 0.0004 (0.0001 -- 0.0011)  max mem: 16413
Val: Total time: 0:00:07 (0.2826 s / it)
* Acc@1 40.041 Acc@5 80.705 loss 2.767
Accuracy of the network on the 482 val images: 40.04%
[2023-08-30 01:53:40,844] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 01:53:40,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 01:53:40,846] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 01:53:40,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 01:53:42,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 01:53:42,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.04%
Epoch: [3]  [  0/160]  eta: 0:22:10  lr: 0.000002  min_lr: 0.000000  loss: 2.7679 (2.7679)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8548 (1.8548)  time: 8.3182 (8.3182 -- 8.3182)  data: 5.4331 (5.4331 -- 5.4331)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 2.7696 (2.7695)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5220 (1.5513)  time: 0.8112 (0.5266 -- 3.9774)  data: 0.0103 (0.0005 -- 0.1728)  max mem: 16413
[2023-08-30 01:54:17,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:54:17,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-30 01:54:17,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:54:17,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000000  loss: 2.7690 (2.7692)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5115 (1.5209)  time: 0.9168 (0.5231 -- 3.8889)  data: 0.2609 (0.0002 -- 3.3743)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 2.7680 (2.7689)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4484 (1.5103)  time: 0.7809 (0.5201 -- 4.1328)  data: 0.2352 (0.0004 -- 3.6174)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 2.7678 (2.7685)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5913 (1.5307)  time: 0.8163 (0.5251 -- 3.0992)  data: 0.2626 (0.0003 -- 2.5452)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 2.7676 (2.7683)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4844 (1.5301)  time: 0.9092 (0.5209 -- 2.8646)  data: 0.3603 (0.0006 -- 2.3213)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 2.7667 (2.7680)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5356 (1.5323)  time: 0.9528 (0.5263 -- 3.8074)  data: 0.4086 (0.0004 -- 3.2815)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 2.7648 (2.7676)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5902 (1.5443)  time: 0.6540 (0.5263 -- 1.7426)  data: 0.0497 (0.0003 -- 0.8435)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 2.7659 (2.7674)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4833 (1.5416)  time: 0.7626 (0.4951 -- 2.4839)  data: 0.0686 (0.0002 -- 1.2342)  max mem: 16413
Epoch: [3] Total time: 0:02:19 (0.8744 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 2.7659 (2.7674)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4833 (1.5416)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.7569 (2.7569)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.3819 (2.3819 -- 2.3819)  data: 2.1457 (2.1457 -- 2.1457)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7578 (2.7584)  acc1: 33.3333 (35.3535)  acc5: 88.8889 (86.8687)  time: 0.4240 (0.2072 -- 2.3819)  data: 0.2013 (0.0009 -- 2.1457)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7576 (2.7579)  acc1: 44.4444 (41.7989)  acc5: 88.8889 (87.8307)  time: 0.2119 (0.1690 -- 0.2850)  data: 0.0046 (0.0001 -- 0.0422)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7584 (2.7584)  acc1: 44.4444 (45.2282)  acc5: 88.8889 (86.3071)  time: 0.1937 (0.1332 -- 0.2850)  data: 0.0036 (0.0001 -- 0.0422)  max mem: 16413
Val: Total time: 0:00:07 (0.2819 s / it)
* Acc@1 44.606 Acc@5 85.062 loss 2.758
Accuracy of the network on the 482 val images: 44.61%
[2023-08-30 01:56:09,897] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 01:56:09,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 01:56:09,899] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 01:56:09,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 01:56:11,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 01:56:11,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.61%
[2023-08-30 01:56:18,732] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:56:18,732] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:56:18,732] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-30 01:56:18,733] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:19:45  lr: 0.000003  min_lr: 0.000000  loss: 2.7702 (2.7702)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6436 (1.6436)  time: 7.4111 (7.4111 -- 7.4111)  data: 6.8973 (6.8973 -- 6.8973)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000000  loss: 2.7643 (2.7645)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5247 (1.5573)  time: 0.8201 (0.5241 -- 3.8157)  data: 0.2744 (0.0003 -- 3.3000)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 2.7639 (2.7638)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5523 (1.5641)  time: 0.8804 (0.5330 -- 3.7621)  data: 0.1929 (0.0004 -- 2.3470)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 2.7621 (2.7630)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4944 (1.5531)  time: 0.8704 (0.5324 -- 2.8053)  data: 0.0013 (0.0007 -- 0.0021)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 2.7624 (2.7629)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5857 (1.5601)  time: 0.8766 (0.5219 -- 2.3198)  data: 0.0213 (0.0004 -- 0.3964)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 2.7614 (2.7624)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4468 (1.5496)  time: 0.9147 (0.5399 -- 3.5777)  data: 0.0020 (0.0004 -- 0.0092)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 2.7557 (2.7615)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5356 (1.5548)  time: 0.7924 (0.5194 -- 2.6664)  data: 0.0013 (0.0004 -- 0.0032)  max mem: 16413
[2023-08-30 01:58:08,530] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:58:08,530] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-30 01:58:08,532] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 01:58:08,533] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 2.7604 (2.7612)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5225 (1.5515)  time: 0.9528 (0.5141 -- 3.9054)  data: 0.0011 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 2.7567 (2.7606)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4786 (1.5453)  time: 0.7272 (0.4943 -- 2.8496)  data: 0.0006 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [4] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 2.7567 (2.7610)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4786 (1.5453)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.7394 (2.7394)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3553 (2.3553 -- 2.3553)  data: 2.1502 (2.1502 -- 2.1502)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7428 (2.7434)  acc1: 33.3333 (38.3838)  acc5: 88.8889 (86.8687)  time: 0.4181 (0.1971 -- 2.3553)  data: 0.2028 (0.0005 -- 2.1502)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7422 (2.7420)  acc1: 44.4444 (42.3280)  acc5: 88.8889 (89.4180)  time: 0.2158 (0.1710 -- 0.3397)  data: 0.0116 (0.0001 -- 0.1479)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7435 (2.7432)  acc1: 44.4444 (44.3983)  acc5: 88.8889 (88.7967)  time: 0.1997 (0.1331 -- 0.3397)  data: 0.0112 (0.0001 -- 0.1479)  max mem: 16413
Val: Total time: 0:00:07 (0.2839 s / it)
* Acc@1 46.680 Acc@5 87.137 loss 2.743
Accuracy of the network on the 482 val images: 46.68%
[2023-08-30 01:58:40,318] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 01:58:40,319] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 01:58:40,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 01:58:40,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 01:58:41,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 01:58:41,716] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.68%
Epoch: [5]  [  0/160]  eta: 0:21:15  lr: 0.000003  min_lr: 0.000000  loss: 2.7478 (2.7478)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4851 (1.4851)  time: 7.9715 (7.9715 -- 7.9715)  data: 5.6147 (5.6147 -- 5.6147)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:37  lr: 0.000003  min_lr: 0.000000  loss: 2.7562 (2.7546)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6287 (1.5903)  time: 0.7816 (0.5276 -- 3.3088)  data: 0.2255 (0.0008 -- 2.7449)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:01:59  lr: 0.000004  min_lr: 0.000000  loss: 2.7494 (2.7531)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5556 (1.5808)  time: 0.8619 (0.5131 -- 1.9738)  data: 0.1963 (0.0004 -- 1.4336)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 2.7533 (2.7529)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5615 (1.5788)  time: 0.9017 (0.5194 -- 2.8707)  data: 0.3248 (0.0003 -- 2.3399)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 2.7516 (2.7522)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4900 (1.5687)  time: 0.9223 (0.5303 -- 3.5556)  data: 0.0260 (0.0004 -- 0.4885)  max mem: 16413
[2023-08-30 02:00:13,807] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:00:13,807] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-30 02:00:13,807] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:00:13,807] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 2.7523 (2.7519)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6046 (1.5750)  time: 0.8506 (0.5249 -- 3.3330)  data: 0.0322 (0.0004 -- 0.6126)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 2.7443 (2.7507)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5240 (1.5773)  time: 0.8770 (0.5341 -- 2.6236)  data: 0.0316 (0.0005 -- 0.6011)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:17  lr: 0.000004  min_lr: 0.000000  loss: 2.7474 (2.7501)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4995 (1.5714)  time: 0.7379 (0.5154 -- 2.6291)  data: 0.0231 (0.0004 -- 0.4355)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 2.7468 (2.7495)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5439 (1.5703)  time: 0.7191 (0.4958 -- 2.1361)  data: 0.0007 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8783 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 2.7468 (2.7496)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5439 (1.5703)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.7127 (2.7127)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.4089 (2.4089 -- 2.4089)  data: 2.1622 (2.1622 -- 2.1622)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7205 (2.7199)  acc1: 44.4444 (39.3939)  acc5: 88.8889 (89.8990)  time: 0.4190 (0.1923 -- 2.4089)  data: 0.2041 (0.0006 -- 2.1622)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7170 (2.7171)  acc1: 44.4444 (40.2116)  acc5: 88.8889 (91.0053)  time: 0.2151 (0.1698 -- 0.3632)  data: 0.0121 (0.0001 -- 0.1571)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7188 (2.7192)  acc1: 44.4444 (41.0788)  acc5: 88.8889 (89.6266)  time: 0.2010 (0.1332 -- 0.3632)  data: 0.0118 (0.0001 -- 0.1571)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 44.191 Acc@5 88.797 loss 2.718
Accuracy of the network on the 482 val images: 44.19%
Max accuracy: 46.68%
Epoch: [6]  [  0/160]  eta: 0:18:06  lr: 0.000004  min_lr: 0.000000  loss: 2.7308 (2.7308)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8970 (1.8970)  time: 6.7916 (6.7916 -- 6.7916)  data: 6.2288 (6.2288 -- 6.2288)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:45  lr: 0.000004  min_lr: 0.000000  loss: 2.7446 (2.7432)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5677 (1.6284)  time: 0.8988 (0.5172 -- 1.9910)  data: 0.2324 (0.0003 -- 1.3720)  max mem: 16413
[2023-08-30 02:01:50,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[9.934882381636529e-08, 9.934882381636529e-08, 1.324650984218204e-07, 1.324650984218204e-07, 1.7662013122909385e-07, 1.7662013122909385e-07, 2.3549350830545847e-07, 2.3549350830545847e-07, 3.13991344407278e-07, 3.13991344407278e-07, 4.186551258763706e-07, 4.186551258763706e-07, 5.582068345018274e-07, 5.582068345018274e-07, 7.442757793357699e-07, 7.442757793357699e-07, 9.923677057810267e-07, 9.923677057810267e-07, 1.3231569410413687e-06, 1.3231569410413687e-06, 1.764209254721825e-06, 1.764209254721825e-06, 2.3522790062957667e-06, 2.3522790062957667e-06, 3.136372008394356e-06, 3.136372008394356e-06, 4.181829344525808e-06, 4.181829344525808e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 02:01:50,370] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.563413649473226, CurrSamplesPerSec=21.132718616547137, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:01:59  lr: 0.000004  min_lr: 0.000000  loss: 2.7437 (2.7420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5891 (1.6386)  time: 0.8107 (0.5291 -- 2.9922)  data: 0.2422 (0.0003 -- 2.4732)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:35  lr: 0.000004  min_lr: 0.000000  loss: 2.7414 (2.7403)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6454 (1.6359)  time: 0.8691 (0.5337 -- 2.6991)  data: 0.3239 (0.0007 -- 2.1721)  max mem: 16413
[2023-08-30 02:02:11,363] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:02:11,363] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-30 02:02:11,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:02:11,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:13  lr: 0.000004  min_lr: 0.000000  loss: 2.7318 (2.7388)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6501 (1.6364)  time: 0.8176 (0.5176 -- 3.1372)  data: 0.2285 (0.0002 -- 2.5967)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:54  lr: 0.000004  min_lr: 0.000000  loss: 2.7294 (2.7372)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5879 (1.6344)  time: 0.8813 (0.5336 -- 2.4796)  data: 0.1711 (0.0003 -- 1.9618)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 2.7211 (2.7347)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5703 (1.6282)  time: 0.9031 (0.5281 -- 2.9723)  data: 0.2809 (0.0003 -- 2.4379)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:17  lr: 0.000005  min_lr: 0.000000  loss: 2.7256 (2.7327)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6613 (1.6374)  time: 0.8244 (0.5307 -- 2.8895)  data: 0.2175 (0.0005 -- 2.3613)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 2.7158 (2.7316)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5353 (1.6286)  time: 0.7416 (0.4970 -- 3.2465)  data: 0.0892 (0.0002 -- 1.0519)  max mem: 16413
Epoch: [6] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 2.7158 (2.7325)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5353 (1.6286)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.6727 (2.6727)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4042 (2.4042 -- 2.4042)  data: 2.1503 (2.1503 -- 2.1503)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6890 (2.6856)  acc1: 44.4444 (38.3838)  acc5: 100.0000 (90.9091)  time: 0.4346 (0.2068 -- 2.4042)  data: 0.2102 (0.0008 -- 2.1503)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6801 (2.6812)  acc1: 44.4444 (38.0952)  acc5: 100.0000 (91.5344)  time: 0.2174 (0.1708 -- 0.3835)  data: 0.0083 (0.0001 -- 0.1413)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6803 (2.6844)  acc1: 44.4444 (38.1743)  acc5: 88.8889 (90.0415)  time: 0.2011 (0.1326 -- 0.3835)  data: 0.0080 (0.0001 -- 0.1413)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 42.739 Acc@5 89.004 loss 2.683
Accuracy of the network on the 482 val images: 42.74%
Max accuracy: 46.68%
Epoch: [7]  [  0/160]  eta: 0:20:48  lr: 0.000005  min_lr: 0.000000  loss: 2.7211 (2.7211)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5003 (1.5003)  time: 7.8002 (7.8002 -- 7.8002)  data: 7.2201 (7.2201 -- 7.2201)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:38  lr: 0.000005  min_lr: 0.000000  loss: 2.7159 (2.7139)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6329 (1.6126)  time: 0.7981 (0.5199 -- 3.0585)  data: 0.2570 (0.0003 -- 2.5302)  max mem: 16413
[2023-08-30 02:04:12,837] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:04:12,837] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-08-30 02:04:12,837] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:04:12,837] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:00  lr: 0.000005  min_lr: 0.000000  loss: 2.7137 (2.7155)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5418 (1.5946)  time: 0.8750 (0.5231 -- 2.4371)  data: 0.3298 (0.0006 -- 1.8627)  max mem: 16413
Epoch: [7]  [ 60/160]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 2.7218 (2.7158)  loss_scale: 65536.0000 (48346.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6901 (1.6103)  time: 0.8655 (0.5314 -- 3.6057)  data: 0.3154 (0.0003 -- 3.0643)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:14  lr: 0.000005  min_lr: 0.000000  loss: 2.7077 (2.7136)  loss_scale: 65536.0000 (52590.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6767 (1.6192)  time: 0.8596 (0.5319 -- 2.5578)  data: 0.3109 (0.0004 -- 1.9980)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:53  lr: 0.000005  min_lr: 0.000000  loss: 2.7099 (2.7134)  loss_scale: 65536.0000 (55154.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6313 (1.6154)  time: 0.7548 (0.5292 -- 3.2330)  data: 0.1987 (0.0002 -- 2.6793)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 2.6993 (2.7115)  loss_scale: 65536.0000 (56870.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6249 (1.6200)  time: 0.9906 (0.5196 -- 3.4733)  data: 0.4446 (0.0004 -- 2.9359)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 2.6938 (2.7091)  loss_scale: 65536.0000 (58099.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6149 (1.6235)  time: 0.8159 (0.5362 -- 2.2341)  data: 0.1033 (0.0003 -- 1.0684)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 2.6926 (2.7065)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6605 (1.6273)  time: 0.7057 (0.4970 -- 3.9625)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8788 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 2.6926 (2.7086)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6605 (1.6273)
Val:  [ 0/27]  eta: 0:00:59  loss: 2.6211 (2.6211)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2114 (2.2114 -- 2.2114)  data: 2.0065 (2.0065 -- 2.0065)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.6500 (2.6407)  acc1: 44.4444 (37.3737)  acc5: 100.0000 (90.9091)  time: 0.3988 (0.1959 -- 2.2114)  data: 0.1851 (0.0008 -- 2.0065)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6291 (2.6342)  acc1: 44.4444 (38.0952)  acc5: 100.0000 (91.5344)  time: 0.2140 (0.1704 -- 0.2950)  data: 0.0055 (0.0001 -- 0.0765)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6332 (2.6386)  acc1: 33.3333 (37.3444)  acc5: 88.8889 (89.6266)  time: 0.1994 (0.1327 -- 0.2950)  data: 0.0053 (0.0001 -- 0.0765)  max mem: 16413
Val: Total time: 0:00:07 (0.2774 s / it)
* Acc@1 41.286 Acc@5 88.797 loss 2.637
Accuracy of the network on the 482 val images: 41.29%
Max accuracy: 46.68%
[2023-08-30 02:06:15,935] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:06:15,935] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:06:15,936] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2023-08-30 02:06:15,936] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [8]  [  0/160]  eta: 0:23:38  lr: 0.000005  min_lr: 0.000000  loss: 2.7092 (2.7092)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3304 (1.3304)  time: 8.8674 (8.8674 -- 8.8674)  data: 5.4411 (5.4411 -- 5.4411)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:41  lr: 0.000005  min_lr: 0.000000  loss: 2.7065 (2.7038)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6237 (1.6526)  time: 0.7672 (0.5204 -- 2.8588)  data: 0.0197 (0.0005 -- 0.3609)  max mem: 16413
[2023-08-30 02:06:38,542] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1308
[2023-08-30 02:06:38,542] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-08-30 02:06:38,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072, reducing to 65536.0
[2023-08-30 02:06:38,543] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1308
[2023-08-30 02:06:38,543] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
Epoch: [8]  [ 40/160]  eta: 0:02:10  lr: 0.000006  min_lr: 0.000000  loss: 2.6943 (2.7023)  loss_scale: 65536.0000 (110292.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6201 (1.6589)  time: 1.0242 (0.5307 -- 4.5481)  data: 0.1005 (0.0002 -- 1.3976)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 2.6781 (2.6928)  loss_scale: 65536.0000 (95618.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6090 (1.6552)  time: 0.7346 (0.5291 -- 2.3025)  data: 0.0021 (0.0004 -- 0.0160)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 2.6823 (2.6892)  loss_scale: 65536.0000 (88190.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7007 (1.6729)  time: 0.8265 (0.5231 -- 3.5627)  data: 0.0018 (0.0004 -- 0.0080)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:54  lr: 0.000006  min_lr: 0.000000  loss: 2.6710 (2.6853)  loss_scale: 65536.0000 (83704.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6558 (1.6882)  time: 0.8268 (0.5199 -- 2.9101)  data: 0.0018 (0.0002 -- 0.0065)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 2.6802 (2.6828)  loss_scale: 65536.0000 (80701.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5828 (1.6791)  time: 0.9273 (0.5204 -- 2.6558)  data: 0.2698 (0.0005 -- 2.0520)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:17  lr: 0.000006  min_lr: 0.000000  loss: 2.6903 (2.6820)  loss_scale: 65536.0000 (78550.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7806 (1.6949)  time: 0.7823 (0.5254 -- 2.0966)  data: 0.1307 (0.0004 -- 1.4085)  max mem: 16413
[2023-08-30 02:08:27,344] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:08:27,344] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:08:27,345] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:08:27,345] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 2.6766 (2.6813)  loss_scale: 65536.0000 (78233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6983 (1.6908)  time: 0.7549 (0.4965 -- 2.9159)  data: 0.2321 (0.0002 -- 2.3819)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 2.6766 (2.6821)  loss_scale: 65536.0000 (78233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6983 (1.6908)
Val:  [ 0/27]  eta: 0:00:58  loss: 2.5621 (2.5621)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.1752 (2.1752 -- 2.1752)  data: 1.9326 (1.9326 -- 1.9326)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.6022 (2.5894)  acc1: 44.4444 (38.3838)  acc5: 100.0000 (90.9091)  time: 0.4059 (0.1969 -- 2.1752)  data: 0.1844 (0.0007 -- 1.9326)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.5710 (2.5803)  acc1: 44.4444 (40.7407)  acc5: 88.8889 (91.5344)  time: 0.2172 (0.1698 -- 0.2991)  data: 0.0091 (0.0001 -- 0.0833)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.5827 (2.5862)  acc1: 44.4444 (40.2490)  acc5: 88.8889 (90.0415)  time: 0.1996 (0.1330 -- 0.2991)  data: 0.0088 (0.0001 -- 0.0833)  max mem: 16413
Val: Total time: 0:00:07 (0.2780 s / it)
* Acc@1 43.361 Acc@5 89.004 loss 2.585
Accuracy of the network on the 482 val images: 43.36%
Max accuracy: 46.68%
Epoch: [9]  [  0/160]  eta: 0:28:06  lr: 0.000006  min_lr: 0.000000  loss: 2.6774 (2.6774)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7213 (1.7213)  time: 10.5417 (10.5417 -- 10.5417)  data: 10.0233 (10.0233 -- 10.0233)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:02:54  lr: 0.000006  min_lr: 0.000000  loss: 2.6727 (2.6664)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7523 (1.7464)  time: 0.7790 (0.5255 -- 2.8355)  data: 0.2279 (0.0001 -- 2.2794)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:03  lr: 0.000006  min_lr: 0.000000  loss: 2.6836 (2.6666)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7308 (1.7513)  time: 0.7972 (0.5367 -- 2.5400)  data: 0.2398 (0.0004 -- 2.0242)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 2.6658 (2.6694)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6789 (1.7391)  time: 0.8413 (0.5341 -- 2.6285)  data: 0.1249 (0.0002 -- 2.0938)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:17  lr: 0.000006  min_lr: 0.000000  loss: 2.6612 (2.6667)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7888 (1.7551)  time: 0.9770 (0.5230 -- 2.7310)  data: 0.2870 (0.0002 -- 2.2093)  max mem: 16413
[2023-08-30 02:10:02,178] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1530
[2023-08-30 02:10:02,178] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:10:02,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-08-30 02:10:02,181] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1530
[2023-08-30 02:10:02,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [9]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 2.6404 (2.6619)  loss_scale: 65536.0000 (123934.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8371 (1.7690)  time: 0.8296 (0.5212 -- 3.1836)  data: 0.2083 (0.0003 -- 2.6827)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 2.6454 (2.6589)  loss_scale: 65536.0000 (114281.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7187 (1.7691)  time: 0.9354 (0.5237 -- 2.6508)  data: 0.2637 (0.0003 -- 2.0555)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 2.6365 (2.6553)  loss_scale: 65536.0000 (107367.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7574 (1.7757)  time: 0.7291 (0.5242 -- 2.5640)  data: 0.1362 (0.0003 -- 2.0101)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 2.6337 (2.6522)  loss_scale: 65536.0000 (102400.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8423 (1.7923)  time: 0.7296 (0.4983 -- 2.1758)  data: 0.1590 (0.0002 -- 1.6716)  max mem: 16413
Epoch: [9] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 2.6337 (2.6515)  loss_scale: 65536.0000 (102400.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8423 (1.7923)
Val:  [ 0/27]  eta: 0:01:01  loss: 2.4957 (2.4957)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2594 (2.2594 -- 2.2594)  data: 2.0486 (2.0486 -- 2.0486)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.5512 (2.5351)  acc1: 33.3333 (38.3838)  acc5: 100.0000 (89.8990)  time: 0.4069 (0.1976 -- 2.2594)  data: 0.1959 (0.0008 -- 2.0486)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.5130 (2.5237)  acc1: 33.3333 (37.5661)  acc5: 100.0000 (91.5344)  time: 0.2208 (0.1695 -- 0.4636)  data: 0.0175 (0.0001 -- 0.2415)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.5265 (2.5315)  acc1: 33.3333 (36.0996)  acc5: 88.8889 (88.7967)  time: 0.2055 (0.1328 -- 0.4636)  data: 0.0172 (0.0001 -- 0.2415)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 40.041 Acc@5 88.797 loss 2.529
Accuracy of the network on the 482 val images: 40.04%
Max accuracy: 46.68%
Epoch: [10]  [  0/160]  eta: 0:23:05  lr: 0.000007  min_lr: 0.000000  loss: 2.6752 (2.6752)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0119 (2.0119)  time: 8.6577 (8.6577 -- 8.6577)  data: 7.3970 (7.3970 -- 7.3970)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:03:00  lr: 0.000007  min_lr: 0.000000  loss: 2.6274 (2.6243)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7854 (1.7952)  time: 0.9201 (0.5180 -- 5.0751)  data: 0.0910 (0.0002 -- 1.1961)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:06  lr: 0.000007  min_lr: 0.000000  loss: 2.6377 (2.6344)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8075 (1.8259)  time: 0.8126 (0.5188 -- 2.9104)  data: 0.0020 (0.0002 -- 0.0134)  max mem: 16413
[2023-08-30 02:12:03,736] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:12:03,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:12:03,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:12:03,738] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 2.5962 (2.6281)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7906 (1.8429)  time: 0.7413 (0.5311 -- 2.4112)  data: 0.0015 (0.0005 -- 0.0064)  max mem: 16413
[2023-08-30 02:12:12,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1671
[2023-08-30 02:12:12,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1671
[2023-08-30 02:12:12,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:12:12,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:12:12,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 2.6082 (2.6235)  loss_scale: 65536.0000 (75245.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8715 (1.8507)  time: 0.9847 (0.5300 -- 4.5742)  data: 0.0139 (0.0002 -- 0.2400)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 2.6284 (2.6231)  loss_scale: 65536.0000 (73322.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8790 (1.8643)  time: 0.8335 (0.5220 -- 3.8548)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 2.6281 (2.6204)  loss_scale: 65536.0000 (72035.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8715 (1.8709)  time: 0.8507 (0.5228 -- 2.8527)  data: 0.0015 (0.0003 -- 0.0085)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 2.6081 (2.6182)  loss_scale: 65536.0000 (71113.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7786 (1.8667)  time: 0.8443 (0.5295 -- 3.6142)  data: 0.0022 (0.0005 -- 0.0157)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 2.6407 (2.6186)  loss_scale: 65536.0000 (70451.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8850 (1.8690)  time: 0.6853 (0.4981 -- 1.8057)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [10] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 2.6407 (2.6184)  loss_scale: 65536.0000 (70451.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8850 (1.8690)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.4288 (2.4288)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4231 (2.4231 -- 2.4231)  data: 2.1932 (2.1932 -- 2.1932)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4965 (2.4791)  acc1: 44.4444 (39.3939)  acc5: 100.0000 (90.9091)  time: 0.4185 (0.1835 -- 2.4231)  data: 0.2003 (0.0004 -- 2.1932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4514 (2.4645)  acc1: 44.4444 (40.7407)  acc5: 88.8889 (91.0053)  time: 0.2104 (0.1708 -- 0.2410)  data: 0.0007 (0.0001 -- 0.0023)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4629 (2.4742)  acc1: 44.4444 (40.6639)  acc5: 88.8889 (88.3817)  time: 0.1920 (0.1333 -- 0.2410)  data: 0.0004 (0.0001 -- 0.0012)  max mem: 16413
Val: Total time: 0:00:07 (0.2822 s / it)
* Acc@1 43.154 Acc@5 88.589 loss 2.473
Accuracy of the network on the 482 val images: 43.15%
Max accuracy: 46.68%
Epoch: [11]  [  0/160]  eta: 0:22:46  lr: 0.000007  min_lr: 0.000000  loss: 2.6363 (2.6363)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1702 (2.1702)  time: 8.5380 (8.5380 -- 8.5380)  data: 8.0072 (8.0072 -- 8.0072)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:47  lr: 0.000007  min_lr: 0.000000  loss: 2.6193 (2.6146)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9190 (1.9868)  time: 0.8301 (0.5334 -- 3.3444)  data: 0.2160 (0.0005 -- 1.6444)  max mem: 16413
[2023-08-30 02:14:16,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:14:16,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:14:16,715] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:14:16,715] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [ 40/160]  eta: 0:02:00  lr: 0.000008  min_lr: 0.000000  loss: 2.5948 (2.6092)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9421 (1.9400)  time: 0.8040 (0.5353 -- 2.3745)  data: 0.1564 (0.0004 -- 1.8048)  max mem: 16413
[2023-08-30 02:14:25,849] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1811
[2023-08-30 02:14:25,849] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1811
[2023-08-30 02:14:25,849] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:14:25,849] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:14:25,849] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 60/160]  eta: 0:01:34  lr: 0.000008  min_lr: 0.000000  loss: 2.6057 (2.6062)  loss_scale: 65536.0000 (77353.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9253 (1.9885)  time: 0.8279 (0.5209 -- 2.5175)  data: 0.0730 (0.0004 -- 1.0717)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:13  lr: 0.000008  min_lr: 0.000000  loss: 2.5813 (2.5996)  loss_scale: 65536.0000 (74435.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9425 (1.9865)  time: 0.8493 (0.5377 -- 3.3644)  data: 0.2600 (0.0001 -- 2.8278)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 2.6206 (2.5997)  loss_scale: 65536.0000 (72673.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0206 (1.9879)  time: 0.9918 (0.5260 -- 3.3587)  data: 0.0633 (0.0004 -- 0.9013)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 2.5466 (2.5932)  loss_scale: 65536.0000 (71493.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0437 (1.9936)  time: 0.8956 (0.5189 -- 4.7109)  data: 0.0016 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 2.5456 (2.5913)  loss_scale: 65536.0000 (70648.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8476 (1.9852)  time: 0.8836 (0.5326 -- 3.6051)  data: 0.0013 (0.0006 -- 0.0024)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 2.5731 (2.5889)  loss_scale: 65536.0000 (70041.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0906 (1.9971)  time: 0.6159 (0.4966 -- 2.3323)  data: 0.0061 (0.0001 -- 0.1109)  max mem: 16413
Epoch: [11] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 2.5731 (2.5858)  loss_scale: 65536.0000 (70041.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0906 (1.9971)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.3607 (2.3607)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3773 (2.3773 -- 2.3773)  data: 2.1616 (2.1616 -- 2.1616)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4325 (2.4179)  acc1: 44.4444 (41.4141)  acc5: 100.0000 (91.9192)  time: 0.4190 (0.1980 -- 2.3773)  data: 0.2112 (0.0007 -- 2.1616)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3952 (2.4005)  acc1: 44.4444 (41.7989)  acc5: 100.0000 (93.6508)  time: 0.2141 (0.1693 -- 0.3779)  data: 0.0122 (0.0001 -- 0.1524)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3982 (2.4124)  acc1: 44.4444 (41.9087)  acc5: 88.8889 (90.4564)  time: 0.2012 (0.1332 -- 0.3779)  data: 0.0120 (0.0001 -- 0.1524)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 44.606 Acc@5 90.456 loss 2.411
Accuracy of the network on the 482 val images: 44.61%
Max accuracy: 46.68%
Epoch: [12]  [  0/160]  eta: 0:22:54  lr: 0.000008  min_lr: 0.000000  loss: 2.3801 (2.3801)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8079 (1.8079)  time: 8.5914 (8.5914 -- 8.5914)  data: 4.6368 (4.6368 -- 4.6368)  max mem: 16413
[2023-08-30 02:16:29,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:16:29,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:16:29,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:16:29,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 20/160]  eta: 0:02:43  lr: 0.000008  min_lr: 0.000000  loss: 2.5255 (2.5287)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0443 (2.1161)  time: 0.7931 (0.5213 -- 3.1766)  data: 0.1686 (0.0004 -- 2.6318)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:02:02  lr: 0.000008  min_lr: 0.000000  loss: 2.6073 (2.5480)  loss_scale: 131072.0000 (99103.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0417 (2.0724)  time: 0.8638 (0.5164 -- 3.5055)  data: 0.3146 (0.0004 -- 2.9822)  max mem: 16413
[2023-08-30 02:16:56,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1970
[2023-08-30 02:16:56,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1970
[2023-08-30 02:16:56,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:16:56,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-08-30 02:16:56,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [12]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 2.5870 (2.5566)  loss_scale: 65536.0000 (97766.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0738 (2.0868)  time: 0.9907 (0.5245 -- 3.4213)  data: 0.3989 (0.0004 -- 2.8865)  max mem: 16413
[2023-08-30 02:17:21,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[1.9879709590481905e-07, 1.9879709590481905e-07, 2.650627945397587e-07, 2.650627945397587e-07, 3.53417059386345e-07, 3.53417059386345e-07, 4.7122274584845994e-07, 4.7122274584845994e-07, 6.282969944646132e-07, 6.282969944646132e-07, 8.377293259528177e-07, 8.377293259528177e-07, 1.1169724346037568e-06, 1.1169724346037568e-06, 1.4892965794716759e-06, 1.4892965794716759e-06, 1.9857287726289013e-06, 1.9857287726289013e-06, 2.6476383635052016e-06, 2.6476383635052016e-06, 3.5301844846736022e-06, 3.5301844846736022e-06, 4.70691264623147e-06, 4.70691264623147e-06, 6.275883528308626e-06, 6.275883528308626e-06, 8.367844704411501e-06, 8.367844704411501e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 02:17:21,677] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.605809944174148, CurrSamplesPerSec=21.517646842550107, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 2.5438 (2.5500)  loss_scale: 65536.0000 (89808.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2202 (2.1607)  time: 0.7711 (0.5106 -- 4.6641)  data: 0.0213 (0.0003 -- 0.4008)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 2.5439 (2.5521)  loss_scale: 65536.0000 (85002.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2545 (2.1819)  time: 0.9001 (0.5351 -- 3.7783)  data: 0.0014 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 2.5250 (2.5501)  loss_scale: 65536.0000 (81784.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1757 (2.1843)  time: 0.7668 (0.5281 -- 2.2601)  data: 0.0353 (0.0002 -- 0.5492)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 2.4901 (2.5432)  loss_scale: 65536.0000 (79479.8298)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1293 (2.1912)  time: 0.8325 (0.5218 -- 2.7342)  data: 0.0034 (0.0005 -- 0.0160)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.5265 (2.5409)  loss_scale: 65536.0000 (77824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0239 (2.1921)  time: 0.7413 (0.4991 -- 2.1260)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [12] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.5265 (2.5420)  loss_scale: 65536.0000 (77824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0239 (2.1921)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.2828 (2.2828)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3974 (2.3974 -- 2.3974)  data: 2.1816 (2.1816 -- 2.1816)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.3677 (2.3518)  acc1: 44.4444 (40.4040)  acc5: 100.0000 (93.9394)  time: 0.4148 (0.1993 -- 2.3974)  data: 0.1996 (0.0007 -- 2.1816)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3232 (2.3339)  acc1: 44.4444 (39.6825)  acc5: 100.0000 (93.6508)  time: 0.2125 (0.1709 -- 0.2522)  data: 0.0036 (0.0001 -- 0.0523)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3316 (2.3473)  acc1: 44.4444 (39.4191)  acc5: 88.8889 (90.4564)  time: 0.1960 (0.1332 -- 0.2522)  data: 0.0032 (0.0001 -- 0.0523)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 42.946 Acc@5 90.041 loss 2.346
Accuracy of the network on the 482 val images: 42.95%
Max accuracy: 46.68%
Epoch: [13]  [  0/160]  eta: 0:23:15  lr: 0.000009  min_lr: 0.000000  loss: 2.6230 (2.6230)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2343 (2.2343)  time: 8.7224 (8.7224 -- 8.7224)  data: 6.5563 (6.5563 -- 6.5563)  max mem: 16413
[2023-08-30 02:19:00,746] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:19:00,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:19:00,749] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:19:00,749] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 20/160]  eta: 0:03:01  lr: 0.000009  min_lr: 0.000000  loss: 2.5540 (2.5428)  loss_scale: 65536.0000 (71777.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2581 (2.4136)  time: 0.9273 (0.5165 -- 4.6641)  data: 0.0016 (0.0004 -- 0.0053)  max mem: 16413
[2023-08-30 02:19:12,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-08-30 02:19:12,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:19:12,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-08-30 02:19:12,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:19:12,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 40/160]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 2.5267 (2.5349)  loss_scale: 131072.0000 (87914.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2809 (2.3982)  time: 0.8729 (0.5195 -- 3.4762)  data: 0.0013 (0.0002 -- 0.0058)  max mem: 16413
[2023-08-30 02:19:31,454] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2139
[2023-08-30 02:19:31,454] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2139
[2023-08-30 02:19:31,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:19:31,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:19:31,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 60/160]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 2.5372 (2.5357)  loss_scale: 65536.0000 (79502.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2803 (2.3740)  time: 0.6633 (0.5336 -- 1.9886)  data: 0.0022 (0.0002 -- 0.0149)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 2.5675 (2.5341)  loss_scale: 32768.0000 (67963.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9841 (2.3193)  time: 1.0518 (0.5283 -- 4.2404)  data: 0.0020 (0.0007 -- 0.0099)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 2.5481 (2.5325)  loss_scale: 32768.0000 (60993.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3049 (2.3539)  time: 0.7525 (0.5235 -- 4.4210)  data: 0.0019 (0.0004 -- 0.0137)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 2.4677 (2.5285)  loss_scale: 32768.0000 (56328.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0612 (2.3442)  time: 0.9477 (0.5268 -- 3.2485)  data: 0.0018 (0.0007 -- 0.0052)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 2.4943 (2.5192)  loss_scale: 32768.0000 (52986.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3428 (2.3450)  time: 0.7412 (0.5256 -- 2.2571)  data: 0.0024 (0.0002 -- 0.0104)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.4856 (2.5147)  loss_scale: 32768.0000 (50585.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3357 (2.3516)  time: 0.6840 (0.4968 -- 2.5000)  data: 0.0007 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [13] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.4856 (2.5141)  loss_scale: 32768.0000 (50585.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3357 (2.3516)
Val:  [ 0/27]  eta: 0:01:00  loss: 2.2165 (2.2165)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2465 (2.2465 -- 2.2465)  data: 2.0162 (2.0162 -- 2.0162)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.2964 (2.2892)  acc1: 44.4444 (41.4141)  acc5: 100.0000 (93.9394)  time: 0.4128 (0.2008 -- 2.2465)  data: 0.1966 (0.0005 -- 2.0162)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.2612 (2.2688)  acc1: 33.3333 (38.0952)  acc5: 100.0000 (94.1799)  time: 0.2212 (0.1706 -- 0.3769)  data: 0.0118 (0.0001 -- 0.1367)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2702 (2.2853)  acc1: 33.3333 (37.3444)  acc5: 88.8889 (90.8714)  time: 0.2064 (0.1333 -- 0.3769)  data: 0.0115 (0.0001 -- 0.1367)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 41.701 Acc@5 90.456 loss 2.283
Accuracy of the network on the 482 val images: 41.70%
Max accuracy: 46.68%
Epoch: [14]  [  0/160]  eta: 0:21:06  lr: 0.000009  min_lr: 0.000000  loss: 2.2251 (2.2251)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5054 (2.5054)  time: 7.9130 (7.9130 -- 7.9130)  data: 7.3825 (7.3825 -- 7.3825)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:46  lr: 0.000009  min_lr: 0.000000  loss: 2.5489 (2.5077)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5619 (2.5748)  time: 0.8497 (0.5210 -- 3.4190)  data: 0.2986 (0.0001 -- 2.8492)  max mem: 16413
[2023-08-30 02:21:34,329] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:21:34,329] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:21:34,329] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:21:34,329] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 40/160]  eta: 0:02:02  lr: 0.000010  min_lr: 0.000000  loss: 2.4752 (2.4927)  loss_scale: 65536.0000 (43157.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3286 (2.5403)  time: 0.8515 (0.5144 -- 4.6642)  data: 0.3095 (0.0003 -- 4.1488)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000000  loss: 2.4855 (2.5009)  loss_scale: 65536.0000 (50494.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3188 (2.5284)  time: 0.8147 (0.5222 -- 2.9240)  data: 0.2589 (0.0004 -- 2.3659)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 2.4675 (2.4962)  loss_scale: 65536.0000 (54208.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2705 (2.5063)  time: 0.9813 (0.5243 -- 2.1867)  data: 0.1216 (0.0002 -- 1.0807)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 2.5236 (2.4939)  loss_scale: 65536.0000 (56451.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3142 (2.4952)  time: 0.7806 (0.5360 -- 1.9500)  data: 0.0141 (0.0003 -- 0.2552)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 2.4822 (2.4898)  loss_scale: 65536.0000 (57953.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5039 (2.5371)  time: 0.8800 (0.5321 -- 2.2766)  data: 0.1532 (0.0003 -- 1.5834)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 2.5106 (2.4909)  loss_scale: 65536.0000 (59028.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5775 (2.5554)  time: 0.8862 (0.5282 -- 3.2186)  data: 0.3037 (0.0004 -- 2.7024)  max mem: 16413
[2023-08-30 02:23:23,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:23:23,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:23:23,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:23:23,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 2.4672 (2.4875)  loss_scale: 65536.0000 (61440.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4895 (2.5516)  time: 0.7664 (0.4982 -- 2.6531)  data: 0.2252 (0.0002 -- 2.1223)  max mem: 16413
Epoch: [14] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 2.4672 (2.4882)  loss_scale: 65536.0000 (61440.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4895 (2.5516)
Val:  [ 0/27]  eta: 0:00:58  loss: 2.1549 (2.1549)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.1836 (2.1836 -- 2.1836)  data: 1.9874 (1.9874 -- 1.9874)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.2160 (2.2249)  acc1: 44.4444 (39.3939)  acc5: 100.0000 (93.9394)  time: 0.4067 (0.1980 -- 2.1836)  data: 0.1896 (0.0003 -- 1.9874)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1961 (2.2016)  acc1: 44.4444 (38.0952)  acc5: 100.0000 (94.7090)  time: 0.2235 (0.1691 -- 0.3468)  data: 0.0129 (0.0001 -- 0.1377)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2171 (2.2217)  acc1: 33.3333 (37.3444)  acc5: 88.8889 (91.2863)  time: 0.2052 (0.1333 -- 0.3468)  data: 0.0119 (0.0001 -- 0.1377)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 40.664 Acc@5 91.079 loss 2.215
Accuracy of the network on the 482 val images: 40.66%
Max accuracy: 46.68%
Epoch: [15]  [  0/160]  eta: 0:16:19  lr: 0.000010  min_lr: 0.000000  loss: 2.6386 (2.6386)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9179 (2.9179)  time: 6.1208 (6.1208 -- 6.1208)  data: 5.5485 (5.5485 -- 5.5485)  max mem: 16413
[2023-08-30 02:23:50,515] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2412
[2023-08-30 02:23:50,515] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2412
[2023-08-30 02:23:50,515] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:23:50,515] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:23:50,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 20/160]  eta: 0:02:38  lr: 0.000010  min_lr: 0.000000  loss: 2.4238 (2.4296)  loss_scale: 131072.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3582 (2.4750)  time: 0.8863 (0.5344 -- 2.5817)  data: 0.0858 (0.0002 -- 1.6741)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:01:56  lr: 0.000010  min_lr: 0.000000  loss: 2.4145 (2.4250)  loss_scale: 65536.0000 (84717.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7482 (2.7493)  time: 0.8018 (0.5258 -- 2.5452)  data: 0.0133 (0.0004 -- 0.2359)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000000  loss: 2.4851 (2.4287)  loss_scale: 65536.0000 (78428.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6981 (2.7625)  time: 0.9656 (0.5255 -- 4.2023)  data: 0.0949 (0.0005 -- 1.3121)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:14  lr: 0.000010  min_lr: 0.000000  loss: 2.4117 (2.4281)  loss_scale: 65536.0000 (75245.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6912 (2.7456)  time: 0.8176 (0.5263 -- 3.4317)  data: 0.0021 (0.0003 -- 0.0172)  max mem: 16413
[2023-08-30 02:25:08,214] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2500
[2023-08-30 02:25:08,214] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2500
[2023-08-30 02:25:08,214] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:25:08,214] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:25:08,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 2.4409 (2.4284)  loss_scale: 65536.0000 (72998.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5362 (2.8009)  time: 1.0069 (0.5129 -- 4.2030)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 2.5137 (2.4358)  loss_scale: 32768.0000 (66348.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8334 (2.8456)  time: 0.7935 (0.5198 -- 2.8389)  data: 0.0012 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 2.4473 (2.4359)  loss_scale: 32768.0000 (61585.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6572 (2.8589)  time: 0.8833 (0.5228 -- 3.7363)  data: 0.2434 (0.0003 -- 3.2122)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 2.3876 (2.4320)  loss_scale: 32768.0000 (58163.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7103 (2.8565)  time: 0.7589 (0.4966 -- 2.6656)  data: 0.2393 (0.0002 -- 2.1591)  max mem: 16413
Epoch: [15] Total time: 0:02:23 (0.8992 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 2.3876 (2.4407)  loss_scale: 32768.0000 (58163.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7103 (2.8565)
Val:  [ 0/27]  eta: 0:01:01  loss: 2.0855 (2.0855)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2639 (2.2639 -- 2.2639)  data: 2.0468 (2.0468 -- 2.0468)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.1709 (2.1590)  acc1: 44.4444 (40.4040)  acc5: 100.0000 (93.9394)  time: 0.4258 (0.2028 -- 2.2639)  data: 0.2144 (0.0007 -- 2.0468)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1134 (2.1341)  acc1: 44.4444 (39.6825)  acc5: 100.0000 (95.2381)  time: 0.2232 (0.1710 -- 0.5147)  data: 0.0200 (0.0001 -- 0.3026)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1256 (2.1554)  acc1: 44.4444 (38.5892)  acc5: 100.0000 (92.9461)  time: 0.2088 (0.1332 -- 0.5147)  data: 0.0197 (0.0001 -- 0.3026)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 42.739 Acc@5 92.116 loss 2.150
Accuracy of the network on the 482 val images: 42.74%
Max accuracy: 46.68%
Epoch: [16]  [  0/160]  eta: 0:17:04  lr: 0.000011  min_lr: 0.000000  loss: 2.4404 (2.4404)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3781 (2.3781)  time: 6.4056 (6.4056 -- 6.4056)  data: 5.1567 (5.1567 -- 5.1567)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 2.4277 (2.4205)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5441 (3.0901)  time: 0.9013 (0.5224 -- 4.4791)  data: 0.3410 (0.0006 -- 3.9539)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000000  loss: 2.4726 (2.4314)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7186 (3.3593)  time: 0.9227 (0.5188 -- 3.6657)  data: 0.3660 (0.0004 -- 3.1319)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:32  lr: 0.000011  min_lr: 0.000000  loss: 2.4000 (2.4175)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8112 (3.2045)  time: 0.6895 (0.5354 -- 2.2949)  data: 0.1164 (0.0002 -- 1.7526)  max mem: 16413
[2023-08-30 02:27:07,840] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:27:07,840] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:27:07,840] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:27:07,840] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 2.4232 (2.4169)  loss_scale: 65536.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8081 (3.1647)  time: 0.9155 (0.5367 -- 2.0591)  data: 0.1364 (0.0003 -- 1.1922)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 2.3547 (2.4128)  loss_scale: 65536.0000 (43149.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8307 (3.1510)  time: 0.8963 (0.5171 -- 3.6932)  data: 0.0506 (0.0004 -- 0.9854)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 2.4119 (2.4153)  loss_scale: 65536.0000 (46850.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5652 (3.1390)  time: 0.9038 (0.5290 -- 2.5667)  data: 0.1569 (0.0004 -- 1.5035)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:17  lr: 0.000011  min_lr: 0.000000  loss: 2.3817 (2.4108)  loss_scale: 65536.0000 (49500.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6193 (3.1132)  time: 0.7932 (0.5205 -- 3.9814)  data: 0.1942 (0.0003 -- 3.4726)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 2.3892 (2.4179)  loss_scale: 65536.0000 (51404.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6998 (3.0840)  time: 0.7091 (0.4967 -- 2.6962)  data: 0.1168 (0.0002 -- 2.1810)  max mem: 16413
Epoch: [16] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 2.3892 (2.4122)  loss_scale: 65536.0000 (51404.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6998 (3.0840)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.0180 (2.0180)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4353 (2.4353 -- 2.4353)  data: 2.1382 (2.1382 -- 2.1382)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0994 (2.1006)  acc1: 44.4444 (41.4141)  acc5: 100.0000 (93.9394)  time: 0.4218 (0.2045 -- 2.4353)  data: 0.1955 (0.0007 -- 2.1382)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.0451 (2.0731)  acc1: 44.4444 (40.2116)  acc5: 100.0000 (94.7090)  time: 0.2113 (0.1709 -- 0.2421)  data: 0.0009 (0.0002 -- 0.0019)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1233 (2.0985)  acc1: 33.3333 (37.7593)  acc5: 88.8889 (92.1162)  time: 0.1937 (0.1334 -- 0.2240)  data: 0.0006 (0.0001 -- 0.0019)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 42.739 Acc@5 91.701 loss 2.092
Accuracy of the network on the 482 val images: 42.74%
Max accuracy: 46.68%
Epoch: [17]  [  0/160]  eta: 0:22:37  lr: 0.000011  min_lr: 0.000000  loss: 2.5339 (2.5339)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1757 (3.1757)  time: 8.4865 (8.4865 -- 8.4865)  data: 7.9506 (7.9506 -- 7.9506)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:41  lr: 0.000011  min_lr: 0.000000  loss: 2.3927 (2.3979)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9044 (2.9865)  time: 0.7866 (0.5258 -- 4.5500)  data: 0.2359 (0.0004 -- 4.0145)  max mem: 16413
[2023-08-30 02:29:11,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:29:11,724] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-30 02:29:11,725] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:29:11,725] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 40/160]  eta: 0:02:07  lr: 0.000012  min_lr: 0.000000  loss: 2.3723 (2.4120)  loss_scale: 65536.0000 (71929.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9093 (3.0091)  time: 0.9608 (0.5121 -- 4.4069)  data: 0.2843 (0.0003 -- 3.8506)  max mem: 16413
[2023-08-30 02:29:29,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2776
[2023-08-30 02:29:29,784] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-30 02:29:29,784] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-08-30 02:29:29,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2776
[2023-08-30 02:29:29,785] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [17]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 2.3499 (2.3903)  loss_scale: 131072.0000 (85948.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0674 (2.9994)  time: 0.8118 (0.5150 -- 3.4374)  data: 0.1020 (0.0004 -- 1.1977)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 2.4405 (2.3981)  loss_scale: 65536.0000 (80908.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2385 (3.0495)  time: 0.8306 (0.5192 -- 2.6007)  data: 0.2839 (0.0006 -- 2.0650)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 2.4079 (2.3971)  loss_scale: 65536.0000 (77864.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3276 (3.1426)  time: 0.8735 (0.5224 -- 3.9271)  data: 0.3021 (0.0003 -- 3.4029)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 2.4573 (2.3992)  loss_scale: 65536.0000 (75826.7769)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1090 (3.1688)  time: 0.9622 (0.5333 -- 4.3965)  data: 0.4182 (0.0003 -- 3.8766)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 2.4097 (2.4008)  loss_scale: 65536.0000 (74367.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2609 (3.2004)  time: 0.8515 (0.5170 -- 4.2302)  data: 0.3060 (0.0001 -- 3.6886)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 2.3504 (2.3919)  loss_scale: 65536.0000 (73318.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3798 (3.2243)  time: 0.6417 (0.4972 -- 2.7477)  data: 0.1266 (0.0002 -- 2.2393)  max mem: 16413
Epoch: [17] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 2.3504 (2.3858)  loss_scale: 65536.0000 (73318.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3798 (3.2243)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.9582 (1.9582)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2814 (2.2814 -- 2.2814)  data: 2.0533 (2.0533 -- 2.0533)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0368 (2.0381)  acc1: 44.4444 (45.4545)  acc5: 100.0000 (93.9394)  time: 0.4221 (0.1942 -- 2.2814)  data: 0.1970 (0.0005 -- 2.0533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9819 (2.0053)  acc1: 44.4444 (43.9153)  acc5: 100.0000 (94.7090)  time: 0.2226 (0.1703 -- 0.3372)  data: 0.0076 (0.0001 -- 0.0989)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0368 (2.0348)  acc1: 44.4444 (42.3237)  acc5: 88.8889 (92.1162)  time: 0.2038 (0.1331 -- 0.3372)  data: 0.0070 (0.0001 -- 0.0989)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 46.266 Acc@5 91.909 loss 2.025
Accuracy of the network on the 482 val images: 46.27%
Max accuracy: 46.68%
Epoch: [18]  [  0/160]  eta: 0:23:29  lr: 0.000012  min_lr: 0.000000  loss: 2.3962 (2.3962)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5178 (3.5178)  time: 8.8074 (8.8074 -- 8.8074)  data: 8.2586 (8.2586 -- 8.2586)  max mem: 16413
[2023-08-30 02:31:19,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2892
[2023-08-30 02:31:19,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2892
[2023-08-30 02:31:19,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:31:19,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:31:19,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 2.4041 (2.3862)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3810 (3.4736)  time: 0.7768 (0.5182 -- 2.7049)  data: 0.1965 (0.0005 -- 2.1219)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000000  loss: 2.3479 (2.3611)  loss_scale: 32768.0000 (42358.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5239 (3.5453)  time: 0.9724 (0.5239 -- 4.0897)  data: 0.4261 (0.0004 -- 3.5465)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:39  lr: 0.000012  min_lr: 0.000000  loss: 2.3857 (2.3599)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1885 (3.5425)  time: 0.8561 (0.5259 -- 2.9435)  data: 0.2186 (0.0003 -- 2.3994)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 2.3625 (2.3561)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8507 (3.5864)  time: 0.8734 (0.5266 -- 2.7895)  data: 0.3139 (0.0002 -- 2.2614)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 2.3016 (2.3469)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2177 (3.5804)  time: 0.7662 (0.5306 -- 2.1804)  data: 0.2172 (0.0002 -- 1.6317)  max mem: 16413
[2023-08-30 02:32:53,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=11, lr=[2.982453679932728e-07, 2.982453679932728e-07, 3.9766049065769705e-07, 3.9766049065769705e-07, 5.30213987543596e-07, 5.30213987543596e-07, 7.069519833914614e-07, 7.069519833914614e-07, 9.426026445219485e-07, 9.426026445219485e-07, 1.2568035260292646e-06, 1.2568035260292646e-06, 1.6757380347056862e-06, 1.6757380347056862e-06, 2.234317379607582e-06, 2.234317379607582e-06, 2.9790898394767754e-06, 2.9790898394767754e-06, 3.972119785969034e-06, 3.972119785969034e-06, 5.296159714625379e-06, 5.296159714625379e-06, 7.061546286167172e-06, 7.061546286167172e-06, 9.415395048222896e-06, 9.415395048222896e-06, 1.2553860064297194e-05, 1.2553860064297194e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 02:32:53,404] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.619775479442506, CurrSamplesPerSec=23.222217300792842, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000000  loss: 2.3503 (2.3494)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5612 (3.6751)  time: 1.0460 (0.5219 -- 3.8286)  data: 0.4969 (0.0004 -- 3.3083)  max mem: 16413
[2023-08-30 02:33:05,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3014
[2023-08-30 02:33:05,675] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:33:05,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3014
[2023-08-30 02:33:05,675] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:33:05,675] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 2.2876 (2.3418)  loss_scale: 32768.0000 (34743.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5470 (3.6542)  time: 0.6912 (0.5133 -- 2.4853)  data: 0.1316 (0.0001 -- 1.9004)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 2.2185 (2.3351)  loss_scale: 16384.0000 (32563.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7336 (3.7370)  time: 0.6510 (0.4955 -- 2.0445)  data: 0.1225 (0.0002 -- 1.5137)  max mem: 16413
Epoch: [18] Total time: 0:02:20 (0.8808 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 2.2185 (2.3361)  loss_scale: 16384.0000 (32563.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7336 (3.7370)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.8984 (1.8984)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3671 (2.3671 -- 2.3671)  data: 2.1023 (2.1023 -- 2.1023)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.9747 (1.9715)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (94.9495)  time: 0.4167 (0.2069 -- 2.3671)  data: 0.1922 (0.0007 -- 2.1023)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8998 (1.9363)  acc1: 44.4444 (42.8571)  acc5: 100.0000 (95.7672)  time: 0.2182 (0.1724 -- 0.3272)  data: 0.0089 (0.0001 -- 0.1366)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.9782 (1.9669)  acc1: 44.4444 (41.4938)  acc5: 88.8889 (93.3610)  time: 0.2000 (0.1336 -- 0.3272)  data: 0.0086 (0.0001 -- 0.1366)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 45.643 Acc@5 92.531 loss 1.954
Accuracy of the network on the 482 val images: 45.64%
Max accuracy: 46.68%
Epoch: [19]  [  0/160]  eta: 0:18:19  lr: 0.000013  min_lr: 0.000000  loss: 2.2196 (2.2196)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4302 (2.4302)  time: 6.8692 (6.8692 -- 6.8692)  data: 5.9206 (5.9206 -- 5.9206)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:44  lr: 0.000013  min_lr: 0.000000  loss: 2.3191 (2.3325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8701 (4.0801)  time: 0.8932 (0.5138 -- 3.4480)  data: 0.2407 (0.0007 -- 1.7862)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:02:03  lr: 0.000013  min_lr: 0.000000  loss: 2.3676 (2.3668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0764 (4.1762)  time: 0.8709 (0.5272 -- 3.5143)  data: 0.3088 (0.0003 -- 2.9167)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000000  loss: 2.2812 (2.3533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5946 (4.0222)  time: 0.9310 (0.5299 -- 3.2228)  data: 0.2118 (0.0003 -- 2.6918)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000000  loss: 2.3806 (2.3486)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8947 (4.0209)  time: 0.7715 (0.5264 -- 3.5333)  data: 0.0016 (0.0003 -- 0.0116)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 2.2656 (2.3420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9122 (4.1060)  time: 0.8963 (0.5305 -- 2.8977)  data: 0.2591 (0.0002 -- 2.3389)  max mem: 16413
[2023-08-30 02:35:06,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:35:06,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:35:06,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 02:35:06,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 2.3776 (2.3480)  loss_scale: 32768.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8904 (4.1046)  time: 0.8841 (0.5177 -- 3.6598)  data: 0.3387 (0.0004 -- 3.1402)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 2.3492 (2.3490)  loss_scale: 32768.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2915 (4.0325)  time: 0.8239 (0.5230 -- 3.0531)  data: 0.2084 (0.0004 -- 2.5401)  max mem: 16413
[2023-08-30 02:35:45,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3186
[2023-08-30 02:35:45,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3186
[2023-08-30 02:35:45,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:35:45,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:35:45,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 2.3317 (2.3479)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6100 (4.0226)  time: 0.7541 (0.4979 -- 2.0096)  data: 0.0015 (0.0002 -- 0.0161)  max mem: 16413
Epoch: [19] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 2.3317 (2.3324)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6100 (4.0226)
[2023-08-30 02:35:54,072] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-30 02:35:54,074] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-30 02:35:54,074] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-30 02:35:54,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-30 02:35:55,081] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-30 02:35:55,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 1.8541 (1.8541)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4923 (2.4923 -- 2.4923)  data: 2.2927 (2.2927 -- 2.2927)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.9059 (1.9159)  acc1: 44.4444 (45.4545)  acc5: 100.0000 (93.9394)  time: 0.4172 (0.1875 -- 2.4923)  data: 0.2093 (0.0006 -- 2.2927)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8459 (1.8765)  acc1: 44.4444 (42.8571)  acc5: 100.0000 (95.2381)  time: 0.2127 (0.1692 -- 0.3350)  data: 0.0104 (0.0001 -- 0.1119)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.9059 (1.9102)  acc1: 44.4444 (41.4938)  acc5: 88.8889 (92.9461)  time: 0.1983 (0.1328 -- 0.3350)  data: 0.0101 (0.0001 -- 0.1119)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 46.266 Acc@5 92.324 loss 1.894
Accuracy of the network on the 482 val images: 46.27%
Max accuracy: 46.68%
Epoch: [20]  [  0/160]  eta: 0:16:23  lr: 0.000013  min_lr: 0.000000  loss: 2.4536 (2.4536)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2967 (3.2967)  time: 6.1452 (6.1452 -- 6.1452)  data: 5.6202 (5.6202 -- 5.6202)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:39  lr: 0.000013  min_lr: 0.000000  loss: 2.2427 (2.2410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0489 (4.6336)  time: 0.8881 (0.5295 -- 3.0487)  data: 0.1191 (0.0004 -- 1.2376)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:00  lr: 0.000014  min_lr: 0.000000  loss: 2.2553 (2.2468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6713 (5.0213)  time: 0.8604 (0.5345 -- 3.0706)  data: 0.1980 (0.0003 -- 2.3252)  max mem: 16413
Epoch: [20]  [ 60/160]  eta: 0:01:34  lr: 0.000014  min_lr: 0.000000  loss: 2.3826 (2.2782)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7123 (4.9389)  time: 0.8306 (0.5181 -- 2.8506)  data: 0.2275 (0.0003 -- 2.3355)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000000  loss: 2.1747 (2.2636)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6114 (4.7648)  time: 0.8988 (0.5257 -- 2.1925)  data: 0.1342 (0.0004 -- 1.2322)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 2.3075 (2.2711)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0288 (4.5839)  time: 0.8816 (0.5294 -- 2.2213)  data: 0.1243 (0.0004 -- 1.5427)  max mem: 16413
[2023-08-30 02:37:48,743] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:37:48,743] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 02:37:48,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:37:48,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 2.2355 (2.2764)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8183 (4.5010)  time: 0.9499 (0.5303 -- 4.1852)  data: 0.0368 (0.0003 -- 0.6930)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 2.4059 (2.2832)  loss_scale: 32768.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9470 (4.5527)  time: 0.7386 (0.5250 -- 1.9855)  data: 0.0738 (0.0001 -- 1.4395)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 2.2328 (2.2819)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9687 (4.5334)  time: 0.7612 (0.4962 -- 2.5746)  data: 0.0669 (0.0003 -- 1.1103)  max mem: 16413
Epoch: [20] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 2.2328 (2.2761)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9687 (4.5334)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.7951 (1.7951)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4499 (2.4499 -- 2.4499)  data: 2.2195 (2.2195 -- 2.2195)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.8268 (1.8496)  acc1: 44.4444 (46.4646)  acc5: 100.0000 (94.9495)  time: 0.4174 (0.2028 -- 2.4499)  data: 0.2036 (0.0007 -- 2.2195)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8096 (1.8136)  acc1: 44.4444 (44.9735)  acc5: 100.0000 (95.7672)  time: 0.2119 (0.1704 -- 0.3379)  data: 0.0056 (0.0001 -- 0.0897)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8268 (1.8497)  acc1: 44.4444 (44.3983)  acc5: 88.8889 (93.7759)  time: 0.1966 (0.1329 -- 0.3379)  data: 0.0049 (0.0001 -- 0.0897)  max mem: 16413
Val: Total time: 0:00:07 (0.2844 s / it)
* Acc@1 49.585 Acc@5 92.739 loss 1.832
Accuracy of the network on the 482 val images: 49.59%
[2023-08-30 02:38:32,351] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:38:32,353] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:38:32,353] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:38:32,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:38:33,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:38:33,693] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 49.59%
Epoch: [21]  [  0/160]  eta: 0:22:06  lr: 0.000014  min_lr: 0.000000  loss: 2.5897 (2.5897)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6934 (3.6934)  time: 8.2878 (8.2878 -- 8.2878)  data: 5.2119 (5.2119 -- 5.2119)  max mem: 16413
[2023-08-30 02:38:44,156] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3364
[2023-08-30 02:38:44,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:38:44,156] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3364
[2023-08-30 02:38:44,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:38:44,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 20/160]  eta: 0:02:35  lr: 0.000014  min_lr: 0.000000  loss: 2.3141 (2.3072)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1170 (5.1372)  time: 0.7541 (0.5215 -- 2.6478)  data: 0.0016 (0.0007 -- 0.0048)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:03  lr: 0.000014  min_lr: 0.000000  loss: 2.3559 (2.3134)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7749 (4.7079)  time: 0.9343 (0.5260 -- 3.3468)  data: 0.1073 (0.0002 -- 1.0096)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 2.2815 (2.3057)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7756 (4.6824)  time: 0.8598 (0.5201 -- 2.9301)  data: 0.1755 (0.0005 -- 0.9716)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 2.2448 (2.3013)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7835 (4.5349)  time: 0.9068 (0.5348 -- 2.6532)  data: 0.2687 (0.0003 -- 2.0641)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 2.1492 (2.2882)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1559 (4.5914)  time: 0.8204 (0.5285 -- 2.5134)  data: 0.0877 (0.0006 -- 0.8211)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 2.2294 (2.2808)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5141 (4.6254)  time: 0.8049 (0.5186 -- 3.9537)  data: 0.0019 (0.0002 -- 0.0103)  max mem: 16413
[2023-08-30 02:40:37,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:40:37,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:40:37,557] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 02:40:37,557] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 2.1547 (2.2734)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2868 (4.6251)  time: 0.9137 (0.5310 -- 4.3350)  data: 0.0012 (0.0005 -- 0.0020)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 2.2704 (2.2743)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3478 (4.6417)  time: 0.6994 (0.4962 -- 2.2475)  data: 0.0008 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 2.2704 (2.2653)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3478 (4.6417)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.7432 (1.7432)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4101 (2.4101 -- 2.4101)  data: 2.2084 (2.2084 -- 2.2084)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7791 (1.7956)  acc1: 55.5556 (51.5152)  acc5: 100.0000 (94.9495)  time: 0.4161 (0.1983 -- 2.4101)  data: 0.2094 (0.0007 -- 2.2084)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7408 (1.7564)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (95.7672)  time: 0.2124 (0.1699 -- 0.3075)  data: 0.0094 (0.0001 -- 0.0905)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7791 (1.7939)  acc1: 55.5556 (50.6224)  acc5: 100.0000 (94.6058)  time: 0.1993 (0.1372 -- 0.3075)  data: 0.0092 (0.0001 -- 0.0905)  max mem: 16413
Val: Total time: 0:00:07 (0.2836 s / it)
* Acc@1 52.905 Acc@5 93.568 loss 1.777
Accuracy of the network on the 482 val images: 52.90%
[2023-08-30 02:41:03,051] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:41:03,053] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:41:03,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:41:03,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:41:04,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:41:04,461] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 52.90%
Epoch: [22]  [  0/160]  eta: 0:19:15  lr: 0.000015  min_lr: 0.000000  loss: 2.1110 (2.1110)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4971 (4.4971)  time: 7.2222 (7.2222 -- 7.2222)  data: 6.6571 (6.6571 -- 6.6571)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:30  lr: 0.000015  min_lr: 0.000000  loss: 2.2271 (2.2144)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3883 (5.2486)  time: 0.7654 (0.5273 -- 2.7516)  data: 0.1595 (0.0006 -- 2.2219)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:02  lr: 0.000015  min_lr: 0.000000  loss: 2.3033 (2.2418)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8282 (5.0057)  time: 0.9601 (0.5245 -- 3.5552)  data: 0.1807 (0.0005 -- 2.2292)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:35  lr: 0.000015  min_lr: 0.000000  loss: 2.3479 (2.2813)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1373 (4.8861)  time: 0.8339 (0.5344 -- 2.5393)  data: 0.0499 (0.0003 -- 0.6857)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:13  lr: 0.000015  min_lr: 0.000000  loss: 2.1784 (2.2706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3023 (4.9741)  time: 0.7956 (0.5283 -- 3.2441)  data: 0.0018 (0.0001 -- 0.0045)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 2.2494 (2.2643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0371 (5.0949)  time: 0.9377 (0.5196 -- 4.4412)  data: 0.0832 (0.0004 -- 1.6406)  max mem: 16413
[2023-08-30 02:42:38,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:42:38,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:42:38,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:42:38,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 2.1169 (2.2548)  loss_scale: 65536.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1689 (4.9750)  time: 0.8790 (0.5282 -- 3.1645)  data: 0.0022 (0.0004 -- 0.0135)  max mem: 16413
[2023-08-30 02:43:00,000] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3646
[2023-08-30 02:43:00,001] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:43:00,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3646
[2023-08-30 02:43:00,001] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:43:00,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 2.2569 (2.2585)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0000 (4.9974)  time: 0.8364 (0.5423 -- 2.1701)  data: 0.0016 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 2.3020 (2.2590)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4609 (4.9775)  time: 0.6750 (0.4968 -- 1.4663)  data: 0.0006 (0.0001 -- 0.0016)  max mem: 16413
Epoch: [22] Total time: 0:02:19 (0.8718 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 2.3020 (2.2503)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4609 (4.9775)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.6962 (1.6962)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.5912 (2.5912 -- 2.5912)  data: 2.3736 (2.3736 -- 2.3736)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7297 (1.7435)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (94.9495)  time: 0.4296 (0.1928 -- 2.5912)  data: 0.2173 (0.0004 -- 2.3736)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6791 (1.7035)  acc1: 55.5556 (53.4392)  acc5: 100.0000 (95.7672)  time: 0.2065 (0.1693 -- 0.2448)  data: 0.0024 (0.0001 -- 0.0278)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7482 (1.7422)  acc1: 55.5556 (52.2822)  acc5: 100.0000 (95.4357)  time: 0.1918 (0.1329 -- 0.2349)  data: 0.0019 (0.0001 -- 0.0278)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 55.187 Acc@5 93.776 loss 1.727
Accuracy of the network on the 482 val images: 55.19%
[2023-08-30 02:43:31,779] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:43:31,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:43:31,780] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:43:31,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:43:33,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:43:33,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.19%
Epoch: [23]  [  0/160]  eta: 0:19:12  lr: 0.000015  min_lr: 0.000000  loss: 2.0371 (2.0371)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0792 (6.0792)  time: 7.2001 (7.2001 -- 7.2001)  data: 6.6501 (6.6501 -- 6.6501)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:44  lr: 0.000015  min_lr: 0.000000  loss: 2.1770 (2.2155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9477 (5.4549)  time: 0.8716 (0.5296 -- 3.3575)  data: 0.2825 (0.0008 -- 2.8368)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:04  lr: 0.000016  min_lr: 0.000000  loss: 2.1974 (2.1887)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6047 (5.0504)  time: 0.8971 (0.5295 -- 3.4713)  data: 0.3043 (0.0004 -- 2.9237)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 2.1386 (2.1942)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2641 (4.9730)  time: 0.7794 (0.5282 -- 2.8628)  data: 0.2243 (0.0004 -- 2.3054)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 2.2637 (2.2118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0987 (4.8056)  time: 0.9616 (0.5083 -- 3.7363)  data: 0.4166 (0.0005 -- 3.2158)  max mem: 16413
[2023-08-30 02:45:01,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:45:01,778] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:45:01,778] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:45:01,778] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 2.2981 (2.2219)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5853 (4.9276)  time: 0.7782 (0.5174 -- 3.4193)  data: 0.2184 (0.0002 -- 2.8643)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.1283 (2.2124)  loss_scale: 65536.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8856 (4.9321)  time: 0.8269 (0.5404 -- 4.0176)  data: 0.2681 (0.0004 -- 3.4858)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 2.1458 (2.2091)  loss_scale: 65536.0000 (43458.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2575 (4.8852)  time: 0.8150 (0.5326 -- 4.2484)  data: 0.2609 (0.0004 -- 3.7126)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 2.2142 (2.2111)  loss_scale: 65536.0000 (46080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2030 (4.9705)  time: 0.7546 (0.4984 -- 2.4216)  data: 0.1342 (0.0002 -- 1.9190)  max mem: 16413
Epoch: [23] Total time: 0:02:20 (0.8774 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 2.2142 (2.2102)  loss_scale: 65536.0000 (46080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2030 (4.9705)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.6505 (1.6505)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4327 (2.4327 -- 2.4327)  data: 2.1820 (2.1820 -- 2.1820)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6505 (1.6897)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.4222 (0.2047 -- 2.4327)  data: 0.2036 (0.0008 -- 2.1820)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6109 (1.6503)  acc1: 55.5556 (56.6138)  acc5: 100.0000 (95.7672)  time: 0.2147 (0.1695 -- 0.2981)  data: 0.0080 (0.0001 -- 0.0992)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7003 (1.6912)  acc1: 55.5556 (55.1867)  acc5: 100.0000 (95.4357)  time: 0.1975 (0.1328 -- 0.2981)  data: 0.0074 (0.0001 -- 0.0992)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 56.639 Acc@5 93.776 loss 1.674
Accuracy of the network on the 482 val images: 56.64%
[2023-08-30 02:46:01,277] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:46:01,279] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:46:01,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:46:01,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:46:02,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:46:02,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.64%
[2023-08-30 02:46:08,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3840
[2023-08-30 02:46:08,494] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:46:08,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3840
[2023-08-30 02:46:08,494] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:46:08,495] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [  0/160]  eta: 0:15:32  lr: 0.000016  min_lr: 0.000000  loss: 2.1608 (2.1608)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5364 (7.5364)  time: 5.8289 (5.8289 -- 5.8289)  data: 4.6578 (4.6578 -- 4.6578)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:37  lr: 0.000016  min_lr: 0.000000  loss: 2.1670 (2.2212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8181 (5.5853)  time: 0.8891 (0.5374 -- 3.1858)  data: 0.3084 (0.0004 -- 2.6357)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:04  lr: 0.000016  min_lr: 0.000000  loss: 2.2741 (2.2279)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6595 (5.4930)  time: 0.9519 (0.5291 -- 3.2821)  data: 0.4087 (0.0003 -- 2.7327)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000000  loss: 2.1072 (2.2068)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7775 (5.3766)  time: 0.8614 (0.5440 -- 4.0739)  data: 0.3031 (0.0004 -- 3.5415)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000016  min_lr: 0.000000  loss: 2.1215 (2.1968)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0238 (5.4536)  time: 0.9884 (0.5332 -- 4.0023)  data: 0.4356 (0.0004 -- 3.4596)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 2.0546 (2.1767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4399 (5.4270)  time: 0.7258 (0.5324 -- 2.5979)  data: 0.1827 (0.0003 -- 2.0780)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 2.2208 (2.1763)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3585 (5.5985)  time: 0.8876 (0.5267 -- 3.6305)  data: 0.3365 (0.0003 -- 3.0892)  max mem: 16413
[2023-08-30 02:48:02,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:48:02,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:48:02,954] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:48:02,954] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 2.2265 (2.1852)  loss_scale: 65536.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0293 (5.5607)  time: 0.8607 (0.5161 -- 2.7344)  data: 0.2135 (0.0006 -- 2.1966)  max mem: 16413
[2023-08-30 02:48:12,873] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3982
[2023-08-30 02:48:12,873] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:48:12,873] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3982
[2023-08-30 02:48:12,873] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:48:12,873] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-30 02:48:23,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=17, lr=[3.9769364008172653e-07, 3.9769364008172653e-07, 5.302581867756355e-07, 5.302581867756355e-07, 7.070109157008472e-07, 7.070109157008472e-07, 9.426812209344629e-07, 9.426812209344629e-07, 1.256908294579284e-06, 1.256908294579284e-06, 1.6758777261057118e-06, 1.6758777261057118e-06, 2.234503634807616e-06, 2.234503634807616e-06, 2.979338179743488e-06, 2.979338179743488e-06, 3.97245090632465e-06, 3.97245090632465e-06, 5.296601208432867e-06, 5.296601208432867e-06, 7.062134944577157e-06, 7.062134944577157e-06, 9.416179926102876e-06, 9.416179926102876e-06, 1.2554906568137167e-05, 1.2554906568137167e-05, 1.673987542418289e-05, 1.673987542418289e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 02:48:23,913] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.751613097340062, CurrSamplesPerSec=24.710884265234892, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 2.0831 (2.1779)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2048 (5.5430)  time: 0.6327 (0.4976 -- 1.5777)  data: 0.0686 (0.0002 -- 1.0446)  max mem: 16413
Epoch: [24] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 2.0831 (2.1852)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2048 (5.5430)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.5951 (1.5951)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4480 (2.4480 -- 2.4480)  data: 2.2227 (2.2227 -- 2.2227)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6099 (1.6368)  acc1: 55.5556 (52.5253)  acc5: 100.0000 (93.9394)  time: 0.4209 (0.2008 -- 2.4480)  data: 0.2032 (0.0010 -- 2.2227)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6099 (1.6039)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (94.7090)  time: 0.2160 (0.1702 -- 0.3239)  data: 0.0083 (0.0001 -- 0.1504)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6497 (1.6443)  acc1: 44.4444 (49.3776)  acc5: 100.0000 (94.6058)  time: 0.2000 (0.1332 -- 0.3239)  data: 0.0080 (0.0001 -- 0.1504)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 55.602 Acc@5 93.776 loss 1.626
Accuracy of the network on the 482 val images: 55.60%
Max accuracy: 56.64%
Epoch: [25]  [  0/160]  eta: 0:20:19  lr: 0.000017  min_lr: 0.000000  loss: 2.2881 (2.2881)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8221 (5.8221)  time: 7.6233 (7.6233 -- 7.6233)  data: 7.0743 (7.0743 -- 7.0743)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:02:40  lr: 0.000017  min_lr: 0.000000  loss: 2.1640 (2.2212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7876 (4.9466)  time: 0.8218 (0.5370 -- 3.3379)  data: 0.2656 (0.0002 -- 2.7870)  max mem: 16413
[2023-08-30 02:49:08,733] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4037
[2023-08-30 02:49:08,733] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4037
[2023-08-30 02:49:08,733] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:49:08,733] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:49:08,733] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 2.0810 (2.1609)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7975 (5.6008)  time: 0.8900 (0.5232 -- 3.7345)  data: 0.3418 (0.0002 -- 3.2114)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:35  lr: 0.000017  min_lr: 0.000000  loss: 2.2396 (2.1870)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2029 (5.7187)  time: 0.8278 (0.5388 -- 3.9913)  data: 0.2771 (0.0001 -- 3.4552)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:13  lr: 0.000017  min_lr: 0.000000  loss: 2.1236 (2.1909)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0095 (5.6888)  time: 0.8248 (0.5313 -- 2.4003)  data: 0.2352 (0.0004 -- 1.8449)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 2.1395 (2.1862)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1758 (5.6513)  time: 0.9802 (0.5280 -- 4.8471)  data: 0.3543 (0.0003 -- 4.3111)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 2.2472 (2.1854)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1273 (5.6277)  time: 0.8171 (0.5259 -- 3.6994)  data: 0.2670 (0.0004 -- 3.1846)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 2.1291 (2.1845)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3860 (5.5881)  time: 0.9035 (0.5282 -- 3.3882)  data: 0.3569 (0.0003 -- 2.8669)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 2.1079 (2.1667)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0716 (5.6511)  time: 0.6247 (0.4978 -- 1.7515)  data: 0.0992 (0.0002 -- 1.2171)  max mem: 16413
Epoch: [25] Total time: 0:02:20 (0.8805 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 2.1079 (2.1530)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0716 (5.6511)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5497 (1.5497)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4123 (2.4123 -- 2.4123)  data: 2.1903 (2.1903 -- 2.1903)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5497 (1.5849)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (93.9394)  time: 0.4222 (0.2030 -- 2.4123)  data: 0.2021 (0.0006 -- 2.1903)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5337 (1.5564)  acc1: 55.5556 (53.9683)  acc5: 100.0000 (94.7090)  time: 0.2121 (0.1707 -- 0.2436)  data: 0.0023 (0.0001 -- 0.0124)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5865 (1.5986)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (95.0207)  time: 0.1953 (0.1329 -- 0.2411)  data: 0.0015 (0.0001 -- 0.0124)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 57.884 Acc@5 93.776 loss 1.580
Accuracy of the network on the 482 val images: 57.88%
[2023-08-30 02:51:00,278] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:51:00,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:51:00,280] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:51:00,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:51:01,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:51:01,678] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.88%
Epoch: [26]  [  0/160]  eta: 0:18:30  lr: 0.000017  min_lr: 0.000000  loss: 2.2828 (2.2828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9861 (5.9861)  time: 6.9426 (6.9426 -- 6.9426)  data: 6.4101 (6.4101 -- 6.4101)  max mem: 16413
[2023-08-30 02:51:11,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:51:11,990] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 02:51:11,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:51:11,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 20/160]  eta: 0:02:42  lr: 0.000017  min_lr: 0.000000  loss: 2.0574 (2.1597)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2380 (5.2218)  time: 0.8699 (0.5273 -- 3.8360)  data: 0.1211 (0.0005 -- 2.3862)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000000  loss: 2.2001 (2.1708)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6084 (5.6207)  time: 0.8658 (0.5325 -- 3.7557)  data: 0.0016 (0.0004 -- 0.0062)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 2.2010 (2.1681)  loss_scale: 32768.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6232 (5.4427)  time: 0.8503 (0.5364 -- 4.1409)  data: 0.2793 (0.0008 -- 3.6202)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 2.3096 (2.1860)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8029 (5.4525)  time: 0.7858 (0.5420 -- 2.2765)  data: 0.2010 (0.0004 -- 1.7255)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:54  lr: 0.000018  min_lr: 0.000000  loss: 2.2082 (2.1861)  loss_scale: 32768.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9079 (5.6085)  time: 0.8508 (0.5183 -- 3.5453)  data: 0.3053 (0.0003 -- 3.0214)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 2.1579 (2.1766)  loss_scale: 32768.0000 (31955.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4831 (5.9022)  time: 1.0287 (0.5325 -- 3.7548)  data: 0.4772 (0.0007 -- 3.2509)  max mem: 16413
[2023-08-30 02:53:06,675] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:53:06,675] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:53:06,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:53:06,676] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:53:08,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4297
[2023-08-30 02:53:08,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4297
[2023-08-30 02:53:08,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:53:08,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:53:08,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.1512 (2.1704)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2670 (6.0014)  time: 0.8171 (0.5128 -- 4.3363)  data: 0.2704 (0.0003 -- 3.7986)  max mem: 16413
[2023-08-30 02:53:23,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4318
[2023-08-30 02:53:23,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4318
[2023-08-30 02:53:23,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:53:23,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:53:23,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 2.2738 (2.1741)  loss_scale: 32768.0000 (32563.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5205 (6.0293)  time: 0.7228 (0.4811 -- 4.6906)  data: 0.2091 (0.0001 -- 4.1698)  max mem: 16413
Epoch: [26] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 2.2738 (2.1533)  loss_scale: 32768.0000 (32563.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5205 (6.0293)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5225 (1.5225)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4144 (2.4144 -- 2.4144)  data: 2.1574 (2.1574 -- 2.1574)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5020 (1.5316)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (93.9394)  time: 0.4198 (0.1959 -- 2.4144)  data: 0.1977 (0.0007 -- 2.1574)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4558 (1.4967)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (95.2381)  time: 0.2160 (0.1721 -- 0.3620)  data: 0.0090 (0.0001 -- 0.1600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5020 (1.5404)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (94.6058)  time: 0.1970 (0.1326 -- 0.3620)  data: 0.0087 (0.0001 -- 0.1600)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 58.506 Acc@5 93.568 loss 1.521
Accuracy of the network on the 482 val images: 58.51%
[2023-08-30 02:53:31,703] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:53:31,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:53:31,705] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:53:31,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:53:33,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:53:33,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.51%
Epoch: [27]  [  0/160]  eta: 0:19:44  lr: 0.000018  min_lr: 0.000000  loss: 2.1229 (2.1229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8334 (7.8334)  time: 7.4004 (7.4004 -- 7.4004)  data: 4.9619 (4.9619 -- 4.9619)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:40  lr: 0.000018  min_lr: 0.000000  loss: 2.0824 (2.1396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9962 (6.4613)  time: 0.8369 (0.5262 -- 3.7290)  data: 0.0192 (0.0005 -- 0.3252)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 2.0256 (2.0693)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7537 (6.2208)  time: 0.8872 (0.5339 -- 3.2780)  data: 0.0979 (0.0004 -- 1.5000)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:35  lr: 0.000018  min_lr: 0.000000  loss: 2.1629 (2.1021)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9772 (6.1461)  time: 0.8317 (0.5282 -- 3.3368)  data: 0.0842 (0.0004 -- 1.6595)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:17  lr: 0.000018  min_lr: 0.000000  loss: 2.0746 (2.1016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4456 (6.1945)  time: 0.9765 (0.5207 -- 3.1930)  data: 0.0135 (0.0004 -- 0.2292)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 2.1010 (2.1103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8865 (6.2369)  time: 0.7755 (0.5138 -- 3.3623)  data: 0.0016 (0.0004 -- 0.0064)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:38  lr: 0.000019  min_lr: 0.000000  loss: 2.0303 (2.1143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1264 (6.3293)  time: 1.0721 (0.5248 -- 5.8391)  data: 0.0017 (0.0003 -- 0.0068)  max mem: 16413
[2023-08-30 02:55:31,813] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:55:31,813] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:55:31,813] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 02:55:31,813] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.1085 (2.1194)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6482 (6.2744)  time: 0.7486 (0.5178 -- 3.6706)  data: 0.0011 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.1770 (2.1222)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7625 (6.2222)  time: 0.6346 (0.4958 -- 2.4762)  data: 0.0010 (0.0002 -- 0.0070)  max mem: 16413
Epoch: [27] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.1770 (2.1519)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7625 (6.2222)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.4910 (1.4910)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2555 (2.2555 -- 2.2555)  data: 2.0272 (2.0272 -- 2.0272)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.4390 (1.5047)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (94.9495)  time: 0.4104 (0.1965 -- 2.2555)  data: 0.1967 (0.0007 -- 2.0272)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4310 (1.4635)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (95.7672)  time: 0.2253 (0.1693 -- 0.5330)  data: 0.0240 (0.0001 -- 0.3402)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4639 (1.5127)  acc1: 55.5556 (55.6017)  acc5: 100.0000 (95.4357)  time: 0.2131 (0.1329 -- 0.5330)  data: 0.0237 (0.0001 -- 0.3402)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 59.336 Acc@5 94.191 loss 1.490
Accuracy of the network on the 482 val images: 59.34%
[2023-08-30 02:56:02,993] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:56:02,994] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:56:02,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:56:02,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:56:04,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:56:04,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.34%
Epoch: [28]  [  0/160]  eta: 0:20:11  lr: 0.000019  min_lr: 0.000000  loss: 2.3140 (2.3140)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6198 (4.6198)  time: 7.5736 (7.5736 -- 7.5736)  data: 5.5377 (5.5377 -- 5.5377)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:36  lr: 0.000019  min_lr: 0.000000  loss: 2.0020 (2.0577)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2517 (5.6499)  time: 0.7950 (0.5329 -- 2.9360)  data: 0.0706 (0.0008 -- 1.1264)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:01:58  lr: 0.000019  min_lr: 0.000000  loss: 2.2652 (2.1470)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4752 (6.6036)  time: 0.8470 (0.5225 -- 2.3428)  data: 0.2791 (0.0004 -- 1.7348)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9685 (2.1078)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6993 (6.3896)  time: 0.9215 (0.5211 -- 3.5008)  data: 0.3698 (0.0005 -- 2.9811)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000000  loss: 2.1898 (2.1245)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4256 (6.3898)  time: 0.9531 (0.5229 -- 3.9670)  data: 0.3978 (0.0004 -- 3.4166)  max mem: 16413
[2023-08-30 02:57:33,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:57:33,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 02:57:33,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:57:33,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 02:57:35,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4579
[2023-08-30 02:57:35,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4579
[2023-08-30 02:57:35,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:57:35,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 02:57:35,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [28]  [100/160]  eta: 0:00:54  lr: 0.000019  min_lr: 0.000000  loss: 2.1988 (2.1386)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9333 (6.2276)  time: 0.6825 (0.5189 -- 2.8990)  data: 0.1243 (0.0003 -- 2.3541)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 2.0102 (2.1159)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9066 (6.2816)  time: 0.9769 (0.5318 -- 4.1472)  data: 0.4274 (0.0004 -- 3.6082)  max mem: 16413
[2023-08-30 02:58:04,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4609
[2023-08-30 02:58:04,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4609
[2023-08-30 02:58:04,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:58:04,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 02:58:04,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.1544 (2.1170)  loss_scale: 16384.0000 (32303.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4432 (6.1832)  time: 0.9046 (0.5292 -- 3.8987)  data: 0.3600 (0.0004 -- 3.3505)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.1976 (2.1204)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3903 (6.1524)  time: 0.6687 (0.4968 -- 2.4570)  data: 0.1481 (0.0002 -- 1.9338)  max mem: 16413
Epoch: [28] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.1976 (2.1142)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3903 (6.1524)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.4543 (1.4543)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3634 (2.3634 -- 2.3634)  data: 2.1205 (2.1205 -- 2.1205)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.3752 (1.4551)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (96.9697)  time: 0.4096 (0.2001 -- 2.3634)  data: 0.1939 (0.0005 -- 2.1205)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3305 (1.4207)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (96.8254)  time: 0.2216 (0.1713 -- 0.5458)  data: 0.0180 (0.0001 -- 0.3449)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4345 (1.4597)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (96.6805)  time: 0.2081 (0.1333 -- 0.5458)  data: 0.0178 (0.0001 -- 0.3449)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 59.959 Acc@5 95.021 loss 1.442
Accuracy of the network on the 482 val images: 59.96%
[2023-08-30 02:58:34,224] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 02:58:34,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 02:58:34,226] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 02:58:34,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 02:58:35,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 02:58:35,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.96%
Epoch: [29]  [  0/160]  eta: 0:19:50  lr: 0.000019  min_lr: 0.000000  loss: 2.0525 (2.0525)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9849 (5.9849)  time: 7.4427 (7.4427 -- 7.4427)  data: 5.3666 (5.3666 -- 5.3666)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:47  lr: 0.000020  min_lr: 0.000000  loss: 2.2578 (2.1996)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8330 (6.1406)  time: 0.8818 (0.5294 -- 3.3958)  data: 0.0300 (0.0004 -- 0.5742)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:02  lr: 0.000020  min_lr: 0.000000  loss: 1.9988 (2.1058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0316 (6.2426)  time: 0.8367 (0.5162 -- 2.6784)  data: 0.2585 (0.0003 -- 2.1499)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 2.1055 (2.1188)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3830 (6.5094)  time: 0.8374 (0.5172 -- 3.6489)  data: 0.2747 (0.0002 -- 3.1258)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 2.0740 (2.1281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1751 (6.5770)  time: 0.8688 (0.5276 -- 4.3269)  data: 0.3179 (0.0004 -- 3.7973)  max mem: 16413
[2023-08-30 03:00:08,125] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:00:08,126] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:00:08,127] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:00:08,127] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 2.0551 (2.1108)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6856 (6.5452)  time: 0.8775 (0.5294 -- 3.2187)  data: 0.3301 (0.0003 -- 2.7131)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0682 (2.1020)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1396 (6.4957)  time: 0.8864 (0.5268 -- 3.2654)  data: 0.3397 (0.0003 -- 2.7477)  max mem: 16413
[2023-08-30 03:00:39,383] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4776
[2023-08-30 03:00:39,384] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:00:39,384] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4776
[2023-08-30 03:00:39,384] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:00:39,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 2.1583 (2.1059)  loss_scale: 32768.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2916 (6.4936)  time: 0.7339 (0.5338 -- 2.1761)  data: 0.1856 (0.0005 -- 1.6427)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0592 (2.1012)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6818 (6.4792)  time: 0.7472 (0.4986 -- 3.5248)  data: 0.2197 (0.0002 -- 2.9894)  max mem: 16413
Epoch: [29] Total time: 0:02:20 (0.8772 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0592 (2.0848)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6818 (6.4792)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.4123 (1.4123)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2186 (2.2186 -- 2.2186)  data: 1.9838 (1.9838 -- 1.9838)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.3217 (1.4176)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4020 (0.2006 -- 2.2186)  data: 0.1815 (0.0009 -- 1.9838)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3124 (1.3809)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (96.2963)  time: 0.2236 (0.1694 -- 0.4923)  data: 0.0138 (0.0001 -- 0.2597)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3856 (1.4280)  acc1: 55.5556 (57.6763)  acc5: 100.0000 (95.8506)  time: 0.2069 (0.1340 -- 0.4923)  data: 0.0134 (0.0001 -- 0.2597)  max mem: 16413
Val: Total time: 0:00:07 (0.2844 s / it)
* Acc@1 61.203 Acc@5 94.813 loss 1.409
Accuracy of the network on the 482 val images: 61.20%
[2023-08-30 03:01:03,956] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:01:03,958] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:01:03,958] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:01:03,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:01:05,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:01:05,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.20%
Epoch: [30]  [  0/160]  eta: 0:15:03  lr: 0.000020  min_lr: 0.000000  loss: 1.6462 (1.6462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2014 (5.2014)  time: 5.6456 (5.6456 -- 5.6456)  data: 4.7696 (4.7696 -- 4.7696)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:02:37  lr: 0.000020  min_lr: 0.000000  loss: 2.0485 (1.9859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5244 (6.4251)  time: 0.9021 (0.5232 -- 2.7547)  data: 0.2115 (0.0008 -- 2.2167)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:01:57  lr: 0.000020  min_lr: 0.000000  loss: 1.9276 (2.0225)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7559 (6.7835)  time: 0.8174 (0.5316 -- 2.3652)  data: 0.1214 (0.0006 -- 1.3694)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 2.0563 (2.0390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4516 (6.9718)  time: 0.9205 (0.5257 -- 2.9650)  data: 0.1134 (0.0005 -- 0.7037)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:12  lr: 0.000020  min_lr: 0.000000  loss: 1.9958 (2.0342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3271 (7.0237)  time: 0.7609 (0.5293 -- 2.0088)  data: 0.0022 (0.0006 -- 0.0074)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 2.1758 (2.0394)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0991 (6.7592)  time: 1.0112 (0.5223 -- 2.2793)  data: 0.3494 (0.0005 -- 1.7395)  max mem: 16413
[2023-08-30 03:02:43,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:02:43,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:02:43,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:02:43,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 2.1889 (2.0558)  loss_scale: 32768.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0387 (6.7192)  time: 0.9636 (0.5236 -- 4.4607)  data: 0.0838 (0.0002 -- 0.9323)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.1398 (2.0677)  loss_scale: 32768.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9667 (6.6933)  time: 0.8633 (0.5266 -- 3.9891)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.8951 (2.0589)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7537 (6.7348)  time: 0.6472 (0.4979 -- 2.0718)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [30] Total time: 0:02:22 (0.8927 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.8951 (2.0656)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7537 (6.7348)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.3753 (1.3753)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2932 (2.2932 -- 2.2932)  data: 2.0711 (2.0711 -- 2.0711)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3167 (1.3777)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4183 (0.1990 -- 2.2932)  data: 0.2015 (0.0005 -- 2.0711)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2540 (1.3479)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (96.2963)  time: 0.2230 (0.1721 -- 0.3584)  data: 0.0138 (0.0001 -- 0.1349)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3497 (1.3927)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (95.8506)  time: 0.2077 (0.1331 -- 0.3584)  data: 0.0135 (0.0001 -- 0.1349)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 59.959 Acc@5 94.606 loss 1.373
Accuracy of the network on the 482 val images: 59.96%
Max accuracy: 61.20%
Epoch: [31]  [  0/160]  eta: 0:16:23  lr: 0.000021  min_lr: 0.000000  loss: 2.0684 (2.0684)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3339 (6.3339)  time: 6.1485 (6.1485 -- 6.1485)  data: 5.6372 (5.6372 -- 5.6372)  max mem: 16413
[2023-08-30 03:03:56,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4976
[2023-08-30 03:03:56,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4976
[2023-08-30 03:03:56,833] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:03:56,834] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:03:56,834] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [ 20/160]  eta: 0:02:51  lr: 0.000021  min_lr: 0.000000  loss: 2.0107 (1.9952)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0761 (6.3884)  time: 0.9802 (0.5289 -- 2.7503)  data: 0.4299 (0.0004 -- 2.1878)  max mem: 16413
[2023-08-30 03:04:16,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[4.971419121701803e-07, 4.971419121701803e-07, 6.628558828935737e-07, 6.628558828935737e-07, 8.838078438580982e-07, 8.838078438580982e-07, 1.1784104584774645e-06, 1.1784104584774645e-06, 1.5712139446366192e-06, 1.5712139446366192e-06, 2.094951926182159e-06, 2.094951926182159e-06, 2.7932692349095452e-06, 2.7932692349095452e-06, 3.7243589798793937e-06, 3.7243589798793937e-06, 4.965811973172525e-06, 4.965811973172525e-06, 6.6210826308966995e-06, 6.6210826308966995e-06, 8.828110174528933e-06, 8.828110174528933e-06, 1.1770813566038578e-05, 1.1770813566038578e-05, 1.5694418088051436e-05, 1.5694418088051436e-05, 2.0925890784068582e-05, 2.0925890784068582e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 03:04:16,400] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.945916550585718, CurrSamplesPerSec=22.0696603190152, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:01:59  lr: 0.000021  min_lr: 0.000000  loss: 1.9171 (1.9939)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7208 (6.4244)  time: 0.7622 (0.5124 -- 2.9330)  data: 0.2153 (0.0004 -- 2.3994)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:34  lr: 0.000021  min_lr: 0.000000  loss: 1.9929 (1.9945)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1881 (6.6522)  time: 0.8382 (0.5296 -- 3.4513)  data: 0.2614 (0.0005 -- 2.8819)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:12  lr: 0.000021  min_lr: 0.000001  loss: 2.0746 (2.0206)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8346 (6.8419)  time: 0.8012 (0.5179 -- 3.1614)  data: 0.2260 (0.0003 -- 2.6266)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 2.2117 (2.0500)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9691 (6.9004)  time: 0.9830 (0.5325 -- 2.7121)  data: 0.4382 (0.0003 -- 2.1826)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 2.1283 (2.0599)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8988 (6.7941)  time: 0.8030 (0.5184 -- 2.6466)  data: 0.2602 (0.0002 -- 2.1183)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000001  loss: 1.8417 (2.0469)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3240 (6.8673)  time: 0.8552 (0.5171 -- 3.0850)  data: 0.3054 (0.0009 -- 2.5819)  max mem: 16413
[2023-08-30 03:05:45,822] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:05:45,822] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:05:45,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:05:45,823] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:05:49,179] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5107
[2023-08-30 03:05:49,179] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5107
[2023-08-30 03:05:49,179] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:05:49,179] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:05:49,179] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.8131 (2.0267)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3884 (6.7793)  time: 0.6736 (0.4959 -- 2.8417)  data: 0.1429 (0.0002 -- 2.3149)  max mem: 16413
Epoch: [31] Total time: 0:02:19 (0.8724 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.8131 (2.0197)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3884 (6.7793)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.3574 (1.3574)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4173 (2.4173 -- 2.4173)  data: 2.1807 (2.1807 -- 2.1807)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2644 (1.3428)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.4240 (0.2009 -- 2.4173)  data: 0.2044 (0.0004 -- 2.1807)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2252 (1.3108)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (95.7672)  time: 0.2193 (0.1699 -- 0.3596)  data: 0.0124 (0.0001 -- 0.1766)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3121 (1.3618)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (95.4357)  time: 0.2018 (0.1333 -- 0.3596)  data: 0.0121 (0.0001 -- 0.1766)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 62.863 Acc@5 94.191 loss 1.338
Accuracy of the network on the 482 val images: 62.86%
[2023-08-30 03:06:03,310] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:06:03,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:06:03,312] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:06:03,312] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:06:04,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:06:04,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [32]  [  0/160]  eta: 0:19:44  lr: 0.000021  min_lr: 0.000001  loss: 2.2477 (2.2477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8782 (6.8782)  time: 7.4008 (7.4008 -- 7.4008)  data: 6.7087 (6.7087 -- 6.7087)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000001  loss: 1.8522 (1.8533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6581 (6.3235)  time: 0.8864 (0.5397 -- 3.6450)  data: 0.1842 (0.0004 -- 2.8005)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000001  loss: 2.0686 (1.9491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0403 (6.7349)  time: 0.8796 (0.5267 -- 2.8319)  data: 0.1294 (0.0003 -- 2.2650)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.8431 (1.9574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7238 (6.6338)  time: 0.8342 (0.5260 -- 3.0089)  data: 0.1411 (0.0003 -- 1.5713)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.9615 (1.9681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4246 (6.4709)  time: 0.8449 (0.5187 -- 3.4263)  data: 0.0127 (0.0001 -- 0.2303)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.1113 (1.9968)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2633 (6.5505)  time: 0.8769 (0.5292 -- 2.4449)  data: 0.1422 (0.0005 -- 1.7756)  max mem: 16413
[2023-08-30 03:07:50,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:07:50,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:07:50,774] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:07:50,774] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0538 (2.0039)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9572 (6.6130)  time: 0.7953 (0.5341 -- 2.3243)  data: 0.0015 (0.0005 -- 0.0026)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 2.0392 (2.0098)  loss_scale: 32768.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7399 (6.8604)  time: 0.8514 (0.5316 -- 3.1360)  data: 0.0024 (0.0005 -- 0.0215)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9391 (2.0104)  loss_scale: 32768.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9378 (6.8628)  time: 0.6685 (0.4968 -- 2.4748)  data: 0.0010 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [32] Total time: 0:02:19 (0.8731 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9391 (2.0388)  loss_scale: 32768.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9378 (6.8628)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.3404 (1.3404)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2744 (2.2744 -- 2.2744)  data: 2.0313 (2.0313 -- 2.0313)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.2482 (1.3227)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4079 (0.1961 -- 2.2744)  data: 0.1890 (0.0008 -- 2.0313)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1729 (1.2812)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1712 -- 0.4094)  data: 0.0194 (0.0001 -- 0.1916)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2702 (1.3318)  acc1: 55.5556 (58.9212)  acc5: 100.0000 (95.0207)  time: 0.2115 (0.1335 -- 0.4094)  data: 0.0190 (0.0001 -- 0.1916)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 62.241 Acc@5 94.191 loss 1.310
Accuracy of the network on the 482 val images: 62.24%
Max accuracy: 62.86%
Epoch: [33]  [  0/160]  eta: 0:17:07  lr: 0.000022  min_lr: 0.000001  loss: 1.8374 (1.8374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9690 (4.9690)  time: 6.4215 (6.4215 -- 6.4215)  data: 5.8016 (5.8016 -- 5.8016)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:56  lr: 0.000022  min_lr: 0.000001  loss: 2.0866 (2.0270)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9824 (6.8699)  time: 0.9995 (0.5227 -- 2.6146)  data: 0.1467 (0.0003 -- 2.0910)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:06  lr: 0.000022  min_lr: 0.000001  loss: 1.8813 (2.0002)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7329 (7.4050)  time: 0.8398 (0.5173 -- 2.6443)  data: 0.2940 (0.0003 -- 2.1281)  max mem: 16413
[2023-08-30 03:09:16,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5322
[2023-08-30 03:09:16,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5322
[2023-08-30 03:09:16,330] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:09:16,330] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:09:16,330] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 2.1489 (2.0233)  loss_scale: 16384.0000 (27664.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4635 (7.6291)  time: 0.7739 (0.5284 -- 1.9749)  data: 0.1111 (0.0004 -- 1.2875)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.0668 (2.0373)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6690 (7.6129)  time: 0.8343 (0.5228 -- 1.9364)  data: 0.2496 (0.0003 -- 1.3998)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0924 (2.0289)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5313 (7.4748)  time: 0.8759 (0.5356 -- 1.8824)  data: 0.1550 (0.0004 -- 1.3462)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9847 (2.0179)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5471 (7.2355)  time: 0.8925 (0.5219 -- 3.2013)  data: 0.1069 (0.0006 -- 0.9468)  max mem: 16413
[2023-08-30 03:10:34,244] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5413
[2023-08-30 03:10:34,245] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:10:34,245] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5413
[2023-08-30 03:10:34,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 03:10:34,245] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [33]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0244 (2.0182)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1325 (7.1581)  time: 0.8436 (0.5248 -- 2.7663)  data: 0.0013 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0998 (2.0317)  loss_scale: 8192.0000 (19302.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5183 (7.0758)  time: 0.6828 (0.4991 -- 1.9779)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8797 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0998 (2.0458)  loss_scale: 8192.0000 (19302.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5183 (7.0758)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.3085 (1.3085)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4952 (2.4952 -- 2.4952)  data: 2.2382 (2.2382 -- 2.2382)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2175 (1.2948)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4325 (0.2004 -- 2.4952)  data: 0.2103 (0.0005 -- 2.2382)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1431 (1.2578)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (96.8254)  time: 0.2121 (0.1693 -- 0.3341)  data: 0.0065 (0.0001 -- 0.0543)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2361 (1.3031)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.8506)  time: 0.1968 (0.1334 -- 0.3341)  data: 0.0062 (0.0001 -- 0.0543)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 62.863 Acc@5 94.606 loss 1.282
Accuracy of the network on the 482 val images: 62.86%
Max accuracy: 62.86%
Epoch: [34]  [  0/160]  eta: 0:20:13  lr: 0.000023  min_lr: 0.000001  loss: 1.6728 (1.6728)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3252 (8.3252)  time: 7.5827 (7.5827 -- 7.5827)  data: 7.0507 (7.0507 -- 7.0507)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000001  loss: 1.8267 (1.9049)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9650 (6.7789)  time: 0.8991 (0.5302 -- 5.3551)  data: 0.3534 (0.0003 -- 4.8085)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0533 (1.9577)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2340 (6.9468)  time: 0.7804 (0.5162 -- 2.6290)  data: 0.1764 (0.0004 -- 2.0808)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1723 (2.0080)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2456 (6.9451)  time: 0.9210 (0.5308 -- 3.8772)  data: 0.2811 (0.0003 -- 3.3658)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 2.1665 (2.0223)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0094 (7.0658)  time: 0.7293 (0.5397 -- 1.9615)  data: 0.1536 (0.0002 -- 1.4044)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0233 (2.0296)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8964 (7.1898)  time: 0.9404 (0.5217 -- 2.8213)  data: 0.1113 (0.0005 -- 1.1046)  max mem: 16413
[2023-08-30 03:12:34,586] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:12:34,586] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:12:34,587] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 03:12:34,587] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [34]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9909 (2.0267)  loss_scale: 16384.0000 (9478.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8791 (7.3260)  time: 0.9550 (0.5308 -- 3.6963)  data: 0.3715 (0.0003 -- 3.1598)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0052 (2.0216)  loss_scale: 16384.0000 (10457.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3672 (7.2748)  time: 0.8433 (0.5116 -- 3.1017)  data: 0.0511 (0.0006 -- 0.9943)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0935 (2.0316)  loss_scale: 16384.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1190 (7.3287)  time: 0.6151 (0.4955 -- 1.5389)  data: 0.0008 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [34] Total time: 0:02:20 (0.8796 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0935 (2.0298)  loss_scale: 16384.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1190 (7.3287)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.2890 (1.2890)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4441 (2.4441 -- 2.4441)  data: 2.1648 (2.1648 -- 2.1648)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2265 (1.2693)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4143 (0.1971 -- 2.4441)  data: 0.1979 (0.0005 -- 2.1648)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1643 (1.2299)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (96.8254)  time: 0.2122 (0.1701 -- 0.3084)  data: 0.0081 (0.0001 -- 0.0965)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2229 (1.2713)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (95.4357)  time: 0.1977 (0.1326 -- 0.3084)  data: 0.0077 (0.0001 -- 0.0965)  max mem: 16413
Val: Total time: 0:00:07 (0.2842 s / it)
* Acc@1 62.656 Acc@5 94.398 loss 1.248
Accuracy of the network on the 482 val images: 62.66%
Max accuracy: 62.86%
Epoch: [35]  [  0/160]  eta: 0:17:21  lr: 0.000023  min_lr: 0.000001  loss: 2.2573 (2.2573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1643 (6.1643)  time: 6.5117 (6.5117 -- 6.5117)  data: 5.0506 (5.0506 -- 5.0506)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9774 (1.9829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5937 (7.8713)  time: 0.8454 (0.5400 -- 2.7174)  data: 0.2353 (0.0008 -- 2.1900)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:01:58  lr: 0.000023  min_lr: 0.000001  loss: 1.9729 (1.9832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0318 (8.2434)  time: 0.8600 (0.5250 -- 3.2712)  data: 0.3158 (0.0002 -- 2.7233)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:34  lr: 0.000023  min_lr: 0.000001  loss: 2.0914 (1.9909)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9752 (7.8303)  time: 0.8419 (0.5199 -- 3.6538)  data: 0.2844 (0.0004 -- 3.1014)  max mem: 16413
[2023-08-30 03:14:34,163] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:14:34,164] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:14:34,164] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:14:34,164] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 80/160]  eta: 0:01:12  lr: 0.000023  min_lr: 0.000001  loss: 1.9981 (1.9924)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9145 (7.6134)  time: 0.8035 (0.5261 -- 2.6714)  data: 0.2540 (0.0006 -- 2.1355)  max mem: 16413
[2023-08-30 03:14:55,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5691
[2023-08-30 03:14:55,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5691
[2023-08-30 03:14:55,051] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:14:55,051] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:14:55,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0323 (2.0055)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9715 (7.8417)  time: 1.0004 (0.5278 -- 5.4818)  data: 0.4449 (0.0003 -- 4.9489)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1659 (2.0233)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9581 (7.7451)  time: 0.8104 (0.5275 -- 1.9394)  data: 0.2528 (0.0006 -- 1.4031)  max mem: 16413
Epoch: [35]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.9132 (2.0204)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4210 (7.6805)  time: 0.8552 (0.5447 -- 2.3868)  data: 0.3020 (0.0003 -- 1.8622)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0121 (2.0227)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9038 (7.7151)  time: 0.6847 (0.4987 -- 2.2890)  data: 0.1592 (0.0001 -- 1.7831)  max mem: 16413
Epoch: [35] Total time: 0:02:20 (0.8754 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0121 (2.0161)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9038 (7.7151)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2599 (1.2599)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3275 (2.3275 -- 2.3275)  data: 2.0689 (2.0689 -- 2.0689)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2148 (1.2464)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.4235 (0.2015 -- 2.3275)  data: 0.1900 (0.0007 -- 2.0689)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0761 (1.2050)  acc1: 55.5556 (59.7884)  acc5: 100.0000 (95.7672)  time: 0.2234 (0.1697 -- 0.3254)  data: 0.0103 (0.0001 -- 0.1294)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2354 (1.2526)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (94.6058)  time: 0.2052 (0.1324 -- 0.3254)  data: 0.0100 (0.0001 -- 0.1294)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 63.071 Acc@5 94.398 loss 1.226
Accuracy of the network on the 482 val images: 63.07%
[2023-08-30 03:15:56,825] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:15:56,826] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:15:56,826] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:15:56,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:15:58,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:15:58,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.07%
Epoch: [36]  [  0/160]  eta: 0:19:25  lr: 0.000023  min_lr: 0.000001  loss: 1.7690 (1.7690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1955 (8.1955)  time: 7.2867 (7.2867 -- 7.2867)  data: 5.2962 (5.2962 -- 5.2962)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 1.8822 (1.9191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5106 (7.1960)  time: 0.8796 (0.5287 -- 2.7999)  data: 0.2420 (0.0006 -- 2.2567)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9369 (1.9158)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7202 (7.6710)  time: 0.8205 (0.5239 -- 2.3396)  data: 0.2682 (0.0003 -- 1.8012)  max mem: 16413
[2023-08-30 03:16:56,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:16:56,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:16:56,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:16:56,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 1.6590 (1.8839)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2263 (7.6360)  time: 0.8720 (0.5267 -- 3.7221)  data: 0.3228 (0.0005 -- 3.1706)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.9716 (1.8985)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7257 (7.7313)  time: 0.8417 (0.5128 -- 3.3257)  data: 0.2918 (0.0003 -- 2.7926)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 1.9256 (1.9055)  loss_scale: 32768.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9371 (7.7046)  time: 0.8173 (0.5211 -- 4.0920)  data: 0.2411 (0.0003 -- 3.5681)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8861 (1.9146)  loss_scale: 32768.0000 (24643.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8876 (7.8779)  time: 0.9662 (0.5260 -- 4.2877)  data: 0.1102 (0.0002 -- 1.4240)  max mem: 16413
[2023-08-30 03:17:58,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5890
[2023-08-30 03:17:58,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5890
[2023-08-30 03:17:58,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:17:58,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:17:58,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0263 (1.9349)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1372 (7.8958)  time: 0.8283 (0.5258 -- 2.7146)  data: 0.2734 (0.0004 -- 2.1848)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8684 (1.9373)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9339 (7.9752)  time: 0.6386 (0.4971 -- 1.4858)  data: 0.1091 (0.0002 -- 0.9806)  max mem: 16413
Epoch: [36] Total time: 0:02:20 (0.8754 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8684 (1.9622)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9339 (7.9752)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.2052 (1.2052)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4314 (2.4314 -- 2.4314)  data: 2.1727 (2.1727 -- 2.1727)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1631 (1.2237)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4267 (0.2082 -- 2.4314)  data: 0.2003 (0.0004 -- 2.1727)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0835 (1.1831)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (96.2963)  time: 0.2162 (0.1697 -- 0.3251)  data: 0.0090 (0.0001 -- 0.1467)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2310 (1.2384)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.0207)  time: 0.1981 (0.1333 -- 0.3251)  data: 0.0086 (0.0001 -- 0.1467)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 62.863 Acc@5 93.983 loss 1.216
Accuracy of the network on the 482 val images: 62.86%
Max accuracy: 63.07%
Epoch: [37]  [  0/160]  eta: 0:19:29  lr: 0.000023  min_lr: 0.000001  loss: 1.5221 (1.5221)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0832 (10.0832)  time: 7.3092 (7.3092 -- 7.3092)  data: 6.6308 (6.6308 -- 6.6308)  max mem: 16413
Epoch: [37]  [ 20/160]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000001  loss: 1.9824 (2.0113)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3627 (7.9835)  time: 0.9115 (0.5331 -- 3.2474)  data: 0.3551 (0.0007 -- 2.7360)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000001  loss: 1.9341 (2.0174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7426 (7.6554)  time: 0.9453 (0.5212 -- 4.2061)  data: 0.4041 (0.0002 -- 3.6617)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000001  loss: 1.8796 (2.0012)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1475 (7.8176)  time: 0.8603 (0.5149 -- 4.0221)  data: 0.3197 (0.0004 -- 3.4973)  max mem: 16413
[2023-08-30 03:19:40,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=29, lr=[5.562653416576599e-07, 5.562653416576599e-07, 7.416871222102132e-07, 7.416871222102132e-07, 9.889161629469508e-07, 9.889161629469508e-07, 1.3185548839292679e-06, 1.3185548839292679e-06, 1.7580731785723572e-06, 1.7580731785723572e-06, 2.3440975714298094e-06, 2.3440975714298094e-06, 3.1254634285730795e-06, 3.1254634285730795e-06, 4.167284571430772e-06, 4.167284571430772e-06, 5.556379428574363e-06, 5.556379428574363e-06, 7.4085059047658174e-06, 7.4085059047658174e-06, 9.87800787302109e-06, 9.87800787302109e-06, 1.317067716402812e-05, 1.317067716402812e-05, 1.7560902885370826e-05, 1.7560902885370826e-05, 2.3414537180494436e-05, 2.3414537180494436e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 03:19:40,528] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=18.152321972900488, CurrSamplesPerSec=20.914249812596506, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 1.9109 (2.0025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4286 (7.7655)  time: 0.8493 (0.5149 -- 4.1092)  data: 0.2993 (0.0003 -- 3.5503)  max mem: 16413
[2023-08-30 03:20:00,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:20:00,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:20:00,306] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:20:00,306] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1933 (2.0185)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4153 (7.8409)  time: 0.8100 (0.5166 -- 3.4711)  data: 0.2554 (0.0003 -- 2.9382)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7731 (1.9899)  loss_scale: 32768.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4668 (7.6818)  time: 0.8344 (0.5282 -- 2.8845)  data: 0.2850 (0.0005 -- 2.3694)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9382 (1.9889)  loss_scale: 32768.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5464 (7.5117)  time: 0.8259 (0.5269 -- 4.0467)  data: 0.2637 (0.0003 -- 3.4600)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9720 (1.9835)  loss_scale: 32768.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3271 (7.4763)  time: 0.7370 (0.4965 -- 4.4029)  data: 0.2171 (0.0003 -- 3.8874)  max mem: 16413
Epoch: [37] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9720 (2.0037)  loss_scale: 32768.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3271 (7.4763)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.2052 (1.2052)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3955 (2.3955 -- 2.3955)  data: 2.1310 (2.1310 -- 2.1310)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1452 (1.1955)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (94.9495)  time: 0.4135 (0.1951 -- 2.3955)  data: 0.1947 (0.0006 -- 2.1310)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0756 (1.1543)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (95.7672)  time: 0.2186 (0.1695 -- 0.4610)  data: 0.0134 (0.0001 -- 0.2529)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1668 (1.2010)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (95.0207)  time: 0.2034 (0.1333 -- 0.4610)  data: 0.0131 (0.0001 -- 0.2529)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 63.485 Acc@5 94.398 loss 1.182
Accuracy of the network on the 482 val images: 63.49%
[2023-08-30 03:20:56,018] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:20:56,019] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:20:56,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:20:56,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:20:57,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:20:57,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.49%
Epoch: [38]  [  0/160]  eta: 0:15:21  lr: 0.000023  min_lr: 0.000001  loss: 1.6681 (1.6681)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2590 (8.2590)  time: 5.7571 (5.7571 -- 5.7571)  data: 5.2167 (5.2167 -- 5.2167)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000001  loss: 2.0573 (1.9257)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0490 (7.6895)  time: 0.9105 (0.5289 -- 3.2198)  data: 0.0826 (0.0005 -- 0.8241)  max mem: 16413
[2023-08-30 03:21:35,296] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6117
[2023-08-30 03:21:35,296] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:21:35,296] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6117
[2023-08-30 03:21:35,297] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:21:35,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 40/160]  eta: 0:01:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1454 (1.9667)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6274 (7.0800)  time: 0.7989 (0.5331 -- 2.5753)  data: 0.1021 (0.0002 -- 1.0931)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 1.7836 (1.9424)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7528 (7.3881)  time: 0.9132 (0.5319 -- 2.8904)  data: 0.0020 (0.0006 -- 0.0141)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 2.0510 (1.9812)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4833 (7.6901)  time: 0.8134 (0.5330 -- 3.1768)  data: 0.0016 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.9404 (1.9832)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3559 (7.7504)  time: 0.9521 (0.5309 -- 4.3659)  data: 0.0020 (0.0002 -- 0.0103)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:35  lr: 0.000023  min_lr: 0.000001  loss: 2.0894 (1.9920)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4586 (7.9627)  time: 0.7576 (0.5299 -- 2.8422)  data: 0.0018 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.7940 (1.9706)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9153 (7.8927)  time: 0.9295 (0.5180 -- 3.4742)  data: 0.0015 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0321 (1.9769)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6390 (7.7710)  time: 0.7467 (0.4971 -- 2.4655)  data: 0.0009 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [38] Total time: 0:02:19 (0.8735 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0321 (1.9644)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6390 (7.7710)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1875 (1.1875)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4943 (2.4943 -- 2.4943)  data: 2.2473 (2.2473 -- 2.2473)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0540 (1.1732)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4101 (0.1937 -- 2.4943)  data: 0.2051 (0.0005 -- 2.2473)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0412 (1.1375)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2164 (0.1699 -- 0.4118)  data: 0.0166 (0.0001 -- 0.2159)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1219 (1.1726)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.8506)  time: 0.2053 (0.1334 -- 0.4118)  data: 0.0164 (0.0001 -- 0.2159)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 67.012 Acc@5 95.021 loss 1.154
Accuracy of the network on the 482 val images: 67.01%
[2023-08-30 03:23:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:23:25,090] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:23:25,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:23:25,090] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:23:26,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:23:26,478] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.01%
Epoch: [39]  [  0/160]  eta: 0:20:42  lr: 0.000023  min_lr: 0.000001  loss: 1.6913 (1.6913)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6032 (7.6032)  time: 7.7684 (7.7684 -- 7.7684)  data: 6.6208 (6.6208 -- 6.6208)  max mem: 16413
[2023-08-30 03:23:37,504] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:23:37,504] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:23:37,504] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:23:37,504] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 20/160]  eta: 0:03:02  lr: 0.000023  min_lr: 0.000001  loss: 2.0518 (2.0735)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3306 (6.8198)  time: 0.9798 (0.5221 -- 6.1719)  data: 0.4409 (0.0005 -- 5.6648)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000001  loss: 2.0356 (1.9920)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0126 (7.0292)  time: 0.8688 (0.5328 -- 3.2976)  data: 0.3145 (0.0002 -- 2.7423)  max mem: 16413
[2023-08-30 03:24:20,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6294
[2023-08-30 03:24:20,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6294
[2023-08-30 03:24:20,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:24:20,822] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:24:20,822] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9436 (1.9783)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1034 (7.4466)  time: 0.7347 (0.5218 -- 2.4263)  data: 0.1860 (0.0003 -- 1.9031)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.2395 (2.0381)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2536 (7.4487)  time: 0.8491 (0.5245 -- 2.6395)  data: 0.0846 (0.0005 -- 1.5806)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.0996 (2.0384)  loss_scale: 16384.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1868 (7.6248)  time: 0.9057 (0.5262 -- 1.9194)  data: 0.1953 (0.0004 -- 1.4057)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8763 (2.0134)  loss_scale: 16384.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8212 (7.6157)  time: 0.8344 (0.5249 -- 3.8407)  data: 0.2871 (0.0005 -- 3.3109)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0219 (1.9986)  loss_scale: 16384.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4490 (7.6339)  time: 0.8519 (0.5218 -- 3.5002)  data: 0.1323 (0.0005 -- 1.5854)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8847 (1.9930)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1224 (7.7477)  time: 0.6858 (0.4985 -- 2.5960)  data: 0.0567 (0.0002 -- 0.6297)  max mem: 16413
Epoch: [39] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8847 (1.9654)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1224 (7.7477)
[2023-08-30 03:25:47,957] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-30 03:25:47,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-30 03:25:47,961] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-30 03:25:47,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-30 03:25:49,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-30 03:25:49,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:00  loss: 1.1210 (1.1210)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2485 (2.2485 -- 2.2485)  data: 2.0285 (2.0285 -- 2.0285)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0548 (1.1660)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4153 (0.2075 -- 2.2485)  data: 0.1968 (0.0005 -- 2.0285)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0468 (1.1211)  acc1: 55.5556 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2265 (0.1697 -- 0.4273)  data: 0.0180 (0.0001 -- 0.2212)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1658 (1.1687)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.2104 (0.1323 -- 0.4273)  data: 0.0178 (0.0001 -- 0.2212)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 65.768 Acc@5 94.606 loss 1.141
Accuracy of the network on the 482 val images: 65.77%
Max accuracy: 67.01%
Epoch: [40]  [  0/160]  eta: 0:20:58  lr: 0.000023  min_lr: 0.000001  loss: 1.7896 (1.7896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9755 (5.9755)  time: 7.8663 (7.8663 -- 7.8663)  data: 7.3102 (7.3102 -- 7.3102)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:57  lr: 0.000023  min_lr: 0.000001  loss: 2.0434 (2.0460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1051 (7.4687)  time: 0.9363 (0.5191 -- 5.8884)  data: 0.3884 (0.0004 -- 5.3783)  max mem: 16413
[2023-08-30 03:26:25,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:26:25,143] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:26:25,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:26:25,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [ 40/160]  eta: 0:02:11  lr: 0.000023  min_lr: 0.000001  loss: 2.0569 (2.0212)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6653 (7.9881)  time: 0.9215 (0.5225 -- 4.1331)  data: 0.3714 (0.0003 -- 3.6162)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:33  lr: 0.000023  min_lr: 0.000001  loss: 2.0316 (2.0160)  loss_scale: 32768.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9039 (8.2141)  time: 0.6047 (0.5323 -- 1.5378)  data: 0.0512 (0.0002 -- 0.9769)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.9391 (2.0072)  loss_scale: 32768.0000 (28115.7531)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4651 (8.2742)  time: 0.9505 (0.5394 -- 3.1941)  data: 0.3861 (0.0002 -- 2.6393)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.0604 (1.9969)  loss_scale: 32768.0000 (29036.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3482 (8.1784)  time: 0.9164 (0.5092 -- 4.2554)  data: 0.3666 (0.0003 -- 3.7409)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8572 (1.9821)  loss_scale: 32768.0000 (29653.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9510 (8.0340)  time: 0.8425 (0.5236 -- 2.5429)  data: 0.2907 (0.0004 -- 2.0214)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0147 (1.9894)  loss_scale: 32768.0000 (30095.4326)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7679 (8.0030)  time: 0.8758 (0.5262 -- 3.0892)  data: 0.2224 (0.0002 -- 2.5332)  max mem: 16413
[2023-08-30 03:28:14,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:28:14,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:28:14,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 03:28:14,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 03:28:14,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6552
[2023-08-30 03:28:14,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6552
[2023-08-30 03:28:14,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 03:28:14,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 03:28:14,943] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7995 (1.9768)  loss_scale: 32768.0000 (30617.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1979 (7.9170)  time: 0.6592 (0.4960 -- 2.5714)  data: 0.1212 (0.0002 -- 2.0612)  max mem: 16413
Epoch: [40] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7995 (1.9519)  loss_scale: 32768.0000 (30617.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1979 (7.9170)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.1056 (1.1056)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2610 (2.2610 -- 2.2610)  data: 1.9804 (1.9804 -- 1.9804)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0218 (1.1344)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (94.9495)  time: 0.4135 (0.1982 -- 2.2610)  data: 0.1868 (0.0006 -- 1.9804)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0101 (1.0922)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (95.7672)  time: 0.2263 (0.1700 -- 0.3274)  data: 0.0147 (0.0001 -- 0.1443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1362 (1.1433)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (95.0207)  time: 0.2085 (0.1338 -- 0.3274)  data: 0.0137 (0.0001 -- 0.1443)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 65.145 Acc@5 94.191 loss 1.123
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 67.01%
Epoch: [41]  [  0/160]  eta: 0:20:52  lr: 0.000023  min_lr: 0.000001  loss: 2.3448 (2.3448)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1784 (7.1784)  time: 7.8309 (7.8309 -- 7.8309)  data: 7.3186 (7.3186 -- 7.3186)  max mem: 16413
[2023-08-30 03:28:38,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6567
[2023-08-30 03:28:38,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6567
[2023-08-30 03:28:38,067] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:28:38,067] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:28:38,067] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [ 20/160]  eta: 0:02:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0108 (1.9635)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5503 (7.8496)  time: 0.9227 (0.5225 -- 4.2630)  data: 0.3683 (0.0006 -- 3.7429)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 1.8738 (1.9387)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2650 (7.7563)  time: 0.8389 (0.5186 -- 3.2978)  data: 0.2928 (0.0004 -- 2.7641)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 1.8563 (1.9197)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7257 (7.8229)  time: 0.8221 (0.5276 -- 3.3786)  data: 0.2747 (0.0004 -- 2.8240)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.9938 (1.9314)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1139 (7.8766)  time: 0.8521 (0.5270 -- 3.4697)  data: 0.2236 (0.0001 -- 2.9584)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0726 (1.9350)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8620 (7.7746)  time: 0.8707 (0.5242 -- 2.8549)  data: 0.3172 (0.0002 -- 2.3281)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9445 (1.9364)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4373 (7.8690)  time: 0.9006 (0.5243 -- 2.4736)  data: 0.3526 (0.0004 -- 1.9638)  max mem: 16413
[2023-08-30 03:30:31,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:30:31,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:30:31,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:30:31,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0466 (1.9392)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1944 (7.8333)  time: 0.8102 (0.5393 -- 2.4831)  data: 0.2542 (0.0007 -- 1.9183)  max mem: 16413
[2023-08-30 03:30:45,085] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6713
[2023-08-30 03:30:45,085] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6713
[2023-08-30 03:30:45,085] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:30:45,085] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:30:45,085] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0480 (1.9471)  loss_scale: 32768.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4588 (7.9277)  time: 0.7096 (0.4973 -- 2.9953)  data: 0.1836 (0.0002 -- 2.4598)  max mem: 16413
Epoch: [41] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0480 (1.9451)  loss_scale: 32768.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4588 (7.9277)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0869 (1.0869)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4303 (2.4303 -- 2.4303)  data: 2.1858 (2.1858 -- 2.1858)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0233 (1.1179)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4186 (0.1989 -- 2.4303)  data: 0.2037 (0.0007 -- 2.1858)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9966 (1.0797)  acc1: 55.5556 (61.3757)  acc5: 100.0000 (96.8254)  time: 0.2182 (0.1753 -- 0.3821)  data: 0.0138 (0.0001 -- 0.1959)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1160 (1.1244)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (95.4357)  time: 0.2033 (0.1332 -- 0.3821)  data: 0.0132 (0.0001 -- 0.1959)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 65.145 Acc@5 94.606 loss 1.096
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 67.01%
Epoch: [42]  [  0/160]  eta: 0:18:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7445 (1.7445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4849 (6.4849)  time: 6.9783 (6.9783 -- 6.9783)  data: 6.1544 (6.1544 -- 6.1544)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:46  lr: 0.000023  min_lr: 0.000001  loss: 1.9718 (1.9392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5827 (8.0483)  time: 0.9009 (0.5264 -- 2.6546)  data: 0.3113 (0.0005 -- 2.1116)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 1.9561 (1.9283)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4316 (8.4326)  time: 0.9341 (0.5267 -- 3.7968)  data: 0.3423 (0.0005 -- 3.2798)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:34  lr: 0.000023  min_lr: 0.000001  loss: 1.8233 (1.9170)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4287 (8.3234)  time: 0.7072 (0.5347 -- 2.3804)  data: 0.1540 (0.0002 -- 1.8372)  max mem: 16413
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.9672 (1.9368)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0847 (8.0751)  time: 0.9879 (0.5108 -- 5.0316)  data: 0.4438 (0.0004 -- 4.4952)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8809 (1.9443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9096 (8.4023)  time: 0.8300 (0.5218 -- 3.8974)  data: 0.2870 (0.0002 -- 3.3557)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9406 (1.9523)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9594 (8.3005)  time: 0.9459 (0.5325 -- 3.9861)  data: 0.3960 (0.0004 -- 3.4614)  max mem: 16413
[2023-08-30 03:32:50,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:32:50,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:32:50,066] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:32:50,067] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:32:58,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6852
[2023-08-30 03:32:58,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6852
[2023-08-30 03:32:58,014] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:32:58,014] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:32:58,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0723 (1.9625)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9501 (8.1179)  time: 0.7385 (0.5180 -- 2.9440)  data: 0.1751 (0.0003 -- 2.4249)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9497 (1.9541)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0876 (8.1959)  time: 0.7049 (0.4968 -- 1.9177)  data: 0.1408 (0.0002 -- 1.4191)  max mem: 16413
Epoch: [42] Total time: 0:02:21 (0.8840 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9497 (1.9336)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0876 (8.1959)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.1075 (1.1075)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4245 (2.4245 -- 2.4245)  data: 2.1988 (2.1988 -- 2.1988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0255 (1.1126)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4176 (0.1996 -- 2.4245)  data: 0.2051 (0.0007 -- 2.1988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9531 (1.0672)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1689 -- 0.5312)  data: 0.0202 (0.0001 -- 0.3445)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0869 (1.1131)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (96.2656)  time: 0.2069 (0.1324 -- 0.5312)  data: 0.0199 (0.0001 -- 0.3445)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 67.220 Acc@5 95.436 loss 1.080
Accuracy of the network on the 482 val images: 67.22%
[2023-08-30 03:33:25,157] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:33:25,159] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:33:25,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:33:25,159] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:33:26,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:33:26,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.22%
Epoch: [43]  [  0/160]  eta: 0:22:28  lr: 0.000023  min_lr: 0.000001  loss: 1.9269 (1.9269)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9717 (9.9717)  time: 8.4306 (8.4306 -- 8.4306)  data: 7.8961 (7.8961 -- 7.8961)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.0336 (2.0077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7473 (7.5491)  time: 0.8175 (0.5192 -- 2.9174)  data: 0.2282 (0.0003 -- 2.3776)  max mem: 16413
Epoch: [43]  [ 40/160]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000001  loss: 1.6949 (1.8978)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5957 (7.9673)  time: 0.7990 (0.5349 -- 2.5591)  data: 0.1010 (0.0003 -- 1.1933)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 1.8792 (1.8742)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3746 (8.3954)  time: 1.0087 (0.5320 -- 3.5282)  data: 0.0833 (0.0005 -- 1.5663)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1029 (1.9065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9465 (8.2985)  time: 0.7421 (0.5282 -- 2.4367)  data: 0.0027 (0.0003 -- 0.0151)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8281 (1.9009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2700 (8.3976)  time: 0.8775 (0.5262 -- 3.3215)  data: 0.0661 (0.0006 -- 0.8267)  max mem: 16413
[2023-08-30 03:35:00,482] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:35:00,483] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:35:00,483] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:35:00,483] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:35:14,671] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6998
[2023-08-30 03:35:14,671] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6998
[2023-08-30 03:35:14,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:35:14,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:35:14,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 03:35:15,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[5.501288981750703e-07, 5.501288981750703e-07, 7.335051975667604e-07, 7.335051975667604e-07, 9.780069300890138e-07, 9.780069300890138e-07, 1.3040092401186852e-06, 1.3040092401186852e-06, 1.7386789868249135e-06, 1.7386789868249135e-06, 2.3182386490998847e-06, 2.3182386490998847e-06, 3.090984865466513e-06, 3.090984865466513e-06, 4.12131315395535e-06, 4.12131315395535e-06, 5.4950842052738e-06, 5.4950842052738e-06, 7.326778940365067e-06, 7.326778940365067e-06, 9.769038587153423e-06, 9.769038587153423e-06, 1.302538478287123e-05, 1.302538478287123e-05, 1.7367179710494975e-05, 1.7367179710494975e-05, 2.31562396139933e-05, 2.31562396139933e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 03:35:15,226] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=18.31577739534764, CurrSamplesPerSec=22.291521748469695, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.6616 (1.8876)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8917 (8.4181)  time: 0.8190 (0.5254 -- 3.2048)  data: 0.2043 (0.0004 -- 2.6842)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9240 (1.8831)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0874 (8.4043)  time: 1.0156 (0.5312 -- 5.1634)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0464 (1.8890)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6769 (8.3613)  time: 0.6159 (0.4952 -- 2.3941)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [43] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0464 (1.9228)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6769 (8.3613)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.0493 (1.0493)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3819 (2.3819 -- 2.3819)  data: 2.1689 (2.1689 -- 2.1689)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0122 (1.0928)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4213 (0.1995 -- 2.3819)  data: 0.2062 (0.0004 -- 2.1689)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9648 (1.0426)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2198 (0.1693 -- 0.3541)  data: 0.0135 (0.0001 -- 0.1685)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0328 (1.0905)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2061 (0.1330 -- 0.3541)  data: 0.0131 (0.0001 -- 0.1685)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 67.427 Acc@5 95.851 loss 1.059
Accuracy of the network on the 482 val images: 67.43%
[2023-08-30 03:35:56,181] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:35:56,183] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:35:56,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:35:56,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:35:57,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:35:57,595] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.43%
Epoch: [44]  [  0/160]  eta: 0:19:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1246 (2.1246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7534 (12.7534)  time: 7.2181 (7.2181 -- 7.2181)  data: 6.3720 (6.3720 -- 6.3720)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:46  lr: 0.000023  min_lr: 0.000001  loss: 1.9757 (1.9790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8219 (8.7694)  time: 0.8867 (0.5270 -- 4.5823)  data: 0.3228 (0.0002 -- 4.0659)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000001  loss: 1.9595 (1.9636)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4352 (8.3430)  time: 0.8610 (0.5288 -- 3.0852)  data: 0.3118 (0.0003 -- 2.5569)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.0596 (1.9820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9306 (8.5728)  time: 0.8030 (0.5269 -- 2.8475)  data: 0.2233 (0.0005 -- 2.3101)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.1317 (1.9908)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4860 (8.6927)  time: 0.9385 (0.5242 -- 5.3655)  data: 0.3931 (0.0004 -- 4.8388)  max mem: 16413
[2023-08-30 03:37:18,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:37:18,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:37:18,813] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:37:18,813] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.8473 (1.9615)  loss_scale: 32768.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2251 (8.4473)  time: 0.8708 (0.5291 -- 4.0443)  data: 0.3238 (0.0003 -- 3.5203)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7826 (1.9219)  loss_scale: 32768.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5887 (8.3462)  time: 0.8732 (0.5264 -- 3.0562)  data: 0.3223 (0.0003 -- 2.5285)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.8360 (1.9162)  loss_scale: 32768.0000 (22658.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1127 (8.2711)  time: 0.7511 (0.5290 -- 4.5502)  data: 0.2013 (0.0003 -- 4.0066)  max mem: 16413
[2023-08-30 03:38:11,044] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7188
[2023-08-30 03:38:11,044] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:38:11,044] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7188
[2023-08-30 03:38:11,044] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:38:11,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8861 (1.9247)  loss_scale: 16384.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2241 (8.2729)  time: 0.7049 (0.4966 -- 2.6040)  data: 0.1760 (0.0001 -- 2.0740)  max mem: 16413
Epoch: [44] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8861 (1.9236)  loss_scale: 16384.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2241 (8.2729)
Val:  [ 0/27]  eta: 0:00:57  loss: 1.0267 (1.0267)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1187 (2.1187 -- 2.1187)  data: 1.8905 (1.8905 -- 1.8905)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9924 (1.0702)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4059 (0.1965 -- 2.1187)  data: 0.1901 (0.0007 -- 1.8905)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9329 (1.0235)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2271 (0.1711 -- 0.4557)  data: 0.0227 (0.0001 -- 0.2027)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9941 (1.0677)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (95.4357)  time: 0.2143 (0.1328 -- 0.4557)  data: 0.0224 (0.0001 -- 0.2027)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 67.842 Acc@5 95.021 loss 1.042
Accuracy of the network on the 482 val images: 67.84%
[2023-08-30 03:38:25,828] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:38:25,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:38:25,829] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:38:25,829] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:38:27,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:38:27,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.84%
Epoch: [45]  [  0/160]  eta: 0:18:54  lr: 0.000023  min_lr: 0.000001  loss: 2.0913 (2.0913)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9470 (5.9470)  time: 7.0934 (7.0934 -- 7.0934)  data: 5.2845 (5.2845 -- 5.2845)  max mem: 16413
Epoch: [45]  [ 20/160]  eta: 0:02:58  lr: 0.000023  min_lr: 0.000001  loss: 2.0265 (1.9469)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2920 (7.8273)  time: 0.9842 (0.5338 -- 3.8690)  data: 0.0307 (0.0006 -- 0.5601)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000001  loss: 1.8436 (1.9034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6008 (7.9147)  time: 0.7906 (0.5196 -- 2.6743)  data: 0.0023 (0.0003 -- 0.0167)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 1.8563 (1.8779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1479 (8.2809)  time: 0.8433 (0.5300 -- 2.8795)  data: 0.0017 (0.0005 -- 0.0051)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.8849 (1.8810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9031 (8.2820)  time: 0.8146 (0.5214 -- 3.3857)  data: 0.0317 (0.0003 -- 0.5889)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0020 (1.9052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9566 (8.4641)  time: 0.9068 (0.5367 -- 2.8151)  data: 0.1583 (0.0008 -- 1.1940)  max mem: 16413
[2023-08-30 03:40:15,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:40:15,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:40:15,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:40:15,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [45]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8032 (1.8950)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7642 (8.4229)  time: 0.8249 (0.5303 -- 2.5935)  data: 0.1831 (0.0006 -- 2.0373)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.8766 (1.8911)  loss_scale: 32768.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1475 (8.3758)  time: 0.9050 (0.5135 -- 2.3485)  data: 0.1827 (0.0003 -- 1.8140)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9379 (1.8976)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9634 (8.2690)  time: 0.6401 (0.4966 -- 1.8731)  data: 0.0848 (0.0002 -- 1.3498)  max mem: 16413
Epoch: [45] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9379 (1.8973)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9634 (8.2690)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.0241 (1.0241)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2717 (2.2717 -- 2.2717)  data: 2.0486 (2.0486 -- 2.0486)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9998 (1.0547)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4299 (0.2104 -- 2.2717)  data: 0.2073 (0.0004 -- 2.0486)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9422 (1.0157)  acc1: 55.5556 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2271 (0.1712 -- 0.3687)  data: 0.0184 (0.0001 -- 0.1340)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0219 (1.0554)  acc1: 55.5556 (63.9004)  acc5: 100.0000 (95.8506)  time: 0.2111 (0.1341 -- 0.3687)  data: 0.0181 (0.0001 -- 0.1340)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 67.427 Acc@5 95.436 loss 1.028
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 67.84%
Epoch: [46]  [  0/160]  eta: 0:18:19  lr: 0.000023  min_lr: 0.000001  loss: 1.9861 (1.9861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7972 (13.7972)  time: 6.8725 (6.8725 -- 6.8725)  data: 5.7479 (5.7479 -- 5.7479)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000001  loss: 2.0315 (2.0476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7994 (9.1672)  time: 0.8642 (0.5265 -- 3.0134)  data: 0.1648 (0.0005 -- 1.2601)  max mem: 16413
[2023-08-30 03:41:26,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7384
[2023-08-30 03:41:26,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7384
[2023-08-30 03:41:26,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:41:26,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:41:26,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [ 40/160]  eta: 0:02:13  lr: 0.000023  min_lr: 0.000001  loss: 2.0473 (2.0417)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7482 (8.7613)  time: 1.0812 (0.5251 -- 4.4731)  data: 0.0024 (0.0003 -- 0.0179)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1626 (2.0576)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2411 (8.5007)  time: 0.6757 (0.5237 -- 1.7540)  data: 0.0313 (0.0004 -- 0.4294)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 1.7432 (1.9987)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5129 (8.3997)  time: 0.9549 (0.5301 -- 3.8585)  data: 0.0011 (0.0005 -- 0.0026)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.8873 (1.9737)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7474 (8.4337)  time: 0.8537 (0.5157 -- 4.8424)  data: 0.0016 (0.0006 -- 0.0038)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.7388 (1.9537)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4837 (8.4607)  time: 0.8633 (0.5362 -- 3.1937)  data: 0.0018 (0.0005 -- 0.0074)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1032 (1.9497)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3255 (8.3997)  time: 0.7932 (0.5246 -- 4.8458)  data: 0.0014 (0.0006 -- 0.0036)  max mem: 16413
[2023-08-30 03:43:14,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:43:14,387] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:43:14,387] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:43:14,387] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8621 (1.9503)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3271 (8.4150)  time: 0.6765 (0.4970 -- 2.2812)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [46] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8621 (1.9299)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3271 (8.4150)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9822 (0.9822)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3679 (2.3679 -- 2.3679)  data: 2.1294 (2.1294 -- 2.1294)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9822 (1.0576)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (94.9495)  time: 0.4342 (0.2038 -- 2.3679)  data: 0.2165 (0.0007 -- 2.1294)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9115 (1.0101)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (95.7672)  time: 0.2207 (0.1701 -- 0.4816)  data: 0.0128 (0.0001 -- 0.2394)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0055 (1.0503)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.2062 (0.1331 -- 0.4816)  data: 0.0125 (0.0001 -- 0.2394)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 66.598 Acc@5 95.021 loss 1.020
Accuracy of the network on the 482 val images: 66.60%
Max accuracy: 67.84%
Epoch: [47]  [  0/160]  eta: 0:22:20  lr: 0.000023  min_lr: 0.000001  loss: 2.2631 (2.2631)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1673 (5.1673)  time: 8.3786 (8.3786 -- 8.3786)  data: 7.8338 (7.8338 -- 7.8338)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000001  loss: 1.8159 (1.8795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5772 (8.4454)  time: 0.8123 (0.5240 -- 3.2129)  data: 0.2572 (0.0001 -- 2.6631)  max mem: 16413
[2023-08-30 03:43:50,403] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7541
[2023-08-30 03:43:50,403] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7541
[2023-08-30 03:43:50,403] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:43:50,403] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:43:50,404] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [ 40/160]  eta: 0:02:09  lr: 0.000023  min_lr: 0.000001  loss: 1.8543 (1.8920)  loss_scale: 16384.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1998 (7.9836)  time: 0.9733 (0.5391 -- 4.4336)  data: 0.3861 (0.0007 -- 3.9042)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000001  loss: 1.6066 (1.8485)  loss_scale: 16384.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4150 (8.2918)  time: 0.8829 (0.5233 -- 4.7898)  data: 0.3390 (0.0002 -- 4.2626)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1539 (1.8913)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8869 (8.0252)  time: 0.8811 (0.5223 -- 3.8962)  data: 0.3291 (0.0003 -- 3.3654)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.8932 (1.8877)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5592 (8.1477)  time: 0.7660 (0.5344 -- 2.2458)  data: 0.2082 (0.0007 -- 1.6642)  max mem: 16413
[2023-08-30 03:45:13,982] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7637
[2023-08-30 03:45:13,982] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7637
[2023-08-30 03:45:13,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:45:13,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:45:13,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [47]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0706 (1.9135)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7338 (8.2653)  time: 0.7888 (0.5259 -- 2.3132)  data: 0.1706 (0.0003 -- 1.7925)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9038 (1.9154)  loss_scale: 8192.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1087 (8.2926)  time: 0.9541 (0.5192 -- 4.8250)  data: 0.0013 (0.0003 -- 0.0094)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9460 (1.9080)  loss_scale: 8192.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9618 (8.2662)  time: 0.6320 (0.4977 -- 1.3236)  data: 0.0599 (0.0002 -- 0.8310)  max mem: 16413
Epoch: [47] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9460 (1.9237)  loss_scale: 8192.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9618 (8.2662)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.9240 (0.9240)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2908 (2.2908 -- 2.2908)  data: 2.0831 (2.0831 -- 2.0831)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9240 (1.0644)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4069 (0.1965 -- 2.2908)  data: 0.1954 (0.0004 -- 2.0831)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9028 (1.0116)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2258 (0.1704 -- 0.5854)  data: 0.0230 (0.0001 -- 0.3908)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9872 (1.0475)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2116 (0.1335 -- 0.5854)  data: 0.0227 (0.0001 -- 0.3908)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 66.390 Acc@5 95.436 loss 1.017
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 67.84%
Epoch: [48]  [  0/160]  eta: 0:22:18  lr: 0.000023  min_lr: 0.000001  loss: 1.3609 (1.3609)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4080 (5.4080)  time: 8.3632 (8.3632 -- 8.3632)  data: 7.8411 (7.8411 -- 7.8411)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000001  loss: 2.0138 (2.0044)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8588 (7.4275)  time: 0.8577 (0.5341 -- 3.9526)  data: 0.3016 (0.0003 -- 3.4078)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:11  lr: 0.000023  min_lr: 0.000001  loss: 1.9155 (1.9590)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7816 (7.8567)  time: 0.9659 (0.5132 -- 5.0820)  data: 0.4285 (0.0001 -- 4.5770)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9784 (1.9743)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5096 (8.3081)  time: 0.7135 (0.5425 -- 2.4624)  data: 0.1629 (0.0003 -- 1.9281)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.8625 (1.9522)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7495 (8.4173)  time: 0.8277 (0.5209 -- 2.6461)  data: 0.2710 (0.0003 -- 2.1318)  max mem: 16413
[2023-08-30 03:47:16,992] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:47:16,992] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 03:47:16,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:47:16,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.9165 (1.9361)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6113 (8.2707)  time: 0.8770 (0.5355 -- 3.9253)  data: 0.3241 (0.0005 -- 3.3949)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7670 (1.9200)  loss_scale: 16384.0000 (10561.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0014 (8.2041)  time: 0.8377 (0.5404 -- 2.9866)  data: 0.2818 (0.0002 -- 2.4347)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.8491 (1.9163)  loss_scale: 16384.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5465 (8.1831)  time: 0.8514 (0.5202 -- 2.4135)  data: 0.2315 (0.0007 -- 1.8466)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7028 (1.9081)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1207 (8.1955)  time: 0.7422 (0.4970 -- 3.5732)  data: 0.1854 (0.0002 -- 3.0372)  max mem: 16413
Epoch: [48] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7028 (1.9011)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1207 (8.1955)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.9751 (0.9751)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2586 (2.2586 -- 2.2586)  data: 2.0226 (2.0226 -- 2.0226)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9751 (1.0411)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4193 (0.1982 -- 2.2586)  data: 0.1998 (0.0007 -- 2.0226)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8727 (0.9905)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2279 (0.1698 -- 0.4675)  data: 0.0231 (0.0001 -- 0.2831)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9865 (1.0411)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (95.4357)  time: 0.2121 (0.1336 -- 0.4675)  data: 0.0227 (0.0001 -- 0.2831)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 66.598 Acc@5 95.436 loss 1.006
Accuracy of the network on the 482 val images: 66.60%
Max accuracy: 67.84%
Epoch: [49]  [  0/160]  eta: 0:24:29  lr: 0.000023  min_lr: 0.000001  loss: 2.3802 (2.3802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5487 (4.5487)  time: 9.1861 (9.1861 -- 9.1861)  data: 8.6633 (8.6633 -- 8.6633)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:56  lr: 0.000023  min_lr: 0.000001  loss: 1.6858 (1.7923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3520 (7.9611)  time: 0.8634 (0.5193 -- 3.0656)  data: 0.3182 (0.0003 -- 2.5078)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 1.6843 (1.8175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6545 (7.5089)  time: 0.7600 (0.5229 -- 3.5686)  data: 0.2138 (0.0002 -- 3.0109)  max mem: 16413
[2023-08-30 03:49:18,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:49:18,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:49:18,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:49:18,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 1.9236 (1.8537)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0203 (7.7603)  time: 0.9517 (0.5207 -- 3.1683)  data: 0.2867 (0.0006 -- 1.9339)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0270 (1.8826)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8442 (7.9459)  time: 0.8641 (0.5233 -- 3.0975)  data: 0.1893 (0.0003 -- 1.3537)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.0298 (1.9012)  loss_scale: 32768.0000 (24008.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5868 (7.8927)  time: 0.8838 (0.5186 -- 2.9850)  data: 0.0429 (0.0003 -- 0.5771)  max mem: 16413
[2023-08-30 03:50:02,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7944
[2023-08-30 03:50:02,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7944
[2023-08-30 03:50:02,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:50:02,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:50:02,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9924 (1.8993)  loss_scale: 16384.0000 (23154.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4604 (8.0406)  time: 0.7950 (0.5320 -- 2.1556)  data: 0.1777 (0.0003 -- 1.6109)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9611 (1.9023)  loss_scale: 16384.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7837 (7.8979)  time: 0.9366 (0.5207 -- 3.0281)  data: 0.3168 (0.0003 -- 2.5079)  max mem: 16413
[2023-08-30 03:50:47,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=41, lr=[5.373154318291099e-07, 5.373154318291099e-07, 7.164205757721467e-07, 7.164205757721467e-07, 9.55227434362862e-07, 9.55227434362862e-07, 1.273636579150483e-06, 1.273636579150483e-06, 1.6981821055339773e-06, 1.6981821055339773e-06, 2.2642428073786362e-06, 2.2642428073786362e-06, 3.0189904098381815e-06, 3.0189904098381815e-06, 4.025320546450909e-06, 4.025320546450909e-06, 5.367094061934545e-06, 5.367094061934545e-06, 7.156125415912727e-06, 7.156125415912727e-06, 9.541500554550302e-06, 9.541500554550302e-06, 1.2722000739400404e-05, 1.2722000739400404e-05, 1.696266765253387e-05, 1.696266765253387e-05, 2.2616890203378495e-05, 2.2616890203378495e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 03:50:47,230] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=18.327012348446292, CurrSamplesPerSec=24.421375014738228, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7726 (1.8939)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5684 (7.9366)  time: 0.6868 (0.4994 -- 2.9691)  data: 0.1241 (0.0002 -- 2.4676)  max mem: 16413
Epoch: [49] Total time: 0:02:23 (0.8967 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7726 (1.8846)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5684 (7.9366)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.9196 (0.9196)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4465 (2.4465 -- 2.4465)  data: 2.1905 (2.1905 -- 2.1905)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9196 (1.0236)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4246 (0.1992 -- 2.4465)  data: 0.2010 (0.0006 -- 2.1905)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9077 (0.9704)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2120 (0.1698 -- 0.2571)  data: 0.0042 (0.0001 -- 0.0600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9587 (1.0201)  acc1: 57.1429 (63.0705)  acc5: 100.0000 (95.4357)  time: 0.1935 (0.1332 -- 0.2454)  data: 0.0035 (0.0001 -- 0.0600)  max mem: 16413
Val: Total time: 0:00:07 (0.2842 s / it)
* Acc@1 66.805 Acc@5 95.643 loss 0.987
Accuracy of the network on the 482 val images: 66.80%
Max accuracy: 67.84%
Epoch: [50]  [  0/160]  eta: 0:18:04  lr: 0.000023  min_lr: 0.000001  loss: 2.2646 (2.2646)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0372 (8.0372)  time: 6.7811 (6.7811 -- 6.7811)  data: 5.8583 (5.8583 -- 5.8583)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 1.7167 (1.8205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1210 (7.6681)  time: 0.9014 (0.5201 -- 3.6729)  data: 0.2591 (0.0003 -- 2.1153)  max mem: 16413
[2023-08-30 03:51:21,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8023
[2023-08-30 03:51:21,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8023
[2023-08-30 03:51:21,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:51:21,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:51:21,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [50]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000001  loss: 1.8133 (1.8301)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7676 (8.0890)  time: 0.8450 (0.5197 -- 3.7972)  data: 0.0026 (0.0004 -- 0.0179)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 1.8096 (1.8394)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6554 (8.0731)  time: 0.8244 (0.5232 -- 3.5430)  data: 0.2458 (0.0003 -- 2.9855)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.7145 (1.8113)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6674 (8.1760)  time: 0.8699 (0.5408 -- 2.3043)  data: 0.2072 (0.0006 -- 1.7317)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8626 (1.8092)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2663 (8.2504)  time: 0.8740 (0.5196 -- 3.8745)  data: 0.2385 (0.0004 -- 3.3261)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7999 (1.8084)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3288 (8.1779)  time: 0.8426 (0.5315 -- 4.2629)  data: 0.2947 (0.0002 -- 3.7343)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9862 (1.8324)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6526 (8.4633)  time: 0.8995 (0.5102 -- 5.2072)  data: 0.3536 (0.0003 -- 4.6830)  max mem: 16413
[2023-08-30 03:53:12,014] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:53:12,014] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 03:53:12,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:53:12,019] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8499 (1.8474)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0958 (8.4246)  time: 0.6717 (0.4953 -- 3.2367)  data: 0.1527 (0.0002 -- 2.7243)  max mem: 16413
Epoch: [50] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8499 (1.8542)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0958 (8.4246)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8727 (0.8727)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3923 (2.3923 -- 2.3923)  data: 2.1482 (2.1482 -- 2.1482)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8727 (0.9966)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4237 (0.2034 -- 2.3923)  data: 0.2026 (0.0008 -- 2.1482)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8549 (0.9600)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2195 (0.1712 -- 0.3410)  data: 0.0121 (0.0001 -- 0.1584)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9405 (1.0054)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (95.8506)  time: 0.2021 (0.1330 -- 0.3410)  data: 0.0116 (0.0001 -- 0.1584)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 67.427 Acc@5 95.643 loss 0.972
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 67.84%
Epoch: [51]  [  0/160]  eta: 0:22:04  lr: 0.000023  min_lr: 0.000001  loss: 1.3382 (1.3382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7756 (12.7756)  time: 8.2775 (8.2775 -- 8.2775)  data: 6.1343 (6.1343 -- 6.1343)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000001  loss: 2.0822 (1.9229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0732 (8.3877)  time: 0.8109 (0.5202 -- 3.4116)  data: 0.0437 (0.0004 -- 0.8451)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:06  lr: 0.000022  min_lr: 0.000001  loss: 1.9792 (1.9082)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8308 (8.3144)  time: 0.9407 (0.5275 -- 2.8866)  data: 0.0026 (0.0001 -- 0.0177)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9721 (1.9242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0184 (8.5909)  time: 0.7829 (0.5328 -- 3.1302)  data: 0.0013 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.9397 (1.9199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8125 (8.5322)  time: 0.8970 (0.5348 -- 3.4741)  data: 0.1035 (0.0002 -- 1.6081)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.7662 (1.9050)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8021 (8.3129)  time: 0.7940 (0.5363 -- 2.2928)  data: 0.2063 (0.0003 -- 1.7610)  max mem: 16413
[2023-08-30 03:55:12,169] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:55:12,170] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:55:12,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:55:12,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [51]  [120/160]  eta: 0:00:35  lr: 0.000022  min_lr: 0.000001  loss: 1.8573 (1.9015)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2006 (8.6115)  time: 0.7870 (0.5293 -- 2.5872)  data: 0.0951 (0.0003 -- 0.9275)  max mem: 16413
[2023-08-30 03:55:16,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8282
[2023-08-30 03:55:16,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:55:16,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8282
[2023-08-30 03:55:16,034] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:55:16,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.7766 (1.8925)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5341 (8.5027)  time: 0.8940 (0.5304 -- 2.6139)  data: 0.1094 (0.0003 -- 0.8330)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8839 (1.8894)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5159 (8.3671)  time: 0.7648 (0.4973 -- 3.4964)  data: 0.0227 (0.0002 -- 0.4357)  max mem: 16413
Epoch: [51] Total time: 0:02:20 (0.8796 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8839 (1.8960)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5159 (8.3671)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.8156 (0.8156)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2128 (2.2128 -- 2.2128)  data: 1.9795 (1.9795 -- 1.9795)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8156 (1.0041)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4010 (0.2009 -- 2.2128)  data: 0.1819 (0.0008 -- 1.9795)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8152 (0.9551)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (96.8254)  time: 0.2225 (0.1714 -- 0.4459)  data: 0.0137 (0.0001 -- 0.2486)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9412 (1.0025)  acc1: 55.5556 (64.3154)  acc5: 100.0000 (96.2656)  time: 0.2041 (0.1331 -- 0.4459)  data: 0.0131 (0.0001 -- 0.2486)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 68.465 Acc@5 95.851 loss 0.972
Accuracy of the network on the 482 val images: 68.46%
[2023-08-30 03:55:52,181] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:55:52,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:55:52,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:55:52,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:55:53,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:55:53,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.46%
Epoch: [52]  [  0/160]  eta: 0:18:26  lr: 0.000022  min_lr: 0.000001  loss: 1.2968 (1.2968)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8610 (10.8610)  time: 6.9150 (6.9150 -- 6.9150)  data: 6.3607 (6.3607 -- 6.3607)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:38  lr: 0.000022  min_lr: 0.000001  loss: 1.9518 (1.8500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7115 (8.0774)  time: 0.8452 (0.5210 -- 3.3819)  data: 0.1549 (0.0002 -- 1.6664)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0682 (1.9164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2273 (7.5549)  time: 0.8611 (0.5188 -- 2.2216)  data: 0.0717 (0.0003 -- 1.4073)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.8831 (1.9139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6756 (7.9037)  time: 0.9067 (0.5319 -- 4.2521)  data: 0.0042 (0.0008 -- 0.0164)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000001  loss: 1.7393 (1.8720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7241 (7.9288)  time: 0.9123 (0.5281 -- 3.5015)  data: 0.0014 (0.0002 -- 0.0023)  max mem: 16413
[2023-08-30 03:57:19,067] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:57:19,067] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 03:57:19,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 03:57:19,072] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 2.0237 (1.9045)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5892 (7.7370)  time: 0.7285 (0.5361 -- 2.4010)  data: 0.0208 (0.0004 -- 0.3875)  max mem: 16413
[2023-08-30 03:57:34,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8431
[2023-08-30 03:57:34,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8431
[2023-08-30 03:57:34,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:57:34,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 03:57:34,781] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0372 (1.9133)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5921 (7.8533)  time: 0.8663 (0.5301 -- 3.5806)  data: 0.3154 (0.0004 -- 3.0416)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8736 (1.9164)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6294 (7.9892)  time: 0.8532 (0.5245 -- 2.1409)  data: 0.1099 (0.0004 -- 1.1039)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8706 (1.9112)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2573 (7.9459)  time: 0.7078 (0.4976 -- 2.5259)  data: 0.0830 (0.0001 -- 0.7640)  max mem: 16413
Epoch: [52] Total time: 0:02:19 (0.8737 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8706 (1.9206)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2573 (7.9459)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.8250 (0.8250)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5501 (2.5501 -- 2.5501)  data: 2.3037 (2.3037 -- 2.3037)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8250 (1.0071)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4298 (0.2000 -- 2.5501)  data: 0.2114 (0.0009 -- 2.3037)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8595 (0.9556)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (96.8254)  time: 0.2094 (0.1707 -- 0.2640)  data: 0.0013 (0.0001 -- 0.0100)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9348 (1.0018)  acc1: 57.1429 (64.7303)  acc5: 100.0000 (96.2656)  time: 0.1914 (0.1332 -- 0.2408)  data: 0.0005 (0.0001 -- 0.0015)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 68.672 Acc@5 96.266 loss 0.970
Accuracy of the network on the 482 val images: 68.67%
[2023-08-30 03:58:21,186] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 03:58:21,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 03:58:21,188] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 03:58:21,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 03:58:22,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 03:58:22,709] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.67%
Epoch: [53]  [  0/160]  eta: 0:21:23  lr: 0.000022  min_lr: 0.000001  loss: 1.7757 (1.7757)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1157 (6.1157)  time: 8.0220 (8.0220 -- 8.0220)  data: 7.4994 (7.4994 -- 7.4994)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9265 (1.7984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6851 (9.3526)  time: 0.7732 (0.5371 -- 3.0030)  data: 0.2042 (0.0005 -- 2.4788)  max mem: 16413
[2023-08-30 03:58:52,323] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8507
[2023-08-30 03:58:52,323] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8507
[2023-08-30 03:58:52,323] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:58:52,323] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 03:58:52,323] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [53]  [ 40/160]  eta: 0:02:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8067 (1.7881)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2164 (9.4905)  time: 0.8873 (0.5279 -- 2.8449)  data: 0.2004 (0.0003 -- 1.6814)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:34  lr: 0.000022  min_lr: 0.000001  loss: 1.8658 (1.8237)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6521 (9.3768)  time: 0.8256 (0.5231 -- 2.1058)  data: 0.1664 (0.0003 -- 1.3849)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.7086 (1.7915)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0878 (8.9665)  time: 0.8851 (0.5195 -- 2.1838)  data: 0.1849 (0.0004 -- 1.3365)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9356 (1.8260)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8588 (8.9726)  time: 0.8937 (0.5382 -- 3.4266)  data: 0.0585 (0.0003 -- 0.6204)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.8524 (1.8286)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8633 (9.0966)  time: 0.8382 (0.5250 -- 3.5304)  data: 0.2301 (0.0005 -- 2.9507)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.6503 (1.8166)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8239 (8.8515)  time: 0.8476 (0.5301 -- 3.6760)  data: 0.2881 (0.0002 -- 3.1323)  max mem: 16413
[2023-08-30 04:00:41,637] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:00:41,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:00:41,638] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:00:41,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9146 (1.8197)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0173 (8.8012)  time: 0.7264 (0.4971 -- 2.3643)  data: 0.1956 (0.0001 -- 1.8630)  max mem: 16413
Epoch: [53] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9146 (1.8485)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0173 (8.8012)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8058 (0.8058)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4465 (2.4465 -- 2.4465)  data: 2.2097 (2.2097 -- 2.2097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8998 (1.0059)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4211 (0.2061 -- 2.4465)  data: 0.2021 (0.0008 -- 2.2097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8486 (0.9413)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.2963)  time: 0.2167 (0.1704 -- 0.3916)  data: 0.0110 (0.0001 -- 0.2034)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9374 (0.9909)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.4357)  time: 0.2005 (0.1334 -- 0.3916)  data: 0.0107 (0.0001 -- 0.2034)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 69.087 Acc@5 95.436 loss 0.956
Accuracy of the network on the 482 val images: 69.09%
[2023-08-30 04:00:51,558] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:00:51,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:00:51,559] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:00:51,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:00:53,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:00:53,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.09%
Epoch: [54]  [  0/160]  eta: 0:20:59  lr: 0.000022  min_lr: 0.000001  loss: 2.0399 (2.0399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1451 (6.1451)  time: 7.8700 (7.8700 -- 7.8700)  data: 7.3362 (7.3362 -- 7.3362)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000001  loss: 2.1129 (2.0427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8074 (8.9572)  time: 0.8335 (0.5415 -- 3.6789)  data: 0.2782 (0.0004 -- 3.1010)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000001  loss: 1.9488 (2.0129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5675 (8.7964)  time: 0.9609 (0.5243 -- 4.3311)  data: 0.4127 (0.0004 -- 3.7603)  max mem: 16413
[2023-08-30 04:01:51,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8699
[2023-08-30 04:01:51,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8699
[2023-08-30 04:01:51,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:01:51,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:01:51,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [54]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 2.0469 (2.0121)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6624 (8.9705)  time: 0.8404 (0.5256 -- 2.7122)  data: 0.1629 (0.0003 -- 2.1863)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.7912 (1.9722)  loss_scale: 8192.0000 (14159.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7458 (9.0825)  time: 0.7464 (0.5249 -- 3.4065)  data: 0.0074 (0.0004 -- 0.0888)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9239 (1.9668)  loss_scale: 8192.0000 (12977.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3882 (8.7845)  time: 0.9204 (0.5239 -- 3.7906)  data: 0.0017 (0.0006 -- 0.0045)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.6384 (1.9305)  loss_scale: 8192.0000 (12186.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8220 (8.6306)  time: 0.8178 (0.5278 -- 2.8430)  data: 0.2225 (0.0002 -- 2.2944)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.7598 (1.9138)  loss_scale: 8192.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6846 (8.4591)  time: 0.9060 (0.5165 -- 2.8673)  data: 0.1701 (0.0003 -- 2.1867)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.6281 (1.8976)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3457 (8.5682)  time: 0.6715 (0.4952 -- 2.2953)  data: 0.0138 (0.0002 -- 0.2644)  max mem: 16413
Epoch: [54] Total time: 0:02:20 (0.8773 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.6281 (1.8737)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3457 (8.5682)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.8148 (0.8148)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3183 (2.3183 -- 2.3183)  data: 2.1019 (2.1019 -- 2.1019)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8148 (0.9961)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4281 (0.2049 -- 2.3183)  data: 0.2148 (0.0011 -- 2.1019)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8145 (0.9445)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (96.8254)  time: 0.2242 (0.1701 -- 0.4671)  data: 0.0199 (0.0001 -- 0.2413)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9347 (0.9907)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (95.8506)  time: 0.2097 (0.1328 -- 0.4671)  data: 0.0195 (0.0001 -- 0.2413)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 69.502 Acc@5 95.851 loss 0.951
Accuracy of the network on the 482 val images: 69.50%
[2023-08-30 04:03:21,267] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:03:21,269] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:03:21,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:03:21,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:03:22,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:03:22,710] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.50%
Epoch: [55]  [  0/160]  eta: 0:19:33  lr: 0.000022  min_lr: 0.000001  loss: 1.4022 (1.4022)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4973 (10.4973)  time: 7.3373 (7.3373 -- 7.3373)  data: 6.7668 (6.7668 -- 6.7668)  max mem: 16413
Epoch: [55]  [ 20/160]  eta: 0:02:41  lr: 0.000022  min_lr: 0.000001  loss: 1.9983 (1.9123)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5046 (9.0288)  time: 0.8478 (0.5373 -- 3.0237)  data: 0.2895 (0.0007 -- 2.4911)  max mem: 16413
[2023-08-30 04:03:52,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:03:52,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:03:52,420] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:03:52,420] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [55]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000001  loss: 1.8596 (1.8986)  loss_scale: 16384.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3060 (8.1070)  time: 0.8217 (0.5327 -- 2.3496)  data: 0.1288 (0.0005 -- 1.2928)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9496 (1.9314)  loss_scale: 16384.0000 (12623.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2397 (8.1585)  time: 0.9112 (0.5285 -- 3.2569)  data: 0.2733 (0.0005 -- 2.7365)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 1.7672 (1.9047)  loss_scale: 16384.0000 (13552.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3006 (8.2281)  time: 0.7811 (0.5185 -- 2.9483)  data: 0.2135 (0.0004 -- 2.4221)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 1.9730 (1.9151)  loss_scale: 16384.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0921 (8.5394)  time: 0.8817 (0.5346 -- 3.5711)  data: 0.0620 (0.0003 -- 1.1234)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:35  lr: 0.000022  min_lr: 0.000001  loss: 1.8041 (1.8999)  loss_scale: 16384.0000 (14488.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3040 (8.4530)  time: 0.7931 (0.5271 -- 1.9148)  data: 0.0698 (0.0003 -- 1.2375)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9364 (1.9079)  loss_scale: 16384.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5526 (8.4048)  time: 0.9924 (0.5272 -- 4.4931)  data: 0.1130 (0.0005 -- 1.2242)  max mem: 16413
[2023-08-30 04:05:41,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:05:41,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:05:41,682] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:05:41,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9177 (1.9081)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4313 (8.3218)  time: 0.6536 (0.4986 -- 2.1021)  data: 0.0011 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [55] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9177 (1.8778)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4313 (8.3218)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7497 (0.7497)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.5018 (2.5018 -- 2.5018)  data: 2.2598 (2.2598 -- 2.2598)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7760 (0.9805)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4181 (0.2006 -- 2.5018)  data: 0.2069 (0.0007 -- 2.2598)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7760 (0.9323)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (96.8254)  time: 0.2123 (0.1696 -- 0.3539)  data: 0.0095 (0.0001 -- 0.1713)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8981 (0.9775)  acc1: 66.6667 (65.9751)  acc5: 100.0000 (95.4357)  time: 0.1977 (0.1336 -- 0.3539)  data: 0.0092 (0.0001 -- 0.1713)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 71.369 Acc@5 95.436 loss 0.942
Accuracy of the network on the 482 val images: 71.37%
[2023-08-30 04:05:50,993] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:05:50,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:05:50,995] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:05:50,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:05:52,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:05:52,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.37%
Epoch: [56]  [  0/160]  eta: 0:24:50  lr: 0.000022  min_lr: 0.000001  loss: 2.4736 (2.4736)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7515 (7.7515)  time: 9.3151 (9.3151 -- 9.3151)  data: 8.8010 (8.8010 -- 8.8010)  max mem: 16413
[2023-08-30 04:06:05,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8967
[2023-08-30 04:06:05,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8967
[2023-08-30 04:06:05,743] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:06:05,743] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:06:05,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [56]  [ 20/160]  eta: 0:03:03  lr: 0.000022  min_lr: 0.000001  loss: 1.8325 (1.9439)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2625 (7.9705)  time: 0.9122 (0.5167 -- 5.2348)  data: 0.3741 (0.0002 -- 4.7170)  max mem: 16413
[2023-08-30 04:06:34,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=47, lr=[5.181404526866733e-07, 5.181404526866733e-07, 6.908539369155644e-07, 6.908539369155644e-07, 9.211385825540859e-07, 9.211385825540859e-07, 1.2281847767387813e-06, 1.2281847767387813e-06, 1.637579702318375e-06, 1.637579702318375e-06, 2.1834396030911665e-06, 2.1834396030911665e-06, 2.9112528041215555e-06, 2.9112528041215555e-06, 3.881670405495407e-06, 3.881670405495407e-06, 5.175560540660543e-06, 5.175560540660543e-06, 6.900747387547391e-06, 6.900747387547391e-06, 9.200996516729854e-06, 9.200996516729854e-06, 1.2267995355639805e-05, 1.2267995355639805e-05, 1.6357327140853076e-05, 1.6357327140853076e-05, 2.1809769521137432e-05, 2.1809769521137432e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 04:06:34,096] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.274228012005747, CurrSamplesPerSec=22.364830289689053, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000001  loss: 1.7553 (1.8791)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3793 (8.2621)  time: 0.8188 (0.5173 -- 4.0925)  data: 0.2738 (0.0003 -- 3.5721)  max mem: 16413
[2023-08-30 04:06:51,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9016
[2023-08-30 04:06:51,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9016
[2023-08-30 04:06:51,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:06:51,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:06:51,039] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [56]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 1.9078 (1.8580)  loss_scale: 16384.0000 (17592.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3573 (8.1546)  time: 0.8400 (0.5169 -- 3.4331)  data: 0.2924 (0.0004 -- 2.9290)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000001  loss: 1.7077 (1.8555)  loss_scale: 8192.0000 (15271.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0917 (8.1510)  time: 0.8717 (0.5320 -- 2.7115)  data: 0.3251 (0.0002 -- 2.1652)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.7800 (1.8442)  loss_scale: 8192.0000 (13869.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9249 (8.0694)  time: 0.7773 (0.5275 -- 2.9958)  data: 0.2234 (0.0003 -- 2.4603)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 1.8864 (1.8441)  loss_scale: 8192.0000 (12931.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1426 (8.0328)  time: 1.0101 (0.5153 -- 4.2236)  data: 0.4597 (0.0005 -- 3.7030)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.6251 (1.8188)  loss_scale: 8192.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0640 (8.2379)  time: 0.8152 (0.5242 -- 3.7002)  data: 0.2689 (0.0003 -- 3.1727)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.6216 (1.8115)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5240 (8.3237)  time: 0.7072 (0.4970 -- 4.3839)  data: 0.1944 (0.0002 -- 3.8785)  max mem: 16413
Epoch: [56] Total time: 0:02:23 (0.8991 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.6216 (1.8397)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5240 (8.3237)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7600 (0.7600)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2725 (2.2725 -- 2.2725)  data: 2.0267 (2.0267 -- 2.0267)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7600 (0.9739)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (96.9697)  time: 0.4239 (0.2007 -- 2.2725)  data: 0.2028 (0.0004 -- 2.0267)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8169 (0.9221)  acc1: 55.5556 (65.0794)  acc5: 100.0000 (96.8254)  time: 0.2271 (0.1692 -- 0.3863)  data: 0.0203 (0.0001 -- 0.1991)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8970 (0.9665)  acc1: 55.5556 (63.9004)  acc5: 100.0000 (95.8506)  time: 0.2094 (0.1333 -- 0.3863)  data: 0.0194 (0.0001 -- 0.1991)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 69.710 Acc@5 95.228 loss 0.933
Accuracy of the network on the 482 val images: 69.71%
Max accuracy: 71.37%
Epoch: [57]  [  0/160]  eta: 0:20:22  lr: 0.000022  min_lr: 0.000001  loss: 2.1405 (2.1405)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1007 (7.1007)  time: 7.6395 (7.6395 -- 7.6395)  data: 7.0952 (7.0952 -- 7.0952)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 1.7936 (1.8958)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0587 (7.8932)  time: 0.8202 (0.5208 -- 4.3721)  data: 0.1077 (0.0005 -- 1.6090)  max mem: 16413
[2023-08-30 04:08:53,687] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:08:53,688] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:08:53,690] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:08:53,690] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [57]  [ 40/160]  eta: 0:02:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9438 (1.9320)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8679 (8.0147)  time: 0.8544 (0.5264 -- 2.8620)  data: 0.1029 (0.0004 -- 1.5357)  max mem: 16413
[2023-08-30 04:09:21,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9178
[2023-08-30 04:09:21,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9178
[2023-08-30 04:09:21,893] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:09:21,893] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:09:21,893] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9289 (1.9231)  loss_scale: 16384.0000 (12623.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3168 (8.2820)  time: 0.8811 (0.5219 -- 2.8845)  data: 0.1421 (0.0007 -- 1.8004)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.8235 (1.8841)  loss_scale: 8192.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2597 (8.4117)  time: 0.8761 (0.5218 -- 3.6958)  data: 0.2721 (0.0008 -- 3.1571)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9726 (1.8848)  loss_scale: 8192.0000 (10868.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2735 (8.2662)  time: 0.8790 (0.5236 -- 3.4855)  data: 0.1303 (0.0002 -- 1.6017)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.6413 (1.8470)  loss_scale: 8192.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9817 (8.3567)  time: 0.8555 (0.5340 -- 3.0983)  data: 0.2489 (0.0004 -- 2.5418)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8906 (1.8463)  loss_scale: 8192.0000 (10109.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9089 (8.2769)  time: 0.9280 (0.5245 -- 2.6355)  data: 0.2090 (0.0004 -- 2.0845)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8257 (1.8484)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4427 (8.3820)  time: 0.6784 (0.4985 -- 2.6558)  data: 0.1569 (0.0002 -- 2.1366)  max mem: 16413
Epoch: [57] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8257 (1.8485)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4427 (8.3820)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7196 (0.7196)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2679 (2.2679 -- 2.2679)  data: 2.0288 (2.0288 -- 2.0288)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7333 (0.9552)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4329 (0.2079 -- 2.2679)  data: 0.2071 (0.0007 -- 2.0288)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7919 (0.9146)  acc1: 77.7778 (68.2540)  acc5: 100.0000 (96.8254)  time: 0.2235 (0.1696 -- 0.4841)  data: 0.0127 (0.0001 -- 0.2402)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8872 (0.9598)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.8506)  time: 0.2053 (0.1328 -- 0.4841)  data: 0.0124 (0.0001 -- 0.2402)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 71.577 Acc@5 95.851 loss 0.922
Accuracy of the network on the 482 val images: 71.58%
[2023-08-30 04:10:54,577] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:10:54,579] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:10:54,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:10:54,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:10:55,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:10:55,967] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.58%
Epoch: [58]  [  0/160]  eta: 0:16:17  lr: 0.000022  min_lr: 0.000001  loss: 1.1317 (1.1317)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1371 (7.1371)  time: 6.1084 (6.1084 -- 6.1084)  data: 5.5712 (5.5712 -- 5.5712)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:45  lr: 0.000022  min_lr: 0.000001  loss: 1.6955 (1.7280)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1728 (7.7922)  time: 0.9396 (0.5252 -- 3.7388)  data: 0.3931 (0.0005 -- 3.2182)  max mem: 16413
[2023-08-30 04:11:27,368] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:11:27,368] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:11:27,369] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:11:27,369] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [58]  [ 40/160]  eta: 0:02:01  lr: 0.000021  min_lr: 0.000001  loss: 1.9339 (1.8371)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9319 (7.4646)  time: 0.8305 (0.5227 -- 3.0627)  data: 0.2832 (0.0003 -- 2.5281)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000001  loss: 1.7973 (1.8587)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9851 (7.5991)  time: 0.8526 (0.5327 -- 3.2090)  data: 0.3036 (0.0003 -- 2.6879)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000001  loss: 1.8527 (1.8415)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7189 (7.6729)  time: 0.8290 (0.5357 -- 2.0775)  data: 0.2741 (0.0010 -- 1.5061)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:54  lr: 0.000021  min_lr: 0.000001  loss: 1.8375 (1.8515)  loss_scale: 16384.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9422 (7.9127)  time: 0.8432 (0.5316 -- 3.4880)  data: 0.2907 (0.0008 -- 2.9571)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.9156 (1.8547)  loss_scale: 16384.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9134 (8.0554)  time: 0.8990 (0.5389 -- 3.3779)  data: 0.3498 (0.0004 -- 2.8587)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000001  loss: 2.0252 (1.8654)  loss_scale: 16384.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3051 (8.1552)  time: 0.8243 (0.5347 -- 4.6732)  data: 0.2603 (0.0003 -- 4.1449)  max mem: 16413
[2023-08-30 04:13:15,455] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:13:15,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:13:15,455] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:13:15,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.6199 (1.8375)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2909 (8.2426)  time: 0.7846 (0.4964 -- 3.3375)  data: 0.2604 (0.0002 -- 2.7851)  max mem: 16413
Epoch: [58] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.6199 (1.8086)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2909 (8.2426)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7159 (0.7159)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3489 (2.3489 -- 2.3489)  data: 2.1363 (2.1363 -- 2.1363)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7525 (0.9745)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4267 (0.1904 -- 2.3489)  data: 0.2142 (0.0006 -- 2.1363)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7525 (0.9141)  acc1: 77.7778 (69.8413)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1705 -- 0.4180)  data: 0.0167 (0.0001 -- 0.2069)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8969 (0.9629)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.8506)  time: 0.2063 (0.1332 -- 0.4180)  data: 0.0164 (0.0001 -- 0.2069)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 71.992 Acc@5 95.851 loss 0.924
Accuracy of the network on the 482 val images: 71.99%
[2023-08-30 04:13:25,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:13:25,466] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:13:25,466] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:13:25,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:13:26,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:13:26,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.99%
Epoch: [59]  [  0/160]  eta: 0:18:46  lr: 0.000021  min_lr: 0.000001  loss: 1.7186 (1.7186)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4611 (10.4611)  time: 7.0378 (7.0378 -- 7.0378)  data: 5.9662 (5.9662 -- 5.9662)  max mem: 16413
[2023-08-30 04:13:36,083] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9444
[2023-08-30 04:13:36,083] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9444
[2023-08-30 04:13:36,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:13:36,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:13:36,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000001  loss: 1.7849 (1.7606)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6172 (9.3658)  time: 0.8922 (0.5130 -- 2.9443)  data: 0.2936 (0.0003 -- 2.2221)  max mem: 16413
Epoch: [59]  [ 40/160]  eta: 0:02:05  lr: 0.000021  min_lr: 0.000001  loss: 1.7943 (1.7919)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7368 (9.2368)  time: 0.8931 (0.5261 -- 3.8438)  data: 0.2367 (0.0004 -- 3.3258)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000001  loss: 2.0086 (1.8428)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4228 (9.4989)  time: 0.8248 (0.5332 -- 2.9071)  data: 0.0690 (0.0002 -- 1.3523)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 1.7669 (1.8529)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5956 (9.4722)  time: 0.9086 (0.5199 -- 3.8978)  data: 0.0017 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.9319 (1.8498)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6540 (9.4065)  time: 0.8575 (0.5310 -- 2.5921)  data: 0.1165 (0.0005 -- 2.0314)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.7994 (1.8455)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1168 (9.2242)  time: 0.7763 (0.5229 -- 2.9748)  data: 0.1234 (0.0003 -- 2.4271)  max mem: 16413
[2023-08-30 04:15:28,898] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:15:28,898] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:15:28,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:15:28,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:15:29,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9575
[2023-08-30 04:15:29,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:15:29,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9575
[2023-08-30 04:15:29,991] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 04:15:29,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8176 (1.8333)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0516 (8.9853)  time: 0.9207 (0.5220 -- 3.3829)  data: 0.1238 (0.0007 -- 2.3214)  max mem: 16413
[2023-08-30 04:15:35,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9581
[2023-08-30 04:15:35,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:15:35,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9581
[2023-08-30 04:15:35,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 04:15:35,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.8845 (1.8360)  loss_scale: 8192.0000 (16025.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7476 (8.9905)  time: 0.7181 (0.4996 -- 4.3620)  data: 0.1986 (0.0002 -- 3.8504)  max mem: 16413
Epoch: [59] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.8845 (1.8441)  loss_scale: 8192.0000 (16025.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7476 (8.9905)
[2023-08-30 04:15:49,196] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-08-30 04:15:49,199] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-08-30 04:15:49,201] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-08-30 04:15:49,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-08-30 04:15:50,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-08-30 04:15:50,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7328 (0.7328)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3408 (2.3408 -- 2.3408)  data: 2.0856 (2.0856 -- 2.0856)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8189 (0.9628)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4213 (0.1944 -- 2.3408)  data: 0.2039 (0.0005 -- 2.0856)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7812 (0.9050)  acc1: 77.7778 (69.3122)  acc5: 100.0000 (96.8254)  time: 0.2226 (0.1699 -- 0.3544)  data: 0.0150 (0.0001 -- 0.1437)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8637 (0.9522)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (96.6805)  time: 0.2076 (0.1330 -- 0.3544)  data: 0.0147 (0.0001 -- 0.1437)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 72.614 Acc@5 96.266 loss 0.904
Accuracy of the network on the 482 val images: 72.61%
[2023-08-30 04:15:58,082] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:15:58,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:15:58,084] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:15:58,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:15:59,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:15:59,337] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.61%
Epoch: [60]  [  0/160]  eta: 0:22:47  lr: 0.000021  min_lr: 0.000001  loss: 2.1140 (2.1140)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9335 (3.9335)  time: 8.5476 (8.5476 -- 8.5476)  data: 8.0025 (8.0025 -- 8.0025)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:58  lr: 0.000021  min_lr: 0.000001  loss: 1.5837 (1.7589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8444 (8.0368)  time: 0.9088 (0.5291 -- 2.8632)  data: 0.3631 (0.0004 -- 2.3591)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000001  loss: 1.7470 (1.7979)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7197 (8.3303)  time: 0.8520 (0.5288 -- 3.0255)  data: 0.2743 (0.0003 -- 2.5012)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000001  loss: 1.8321 (1.8170)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3901 (8.4522)  time: 0.8131 (0.5327 -- 3.6059)  data: 0.2611 (0.0004 -- 3.0793)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:18  lr: 0.000021  min_lr: 0.000001  loss: 1.7636 (1.8133)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9697 (8.8589)  time: 0.9761 (0.5271 -- 3.4619)  data: 0.4244 (0.0002 -- 2.9593)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.8017 (1.8164)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3524 (8.7897)  time: 0.7620 (0.5215 -- 4.1612)  data: 0.2159 (0.0002 -- 3.5996)  max mem: 16413
[2023-08-30 04:17:41,934] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:17:41,934] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:17:41,934] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:17:41,934] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [60]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.8456 (1.8222)  loss_scale: 16384.0000 (8936.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5005 (8.7179)  time: 0.9467 (0.5285 -- 3.9602)  data: 0.3929 (0.0005 -- 3.4008)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.9967 (1.8283)  loss_scale: 16384.0000 (9993.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7209 (8.6078)  time: 0.8294 (0.5335 -- 3.3871)  data: 0.2740 (0.0001 -- 2.8527)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9231 (1.8365)  loss_scale: 16384.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1285 (8.4631)  time: 0.6485 (0.4980 -- 3.0046)  data: 0.1237 (0.0002 -- 2.4612)  max mem: 16413
Epoch: [60] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9231 (1.8480)  loss_scale: 16384.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1285 (8.4631)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6975 (0.6975)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3764 (2.3764 -- 2.3764)  data: 2.1332 (2.1332 -- 2.1332)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8424 (0.9728)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4306 (0.1978 -- 2.3764)  data: 0.2062 (0.0007 -- 2.1332)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7807 (0.9051)  acc1: 77.7778 (69.8413)  acc5: 100.0000 (96.8254)  time: 0.2185 (0.1706 -- 0.3377)  data: 0.0092 (0.0001 -- 0.1148)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9176 (0.9550)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.8506)  time: 0.1984 (0.1326 -- 0.3377)  data: 0.0085 (0.0001 -- 0.1148)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 72.407 Acc@5 95.643 loss 0.909
Accuracy of the network on the 482 val images: 72.41%
Max accuracy: 72.61%
Epoch: [61]  [  0/160]  eta: 0:16:05  lr: 0.000021  min_lr: 0.000000  loss: 1.5688 (1.5688)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3991 (8.3991)  time: 6.0367 (6.0367 -- 6.0367)  data: 5.4392 (5.4392 -- 5.4392)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:44  lr: 0.000021  min_lr: 0.000000  loss: 1.7193 (1.8102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2150 (9.1420)  time: 0.9327 (0.5233 -- 2.6874)  data: 0.2705 (0.0006 -- 2.1251)  max mem: 16413
Epoch: [61]  [ 40/160]  eta: 0:01:58  lr: 0.000021  min_lr: 0.000000  loss: 1.8741 (1.7780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9967 (8.6282)  time: 0.7828 (0.5371 -- 2.4618)  data: 0.1627 (0.0003 -- 1.8512)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:35  lr: 0.000021  min_lr: 0.000000  loss: 2.0134 (1.8309)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7471 (8.5400)  time: 0.8991 (0.5284 -- 3.6682)  data: 0.2220 (0.0004 -- 3.1489)  max mem: 16413
[2023-08-30 04:19:44,865] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:19:44,865] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:19:44,866] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:19:44,866] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [61]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000000  loss: 1.9422 (1.8578)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6623 (8.2052)  time: 0.8914 (0.5169 -- 4.4401)  data: 0.0559 (0.0005 -- 1.0695)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:54  lr: 0.000021  min_lr: 0.000000  loss: 1.8047 (1.8464)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1030 (8.2094)  time: 0.8132 (0.5231 -- 2.4742)  data: 0.0833 (0.0002 -- 1.6245)  max mem: 16413
[2023-08-30 04:20:21,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9879
[2023-08-30 04:20:21,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:20:21,586] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9879
[2023-08-30 04:20:21,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:20:21,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.7617 (1.8409)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9144 (8.2513)  time: 0.9942 (0.5219 -- 3.0767)  data: 0.1272 (0.0005 -- 1.2670)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.7687 (1.8289)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3257 (8.2914)  time: 0.7688 (0.5341 -- 3.0101)  data: 0.1256 (0.0004 -- 2.4803)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7901 (1.8287)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9073 (8.3754)  time: 0.7181 (0.4989 -- 4.0096)  data: 0.1899 (0.0002 -- 3.4864)  max mem: 16413
Epoch: [61] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7901 (1.8301)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9073 (8.3754)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6778 (0.6778)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3182 (2.3182 -- 2.3182)  data: 2.0920 (2.0920 -- 2.0920)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7362 (0.9597)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4102 (0.1992 -- 2.3182)  data: 0.1912 (0.0007 -- 2.0920)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7362 (0.9017)  acc1: 77.7778 (69.8413)  acc5: 100.0000 (96.8254)  time: 0.2244 (0.1683 -- 0.4857)  data: 0.0189 (0.0001 -- 0.2912)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8599 (0.9435)  acc1: 66.6667 (68.4647)  acc5: 100.0000 (96.2656)  time: 0.2069 (0.1327 -- 0.4857)  data: 0.0186 (0.0001 -- 0.2912)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 74.066 Acc@5 95.851 loss 0.894
Accuracy of the network on the 482 val images: 74.07%
[2023-08-30 04:20:59,129] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:20:59,131] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:20:59,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:20:59,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:21:00,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:21:00,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.07%
Epoch: [62]  [  0/160]  eta: 0:20:34  lr: 0.000021  min_lr: 0.000000  loss: 1.4434 (1.4434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2168 (7.2168)  time: 7.7137 (7.7137 -- 7.7137)  data: 6.2969 (6.2969 -- 6.2969)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:32  lr: 0.000021  min_lr: 0.000000  loss: 1.9386 (1.9736)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0423 (8.1439)  time: 0.7552 (0.5158 -- 3.4664)  data: 0.1855 (0.0003 -- 2.5873)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000000  loss: 1.8348 (1.9139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0012 (8.5758)  time: 0.9608 (0.5217 -- 3.7526)  data: 0.3742 (0.0005 -- 3.2271)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000000  loss: 1.7630 (1.8594)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7048 (8.3607)  time: 0.8437 (0.5248 -- 3.9812)  data: 0.1255 (0.0004 -- 2.0087)  max mem: 16413
[2023-08-30 04:22:15,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=53, lr=[4.93076112372354e-07, 4.93076112372354e-07, 6.574348164964721e-07, 6.574348164964721e-07, 8.765797553286295e-07, 8.765797553286295e-07, 1.1687730071048393e-06, 1.1687730071048393e-06, 1.5583640094731191e-06, 1.5583640094731191e-06, 2.077818679297492e-06, 2.077818679297492e-06, 2.7704249057299893e-06, 2.7704249057299893e-06, 3.6938998743066527e-06, 3.6938998743066527e-06, 4.92519983240887e-06, 4.92519983240887e-06, 6.566933109878494e-06, 6.566933109878494e-06, 8.755910813171324e-06, 8.755910813171324e-06, 1.1674547750895099e-05, 1.1674547750895099e-05, 1.5566063667860134e-05, 1.5566063667860134e-05, 2.0754751557146844e-05, 2.0754751557146844e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 04:22:15,634] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.388600639996802, CurrSamplesPerSec=22.438172443277818, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.6717 (1.8425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1714 (8.6763)  time: 0.9666 (0.5209 -- 4.3502)  data: 0.0011 (0.0002 -- 0.0031)  max mem: 16413
[2023-08-30 04:22:24,708] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:22:24,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:22:24,709] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:22:24,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [100/160]  eta: 0:00:54  lr: 0.000021  min_lr: 0.000000  loss: 1.6410 (1.8040)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4810 (8.5845)  time: 0.7048 (0.5295 -- 2.0615)  data: 0.0016 (0.0004 -- 0.0027)  max mem: 16413
[2023-08-30 04:22:47,031] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10036
[2023-08-30 04:22:47,031] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10036
[2023-08-30 04:22:47,031] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:22:47,031] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:22:47,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [62]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 1.8351 (1.8055)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3906 (8.5358)  time: 0.9399 (0.5305 -- 3.6810)  data: 0.0032 (0.0006 -- 0.0203)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.7513 (1.7958)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9686 (8.5338)  time: 0.8950 (0.5282 -- 4.5155)  data: 0.0254 (0.0003 -- 0.4817)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9726 (1.8079)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5026 (8.6446)  time: 0.6001 (0.4973 -- 1.9362)  data: 0.0012 (0.0002 -- 0.0074)  max mem: 16413
Epoch: [62] Total time: 0:02:20 (0.8784 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9726 (1.8204)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5026 (8.6446)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.6636 (0.6636)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2891 (2.2891 -- 2.2891)  data: 2.0668 (2.0668 -- 2.0668)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7026 (0.9503)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4065 (0.1936 -- 2.2891)  data: 0.1976 (0.0006 -- 2.0668)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7026 (0.8883)  acc1: 77.7778 (69.3122)  acc5: 100.0000 (96.8254)  time: 0.2227 (0.1699 -- 0.4967)  data: 0.0195 (0.0001 -- 0.2798)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8399 (0.9300)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.8506)  time: 0.2095 (0.1333 -- 0.4967)  data: 0.0192 (0.0001 -- 0.2798)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 73.029 Acc@5 95.436 loss 0.889
Accuracy of the network on the 482 val images: 73.03%
Max accuracy: 74.07%
Epoch: [63]  [  0/160]  eta: 0:19:27  lr: 0.000021  min_lr: 0.000000  loss: 0.7992 (0.7992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2812 (8.2812)  time: 7.2940 (7.2940 -- 7.2940)  data: 6.7421 (6.7421 -- 6.7421)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:02:34  lr: 0.000021  min_lr: 0.000000  loss: 1.6885 (1.7176)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3307 (8.6289)  time: 0.7946 (0.5314 -- 3.4108)  data: 0.0526 (0.0003 -- 0.5521)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:01:58  lr: 0.000021  min_lr: 0.000000  loss: 1.6876 (1.7573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5291 (8.6915)  time: 0.8670 (0.5318 -- 3.0941)  data: 0.0461 (0.0003 -- 0.8895)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 1.7554 (1.7644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2715 (8.7562)  time: 0.9487 (0.5232 -- 2.8938)  data: 0.0608 (0.0002 -- 1.0479)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000000  loss: 1.7749 (1.7792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8325 (8.9883)  time: 0.7985 (0.5234 -- 3.1601)  data: 0.1761 (0.0005 -- 2.6424)  max mem: 16413
[2023-08-30 04:24:48,141] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:24:48,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:24:48,147] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:24:48,148] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:24:57,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10176
[2023-08-30 04:24:57,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:24:57,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10176
[2023-08-30 04:24:57,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:24:57,487] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 1.8833 (1.7891)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8420 (9.0480)  time: 0.8855 (0.5311 -- 3.6942)  data: 0.3277 (0.0006 -- 3.1844)  max mem: 16413
Epoch: [63]  [120/160]  eta: 0:00:35  lr: 0.000021  min_lr: 0.000000  loss: 1.6701 (1.7682)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0464 (8.9261)  time: 0.7499 (0.5280 -- 3.6620)  data: 0.2089 (0.0005 -- 3.1166)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.9084 (1.7806)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0455 (8.8786)  time: 0.9591 (0.5358 -- 3.9480)  data: 0.3239 (0.0003 -- 3.4292)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7879 (1.7751)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8717 (8.7853)  time: 0.7652 (0.4936 -- 3.6729)  data: 0.0011 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [63] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7879 (1.7994)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8717 (8.7853)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6991 (0.6991)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3719 (2.3719 -- 2.3719)  data: 2.1521 (2.1521 -- 2.1521)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7106 (0.9541)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4224 (0.2020 -- 2.3719)  data: 0.2095 (0.0005 -- 2.1521)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7106 (0.8943)  acc1: 77.7778 (70.8995)  acc5: 100.0000 (96.2963)  time: 0.2253 (0.1696 -- 0.4371)  data: 0.0200 (0.0001 -- 0.2433)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8494 (0.9306)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (96.2656)  time: 0.2103 (0.1328 -- 0.4371)  data: 0.0197 (0.0001 -- 0.2433)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 73.859 Acc@5 96.266 loss 0.881
Accuracy of the network on the 482 val images: 73.86%
Max accuracy: 74.07%
Epoch: [64]  [  0/160]  eta: 0:17:50  lr: 0.000020  min_lr: 0.000000  loss: 1.9831 (1.9831)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3242 (12.3242)  time: 6.6891 (6.6891 -- 6.6891)  data: 6.0782 (6.0782 -- 6.0782)  max mem: 16413
[2023-08-30 04:26:18,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10257
[2023-08-30 04:26:18,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10257
[2023-08-30 04:26:18,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:26:18,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:26:18,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [64]  [ 20/160]  eta: 0:02:42  lr: 0.000020  min_lr: 0.000000  loss: 1.8225 (1.7726)  loss_scale: 16384.0000 (14823.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4778 (8.6790)  time: 0.8807 (0.5304 -- 3.0004)  data: 0.0630 (0.0005 -- 1.0971)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:03  lr: 0.000020  min_lr: 0.000000  loss: 1.9588 (1.8450)  loss_scale: 8192.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0179 (8.4652)  time: 0.8922 (0.5240 -- 5.3432)  data: 0.0631 (0.0003 -- 1.2377)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000020  min_lr: 0.000000  loss: 1.8538 (1.8402)  loss_scale: 8192.0000 (10475.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2595 (8.2549)  time: 0.9362 (0.5140 -- 2.6049)  data: 0.1126 (0.0001 -- 1.7046)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.6850 (1.8133)  loss_scale: 8192.0000 (9911.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5437 (8.3398)  time: 0.7860 (0.5259 -- 3.1537)  data: 0.0190 (0.0004 -- 0.3589)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.8872 (1.8205)  loss_scale: 8192.0000 (9570.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0787 (8.2974)  time: 0.9085 (0.5206 -- 3.1816)  data: 0.0949 (0.0003 -- 1.3948)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9030 (1.8254)  loss_scale: 8192.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4430 (8.6049)  time: 0.8351 (0.5274 -- 3.0860)  data: 0.1874 (0.0004 -- 2.5339)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 2.0796 (1.8517)  loss_scale: 8192.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6977 (8.4412)  time: 0.8796 (0.5303 -- 4.3493)  data: 0.3246 (0.0003 -- 3.7939)  max mem: 16413
[2023-08-30 04:28:11,265] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:28:11,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:28:11,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:28:11,267] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7781 (1.8423)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7482 (8.4172)  time: 0.6360 (0.4980 -- 2.1222)  data: 0.1034 (0.0002 -- 1.5790)  max mem: 16413
Epoch: [64] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7781 (1.8304)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7482 (8.4172)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.6908 (0.6908)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2787 (2.2787 -- 2.2787)  data: 2.0725 (2.0725 -- 2.0725)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7316 (0.9633)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4073 (0.2014 -- 2.2787)  data: 0.1903 (0.0006 -- 2.0725)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7635 (0.9004)  acc1: 66.6667 (70.8995)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1696 -- 0.3560)  data: 0.0156 (0.0001 -- 0.1803)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8307 (0.9491)  acc1: 66.6667 (68.4647)  acc5: 100.0000 (95.0207)  time: 0.2084 (0.1335 -- 0.3560)  data: 0.0148 (0.0001 -- 0.1803)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 73.444 Acc@5 95.851 loss 0.898
Accuracy of the network on the 482 val images: 73.44%
Max accuracy: 74.07%
Epoch: [65]  [  0/160]  eta: 0:23:57  lr: 0.000020  min_lr: 0.000000  loss: 2.0846 (2.0846)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5538 (8.5538)  time: 8.9858 (8.9858 -- 8.9858)  data: 5.6381 (5.6381 -- 5.6381)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000000  loss: 1.8822 (1.8355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6281 (8.1154)  time: 0.7958 (0.5223 -- 3.8139)  data: 0.2153 (0.0003 -- 2.5831)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:05  lr: 0.000020  min_lr: 0.000000  loss: 1.6562 (1.7959)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4532 (8.1942)  time: 0.9047 (0.5163 -- 4.9306)  data: 0.0371 (0.0002 -- 0.7065)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.5832 (1.7614)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2923 (8.1087)  time: 0.7690 (0.5314 -- 2.9665)  data: 0.1663 (0.0003 -- 2.4383)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000000  loss: 2.1079 (1.8243)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8588 (8.2247)  time: 0.9787 (0.5163 -- 4.1926)  data: 0.4287 (0.0003 -- 3.6749)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.7526 (1.8054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5611 (8.2671)  time: 0.7280 (0.5292 -- 3.3180)  data: 0.1649 (0.0004 -- 2.7664)  max mem: 16413
[2023-08-30 04:30:11,721] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:30:11,721] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:30:11,721] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:30:11,722] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.7340 (1.7936)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1783 (8.3599)  time: 0.9749 (0.5364 -- 3.7175)  data: 0.0208 (0.0003 -- 0.3795)  max mem: 16413
[2023-08-30 04:30:29,376] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10535
[2023-08-30 04:30:29,376] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10535
[2023-08-30 04:30:29,376] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:30:29,376] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:30:29,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7531 (1.7959)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0469 (8.3330)  time: 0.7531 (0.5315 -- 2.2367)  data: 0.0646 (0.0005 -- 1.2499)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8764 (1.8040)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8405 (8.3044)  time: 0.6965 (0.4968 -- 2.5281)  data: 0.0033 (0.0002 -- 0.0490)  max mem: 16413
Epoch: [65] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8764 (1.8382)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8405 (8.3044)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6504 (0.6504)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3341 (2.3341 -- 2.3341)  data: 2.0815 (2.0815 -- 2.0815)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6791 (0.9485)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (96.9697)  time: 0.4245 (0.2040 -- 2.3341)  data: 0.2042 (0.0006 -- 2.0815)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6791 (0.8855)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1724 -- 0.3627)  data: 0.0148 (0.0001 -- 0.1554)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8399 (0.9270)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (96.2656)  time: 0.2041 (0.1330 -- 0.3627)  data: 0.0145 (0.0001 -- 0.1554)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 74.481 Acc@5 96.058 loss 0.880
Accuracy of the network on the 482 val images: 74.48%
[2023-08-30 04:30:54,467] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:30:54,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:30:54,469] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:30:54,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:30:55,971] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:30:55,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.48%
Epoch: [66]  [  0/160]  eta: 0:17:45  lr: 0.000020  min_lr: 0.000000  loss: 1.7132 (1.7132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2832 (8.2832)  time: 6.6595 (6.6595 -- 6.6595)  data: 6.1158 (6.1158 -- 6.1158)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:49  lr: 0.000020  min_lr: 0.000000  loss: 1.8832 (1.7935)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8988 (8.3949)  time: 0.9420 (0.5180 -- 3.7881)  data: 0.1805 (0.0004 -- 1.4506)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 1.7159 (1.7808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4012 (8.6679)  time: 0.8544 (0.5254 -- 4.0268)  data: 0.0800 (0.0003 -- 0.6681)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:37  lr: 0.000020  min_lr: 0.000000  loss: 1.8297 (1.8118)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7278 (8.9610)  time: 0.8528 (0.5168 -- 3.7675)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.6417 (1.8216)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5974 (8.8104)  time: 0.8434 (0.5231 -- 3.3768)  data: 0.1152 (0.0002 -- 0.9483)  max mem: 16413
[2023-08-30 04:32:29,254] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10660
[2023-08-30 04:32:29,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:32:29,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 04:32:29,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10660
[2023-08-30 04:32:29,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [66]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.8167 (1.8275)  loss_scale: 16384.0000 (16302.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5012 (8.7705)  time: 0.8377 (0.5281 -- 2.1206)  data: 0.1132 (0.0003 -- 1.0853)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.6298 (1.8066)  loss_scale: 8192.0000 (14962.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5426 (8.7785)  time: 0.8476 (0.5192 -- 2.1404)  data: 0.3001 (0.0005 -- 1.6127)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7025 (1.8011)  loss_scale: 8192.0000 (14001.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9911 (8.7322)  time: 0.8996 (0.5398 -- 4.0801)  data: 0.3426 (0.0004 -- 3.5382)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.6656 (1.7880)  loss_scale: 8192.0000 (13312.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2604 (8.8435)  time: 0.7230 (0.4966 -- 3.9459)  data: 0.2007 (0.0002 -- 3.3969)  max mem: 16413
Epoch: [66] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.6656 (1.7918)  loss_scale: 8192.0000 (13312.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2604 (8.8435)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.6480 (0.6480)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2529 (2.2529 -- 2.2529)  data: 2.0278 (2.0278 -- 2.0278)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6594 (0.9426)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4221 (0.1920 -- 2.2529)  data: 0.2145 (0.0007 -- 2.0278)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6911 (0.8829)  acc1: 77.7778 (70.8995)  acc5: 100.0000 (96.2963)  time: 0.2236 (0.1689 -- 0.5567)  data: 0.0211 (0.0001 -- 0.3163)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8216 (0.9243)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (95.4357)  time: 0.2126 (0.1335 -- 0.5567)  data: 0.0206 (0.0001 -- 0.3163)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 73.651 Acc@5 95.643 loss 0.876
Accuracy of the network on the 482 val images: 73.65%
Max accuracy: 74.48%
Epoch: [67]  [  0/160]  eta: 0:21:11  lr: 0.000020  min_lr: 0.000000  loss: 2.0848 (2.0848)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3704 (6.3704)  time: 7.9474 (7.9474 -- 7.9474)  data: 7.4032 (7.4032 -- 7.4032)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:43  lr: 0.000020  min_lr: 0.000000  loss: 1.8331 (1.9137)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2238 (8.3228)  time: 0.8268 (0.5255 -- 2.3975)  data: 0.1187 (0.0003 -- 1.8335)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:01:55  lr: 0.000020  min_lr: 0.000000  loss: 1.6947 (1.8671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8450 (8.2793)  time: 0.7516 (0.5288 -- 3.5175)  data: 0.0219 (0.0004 -- 0.4130)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:38  lr: 0.000020  min_lr: 0.000000  loss: 1.7915 (1.8624)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2182 (8.1688)  time: 1.0264 (0.5281 -- 4.0145)  data: 0.0403 (0.0003 -- 0.7812)  max mem: 16413
[2023-08-30 04:34:33,919] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:34:33,919] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:34:33,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:34:33,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [67]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.8198 (1.8408)  loss_scale: 16384.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7693 (8.0444)  time: 0.8169 (0.5218 -- 2.7654)  data: 0.0461 (0.0004 -- 0.8859)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.9636 (1.8506)  loss_scale: 16384.0000 (10787.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5065 (8.1741)  time: 0.8852 (0.5326 -- 2.5953)  data: 0.1530 (0.0003 -- 1.7864)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.7901 (1.8445)  loss_scale: 16384.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1371 (8.3431)  time: 0.8091 (0.5227 -- 2.4900)  data: 0.2067 (0.0004 -- 1.9443)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8973 (1.8458)  loss_scale: 16384.0000 (12375.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5085 (8.3728)  time: 0.9820 (0.5264 -- 2.9540)  data: 0.2187 (0.0003 -- 2.2151)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7157 (1.8342)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2445 (8.3657)  time: 0.7357 (0.5012 -- 2.7303)  data: 0.1324 (0.0001 -- 2.2151)  max mem: 16413
Epoch: [67] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7157 (1.8184)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2445 (8.3657)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.6417 (0.6417)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2447 (2.2447 -- 2.2447)  data: 2.0027 (2.0027 -- 2.0027)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6713 (0.9447)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4113 (0.2006 -- 2.2447)  data: 0.1842 (0.0006 -- 2.0027)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7342 (0.8770)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2242 (0.1697 -- 0.3671)  data: 0.0106 (0.0001 -- 0.1855)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8125 (0.9135)  acc1: 66.6667 (70.1245)  acc5: 100.0000 (95.8506)  time: 0.2072 (0.1337 -- 0.3671)  data: 0.0103 (0.0001 -- 0.1855)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 74.274 Acc@5 95.643 loss 0.867
Accuracy of the network on the 482 val images: 74.27%
Max accuracy: 74.48%
Epoch: [68]  [  0/160]  eta: 0:19:21  lr: 0.000020  min_lr: 0.000000  loss: 2.3556 (2.3556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8712 (5.8712)  time: 7.2599 (7.2599 -- 7.2599)  data: 6.6866 (6.6866 -- 6.6866)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:37  lr: 0.000020  min_lr: 0.000000  loss: 1.7415 (1.7810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0319 (8.2344)  time: 0.8171 (0.5272 -- 3.2810)  data: 0.1323 (0.0007 -- 1.7287)  max mem: 16413
[2023-08-30 04:36:27,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10909
[2023-08-30 04:36:27,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:36:27,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10909
[2023-08-30 04:36:27,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:36:27,458] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [68]  [ 40/160]  eta: 0:02:08  lr: 0.000020  min_lr: 0.000000  loss: 1.9128 (1.8209)  loss_scale: 8192.0000 (13986.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3038 (9.1221)  time: 1.0135 (0.5293 -- 3.4266)  data: 0.0024 (0.0004 -- 0.0171)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.8319 (1.8103)  loss_scale: 8192.0000 (12086.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7213 (8.8623)  time: 0.7320 (0.5244 -- 2.3743)  data: 0.0017 (0.0005 -- 0.0059)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000000  loss: 1.8369 (1.8271)  loss_scale: 8192.0000 (11124.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6144 (8.9212)  time: 0.9223 (0.5196 -- 3.0657)  data: 0.0020 (0.0004 -- 0.0072)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.9349 (1.8280)  loss_scale: 8192.0000 (10544.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4644 (8.9145)  time: 0.8284 (0.5192 -- 2.2406)  data: 0.0016 (0.0004 -- 0.0033)  max mem: 16413
[2023-08-30 04:37:47,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=59, lr=[4.6273957812846756e-07, 4.6273957812846756e-07, 6.1698610417129e-07, 6.1698610417129e-07, 8.226481388950535e-07, 8.226481388950535e-07, 1.0968641851934045e-06, 1.0968641851934045e-06, 1.4624855802578728e-06, 1.4624855802578728e-06, 1.9499807736771634e-06, 1.9499807736771634e-06, 2.599974364902885e-06, 2.599974364902885e-06, 3.4666324865371797e-06, 3.4666324865371797e-06, 4.62217664871624e-06, 4.62217664871624e-06, 6.1629021982883195e-06, 6.1629021982883195e-06, 8.217202931051093e-06, 8.217202931051093e-06, 1.095627057473479e-05, 1.095627057473479e-05, 1.4608360766313053e-05, 1.4608360766313053e-05, 1.9477814355084072e-05, 1.9477814355084072e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 04:37:47,538] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=18.21673023677005, CurrSamplesPerSec=22.393876389671835, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.9625 (1.8454)  loss_scale: 8192.0000 (10155.3719)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6205 (8.6560)  time: 0.9521 (0.5207 -- 3.0403)  data: 0.0016 (0.0004 -- 0.0094)  max mem: 16413
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7202 (1.8437)  loss_scale: 8192.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8784 (8.6537)  time: 0.7396 (0.5164 -- 3.4765)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-30 04:38:15,920] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:38:15,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:38:15,921] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:38:15,962] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.7302 (1.8182)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0854 (8.6017)  time: 0.7215 (0.5000 -- 2.7179)  data: 0.0015 (0.0002 -- 0.0159)  max mem: 16413
Epoch: [68] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.7302 (1.8076)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0854 (8.6017)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.6196 (0.6196)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.6048 (2.6048 -- 2.6048)  data: 2.3687 (2.3687 -- 2.3687)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6869 (0.9331)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (96.9697)  time: 0.4328 (0.2017 -- 2.6048)  data: 0.2167 (0.0007 -- 2.3687)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6869 (0.8537)  acc1: 77.7778 (71.4286)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1702 -- 0.4329)  data: 0.0135 (0.0001 -- 0.2520)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8037 (0.9015)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (96.2656)  time: 0.2002 (0.1325 -- 0.4329)  data: 0.0132 (0.0001 -- 0.2520)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 74.896 Acc@5 96.266 loss 0.852
Accuracy of the network on the 482 val images: 74.90%
[2023-08-30 04:38:24,716] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:38:24,718] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:38:24,718] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:38:24,718] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:38:25,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:38:25,958] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.90%
Epoch: [69]  [  0/160]  eta: 0:25:20  lr: 0.000019  min_lr: 0.000000  loss: 1.8430 (1.8430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9774 (5.9774)  time: 9.5048 (9.5048 -- 9.5048)  data: 8.9542 (8.9542 -- 8.9542)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:56  lr: 0.000019  min_lr: 0.000000  loss: 1.7657 (1.8420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5159 (9.4874)  time: 0.8483 (0.5126 -- 3.8434)  data: 0.2907 (0.0001 -- 3.3317)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:04  lr: 0.000019  min_lr: 0.000000  loss: 1.8968 (1.8212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1870 (9.2325)  time: 0.7992 (0.5277 -- 3.1962)  data: 0.2500 (0.0004 -- 2.6719)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.6588 (1.7944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8667 (9.0468)  time: 0.8103 (0.5382 -- 2.8274)  data: 0.1382 (0.0004 -- 2.3118)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000000  loss: 1.8702 (1.8237)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4355 (9.0525)  time: 0.9307 (0.5135 -- 3.4438)  data: 0.3819 (0.0004 -- 2.9220)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.6119 (1.8081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4225 (8.8235)  time: 0.8692 (0.5275 -- 3.7473)  data: 0.3089 (0.0002 -- 3.2094)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8568 (1.8210)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5787 (8.7589)  time: 0.8040 (0.5326 -- 2.8153)  data: 0.2526 (0.0005 -- 2.2913)  max mem: 16413
[2023-08-30 04:40:23,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:40:23,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:40:23,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:40:23,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7946 (1.8197)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9251 (8.6644)  time: 0.8567 (0.5228 -- 3.0725)  data: 0.1931 (0.0003 -- 2.5623)  max mem: 16413
[2023-08-30 04:40:45,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11195
[2023-08-30 04:40:45,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11195
[2023-08-30 04:40:45,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:40:45,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:40:45,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9720 (1.8354)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7469 (8.6303)  time: 0.7539 (0.4947 -- 2.9098)  data: 0.2306 (0.0002 -- 2.3668)  max mem: 16413
Epoch: [69] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9720 (1.8555)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7469 (8.6303)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6294 (0.6294)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3843 (2.3843 -- 2.3843)  data: 2.1788 (2.1788 -- 2.1788)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6718 (0.9283)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4200 (0.1927 -- 2.3843)  data: 0.2065 (0.0004 -- 2.1788)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6718 (0.8744)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2177 (0.1705 -- 0.3187)  data: 0.0108 (0.0001 -- 0.1185)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7992 (0.9104)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (96.2656)  time: 0.2028 (0.1337 -- 0.3187)  data: 0.0102 (0.0001 -- 0.1185)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 75.104 Acc@5 96.473 loss 0.855
Accuracy of the network on the 482 val images: 75.10%
[2023-08-30 04:40:56,102] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:40:56,104] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:40:56,104] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:40:56,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:40:57,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:40:57,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.10%
Epoch: [70]  [  0/160]  eta: 0:23:44  lr: 0.000019  min_lr: 0.000000  loss: 1.6640 (1.6640)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9727 (11.9727)  time: 8.9010 (8.9010 -- 8.9010)  data: 6.8158 (6.8158 -- 6.8158)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:53  lr: 0.000019  min_lr: 0.000000  loss: 1.8387 (1.7125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2877 (8.1834)  time: 0.8545 (0.5245 -- 4.8010)  data: 0.2577 (0.0005 -- 4.2903)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:02:09  lr: 0.000019  min_lr: 0.000000  loss: 1.9014 (1.7932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6997 (8.0318)  time: 0.9063 (0.5244 -- 4.4419)  data: 0.0795 (0.0002 -- 1.5365)  max mem: 16413
[2023-08-30 04:41:54,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11257
[2023-08-30 04:41:54,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11257
[2023-08-30 04:41:54,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:41:54,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:41:54,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [70]  [ 60/160]  eta: 0:01:35  lr: 0.000019  min_lr: 0.000000  loss: 2.0836 (1.8620)  loss_scale: 16384.0000 (15846.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2713 (8.1908)  time: 0.7128 (0.5157 -- 2.3195)  data: 0.0016 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000000  loss: 1.8368 (1.8621)  loss_scale: 8192.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9365 (8.2780)  time: 1.0241 (0.5208 -- 3.5348)  data: 0.0225 (0.0003 -- 0.3091)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.8323 (1.8561)  loss_scale: 8192.0000 (12815.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0856 (8.3073)  time: 0.8068 (0.5219 -- 3.2264)  data: 0.0011 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7259 (1.8475)  loss_scale: 8192.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9405 (8.4363)  time: 0.7681 (0.5292 -- 1.9374)  data: 0.0142 (0.0004 -- 0.2487)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 2.0907 (1.8494)  loss_scale: 8192.0000 (11503.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9534 (8.5012)  time: 0.8154 (0.5275 -- 2.8529)  data: 0.1071 (0.0004 -- 1.2106)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.7293 (1.8442)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2713 (8.4759)  time: 0.7145 (0.4975 -- 3.4842)  data: 0.1820 (0.0002 -- 2.9579)  max mem: 16413
Epoch: [70] Total time: 0:02:20 (0.8777 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.7293 (1.8124)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2713 (8.4759)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5973 (0.5973)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4174 (2.4174 -- 2.4174)  data: 2.1857 (2.1857 -- 2.1857)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6967 (0.9350)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4118 (0.1971 -- 2.4174)  data: 0.1996 (0.0002 -- 2.1857)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6967 (0.8682)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2131 (0.1694 -- 0.3884)  data: 0.0100 (0.0001 -- 0.1880)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7887 (0.9063)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (96.2656)  time: 0.1989 (0.1332 -- 0.3884)  data: 0.0099 (0.0001 -- 0.1880)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 75.311 Acc@5 95.851 loss 0.856
Accuracy of the network on the 482 val images: 75.31%
[2023-08-30 04:43:25,686] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:43:25,688] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:43:25,688] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:43:25,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:43:27,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:43:27,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.31%
Epoch: [71]  [  0/160]  eta: 0:21:18  lr: 0.000019  min_lr: 0.000000  loss: 1.9461 (1.9461)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3075 (8.3075)  time: 7.9921 (7.9921 -- 7.9921)  data: 5.1074 (5.1074 -- 5.1074)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:35  lr: 0.000019  min_lr: 0.000000  loss: 1.7819 (1.7825)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5696 (9.1949)  time: 0.7646 (0.5156 -- 2.9687)  data: 0.0518 (0.0008 -- 0.7490)  max mem: 16413
[2023-08-30 04:43:57,325] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:43:57,325] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:43:57,325] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:43:57,325] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [71]  [ 40/160]  eta: 0:02:07  lr: 0.000019  min_lr: 0.000000  loss: 1.7222 (1.7581)  loss_scale: 16384.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4387 (8.3939)  time: 1.0109 (0.5341 -- 4.1485)  data: 0.0781 (0.0005 -- 1.2645)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000019  min_lr: 0.000000  loss: 1.8506 (1.7868)  loss_scale: 16384.0000 (12892.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8982 (8.6442)  time: 0.9006 (0.5235 -- 4.2474)  data: 0.0021 (0.0001 -- 0.0092)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:18  lr: 0.000019  min_lr: 0.000000  loss: 1.4944 (1.7636)  loss_scale: 16384.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5064 (8.4922)  time: 0.9157 (0.5203 -- 4.1742)  data: 0.0014 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.9085 (1.7911)  loss_scale: 16384.0000 (14275.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4422 (8.6360)  time: 0.7067 (0.5306 -- 2.4452)  data: 0.0020 (0.0002 -- 0.0133)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.8832 (1.8004)  loss_scale: 16384.0000 (14623.7355)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2156 (8.8564)  time: 0.9228 (0.5411 -- 3.1496)  data: 0.0387 (0.0004 -- 0.4233)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8623 (1.8056)  loss_scale: 16384.0000 (14873.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0631 (8.6818)  time: 0.8718 (0.5210 -- 3.4664)  data: 0.1310 (0.0003 -- 1.3726)  max mem: 16413
[2023-08-30 04:45:47,515] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:45:47,515] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:45:47,515] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:45:47,515] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.6447 (1.7912)  loss_scale: 16384.0000 (15667.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2339 (8.6844)  time: 0.6807 (0.4966 -- 3.7826)  data: 0.0006 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.6447 (1.8094)  loss_scale: 16384.0000 (15667.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2339 (8.6844)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6025 (0.6025)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3668 (2.3668 -- 2.3668)  data: 2.1397 (2.1397 -- 2.1397)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7121 (0.9358)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4140 (0.2083 -- 2.3668)  data: 0.1958 (0.0006 -- 2.1397)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7121 (0.8611)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2237 (0.1708 -- 0.3968)  data: 0.0169 (0.0001 -- 0.1896)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7969 (0.9012)  acc1: 71.4286 (71.3693)  acc5: 100.0000 (96.2656)  time: 0.2072 (0.1366 -- 0.3968)  data: 0.0165 (0.0001 -- 0.1896)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 75.311 Acc@5 96.266 loss 0.851
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 75.31%
Epoch: [72]  [  0/160]  eta: 0:16:12  lr: 0.000019  min_lr: 0.000000  loss: 1.2558 (1.2558)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5105 (5.5105)  time: 6.0807 (6.0807 -- 6.0807)  data: 5.5145 (5.5145 -- 5.5145)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:43  lr: 0.000019  min_lr: 0.000000  loss: 1.8289 (1.8033)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9794 (8.3193)  time: 0.9252 (0.5263 -- 3.8054)  data: 0.3028 (0.0004 -- 3.2686)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:12  lr: 0.000019  min_lr: 0.000000  loss: 1.8846 (1.8150)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9155 (8.2107)  time: 1.0309 (0.5310 -- 4.4919)  data: 0.4845 (0.0003 -- 3.9602)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8301 (1.8104)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1890 (8.2723)  time: 0.6929 (0.5184 -- 3.2863)  data: 0.1432 (0.0004 -- 2.7642)  max mem: 16413
[2023-08-30 04:47:05,584] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11590
[2023-08-30 04:47:05,584] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11590
[2023-08-30 04:47:05,585] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:47:05,585] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:47:05,585] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000000  loss: 1.8444 (1.8227)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7012 (8.5568)  time: 0.9391 (0.5274 -- 3.6722)  data: 0.3921 (0.0005 -- 3.1685)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.8854 (1.8270)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6395 (8.3955)  time: 0.8062 (0.5220 -- 3.4550)  data: 0.2540 (0.0005 -- 2.9443)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.7931 (1.8225)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3133 (8.4480)  time: 0.9031 (0.5192 -- 3.5958)  data: 0.3475 (0.0002 -- 3.0803)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8071 (1.8294)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7341 (8.8265)  time: 0.8570 (0.5214 -- 4.0133)  data: 0.3083 (0.0003 -- 3.4906)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8265 (1.8405)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8978 (8.7631)  time: 0.7030 (0.4980 -- 3.8975)  data: 0.1902 (0.0001 -- 3.3906)  max mem: 16413
Epoch: [72] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8265 (1.8226)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8978 (8.7631)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6271 (0.6271)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3167 (2.3167 -- 2.3167)  data: 2.0677 (2.0677 -- 2.0677)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7386 (0.9171)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4160 (0.2065 -- 2.3167)  data: 0.1926 (0.0008 -- 2.0677)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7055 (0.8529)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.8254)  time: 0.2217 (0.1690 -- 0.3710)  data: 0.0124 (0.0001 -- 0.1918)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7796 (0.8926)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (96.2656)  time: 0.2055 (0.1325 -- 0.3710)  data: 0.0119 (0.0001 -- 0.1918)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 75.726 Acc@5 96.266 loss 0.839
Accuracy of the network on the 482 val images: 75.73%
[2023-08-30 04:48:28,496] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:48:28,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:48:28,498] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:48:28,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:48:30,019] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:48:30,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.73%
Epoch: [73]  [  0/160]  eta: 0:18:29  lr: 0.000018  min_lr: 0.000000  loss: 2.4233 (2.4233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9723 (6.9723)  time: 6.9320 (6.9320 -- 6.9320)  data: 6.4042 (6.4042 -- 6.4042)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:48  lr: 0.000018  min_lr: 0.000000  loss: 1.7639 (1.8457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4310 (8.7567)  time: 0.9193 (0.5154 -- 3.5166)  data: 0.3136 (0.0004 -- 2.6305)  max mem: 16413
[2023-08-30 04:49:11,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:49:11,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:49:11,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:49:11,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [ 40/160]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000000  loss: 1.7156 (1.7931)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7210 (8.4160)  time: 0.8143 (0.5221 -- 3.2106)  data: 0.2686 (0.0003 -- 2.6855)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:40  lr: 0.000018  min_lr: 0.000000  loss: 1.9773 (1.8289)  loss_scale: 32768.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9893 (8.5047)  time: 0.9878 (0.5163 -- 3.8376)  data: 0.4362 (0.0004 -- 3.2726)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.7563 (1.8181)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8152 (8.3088)  time: 0.8296 (0.5078 -- 3.8281)  data: 0.2867 (0.0003 -- 3.3047)  max mem: 16413
[2023-08-30 04:50:01,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11776
[2023-08-30 04:50:01,014] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:50:01,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11776
[2023-08-30 04:50:01,015] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:50:01,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.7099 (1.8111)  loss_scale: 32768.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6042 (8.3494)  time: 0.8132 (0.5274 -- 2.8299)  data: 0.2604 (0.0004 -- 2.2928)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7641 (1.7910)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9803 (8.6579)  time: 0.8477 (0.5206 -- 3.6315)  data: 0.2984 (0.0005 -- 3.0988)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9184 (1.7991)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0467 (8.7814)  time: 0.8995 (0.5323 -- 3.6369)  data: 0.3461 (0.0005 -- 3.1101)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8442 (1.8095)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0487 (8.7633)  time: 0.6954 (0.5000 -- 2.3228)  data: 0.1312 (0.0002 -- 1.7769)  max mem: 16413
Epoch: [73] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8442 (1.8120)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0487 (8.7633)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5817 (0.5817)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4078 (2.4078 -- 2.4078)  data: 2.1871 (2.1871 -- 2.1871)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6971 (0.9174)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (96.9697)  time: 0.4267 (0.1968 -- 2.4078)  data: 0.2135 (0.0007 -- 2.1871)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6971 (0.8406)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (96.8254)  time: 0.2169 (0.1714 -- 0.3800)  data: 0.0119 (0.0001 -- 0.1483)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7682 (0.8820)  acc1: 77.7778 (70.9544)  acc5: 100.0000 (96.2656)  time: 0.2028 (0.1332 -- 0.3800)  data: 0.0114 (0.0001 -- 0.1483)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 75.726 Acc@5 96.473 loss 0.830
Accuracy of the network on the 482 val images: 75.73%
[2023-08-30 04:51:00,376] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:51:00,377] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:51:00,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:51:00,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:51:01,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:51:01,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.73%
Epoch: [74]  [  0/160]  eta: 0:19:24  lr: 0.000018  min_lr: 0.000000  loss: 1.2106 (1.2106)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1145 (8.1145)  time: 7.2757 (7.2757 -- 7.2757)  data: 6.7573 (6.7573 -- 6.7573)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:32  lr: 0.000018  min_lr: 0.000000  loss: 1.7369 (1.7698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8176 (7.6496)  time: 0.7779 (0.5264 -- 2.2748)  data: 0.1201 (0.0004 -- 1.6146)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:01:59  lr: 0.000018  min_lr: 0.000000  loss: 1.8150 (1.8001)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4622 (8.0139)  time: 0.9001 (0.5204 -- 3.0106)  data: 0.3078 (0.0003 -- 2.4194)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7845 (1.7953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5002 (8.2631)  time: 0.9041 (0.5234 -- 2.4509)  data: 0.2803 (0.0003 -- 1.9226)  max mem: 16413
[2023-08-30 04:52:04,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:52:04,679] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 04:52:04,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:52:04,679] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [74]  [ 80/160]  eta: 0:01:14  lr: 0.000018  min_lr: 0.000000  loss: 1.8345 (1.8189)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8427 (8.0809)  time: 0.8267 (0.5158 -- 1.9734)  data: 0.1305 (0.0001 -- 1.4542)  max mem: 16413
[2023-08-30 04:52:31,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11936
[2023-08-30 04:52:31,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11936
[2023-08-30 04:52:31,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:52:31,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 04:52:31,487] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [100/160]  eta: 0:00:54  lr: 0.000018  min_lr: 0.000000  loss: 1.5872 (1.7778)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0468 (8.0635)  time: 0.8187 (0.5224 -- 2.4716)  data: 0.0543 (0.0002 -- 0.6641)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.8422 (1.7813)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2368 (8.2181)  time: 1.0066 (0.5331 -- 3.3565)  data: 0.0013 (0.0005 -- 0.0025)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.0119 (1.7976)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7148 (8.3402)  time: 0.8728 (0.5239 -- 3.5545)  data: 0.1582 (0.0006 -- 3.0459)  max mem: 16413
[2023-08-30 04:53:24,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=64, lr=[4.278778361092828e-07, 4.278778361092828e-07, 5.705037814790438e-07, 5.705037814790438e-07, 7.606717086387249e-07, 7.606717086387249e-07, 1.0142289448516333e-06, 1.0142289448516333e-06, 1.3523052598021778e-06, 1.3523052598021778e-06, 1.8030736797362369e-06, 1.8030736797362369e-06, 2.4040982396483157e-06, 2.4040982396483157e-06, 3.205464319531088e-06, 3.205464319531088e-06, 4.2739524260414505e-06, 4.2739524260414505e-06, 5.698603234721934e-06, 5.698603234721934e-06, 7.5981376462959115e-06, 7.5981376462959115e-06, 1.0130850195061216e-05, 1.0130850195061216e-05, 1.3507800260081622e-05, 1.3507800260081622e-05, 1.8010400346775495e-05, 1.8010400346775495e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 04:53:24,348] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=18.235564645934744, CurrSamplesPerSec=24.64012689274332, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.6805 (1.7938)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5814 (8.3918)  time: 0.6815 (0.4990 -- 2.7677)  data: 0.1580 (0.0002 -- 2.2241)  max mem: 16413
Epoch: [74] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.6805 (1.8019)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5814 (8.3918)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.5924 (0.5924)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2184 (2.2184 -- 2.2184)  data: 2.0143 (2.0143 -- 2.0143)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6768 (0.9280)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (97.9798)  time: 0.4093 (0.2024 -- 2.2184)  data: 0.1952 (0.0005 -- 2.0143)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6768 (0.8591)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (97.3545)  time: 0.2305 (0.1703 -- 0.6352)  data: 0.0291 (0.0001 -- 0.4467)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7662 (0.8910)  acc1: 77.7778 (72.1992)  acc5: 100.0000 (96.6805)  time: 0.2156 (0.1365 -- 0.6352)  data: 0.0287 (0.0001 -- 0.4467)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 76.556 Acc@5 96.680 loss 0.838
Accuracy of the network on the 482 val images: 76.56%
[2023-08-30 04:53:32,183] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:53:32,185] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:53:32,185] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:53:32,185] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:53:33,604] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:53:33,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.56%
Epoch: [75]  [  0/160]  eta: 0:19:02  lr: 0.000018  min_lr: 0.000000  loss: 2.0254 (2.0254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3467 (7.3467)  time: 7.1387 (7.1387 -- 7.1387)  data: 6.5970 (6.5970 -- 6.5970)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:38  lr: 0.000018  min_lr: 0.000000  loss: 1.5766 (1.7468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4822 (7.9195)  time: 0.8337 (0.5311 -- 3.5319)  data: 0.1544 (0.0005 -- 1.9845)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000000  loss: 1.9260 (1.8272)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0836 (8.3983)  time: 0.8875 (0.5242 -- 3.1226)  data: 0.0714 (0.0001 -- 0.6407)  max mem: 16413
[2023-08-30 04:54:31,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12058
[2023-08-30 04:54:31,075] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:54:31,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12058
[2023-08-30 04:54:31,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:54:31,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [75]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9636 (1.8549)  loss_scale: 16384.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0814 (8.4065)  time: 0.8505 (0.5186 -- 2.4782)  data: 0.0543 (0.0002 -- 0.7547)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.6766 (1.8191)  loss_scale: 8192.0000 (14057.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5972 (8.4271)  time: 0.9280 (0.5291 -- 3.2122)  data: 0.2519 (0.0005 -- 2.6777)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.7806 (1.8102)  loss_scale: 8192.0000 (12896.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2583 (8.3709)  time: 0.8132 (0.5320 -- 3.5531)  data: 0.0209 (0.0003 -- 0.3899)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.9705 (1.8258)  loss_scale: 8192.0000 (12118.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3045 (8.3013)  time: 0.9307 (0.5190 -- 3.1541)  data: 0.0018 (0.0004 -- 0.0132)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8524 (1.8250)  loss_scale: 8192.0000 (11561.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9218 (8.3303)  time: 0.8317 (0.5240 -- 3.0068)  data: 0.0014 (0.0002 -- 0.0089)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.6787 (1.8110)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0117 (8.3411)  time: 0.7194 (0.4980 -- 2.6607)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [75] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.6787 (1.8024)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0117 (8.3411)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5702 (0.5702)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3876 (2.3876 -- 2.3876)  data: 2.1811 (2.1811 -- 2.1811)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6921 (0.9112)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4160 (0.2041 -- 2.3876)  data: 0.1998 (0.0009 -- 2.1811)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6921 (0.8407)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.2963)  time: 0.2192 (0.1708 -- 0.3512)  data: 0.0100 (0.0001 -- 0.1035)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7622 (0.8759)  acc1: 71.4286 (71.7842)  acc5: 100.0000 (95.8506)  time: 0.2046 (0.1335 -- 0.3512)  data: 0.0097 (0.0001 -- 0.1035)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 76.349 Acc@5 96.266 loss 0.822
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 76.56%
Epoch: [76]  [  0/160]  eta: 0:20:58  lr: 0.000018  min_lr: 0.000000  loss: 1.6754 (1.6754)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5953 (6.5953)  time: 7.8686 (7.8686 -- 7.8686)  data: 6.0978 (6.0978 -- 6.0978)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:41  lr: 0.000018  min_lr: 0.000000  loss: 1.7027 (1.7088)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0517 (7.9869)  time: 0.8195 (0.5250 -- 2.9826)  data: 0.2192 (0.0002 -- 2.4524)  max mem: 16413
[2023-08-30 04:56:35,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:56:35,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 04:56:35,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 04:56:35,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [76]  [ 40/160]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000000  loss: 1.5504 (1.6628)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3157 (8.1045)  time: 0.8839 (0.5323 -- 3.1696)  data: 0.0520 (0.0003 -- 1.0171)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:40  lr: 0.000018  min_lr: 0.000000  loss: 1.8870 (1.7296)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9004 (8.6067)  time: 0.9615 (0.5260 -- 3.7255)  data: 0.0018 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [76]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.9008 (1.7581)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2410 (8.8294)  time: 0.8144 (0.5264 -- 2.8388)  data: 0.0016 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.6454 (1.7440)  loss_scale: 16384.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2788 (8.7542)  time: 0.7718 (0.5263 -- 4.3594)  data: 0.0019 (0.0004 -- 0.0110)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.8939 (1.7690)  loss_scale: 16384.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5700 (8.7611)  time: 0.9593 (0.5234 -- 4.1287)  data: 0.0014 (0.0002 -- 0.0048)  max mem: 16413
[2023-08-30 04:58:12,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12300
[2023-08-30 04:58:12,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12300
[2023-08-30 04:58:12,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:58:12,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 04:58:12,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7045 (1.7628)  loss_scale: 16384.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5054 (8.7040)  time: 0.8033 (0.5350 -- 3.5121)  data: 0.0015 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7162 (1.7623)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3021 (8.6292)  time: 0.7112 (0.4980 -- 2.3935)  data: 0.0010 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [76] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7162 (1.7537)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3021 (8.6292)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5592 (0.5592)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4270 (2.4270 -- 2.4270)  data: 2.2034 (2.2034 -- 2.2034)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6235 (0.9025)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (97.9798)  time: 0.4330 (0.2030 -- 2.4270)  data: 0.2141 (0.0008 -- 2.2034)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6235 (0.8233)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (97.3545)  time: 0.2262 (0.1686 -- 0.4636)  data: 0.0210 (0.0001 -- 0.2656)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7646 (0.8607)  acc1: 71.4286 (73.4440)  acc5: 100.0000 (96.6805)  time: 0.2089 (0.1326 -- 0.4636)  data: 0.0207 (0.0001 -- 0.2656)  max mem: 16413
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 78.008 Acc@5 96.888 loss 0.808
Accuracy of the network on the 482 val images: 78.01%
[2023-08-30 04:58:33,714] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 04:58:33,716] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 04:58:33,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 04:58:33,716] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 04:58:35,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 04:58:35,108] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.01%
Epoch: [77]  [  0/160]  eta: 0:19:43  lr: 0.000018  min_lr: 0.000000  loss: 2.4146 (2.4146)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3115 (6.3115)  time: 7.3946 (7.3946 -- 7.3946)  data: 6.8507 (6.8507 -- 6.8507)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000000  loss: 1.7638 (1.8266)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8599 (8.9288)  time: 0.8648 (0.5264 -- 3.9889)  data: 0.3160 (0.0002 -- 3.4678)  max mem: 16413
Epoch: [77]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 1.7816 (1.7935)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8536 (9.0837)  time: 0.8566 (0.5453 -- 3.4793)  data: 0.2831 (0.0009 -- 2.9586)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:37  lr: 0.000017  min_lr: 0.000000  loss: 1.8623 (1.7962)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4512 (8.9286)  time: 0.8961 (0.5342 -- 3.8460)  data: 0.2521 (0.0003 -- 1.9639)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000017  min_lr: 0.000000  loss: 1.9419 (1.8176)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6410 (8.8579)  time: 0.8841 (0.5189 -- 3.2324)  data: 0.3423 (0.0003 -- 2.6997)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 1.7096 (1.8106)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9973 (8.8670)  time: 0.8477 (0.5239 -- 3.5966)  data: 0.3056 (0.0004 -- 3.0640)  max mem: 16413
[2023-08-30 05:00:16,711] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:00:16,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:00:16,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:00:16,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.7428 (1.8007)  loss_scale: 16384.0000 (9004.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3188 (9.0070)  time: 0.7993 (0.5266 -- 3.1847)  data: 0.2487 (0.0003 -- 2.6739)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.6462 (1.7849)  loss_scale: 16384.0000 (10051.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7918 (8.9793)  time: 0.9412 (0.5256 -- 4.0356)  data: 0.3881 (0.0006 -- 3.4960)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8313 (1.7878)  loss_scale: 16384.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6595 (8.7921)  time: 0.7042 (0.4967 -- 4.3755)  data: 0.1933 (0.0002 -- 3.8594)  max mem: 16413
Epoch: [77] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8313 (1.8025)  loss_scale: 16384.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6595 (8.7921)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5823 (0.5823)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2850 (2.2850 -- 2.2850)  data: 2.0419 (2.0419 -- 2.0419)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7273 (0.8992)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4153 (0.2018 -- 2.2850)  data: 0.1883 (0.0007 -- 2.0419)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7062 (0.8306)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2242 (0.1690 -- 0.4440)  data: 0.0144 (0.0001 -- 0.2546)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7373 (0.8635)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (96.2656)  time: 0.2040 (0.1335 -- 0.4440)  data: 0.0141 (0.0001 -- 0.2546)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 77.593 Acc@5 96.680 loss 0.811
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.01%
Epoch: [78]  [  0/160]  eta: 0:20:43  lr: 0.000017  min_lr: 0.000000  loss: 2.1574 (2.1574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6759 (6.6759)  time: 7.7730 (7.7730 -- 7.7730)  data: 7.2394 (7.2394 -- 7.2394)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:43  lr: 0.000017  min_lr: 0.000000  loss: 1.6308 (1.7735)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4518 (8.2731)  time: 0.8383 (0.5302 -- 2.3525)  data: 0.1652 (0.0006 -- 1.8255)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:03  lr: 0.000017  min_lr: 0.000000  loss: 1.7487 (1.7796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7988 (8.8676)  time: 0.8748 (0.5263 -- 3.0484)  data: 0.0174 (0.0007 -- 0.3050)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 1.8776 (1.8189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9675 (8.9490)  time: 0.9573 (0.5242 -- 4.8998)  data: 0.0149 (0.0004 -- 0.2794)  max mem: 16413
[2023-08-30 05:02:20,936] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:02:20,936] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:02:20,937] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:02:20,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:02:22,041] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12559
[2023-08-30 05:02:22,041] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:02:22,041] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12559
[2023-08-30 05:02:22,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:02:22,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.6942 (1.8022)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2804 (8.8482)  time: 0.7874 (0.5140 -- 3.9006)  data: 0.0018 (0.0002 -- 0.0086)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 1.8701 (1.8139)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0666 (8.7699)  time: 0.9480 (0.5309 -- 3.7104)  data: 0.0018 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.7848 (1.8147)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4316 (8.7686)  time: 0.8366 (0.5192 -- 4.1717)  data: 0.0019 (0.0002 -- 0.0156)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9267 (1.8200)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9533 (8.6581)  time: 0.8892 (0.5206 -- 3.4948)  data: 0.0013 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9024 (1.8213)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8310 (8.6850)  time: 0.6823 (0.4970 -- 2.5134)  data: 0.0008 (0.0001 -- 0.0048)  max mem: 16413
Epoch: [78] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9024 (1.7972)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8310 (8.6850)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5417 (0.5417)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2557 (2.2557 -- 2.2557)  data: 2.0446 (2.0446 -- 2.0446)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7037 (0.8985)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4198 (0.1989 -- 2.2557)  data: 0.2055 (0.0005 -- 2.0446)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6878 (0.8246)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2246 (0.1701 -- 0.4212)  data: 0.0154 (0.0001 -- 0.2065)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7390 (0.8596)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2096 (0.1332 -- 0.4212)  data: 0.0151 (0.0001 -- 0.2065)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 78.423 Acc@5 96.473 loss 0.802
Accuracy of the network on the 482 val images: 78.42%
[2023-08-30 05:03:34,949] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:03:34,951] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:03:34,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:03:34,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:03:36,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:03:36,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.42%
Epoch: [79]  [  0/160]  eta: 0:18:26  lr: 0.000017  min_lr: 0.000000  loss: 1.6224 (1.6224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7820 (5.7820)  time: 6.9134 (6.9134 -- 6.9134)  data: 5.9128 (5.9128 -- 5.9128)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:43  lr: 0.000017  min_lr: 0.000000  loss: 1.8041 (1.8207)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0433 (8.8567)  time: 0.8822 (0.5309 -- 3.4864)  data: 0.3341 (0.0003 -- 2.9525)  max mem: 16413
[2023-08-30 05:04:01,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12661
[2023-08-30 05:04:01,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12661
[2023-08-30 05:04:01,388] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:04:01,388] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:04:01,388] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [79]  [ 40/160]  eta: 0:02:07  lr: 0.000017  min_lr: 0.000000  loss: 1.5947 (1.7122)  loss_scale: 8192.0000 (12387.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2367 (8.9494)  time: 0.9535 (0.5061 -- 4.7356)  data: 0.4129 (0.0003 -- 4.2289)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:39  lr: 0.000017  min_lr: 0.000000  loss: 1.7992 (1.7290)  loss_scale: 8192.0000 (11012.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8358 (8.8175)  time: 0.8461 (0.5177 -- 4.6791)  data: 0.2964 (0.0003 -- 4.1252)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:19  lr: 0.000017  min_lr: 0.000000  loss: 1.8464 (1.7190)  loss_scale: 8192.0000 (10315.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7253 (8.9344)  time: 0.9882 (0.5299 -- 4.5328)  data: 0.4350 (0.0002 -- 4.0083)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.6035 (1.7154)  loss_scale: 8192.0000 (9895.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7507 (9.0402)  time: 0.6888 (0.5261 -- 2.3707)  data: 0.1340 (0.0003 -- 1.8233)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.6204 (1.7053)  loss_scale: 8192.0000 (9613.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9782 (8.9446)  time: 0.8972 (0.5084 -- 3.5152)  data: 0.3451 (0.0004 -- 2.9947)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7490 (1.7114)  loss_scale: 8192.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8399 (8.9038)  time: 0.8535 (0.5205 -- 3.7495)  data: 0.3077 (0.0005 -- 3.2120)  max mem: 16413
[2023-08-30 05:05:52,307] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:05:52,307] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:05:52,307] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:05:52,307] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8366 (1.7408)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0045 (8.7739)  time: 0.6490 (0.4955 -- 2.0746)  data: 0.1298 (0.0002 -- 1.5444)  max mem: 16413
Epoch: [79] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8366 (1.7860)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0045 (8.7739)
[2023-08-30 05:05:57,857] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-08-30 05:05:57,859] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-08-30 05:05:57,859] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-08-30 05:05:57,866] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-08-30 05:05:58,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-08-30 05:05:58,880] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4874 (0.4874)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4072 (2.4072 -- 2.4072)  data: 2.1070 (2.1070 -- 2.1070)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6356 (0.8898)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4290 (0.2002 -- 2.4072)  data: 0.2116 (0.0008 -- 2.1070)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6356 (0.8196)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.8254)  time: 0.2164 (0.1703 -- 0.4095)  data: 0.0142 (0.0001 -- 0.1967)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7658 (0.8527)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (96.2656)  time: 0.2036 (0.1333 -- 0.4095)  data: 0.0139 (0.0001 -- 0.1967)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 78.838 Acc@5 96.266 loss 0.793
Accuracy of the network on the 482 val images: 78.84%
[2023-08-30 05:06:06,723] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:06:06,725] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:06:06,725] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:06:06,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:06:08,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:06:08,198] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.84%
Epoch: [80]  [  0/160]  eta: 0:18:36  lr: 0.000017  min_lr: 0.000000  loss: 2.0105 (2.0105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5989 (6.5989)  time: 6.9779 (6.9779 -- 6.9779)  data: 6.4119 (6.4119 -- 6.4119)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:45  lr: 0.000017  min_lr: 0.000000  loss: 1.8002 (1.8263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5642 (8.4725)  time: 0.8960 (0.5305 -- 4.1007)  data: 0.0015 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:01  lr: 0.000017  min_lr: 0.000000  loss: 1.7908 (1.8262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9780 (9.0067)  time: 0.8374 (0.5349 -- 3.5951)  data: 0.0021 (0.0004 -- 0.0149)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:35  lr: 0.000017  min_lr: 0.000000  loss: 1.9477 (1.8564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3160 (9.2291)  time: 0.8448 (0.5179 -- 2.9323)  data: 0.0021 (0.0002 -- 0.0097)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 2.0149 (1.8733)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5987 (8.9589)  time: 0.8873 (0.5190 -- 4.5719)  data: 0.0017 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.7346 (1.8405)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3588 (9.0140)  time: 0.8171 (0.5300 -- 3.8139)  data: 0.0020 (0.0002 -- 0.0136)  max mem: 16413
[2023-08-30 05:07:56,200] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:07:56,200] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:07:56,201] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:07:56,202] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.8676 (1.8258)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2086 (8.9051)  time: 0.8738 (0.5403 -- 2.5444)  data: 0.1208 (0.0003 -- 1.0595)  max mem: 16413
[2023-08-30 05:08:05,499] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12928
[2023-08-30 05:08:05,499] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12928
[2023-08-30 05:08:05,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:08:05,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:08:05,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.5796 (1.7997)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8077 (8.8357)  time: 0.9342 (0.5256 -- 3.5572)  data: 0.1054 (0.0008 -- 1.5632)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9354 (1.8133)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3393 (8.8649)  time: 0.6675 (0.4966 -- 3.1830)  data: 0.0007 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [80] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9354 (1.7772)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3393 (8.8649)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5320 (0.5320)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3843 (2.3843 -- 2.3843)  data: 2.1648 (2.1648 -- 2.1648)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7061 (0.8836)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4228 (0.1939 -- 2.3843)  data: 0.2122 (0.0006 -- 2.1648)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6443 (0.8159)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2227 (0.1690 -- 0.4124)  data: 0.0176 (0.0001 -- 0.1805)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7323 (0.8463)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (96.2656)  time: 0.2097 (0.1331 -- 0.4124)  data: 0.0173 (0.0001 -- 0.1805)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 78.631 Acc@5 96.473 loss 0.794
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 78.84%
Epoch: [81]  [  0/160]  eta: 0:19:49  lr: 0.000016  min_lr: 0.000000  loss: 1.8683 (1.8683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1652 (9.1652)  time: 7.4315 (7.4315 -- 7.4315)  data: 5.6157 (5.6157 -- 5.6157)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:48  lr: 0.000016  min_lr: 0.000000  loss: 1.9191 (1.8520)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4819 (8.8140)  time: 0.8889 (0.5326 -- 2.3075)  data: 0.2577 (0.0005 -- 1.5117)  max mem: 16413
[2023-08-30 05:09:18,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=69, lr=[3.893492981028005e-07, 3.893492981028005e-07, 5.191323974704007e-07, 5.191323974704007e-07, 6.921765299605342e-07, 6.921765299605342e-07, 9.22902039947379e-07, 9.22902039947379e-07, 1.230536053263172e-06, 1.230536053263172e-06, 1.6407147376842294e-06, 1.6407147376842294e-06, 2.187619650245639e-06, 2.187619650245639e-06, 2.9168262003275188e-06, 2.9168262003275188e-06, 3.889101600436692e-06, 3.889101600436692e-06, 5.185468800582256e-06, 5.185468800582256e-06, 6.9139584007763405e-06, 6.9139584007763405e-06, 9.218611201035122e-06, 9.218611201035122e-06, 1.2291481601380162e-05, 1.2291481601380162e-05, 1.638864213517355e-05, 1.638864213517355e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 05:09:18,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=18.13637300270289, CurrSamplesPerSec=20.700081555568627, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:01  lr: 0.000016  min_lr: 0.000000  loss: 1.8284 (1.7994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3664 (8.8317)  time: 0.8152 (0.5191 -- 3.3503)  data: 0.2661 (0.0006 -- 2.8434)  max mem: 16413
[2023-08-30 05:09:25,437] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13006
[2023-08-30 05:09:25,437] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13006
[2023-08-30 05:09:25,437] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:09:25,438] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 05:09:25,437] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [81]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 1.7411 (1.8163)  loss_scale: 8192.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6960 (9.0164)  time: 0.8945 (0.5277 -- 2.9788)  data: 0.3194 (0.0004 -- 2.4477)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.7049 (1.7571)  loss_scale: 8192.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1076 (9.1091)  time: 0.8124 (0.5245 -- 2.0515)  data: 0.2352 (0.0007 -- 1.5138)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7540 (1.7443)  loss_scale: 8192.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1226 (9.0582)  time: 0.9451 (0.5257 -- 2.2339)  data: 0.3976 (0.0003 -- 1.6905)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.6489 (1.7378)  loss_scale: 8192.0000 (11306.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7934 (9.1239)  time: 0.7870 (0.5325 -- 2.4741)  data: 0.2318 (0.0007 -- 1.9265)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.4756 (1.7254)  loss_scale: 8192.0000 (10864.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9494 (9.1167)  time: 0.8656 (0.5294 -- 2.6362)  data: 0.1875 (0.0004 -- 1.6744)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7939 (1.7439)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9653 (9.0476)  time: 0.6887 (0.4993 -- 2.9956)  data: 0.1257 (0.0002 -- 2.4868)  max mem: 16413
Epoch: [81] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7939 (1.7814)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9653 (9.0476)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5275 (0.5275)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2805 (2.2805 -- 2.2805)  data: 2.0464 (2.0464 -- 2.0464)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6329 (0.8916)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4093 (0.2046 -- 2.2805)  data: 0.1871 (0.0009 -- 2.0464)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6166 (0.8059)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2147 (0.1722 -- 0.2869)  data: 0.0048 (0.0001 -- 0.0812)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7219 (0.8382)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.4357)  time: 0.1958 (0.1330 -- 0.2869)  data: 0.0045 (0.0001 -- 0.0812)  max mem: 16413
Val: Total time: 0:00:07 (0.2804 s / it)
* Acc@1 78.631 Acc@5 96.266 loss 0.784
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 78.84%
Epoch: [82]  [  0/160]  eta: 0:19:07  lr: 0.000016  min_lr: 0.000000  loss: 1.2944 (1.2944)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6793 (8.6793)  time: 7.1705 (7.1705 -- 7.1705)  data: 6.1424 (6.1424 -- 6.1424)  max mem: 16413
[2023-08-30 05:11:25,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:11:25,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:11:25,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:11:25,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [82]  [ 20/160]  eta: 0:02:43  lr: 0.000016  min_lr: 0.000000  loss: 1.7967 (1.7761)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4429 (9.2611)  time: 0.8669 (0.5241 -- 3.9168)  data: 0.1653 (0.0006 -- 2.8683)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 1.6622 (1.7037)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7868 (9.0939)  time: 0.8747 (0.5230 -- 3.3249)  data: 0.0020 (0.0003 -- 0.0109)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:34  lr: 0.000016  min_lr: 0.000000  loss: 1.6132 (1.6865)  loss_scale: 16384.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4285 (9.1305)  time: 0.7853 (0.5290 -- 2.9070)  data: 0.0325 (0.0005 -- 0.3979)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:13  lr: 0.000016  min_lr: 0.000000  loss: 1.8448 (1.7171)  loss_scale: 16384.0000 (14866.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0633 (9.1158)  time: 0.8460 (0.5201 -- 2.3305)  data: 0.0257 (0.0002 -- 0.4900)  max mem: 16413
[2023-08-30 05:12:38,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13219
[2023-08-30 05:12:38,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13219
[2023-08-30 05:12:38,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:12:38,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:12:38,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [82]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7915 (1.7344)  loss_scale: 16384.0000 (15005.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8923 (9.1074)  time: 0.9906 (0.5095 -- 3.5903)  data: 0.0644 (0.0003 -- 1.2646)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.9239 (1.7484)  loss_scale: 8192.0000 (13879.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1388 (8.9379)  time: 0.7637 (0.5233 -- 3.1782)  data: 0.0430 (0.0004 -- 0.4464)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.6450 (1.7464)  loss_scale: 8192.0000 (13072.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9115 (8.9046)  time: 1.0310 (0.5208 -- 4.2110)  data: 0.4827 (0.0005 -- 3.6581)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7387 (1.7517)  loss_scale: 8192.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0363 (8.9404)  time: 0.6141 (0.4953 -- 1.6838)  data: 0.0920 (0.0002 -- 1.1657)  max mem: 16413
Epoch: [82] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7387 (1.7715)  loss_scale: 8192.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0363 (8.9404)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5117 (0.5117)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2601 (2.2601 -- 2.2601)  data: 2.0067 (2.0067 -- 2.0067)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6955 (0.8711)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4171 (0.2007 -- 2.2601)  data: 0.1978 (0.0008 -- 2.0067)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6261 (0.7991)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2291 (0.1685 -- 0.5268)  data: 0.0254 (0.0001 -- 0.3373)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7259 (0.8328)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.8506)  time: 0.2111 (0.1328 -- 0.5268)  data: 0.0250 (0.0001 -- 0.3373)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 78.216 Acc@5 96.266 loss 0.781
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 78.84%
Epoch: [83]  [  0/160]  eta: 0:17:46  lr: 0.000016  min_lr: 0.000000  loss: 1.8244 (1.8244)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8972 (7.8972)  time: 6.6643 (6.6643 -- 6.6643)  data: 6.1364 (6.1364 -- 6.1364)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:41  lr: 0.000016  min_lr: 0.000000  loss: 1.9065 (1.7963)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7969 (9.4062)  time: 0.8788 (0.5282 -- 2.4304)  data: 0.1036 (0.0002 -- 1.1402)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:57  lr: 0.000016  min_lr: 0.000000  loss: 1.8590 (1.7941)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6351 (9.4834)  time: 0.7996 (0.5122 -- 2.2615)  data: 0.0040 (0.0002 -- 0.0446)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:41  lr: 0.000016  min_lr: 0.000000  loss: 1.7853 (1.7920)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1030 (9.2152)  time: 1.0849 (0.5278 -- 5.4474)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16413
[2023-08-30 05:14:44,113] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:14:44,113] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:14:44,113] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:14:44,113] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [83]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.6051 (1.7605)  loss_scale: 16384.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8458 (8.8116)  time: 0.7782 (0.5100 -- 3.5592)  data: 0.0020 (0.0001 -- 0.0161)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 2.0016 (1.7927)  loss_scale: 16384.0000 (10868.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0768 (8.7762)  time: 0.8156 (0.5314 -- 2.5796)  data: 0.0020 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.9082 (1.8068)  loss_scale: 16384.0000 (11780.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2012 (8.7816)  time: 0.8242 (0.5304 -- 2.5954)  data: 0.0021 (0.0003 -- 0.0054)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.7957 (1.8090)  loss_scale: 16384.0000 (12433.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9774 (8.8336)  time: 0.8821 (0.5356 -- 2.9882)  data: 0.0295 (0.0006 -- 0.5535)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8028 (1.8031)  loss_scale: 16384.0000 (12902.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8435 (8.8297)  time: 0.7076 (0.4976 -- 2.6662)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [83] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8028 (1.8148)  loss_scale: 16384.0000 (12902.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8435 (8.8297)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4948 (0.4948)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4496 (2.4496 -- 2.4496)  data: 2.2115 (2.2115 -- 2.2115)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7377 (0.8820)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4221 (0.1846 -- 2.4496)  data: 0.2020 (0.0007 -- 2.2115)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6039 (0.8015)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2124 (0.1704 -- 0.2781)  data: 0.0040 (0.0001 -- 0.0362)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7311 (0.8348)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.1980 (0.1327 -- 0.2781)  data: 0.0037 (0.0001 -- 0.0362)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 78.838 Acc@5 96.266 loss 0.776
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 78.84%
Epoch: [84]  [  0/160]  eta: 0:19:48  lr: 0.000016  min_lr: 0.000000  loss: 2.5360 (2.5360)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0094 (7.0094)  time: 7.4259 (7.4259 -- 7.4259)  data: 4.9385 (4.9385 -- 4.9385)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:46  lr: 0.000016  min_lr: 0.000000  loss: 1.8601 (1.8779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2314 (8.1746)  time: 0.8806 (0.5288 -- 3.6649)  data: 0.3017 (0.0006 -- 3.1165)  max mem: 16413
[2023-08-30 05:16:44,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:16:44,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:16:44,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:16:44,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:16:44,627] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13477
[2023-08-30 05:16:44,627] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:16:44,627] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13477
[2023-08-30 05:16:44,627] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:16:44,628] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [84]  [ 40/160]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 1.9035 (1.8480)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0516 (8.4114)  time: 0.8447 (0.5353 -- 3.0134)  data: 0.0836 (0.0006 -- 1.4224)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7454 (1.8589)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8146 (8.6393)  time: 0.8447 (0.5210 -- 2.8701)  data: 0.0520 (0.0004 -- 1.0127)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000000  loss: 1.7140 (1.8377)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5675 (8.5663)  time: 0.9625 (0.5300 -- 3.9355)  data: 0.0804 (0.0003 -- 1.4983)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 1.9615 (1.8383)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2552 (8.6467)  time: 0.9169 (0.5295 -- 4.7809)  data: 0.0017 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.6576 (1.8125)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1945 (8.5902)  time: 0.8416 (0.5217 -- 4.0763)  data: 0.0012 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.5739 (1.7822)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7232 (8.5104)  time: 0.8770 (0.5346 -- 3.3996)  data: 0.0015 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9132 (1.7866)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7291 (8.5967)  time: 0.7583 (0.4971 -- 2.8195)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [84] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9132 (1.7813)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7291 (8.5967)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4732 (0.4732)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3064 (2.3064 -- 2.3064)  data: 2.0864 (2.0864 -- 2.0864)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5681 (0.8510)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4039 (0.1966 -- 2.3064)  data: 0.1923 (0.0007 -- 2.0864)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5934 (0.7808)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2209 (0.1691 -- 0.4423)  data: 0.0136 (0.0001 -- 0.2064)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7186 (0.8143)  acc1: 85.7143 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.2059 (0.1331 -- 0.4423)  data: 0.0132 (0.0001 -- 0.2064)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.764
Accuracy of the network on the 482 val images: 80.29%
[2023-08-30 05:18:35,866] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:18:35,868] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:18:35,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:18:35,868] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:18:37,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:18:37,289] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.29%
Epoch: [85]  [  0/160]  eta: 0:18:12  lr: 0.000015  min_lr: 0.000000  loss: 1.9194 (1.9194)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1670 (7.1670)  time: 6.8254 (6.8254 -- 6.8254)  data: 6.2636 (6.2636 -- 6.2636)  max mem: 16413
[2023-08-30 05:18:47,465] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:18:47,465] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:18:47,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:18:47,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [ 20/160]  eta: 0:02:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8426 (1.7222)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5355 (7.9437)  time: 0.8408 (0.5225 -- 3.5519)  data: 0.1832 (0.0002 -- 2.2152)  max mem: 16413
[2023-08-30 05:19:13,538] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13634
[2023-08-30 05:19:13,538] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:19:13,538] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13634
[2023-08-30 05:19:13,538] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:19:13,538] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [ 40/160]  eta: 0:02:03  lr: 0.000015  min_lr: 0.000000  loss: 1.7224 (1.6908)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4163 (7.9423)  time: 0.9355 (0.5412 -- 3.3320)  data: 0.2873 (0.0004 -- 2.7905)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000000  loss: 1.8175 (1.7257)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4201 (8.2128)  time: 0.8745 (0.5110 -- 4.3479)  data: 0.3209 (0.0003 -- 3.7749)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.7131 (1.7289)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7390 (8.4701)  time: 0.8723 (0.5390 -- 2.8909)  data: 0.3129 (0.0007 -- 2.3370)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.8907 (1.7349)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4156 (8.6704)  time: 0.8572 (0.5242 -- 2.1535)  data: 0.3140 (0.0005 -- 1.6281)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.7956 (1.7381)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8636 (8.8173)  time: 0.8385 (0.5212 -- 3.3301)  data: 0.2859 (0.0003 -- 2.8086)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.6418 (1.7299)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2016 (8.8239)  time: 0.8805 (0.5254 -- 3.0872)  data: 0.3206 (0.0004 -- 2.5493)  max mem: 16413
[2023-08-30 05:20:55,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-08-30 05:20:55,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-08-30 05:20:55,472] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:20:55,472] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:20:55,472] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8297 (1.7433)  loss_scale: 16384.0000 (18892.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5129 (8.9745)  time: 0.6433 (0.4966 -- 2.8894)  data: 0.1230 (0.0002 -- 2.3666)  max mem: 16413
Epoch: [85] Total time: 0:02:21 (0.8824 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8297 (1.7705)  loss_scale: 16384.0000 (18892.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5129 (8.9745)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4830 (0.4830)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3853 (2.3853 -- 2.3853)  data: 2.1296 (2.1296 -- 2.1296)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6042 (0.8451)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4161 (0.1983 -- 2.3853)  data: 0.1949 (0.0005 -- 2.1296)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6041 (0.7783)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2202 (0.1706 -- 0.5009)  data: 0.0147 (0.0001 -- 0.2770)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7032 (0.8170)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.8506)  time: 0.2028 (0.1327 -- 0.5009)  data: 0.0142 (0.0001 -- 0.2770)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.761
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 80.29%
Epoch: [86]  [  0/160]  eta: 0:19:58  lr: 0.000015  min_lr: 0.000000  loss: 1.5633 (1.5633)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1359 (4.1359)  time: 7.4910 (7.4910 -- 7.4910)  data: 6.4058 (6.4058 -- 6.4058)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:41  lr: 0.000015  min_lr: 0.000000  loss: 1.7517 (1.7743)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0245 (8.3955)  time: 0.8331 (0.5277 -- 3.2477)  data: 0.0776 (0.0004 -- 1.5025)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:02  lr: 0.000015  min_lr: 0.000000  loss: 1.6726 (1.7160)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5037 (8.3058)  time: 0.8828 (0.5284 -- 2.9160)  data: 0.2400 (0.0002 -- 2.3966)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:34  lr: 0.000015  min_lr: 0.000000  loss: 1.7180 (1.7363)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2271 (8.4161)  time: 0.7894 (0.5280 -- 2.8005)  data: 0.2195 (0.0006 -- 2.2589)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 1.8824 (1.7479)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3601 (8.6262)  time: 0.9170 (0.5483 -- 3.3372)  data: 0.2007 (0.0004 -- 2.1807)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.7703 (1.7466)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3679 (8.6758)  time: 0.9128 (0.5215 -- 3.0252)  data: 0.2639 (0.0004 -- 2.1364)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.8833 (1.7678)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1433 (8.6508)  time: 0.8442 (0.5298 -- 1.9044)  data: 0.1579 (0.0003 -- 1.3003)  max mem: 16413
[2023-08-30 05:23:00,506] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:23:00,506] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:23:00,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:23:00,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.5548 (1.7537)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1343 (8.5516)  time: 0.9085 (0.5208 -- 2.8241)  data: 0.0013 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7548 (1.7524)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3943 (8.5851)  time: 0.7010 (0.4985 -- 2.2113)  data: 0.0014 (0.0002 -- 0.0150)  max mem: 16413
Epoch: [86] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7548 (1.7162)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3943 (8.5851)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4655 (0.4655)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3940 (2.3940 -- 2.3940)  data: 2.1386 (2.1386 -- 2.1386)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5887 (0.8597)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4357 (0.1957 -- 2.3940)  data: 0.2225 (0.0006 -- 2.1386)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5843 (0.7776)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1692 -- 0.5107)  data: 0.0158 (0.0001 -- 0.3012)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7060 (0.8206)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.8506)  time: 0.2050 (0.1329 -- 0.5107)  data: 0.0155 (0.0001 -- 0.3012)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 79.253 Acc@5 96.058 loss 0.764
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 80.29%
Epoch: [87]  [  0/160]  eta: 0:20:01  lr: 0.000015  min_lr: 0.000000  loss: 2.1600 (2.1600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2028 (12.2028)  time: 7.5115 (7.5115 -- 7.5115)  data: 6.9883 (6.9883 -- 6.9883)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000000  loss: 1.7511 (1.7292)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7308 (8.7929)  time: 0.8535 (0.5422 -- 1.9281)  data: 0.1994 (0.0006 -- 1.4138)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000000  loss: 1.5414 (1.6766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5938 (8.5934)  time: 0.9311 (0.5287 -- 2.5572)  data: 0.2603 (0.0004 -- 1.9924)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:43  lr: 0.000015  min_lr: 0.000000  loss: 1.5960 (1.6880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8592 (8.6756)  time: 0.9909 (0.5116 -- 5.2616)  data: 0.0011 (0.0003 -- 0.0055)  max mem: 16413
[2023-08-30 05:24:53,105] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13997
[2023-08-30 05:24:53,105] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13997
[2023-08-30 05:24:53,105] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:24:53,105] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:24:53,105] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 05:24:54,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=75, lr=[3.481026645836272e-07, 3.481026645836272e-07, 4.6413688611150295e-07, 4.6413688611150295e-07, 6.188491814820039e-07, 6.188491814820039e-07, 8.251322419760053e-07, 8.251322419760053e-07, 1.1001763226346737e-06, 1.1001763226346737e-06, 1.4669017635128982e-06, 1.4669017635128982e-06, 1.9558690180171976e-06, 1.9558690180171976e-06, 2.6078253573562636e-06, 2.6078253573562636e-06, 3.477100476475018e-06, 3.477100476475018e-06, 4.636133968633357e-06, 4.636133968633357e-06, 6.1815119581778096e-06, 6.1815119581778096e-06, 8.242015944237079e-06, 8.242015944237079e-06, 1.0989354592316106e-05, 1.0989354592316106e-05, 1.4652472789754808e-05, 1.4652472789754808e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 05:24:54,219] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=18.06033428659487, CurrSamplesPerSec=21.898492303582437, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 2.0242 (1.7680)  loss_scale: 16384.0000 (15979.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7769 (8.6645)  time: 0.7465 (0.5198 -- 3.4340)  data: 0.0018 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 1.8749 (1.7863)  loss_scale: 8192.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9516 (8.8495)  time: 0.9749 (0.5373 -- 3.4658)  data: 0.0015 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8410 (1.7906)  loss_scale: 8192.0000 (13405.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6138 (8.9111)  time: 0.7798 (0.5270 -- 3.7065)  data: 0.0018 (0.0002 -- 0.0079)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 2.0074 (1.8032)  loss_scale: 8192.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0401 (8.8030)  time: 0.7857 (0.5336 -- 2.4597)  data: 0.0020 (0.0004 -- 0.0093)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7694 (1.8017)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0342 (8.7368)  time: 0.7471 (0.4997 -- 2.4597)  data: 0.0198 (0.0002 -- 0.3784)  max mem: 16413
Epoch: [87] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7694 (1.7654)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0342 (8.7368)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.4718 (0.4718)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2215 (2.2215 -- 2.2215)  data: 1.9314 (1.9314 -- 1.9314)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6037 (0.8530)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4119 (0.1991 -- 2.2215)  data: 0.1846 (0.0005 -- 1.9314)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5717 (0.7749)  acc1: 88.8889 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2260 (0.1721 -- 0.4497)  data: 0.0201 (0.0001 -- 0.2386)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7000 (0.8155)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.2067 (0.1336 -- 0.4497)  data: 0.0196 (0.0001 -- 0.2386)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 79.668 Acc@5 95.851 loss 0.764
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 80.29%
Epoch: [88]  [  0/160]  eta: 0:22:31  lr: 0.000015  min_lr: 0.000000  loss: 1.2746 (1.2746)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0461 (7.0461)  time: 8.4459 (8.4459 -- 8.4459)  data: 7.8607 (7.8607 -- 7.8607)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:36  lr: 0.000014  min_lr: 0.000000  loss: 1.6310 (1.6508)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2085 (8.2262)  time: 0.7494 (0.5293 -- 3.4823)  data: 0.1377 (0.0005 -- 2.2028)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:02:01  lr: 0.000014  min_lr: 0.000000  loss: 1.7382 (1.7158)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8946 (8.0069)  time: 0.9039 (0.5182 -- 4.5774)  data: 0.1505 (0.0007 -- 2.4388)  max mem: 16413
[2023-08-30 05:26:52,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:26:52,533] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:26:52,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:26:52,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [88]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.6429 (1.7158)  loss_scale: 16384.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0468 (8.1358)  time: 0.8563 (0.5232 -- 2.6163)  data: 0.1175 (0.0003 -- 1.8492)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000000  loss: 1.6933 (1.7085)  loss_scale: 16384.0000 (11731.7531)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8484 (8.3452)  time: 0.9028 (0.5250 -- 2.4879)  data: 0.1539 (0.0003 -- 1.9629)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.9209 (1.7499)  loss_scale: 16384.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9104 (8.6380)  time: 0.8232 (0.5171 -- 3.1797)  data: 0.1465 (0.0002 -- 1.7247)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.5204 (1.7332)  loss_scale: 16384.0000 (13269.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5850 (8.5788)  time: 0.9558 (0.5115 -- 3.7626)  data: 0.1728 (0.0003 -- 2.0380)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.8357 (1.7486)  loss_scale: 16384.0000 (13711.4326)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6725 (8.6291)  time: 0.6945 (0.5306 -- 2.0478)  data: 0.0528 (0.0001 -- 0.8570)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7026 (1.7369)  loss_scale: 16384.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5829 (8.5690)  time: 0.8026 (0.4962 -- 3.4762)  data: 0.2786 (0.0002 -- 2.9798)  max mem: 16413
Epoch: [88] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7026 (1.7473)  loss_scale: 16384.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5829 (8.5690)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4487 (0.4487)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2978 (2.2978 -- 2.2978)  data: 2.0911 (2.0911 -- 2.0911)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5660 (0.8543)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4203 (0.1967 -- 2.2978)  data: 0.2093 (0.0008 -- 2.0911)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5660 (0.7788)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2202 (0.1706 -- 0.4276)  data: 0.0139 (0.0001 -- 0.2010)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7147 (0.8137)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2066 (0.1329 -- 0.4276)  data: 0.0136 (0.0001 -- 0.2010)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 80.083 Acc@5 96.058 loss 0.753
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.29%
Epoch: [89]  [  0/160]  eta: 0:22:18  lr: 0.000014  min_lr: 0.000000  loss: 1.2153 (1.2153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7244 (7.7244)  time: 8.3635 (8.3635 -- 8.3635)  data: 7.8283 (7.8283 -- 7.8283)  max mem: 16413
[2023-08-30 05:28:54,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:28:54,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:28:54,666] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:28:54,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [89]  [ 20/160]  eta: 0:02:49  lr: 0.000014  min_lr: 0.000000  loss: 1.5839 (1.6690)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2013 (8.3383)  time: 0.8556 (0.5261 -- 3.3649)  data: 0.1960 (0.0005 -- 2.7688)  max mem: 16413
[2023-08-30 05:29:15,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14278
[2023-08-30 05:29:15,231] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:29:15,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14278
[2023-08-30 05:29:15,231] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:29:15,231] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [ 40/160]  eta: 0:02:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9404 (1.7938)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1456 (8.8495)  time: 0.7791 (0.5218 -- 3.4424)  data: 0.2301 (0.0004 -- 2.9244)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:34  lr: 0.000014  min_lr: 0.000000  loss: 1.7755 (1.7738)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8121 (8.3858)  time: 0.8326 (0.5279 -- 2.1721)  data: 0.2039 (0.0006 -- 1.6616)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 1.5722 (1.7364)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9258 (8.4696)  time: 0.8594 (0.5219 -- 4.7777)  data: 0.3070 (0.0007 -- 4.2244)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.8026 (1.7382)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0438 (8.6055)  time: 1.0322 (0.5111 -- 4.2844)  data: 0.4894 (0.0003 -- 3.7668)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.8090 (1.7526)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3937 (8.6576)  time: 0.7124 (0.5325 -- 2.5235)  data: 0.1558 (0.0005 -- 1.9814)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.9807 (1.7789)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5427 (8.4948)  time: 0.9139 (0.5340 -- 3.3575)  data: 0.3570 (0.0005 -- 2.8314)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7843 (1.7753)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3625 (8.5538)  time: 0.6188 (0.4989 -- 1.1420)  data: 0.0693 (0.0002 -- 0.5792)  max mem: 16413
Epoch: [89] Total time: 0:02:19 (0.8749 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7843 (1.7919)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3625 (8.5538)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.4415 (0.4415)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2411 (2.2411 -- 2.2411)  data: 2.0289 (2.0289 -- 2.0289)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5604 (0.8414)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.2003 -- 2.2411)  data: 0.2042 (0.0005 -- 2.0289)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5604 (0.7668)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2223 (0.1694 -- 0.4059)  data: 0.0118 (0.0001 -- 0.2034)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6976 (0.8079)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.2058 (0.1333 -- 0.4059)  data: 0.0113 (0.0001 -- 0.2034)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 80.083 Acc@5 96.058 loss 0.751
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.29%
Epoch: [90]  [  0/160]  eta: 0:19:56  lr: 0.000014  min_lr: 0.000000  loss: 1.1151 (1.1151)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4174 (5.4174)  time: 7.4796 (7.4796 -- 7.4796)  data: 6.9399 (6.9399 -- 6.9399)  max mem: 16413
[2023-08-30 05:31:14,575] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:31:14,575] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:31:14,575] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:31:14,575] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:31:24,596] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14416
[2023-08-30 05:31:24,596] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14416
[2023-08-30 05:31:24,596] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:31:24,596] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:31:24,596] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [90]  [ 20/160]  eta: 0:02:37  lr: 0.000014  min_lr: 0.000000  loss: 1.6310 (1.6743)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3633 (8.1371)  time: 0.8074 (0.5400 -- 3.2242)  data: 0.2529 (0.0003 -- 2.6946)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:02  lr: 0.000014  min_lr: 0.000000  loss: 1.7673 (1.7225)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3696 (8.2723)  time: 0.9045 (0.5323 -- 3.7652)  data: 0.3110 (0.0002 -- 3.2386)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 1.6361 (1.7280)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9740 (8.5823)  time: 0.9030 (0.5435 -- 4.3196)  data: 0.3500 (0.0004 -- 3.7772)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 1.7393 (1.7420)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9987 (8.7892)  time: 0.8756 (0.5158 -- 4.3412)  data: 0.3219 (0.0003 -- 3.8019)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:54  lr: 0.000014  min_lr: 0.000000  loss: 1.8820 (1.7521)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6274 (8.9674)  time: 0.7585 (0.5398 -- 2.5768)  data: 0.2072 (0.0003 -- 2.0729)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7676 (1.7402)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2676 (8.7625)  time: 0.9136 (0.5268 -- 4.6121)  data: 0.3658 (0.0004 -- 4.0935)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.7401 (1.7459)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5651 (8.8351)  time: 0.8670 (0.5200 -- 4.6731)  data: 0.3177 (0.0003 -- 4.1165)  max mem: 16413
[2023-08-30 05:33:15,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:33:15,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:33:15,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:33:15,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:33:17,000] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14547
[2023-08-30 05:33:17,000] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:33:17,000] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14547
[2023-08-30 05:33:17,000] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 05:33:17,000] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.6121 (1.7312)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1138 (8.9547)  time: 0.6245 (0.4975 -- 2.4999)  data: 0.1031 (0.0003 -- 1.9558)  max mem: 16413
Epoch: [90] Total time: 0:02:20 (0.8751 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.6121 (1.7527)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1138 (8.9547)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4274 (0.4274)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2639 (2.2639 -- 2.2639)  data: 2.0675 (2.0675 -- 2.0675)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5556 (0.8391)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4100 (0.1995 -- 2.2639)  data: 0.1913 (0.0007 -- 2.0675)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5556 (0.7614)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2246 (0.1704 -- 0.4636)  data: 0.0162 (0.0001 -- 0.2664)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6927 (0.8051)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.2061 (0.1329 -- 0.4636)  data: 0.0147 (0.0001 -- 0.2664)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 79.876 Acc@5 96.473 loss 0.745
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.29%
Epoch: [91]  [  0/160]  eta: 0:21:38  lr: 0.000014  min_lr: 0.000000  loss: 2.0678 (2.0678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9911 (9.9911)  time: 8.1132 (8.1132 -- 8.1132)  data: 7.5711 (7.5711 -- 7.5711)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:40  lr: 0.000014  min_lr: 0.000000  loss: 1.6664 (1.7437)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3436 (8.7192)  time: 0.7994 (0.5139 -- 3.7324)  data: 0.2101 (0.0004 -- 3.2076)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:09  lr: 0.000014  min_lr: 0.000000  loss: 1.7295 (1.7664)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5784 (8.7242)  time: 1.0142 (0.5152 -- 4.3001)  data: 0.4680 (0.0002 -- 3.7704)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000000  loss: 1.9169 (1.7833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7455 (8.6325)  time: 0.8224 (0.5319 -- 3.8200)  data: 0.2740 (0.0003 -- 3.2977)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000000  loss: 1.7002 (1.7792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7088 (8.5517)  time: 0.8059 (0.5219 -- 3.5406)  data: 0.2607 (0.0001 -- 2.9949)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.5292 (1.7422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9035 (8.8641)  time: 0.8727 (0.5211 -- 3.3217)  data: 0.3193 (0.0004 -- 2.7754)  max mem: 16413
[2023-08-30 05:35:16,962] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:35:16,963] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:35:16,963] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:35:16,963] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:35:21,718] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14680
[2023-08-30 05:35:21,718] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14680
[2023-08-30 05:35:21,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:35:21,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:35:21,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.5880 (1.7507)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1869 (8.7563)  time: 0.8185 (0.5450 -- 3.0598)  data: 0.2502 (0.0002 -- 2.5160)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.6858 (1.7520)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6188 (8.8268)  time: 0.8988 (0.5196 -- 3.1568)  data: 0.3504 (0.0003 -- 2.6100)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6270 (1.7401)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1326 (8.8988)  time: 0.7472 (0.4989 -- 4.0020)  data: 0.2213 (0.0002 -- 3.4899)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6270 (1.7356)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1326 (8.8988)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4487 (0.4487)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3231 (2.3231 -- 2.3231)  data: 2.0671 (2.0671 -- 2.0671)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5528 (0.8229)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4205 (0.2045 -- 2.3231)  data: 0.2019 (0.0004 -- 2.0671)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5528 (0.7514)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2268 (0.1689 -- 0.4864)  data: 0.0232 (0.0001 -- 0.3052)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6840 (0.7927)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2095 (0.1326 -- 0.4864)  data: 0.0229 (0.0001 -- 0.3052)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 80.083 Acc@5 96.888 loss 0.736
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.29%
Epoch: [92]  [  0/160]  eta: 0:20:33  lr: 0.000013  min_lr: 0.000000  loss: 1.8995 (1.8995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3414 (11.3414)  time: 7.7085 (7.7085 -- 7.7085)  data: 5.6593 (5.6593 -- 5.6593)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:47  lr: 0.000013  min_lr: 0.000000  loss: 1.6442 (1.7325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9261 (9.0803)  time: 0.8740 (0.5266 -- 3.7483)  data: 0.3203 (0.0001 -- 3.1897)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:08  lr: 0.000013  min_lr: 0.000000  loss: 1.8099 (1.7669)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8612 (9.2260)  time: 0.9400 (0.5374 -- 3.5925)  data: 0.3876 (0.0003 -- 3.0610)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000000  loss: 2.0709 (1.8298)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0858 (9.0488)  time: 0.8252 (0.5325 -- 4.1897)  data: 0.2722 (0.0005 -- 3.6724)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000000  loss: 1.8613 (1.8204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3909 (8.8236)  time: 0.8228 (0.5280 -- 2.8655)  data: 0.2721 (0.0002 -- 2.3233)  max mem: 16413
[2023-08-30 05:37:25,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:37:25,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:37:25,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:37:25,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [100/160]  eta: 0:00:54  lr: 0.000013  min_lr: 0.000000  loss: 1.8626 (1.8087)  loss_scale: 32768.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8090 (8.6233)  time: 0.7442 (0.5337 -- 2.7033)  data: 0.1897 (0.0004 -- 2.1560)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7283 (1.7854)  loss_scale: 32768.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2762 (8.6298)  time: 0.8980 (0.5420 -- 2.7988)  data: 0.3377 (0.0003 -- 2.2357)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:17  lr: 0.000013  min_lr: 0.000000  loss: 1.5974 (1.7681)  loss_scale: 32768.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0622 (8.5657)  time: 0.7982 (0.5385 -- 2.6805)  data: 0.2185 (0.0007 -- 2.1409)  max mem: 16413
[2023-08-30 05:38:15,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14871
[2023-08-30 05:38:15,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:38:15,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14871
[2023-08-30 05:38:15,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 05:38:15,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6025 (1.7559)  loss_scale: 32768.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7423 (8.6441)  time: 0.7116 (0.4961 -- 2.1400)  data: 0.0853 (0.0002 -- 0.7670)  max mem: 16413
Epoch: [92] Total time: 0:02:19 (0.8716 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6025 (1.7596)  loss_scale: 32768.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7423 (8.6441)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4466 (0.4466)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3805 (2.3805 -- 2.3805)  data: 2.1757 (2.1757 -- 2.1757)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5555 (0.8172)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4273 (0.2036 -- 2.3805)  data: 0.2068 (0.0006 -- 2.1757)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5712 (0.7596)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.8254)  time: 0.2183 (0.1723 -- 0.3389)  data: 0.0093 (0.0001 -- 0.0898)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7001 (0.7967)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2005 (0.1334 -- 0.3389)  data: 0.0090 (0.0001 -- 0.0898)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 80.083 Acc@5 96.680 loss 0.737
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.29%
Epoch: [93]  [  0/160]  eta: 0:20:23  lr: 0.000013  min_lr: 0.000000  loss: 1.7734 (1.7734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0025 (6.0025)  time: 7.6455 (7.6455 -- 7.6455)  data: 6.8340 (6.8340 -- 6.8340)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:35  lr: 0.000013  min_lr: 0.000000  loss: 1.6388 (1.7325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2271 (8.8719)  time: 0.7838 (0.5252 -- 3.0803)  data: 0.1937 (0.0004 -- 2.1057)  max mem: 16413
Epoch: [93]  [ 40/160]  eta: 0:02:03  lr: 0.000013  min_lr: 0.000000  loss: 1.8593 (1.7693)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0623 (8.9827)  time: 0.9439 (0.5226 -- 4.1118)  data: 0.1155 (0.0006 -- 1.3645)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.6222 (1.7460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0414 (8.7296)  time: 0.8596 (0.5188 -- 4.5894)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000000  loss: 1.8305 (1.7491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6137 (8.5346)  time: 0.8723 (0.5245 -- 3.7839)  data: 0.0116 (0.0002 -- 0.2092)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8115 (1.7469)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7810 (8.6197)  time: 0.8322 (0.5227 -- 3.7243)  data: 0.0325 (0.0003 -- 0.6140)  max mem: 16413
[2023-08-30 05:40:16,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=80, lr=[3.051535645587332e-07, 3.051535645587332e-07, 4.068714194116442e-07, 4.068714194116442e-07, 5.424952258821923e-07, 5.424952258821923e-07, 7.23326967842923e-07, 7.23326967842923e-07, 9.644359571238973e-07, 9.644359571238973e-07, 1.28591460949853e-06, 1.28591460949853e-06, 1.7145528126647066e-06, 1.7145528126647066e-06, 2.2860704168862754e-06, 2.2860704168862754e-06, 3.0480938891817003e-06, 3.0480938891817003e-06, 4.0641251855756e-06, 4.0641251855756e-06, 5.418833580767468e-06, 5.418833580767468e-06, 7.22511144102329e-06, 7.22511144102329e-06, 9.633481921364386e-06, 9.633481921364386e-06, 1.2844642561819182e-05, 1.2844642561819182e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 05:40:16,967] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=18.14942438826122, CurrSamplesPerSec=21.440467696155963, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-08-30 05:40:17,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:40:17,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:40:17,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:40:17,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7832 (1.7286)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0376 (8.7745)  time: 0.8011 (0.5233 -- 3.0487)  data: 0.0173 (0.0003 -- 0.1736)  max mem: 16413
[2023-08-30 05:40:19,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15002
[2023-08-30 05:40:19,160] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:40:19,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15002
[2023-08-30 05:40:19,160] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:40:19,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8382 (1.7463)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0885 (8.7193)  time: 1.0187 (0.5276 -- 4.3093)  data: 0.0930 (0.0008 -- 1.8287)  max mem: 16413
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.4514 (1.7262)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6260 (8.7778)  time: 0.7758 (0.4978 -- 4.3093)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [93] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.4514 (1.7325)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6260 (8.7778)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4471 (0.4471)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3701 (2.3701 -- 2.3701)  data: 2.1492 (2.1492 -- 2.1492)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5090 (0.7978)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4108 (0.1998 -- 2.3701)  data: 0.1969 (0.0004 -- 2.1492)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5090 (0.7253)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2178 (0.1692 -- 0.4329)  data: 0.0138 (0.0001 -- 0.2553)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6647 (0.7731)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2036 (0.1338 -- 0.4329)  data: 0.0133 (0.0001 -- 0.2553)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 80.913 Acc@5 96.888 loss 0.726
Accuracy of the network on the 482 val images: 80.91%
[2023-08-30 05:40:57,328] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:40:57,329] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:40:57,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:40:57,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:40:58,868] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:40:58,868] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.91%
Epoch: [94]  [  0/160]  eta: 0:20:17  lr: 0.000013  min_lr: 0.000000  loss: 1.3020 (1.3020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6788 (8.6788)  time: 7.6065 (7.6065 -- 7.6065)  data: 7.0818 (7.0818 -- 7.0818)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:49  lr: 0.000013  min_lr: 0.000000  loss: 1.6534 (1.6718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0795 (8.4030)  time: 0.8903 (0.5399 -- 4.0706)  data: 0.3181 (0.0004 -- 3.5465)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:02:12  lr: 0.000013  min_lr: 0.000000  loss: 1.7348 (1.6971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0825 (8.3655)  time: 0.9855 (0.5292 -- 4.4210)  data: 0.1991 (0.0004 -- 2.0025)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:42  lr: 0.000013  min_lr: 0.000000  loss: 1.6694 (1.6787)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5942 (8.3784)  time: 0.8762 (0.5177 -- 5.5037)  data: 0.0012 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:18  lr: 0.000013  min_lr: 0.000000  loss: 2.0516 (1.7312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3468 (8.3657)  time: 0.8367 (0.5232 -- 3.2600)  data: 0.0017 (0.0003 -- 0.0052)  max mem: 16413
[2023-08-30 05:42:28,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:42:28,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:42:28,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:42:28,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:42:33,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15138
[2023-08-30 05:42:33,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15138
[2023-08-30 05:42:33,803] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:42:33,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 05:42:33,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [94]  [100/160]  eta: 0:00:57  lr: 0.000013  min_lr: 0.000000  loss: 1.8496 (1.7478)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4160 (8.3741)  time: 0.8342 (0.5271 -- 4.3684)  data: 0.0032 (0.0004 -- 0.0233)  max mem: 16413
[2023-08-30 05:42:35,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15141
[2023-08-30 05:42:35,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:42:35,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15141
[2023-08-30 05:42:35,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:42:35,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [94]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000000  loss: 1.7872 (1.7473)  loss_scale: 8192.0000 (15977.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5291 (8.3380)  time: 1.0298 (0.5106 -- 4.1874)  data: 0.0010 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.5220 (1.7306)  loss_scale: 8192.0000 (14873.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7421 (8.4644)  time: 0.5793 (0.5181 -- 1.2067)  data: 0.0014 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6186 (1.7215)  loss_scale: 8192.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7786 (8.4212)  time: 0.7181 (0.4961 -- 2.2426)  data: 0.1064 (0.0002 -- 1.7200)  max mem: 16413
Epoch: [94] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6186 (1.7391)  loss_scale: 8192.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7786 (8.4212)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4437 (0.4437)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3178 (2.3178 -- 2.3178)  data: 2.1005 (2.1005 -- 2.1005)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5630 (0.8060)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4107 (0.2041 -- 2.3178)  data: 0.1920 (0.0005 -- 2.1005)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5630 (0.7335)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2230 (0.1702 -- 0.3948)  data: 0.0152 (0.0001 -- 0.1877)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6882 (0.7802)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2079 (0.1338 -- 0.3948)  data: 0.0149 (0.0001 -- 0.1877)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 79.876 Acc@5 96.473 loss 0.730
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.91%
Epoch: [95]  [  0/160]  eta: 0:20:47  lr: 0.000012  min_lr: 0.000000  loss: 2.0219 (2.0219)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3307 (8.3307)  time: 7.7989 (7.7989 -- 7.7989)  data: 7.2559 (7.2559 -- 7.2559)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:39  lr: 0.000012  min_lr: 0.000000  loss: 1.9783 (1.8776)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1886 (9.0152)  time: 0.8056 (0.5329 -- 3.2045)  data: 0.1412 (0.0002 -- 1.4534)  max mem: 16413
Epoch: [95]  [ 40/160]  eta: 0:02:03  lr: 0.000012  min_lr: 0.000000  loss: 1.4723 (1.7509)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3260 (8.7486)  time: 0.9174 (0.5338 -- 2.7483)  data: 0.1408 (0.0004 -- 1.4241)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.5574 (1.7006)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1648 (8.7042)  time: 0.8671 (0.5314 -- 2.6825)  data: 0.1568 (0.0005 -- 1.5630)  max mem: 16413
[2023-08-30 05:44:36,504] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:44:36,504] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:44:36,505] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:44:36,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [95]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 1.5859 (1.6849)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1195 (8.6760)  time: 0.9287 (0.5073 -- 3.2569)  data: 0.3148 (0.0004 -- 2.7497)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.5661 (1.6880)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5520 (8.9577)  time: 0.8629 (0.5250 -- 3.1090)  data: 0.1418 (0.0003 -- 2.5446)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:38  lr: 0.000012  min_lr: 0.000000  loss: 1.7949 (1.7058)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5542 (8.8737)  time: 0.9761 (0.5138 -- 4.1400)  data: 0.3519 (0.0003 -- 3.6091)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.6982 (1.7235)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9133 (8.9183)  time: 0.7146 (0.5213 -- 2.4760)  data: 0.0016 (0.0002 -- 0.0089)  max mem: 16413
[2023-08-30 05:45:51,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15359
[2023-08-30 05:45:51,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:45:51,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15359
[2023-08-30 05:45:51,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 05:45:51,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7460 (1.7307)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4827 (8.9552)  time: 0.7221 (0.4842 -- 3.1260)  data: 0.1303 (0.0002 -- 2.5919)  max mem: 16413
Epoch: [95] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7460 (1.7288)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4827 (8.9552)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4098 (0.4098)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3927 (2.3927 -- 2.3927)  data: 2.1805 (2.1805 -- 2.1805)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5169 (0.7902)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4217 (0.1949 -- 2.3927)  data: 0.2086 (0.0005 -- 2.1805)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5169 (0.7148)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2211 (0.1695 -- 0.3502)  data: 0.0165 (0.0001 -- 0.1769)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6590 (0.7644)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2063 (0.1330 -- 0.3502)  data: 0.0160 (0.0001 -- 0.1769)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 80.498 Acc@5 96.680 loss 0.722
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 80.91%
Epoch: [96]  [  0/160]  eta: 0:16:53  lr: 0.000012  min_lr: 0.000000  loss: 1.2425 (1.2425)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7120 (11.7120)  time: 6.3342 (6.3342 -- 6.3342)  data: 5.7463 (5.7463 -- 5.7463)  max mem: 16413
Epoch: [96]  [ 20/160]  eta: 0:02:49  lr: 0.000012  min_lr: 0.000000  loss: 1.7009 (1.7369)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3008 (9.6104)  time: 0.9526 (0.5182 -- 3.2894)  data: 0.3978 (0.0006 -- 2.7507)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:01:59  lr: 0.000012  min_lr: 0.000000  loss: 1.4528 (1.7012)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8112 (9.3366)  time: 0.7733 (0.5188 -- 4.1052)  data: 0.2255 (0.0004 -- 3.5767)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.6203 (1.6772)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9463 (9.0681)  time: 0.9535 (0.5227 -- 3.9947)  data: 0.3920 (0.0004 -- 3.4715)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 1.5773 (1.6786)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7660 (8.9999)  time: 0.8167 (0.5213 -- 4.0600)  data: 0.2684 (0.0003 -- 3.5322)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.6805 (1.6763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1338 (8.9270)  time: 0.9202 (0.5257 -- 4.0792)  data: 0.3212 (0.0004 -- 2.5932)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.6981 (1.6886)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6846 (8.9605)  time: 0.8318 (0.5313 -- 3.6448)  data: 0.2741 (0.0002 -- 3.1141)  max mem: 16413
[2023-08-30 05:47:57,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:47:57,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:47:57,759] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:47:57,759] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.7417 (1.6927)  loss_scale: 16384.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9131 (8.8425)  time: 0.8813 (0.5240 -- 3.3443)  data: 0.3017 (0.0003 -- 2.8048)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7129 (1.6891)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1849 (8.8622)  time: 0.6321 (0.4963 -- 2.6278)  data: 0.0924 (0.0002 -- 1.8327)  max mem: 16413
Epoch: [96] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7129 (1.7578)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1849 (8.8622)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4695 (0.4695)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3027 (2.3027 -- 2.3027)  data: 2.0389 (2.0389 -- 2.0389)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5020 (0.7787)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4098 (0.2002 -- 2.3027)  data: 0.1903 (0.0007 -- 2.0389)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5303 (0.7118)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2187 (0.1721 -- 0.3796)  data: 0.0118 (0.0001 -- 0.1780)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6381 (0.7595)  acc1: 88.8889 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2021 (0.1358 -- 0.3796)  data: 0.0115 (0.0001 -- 0.1780)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.719
Accuracy of the network on the 482 val images: 81.12%
[2023-08-30 05:48:28,393] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:48:28,395] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:48:28,395] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:48:28,395] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:48:29,807] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:48:29,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.12%
Epoch: [97]  [  0/160]  eta: 0:22:56  lr: 0.000012  min_lr: 0.000000  loss: 1.4841 (1.4841)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7026 (7.7026)  time: 8.6047 (8.6047 -- 8.6047)  data: 6.7846 (6.7846 -- 6.7846)  max mem: 16413
[2023-08-30 05:48:53,874] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15538
[2023-08-30 05:48:53,874] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15538
[2023-08-30 05:48:53,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:48:53,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 05:48:53,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [97]  [ 20/160]  eta: 0:02:47  lr: 0.000012  min_lr: 0.000000  loss: 1.5711 (1.5596)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2142 (9.2849)  time: 0.8273 (0.5267 -- 2.5457)  data: 0.0209 (0.0003 -- 0.3721)  max mem: 16413
Epoch: [97]  [ 40/160]  eta: 0:02:05  lr: 0.000012  min_lr: 0.000000  loss: 1.5095 (1.5982)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4489 (8.5610)  time: 0.8891 (0.5101 -- 3.7609)  data: 0.0037 (0.0002 -- 0.0530)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:39  lr: 0.000012  min_lr: 0.000000  loss: 1.8712 (1.6713)  loss_scale: 8192.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0118 (8.7897)  time: 0.8773 (0.5110 -- 3.7410)  data: 0.0017 (0.0004 -- 0.0076)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 1.6528 (1.6828)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9451 (8.7482)  time: 0.8916 (0.5219 -- 4.4465)  data: 0.0012 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.5943 (1.6694)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1044 (8.8148)  time: 0.7988 (0.5357 -- 3.1287)  data: 0.0017 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.8144 (1.6791)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8993 (8.9818)  time: 0.7797 (0.5358 -- 3.8101)  data: 0.0017 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.7159 (1.6867)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1731 (8.9211)  time: 0.8971 (0.5325 -- 3.1159)  data: 0.0179 (0.0004 -- 0.3123)  max mem: 16413
[2023-08-30 05:50:43,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:50:43,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 05:50:43,541] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:50:43,541] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7735 (1.7036)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1884 (8.8932)  time: 0.6677 (0.4985 -- 1.8790)  data: 0.0653 (0.0002 -- 0.8053)  max mem: 16413
Epoch: [97] Total time: 0:02:20 (0.8787 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7735 (1.7262)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1884 (8.8932)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4112 (0.4112)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3730 (2.3730 -- 2.3730)  data: 2.1450 (2.1450 -- 2.1450)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5265 (0.7810)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4172 (0.2048 -- 2.3730)  data: 0.1960 (0.0007 -- 2.1450)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5265 (0.7158)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2162 (0.1724 -- 0.2820)  data: 0.0050 (0.0001 -- 0.0847)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6559 (0.7661)  acc1: 88.8889 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.2009 (0.1340 -- 0.2820)  data: 0.0046 (0.0001 -- 0.0847)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 80.705 Acc@5 96.888 loss 0.717
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.12%
Epoch: [98]  [  0/160]  eta: 0:16:07  lr: 0.000012  min_lr: 0.000000  loss: 1.2153 (1.2153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3476 (11.3476)  time: 6.0451 (6.0451 -- 6.0451)  data: 5.4811 (5.4811 -- 5.4811)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:33  lr: 0.000012  min_lr: 0.000000  loss: 1.7690 (1.7304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4669 (8.5787)  time: 0.8515 (0.5358 -- 3.6824)  data: 0.0811 (0.0006 -- 1.2312)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:01:59  lr: 0.000012  min_lr: 0.000000  loss: 1.8070 (1.7778)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1139 (8.9565)  time: 0.8831 (0.5337 -- 3.8909)  data: 0.1885 (0.0005 -- 2.4465)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.6362 (1.7275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1604 (9.2965)  time: 0.9421 (0.5365 -- 2.8720)  data: 0.1548 (0.0003 -- 2.2950)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000000  loss: 1.6801 (1.7183)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5068 (9.4089)  time: 0.8370 (0.5343 -- 3.5331)  data: 0.0362 (0.0003 -- 0.6999)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.5934 (1.6920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0633 (9.2104)  time: 0.8594 (0.5261 -- 3.1833)  data: 0.0015 (0.0002 -- 0.0035)  max mem: 16413
[2023-08-30 05:52:44,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:52:44,160] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:52:44,201] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:52:44,201] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:52:44,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15796
[2023-08-30 05:52:44,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15796
[2023-08-30 05:52:44,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:52:44,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:52:44,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [120/160]  eta: 0:00:35  lr: 0.000011  min_lr: 0.000000  loss: 1.8849 (1.7120)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9734 (8.9368)  time: 0.7634 (0.5199 -- 2.6963)  data: 0.0018 (0.0007 -- 0.0086)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7652 (1.7298)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8487 (8.8287)  time: 0.9643 (0.5334 -- 3.3768)  data: 0.0015 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8000 (1.7352)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3378 (9.0463)  time: 0.6628 (0.4962 -- 1.5285)  data: 0.0005 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [98] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8000 (1.7346)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3378 (9.0463)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.4249 (0.4249)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2369 (2.2369 -- 2.2369)  data: 2.0293 (2.0293 -- 2.0293)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4713 (0.7703)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4153 (0.2097 -- 2.2369)  data: 0.1941 (0.0007 -- 2.0293)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5465 (0.7096)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2266 (0.1706 -- 0.4767)  data: 0.0202 (0.0001 -- 0.2939)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6426 (0.7530)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2071 (0.1329 -- 0.4767)  data: 0.0199 (0.0001 -- 0.2939)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.712
Accuracy of the network on the 482 val images: 81.54%
[2023-08-30 05:53:26,822] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:53:26,824] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:53:26,824] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:53:26,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:53:28,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:53:28,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.54%
Epoch: [99]  [  0/160]  eta: 0:21:13  lr: 0.000011  min_lr: 0.000000  loss: 1.7007 (1.7007)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6341 (12.6341)  time: 7.9608 (7.9608 -- 7.9608)  data: 7.4273 (7.4273 -- 7.4273)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:38  lr: 0.000011  min_lr: 0.000000  loss: 1.5734 (1.6555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9851 (8.5683)  time: 0.7880 (0.5318 -- 3.8364)  data: 0.2352 (0.0002 -- 3.2678)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:01:59  lr: 0.000011  min_lr: 0.000000  loss: 1.9168 (1.7528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3425 (8.8050)  time: 0.8628 (0.5101 -- 1.9017)  data: 0.2190 (0.0003 -- 1.3570)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:31  lr: 0.000011  min_lr: 0.000000  loss: 1.6046 (1.7313)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0087 (8.8021)  time: 0.7569 (0.5398 -- 1.8958)  data: 0.1863 (0.0003 -- 1.3534)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:13  lr: 0.000011  min_lr: 0.000000  loss: 1.7654 (1.7315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7559 (8.9346)  time: 0.9354 (0.5210 -- 2.8641)  data: 0.3862 (0.0008 -- 2.3319)  max mem: 16413
[2023-08-30 05:54:47,894] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:54:47,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:54:47,895] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:54:47,896] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:54:52,651] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15929
[2023-08-30 05:54:52,651] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15929
[2023-08-30 05:54:52,651] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:54:52,651] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:54:52,652] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [99]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.8190 (1.7683)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1886 (8.8625)  time: 0.9530 (0.5324 -- 4.0192)  data: 0.3015 (0.0003 -- 3.4805)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.5555 (1.7705)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0414 (8.8449)  time: 0.9428 (0.5204 -- 4.0305)  data: 0.3944 (0.0003 -- 3.4787)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8273 (1.7675)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5024 (8.7652)  time: 0.8351 (0.5229 -- 4.4408)  data: 0.2843 (0.0003 -- 3.8925)  max mem: 16413
[2023-08-30 05:55:49,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=87, lr=[2.615595474106221e-07, 2.615595474106221e-07, 3.487460632141628e-07, 3.487460632141628e-07, 4.649947509522171e-07, 4.649947509522171e-07, 6.199930012696228e-07, 6.199930012696228e-07, 8.266573350261637e-07, 8.266573350261637e-07, 1.102209780034885e-06, 1.102209780034885e-06, 1.4696130400465133e-06, 1.4696130400465133e-06, 1.959484053395351e-06, 1.959484053395351e-06, 2.6126454045271346e-06, 2.6126454045271346e-06, 3.4835272060361793e-06, 3.4835272060361793e-06, 4.644702941381572e-06, 4.644702941381572e-06, 6.1929372551754304e-06, 6.1929372551754304e-06, 8.25724967356724e-06, 8.25724967356724e-06, 1.1009666231422987e-05, 1.1009666231422987e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 05:55:49,997] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=18.103902898394526, CurrSamplesPerSec=24.582181554443622, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8372 (1.7584)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9515 (8.7232)  time: 0.6396 (0.5001 -- 2.4889)  data: 0.1177 (0.0002 -- 1.9794)  max mem: 16413
Epoch: [99] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8372 (1.7343)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9515 (8.7232)
[2023-08-30 05:55:50,000] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-08-30 05:55:50,002] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-08-30 05:55:50,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-08-30 05:55:50,008] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-08-30 05:55:50,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-08-30 05:55:50,910] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3856 (0.3856)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2003 (2.2003 -- 2.2003)  data: 1.9861 (1.9861 -- 1.9861)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4856 (0.7677)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4268 (0.2028 -- 2.2003)  data: 0.2103 (0.0010 -- 1.9861)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5024 (0.7035)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2329 (0.1708 -- 0.4115)  data: 0.0245 (0.0001 -- 0.2058)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6575 (0.7494)  acc1: 88.8889 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2148 (0.1334 -- 0.4115)  data: 0.0241 (0.0001 -- 0.2058)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.705
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.54%
Epoch: [100]  [  0/160]  eta: 0:20:40  lr: 0.000011  min_lr: 0.000000  loss: 2.2338 (2.2338)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1277 (7.1277)  time: 7.7513 (7.7513 -- 7.7513)  data: 6.5601 (6.5601 -- 6.5601)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:45  lr: 0.000011  min_lr: 0.000000  loss: 1.8705 (1.7791)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6850 (9.1251)  time: 0.8555 (0.5208 -- 3.6480)  data: 0.3113 (0.0005 -- 3.1258)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:10  lr: 0.000011  min_lr: 0.000000  loss: 1.8415 (1.7985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3805 (9.5152)  time: 0.9832 (0.5372 -- 3.6975)  data: 0.4302 (0.0002 -- 3.1692)  max mem: 16413
[2023-08-30 05:56:57,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:56:57,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:56:57,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:56:57,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [100]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.5626 (1.7559)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0012 (9.3783)  time: 0.7541 (0.5285 -- 2.1089)  data: 0.2068 (0.0004 -- 1.5553)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 1.5004 (1.7212)  loss_scale: 32768.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6240 (9.3558)  time: 0.7948 (0.5257 -- 2.4371)  data: 0.2494 (0.0005 -- 1.9078)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:54  lr: 0.000011  min_lr: 0.000000  loss: 1.8172 (1.7335)  loss_scale: 32768.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9982 (9.3306)  time: 0.8411 (0.5242 -- 2.5142)  data: 0.2613 (0.0003 -- 1.9408)  max mem: 16413
[2023-08-30 05:57:32,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16103
[2023-08-30 05:57:32,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16103
[2023-08-30 05:57:32,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:57:32,872] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:57:32,872] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9697 (1.7499)  loss_scale: 16384.0000 (22477.2231)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0572 (9.1254)  time: 0.9034 (0.5285 -- 2.6463)  data: 0.1179 (0.0003 -- 1.5538)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8015 (1.7503)  loss_scale: 16384.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5063 (9.1273)  time: 0.8914 (0.5373 -- 3.2579)  data: 0.0385 (0.0003 -- 0.4942)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.5929 (1.7419)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3985 (8.9120)  time: 0.6640 (0.4992 -- 1.6994)  data: 0.0009 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [100] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.5929 (1.7592)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3985 (8.9120)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4067 (0.4067)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4103 (2.4103 -- 2.4103)  data: 2.1494 (2.1494 -- 2.1494)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4438 (0.7574)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4337 (0.1964 -- 2.4103)  data: 0.2058 (0.0007 -- 2.1494)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4830 (0.6943)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2182 (0.1692 -- 0.3152)  data: 0.0060 (0.0001 -- 0.0879)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6493 (0.7447)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.1986 (0.1333 -- 0.3152)  data: 0.0055 (0.0001 -- 0.0879)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 81.950 Acc@5 96.680 loss 0.699
Accuracy of the network on the 482 val images: 81.95%
[2023-08-30 05:58:27,554] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 05:58:27,556] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 05:58:27,556] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 05:58:27,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 05:58:29,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 05:58:29,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.95%
Epoch: [101]  [  0/160]  eta: 0:22:25  lr: 0.000011  min_lr: 0.000000  loss: 1.8177 (1.8177)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7282 (9.7282)  time: 8.4088 (8.4088 -- 8.4088)  data: 7.8713 (7.8713 -- 7.8713)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:45  lr: 0.000011  min_lr: 0.000000  loss: 1.7517 (1.7190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6721 (8.9346)  time: 0.8204 (0.5319 -- 3.7016)  data: 0.1811 (0.0002 -- 2.3102)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000000  loss: 1.6508 (1.6976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6693 (8.8728)  time: 0.8961 (0.5296 -- 3.4843)  data: 0.0983 (0.0005 -- 1.0569)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.5914 (1.6870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1274 (8.6486)  time: 0.8391 (0.5202 -- 3.8896)  data: 0.2410 (0.0003 -- 3.3561)  max mem: 16413
[2023-08-30 05:59:40,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:59:40,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:59:40,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 05:59:40,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 05:59:41,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16235
[2023-08-30 05:59:41,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16235
[2023-08-30 05:59:41,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:59:41,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 05:59:41,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [101]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 1.6148 (1.6854)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0049 (8.9574)  time: 0.8989 (0.5302 -- 3.3608)  data: 0.3487 (0.0005 -- 2.8224)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.6670 (1.6995)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6964 (8.9819)  time: 0.7928 (0.5364 -- 2.4571)  data: 0.1678 (0.0003 -- 1.9171)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.6172 (1.6744)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5043 (8.9550)  time: 0.8816 (0.5326 -- 4.1742)  data: 0.0023 (0.0002 -- 0.0225)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.8839 (1.7031)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6559 (8.9956)  time: 0.9080 (0.5314 -- 3.3123)  data: 0.0477 (0.0002 -- 0.4361)  max mem: 16413
[2023-08-30 06:00:46,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16312
[2023-08-30 06:00:46,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16312
[2023-08-30 06:00:46,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:00:46,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:00:46,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7201 (1.7026)  loss_scale: 16384.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9548 (8.8746)  time: 0.5910 (0.4911 -- 1.4709)  data: 0.0290 (0.0002 -- 0.2873)  max mem: 16413
Epoch: [101] Total time: 0:02:20 (0.8777 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7201 (1.7125)  loss_scale: 16384.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9548 (8.8746)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3900 (0.3900)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5304 (2.5304 -- 2.5304)  data: 2.2865 (2.2865 -- 2.2865)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4559 (0.7754)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4370 (0.1923 -- 2.5304)  data: 0.2218 (0.0004 -- 2.2865)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5055 (0.7033)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2128 (0.1701 -- 0.3491)  data: 0.0078 (0.0001 -- 0.1429)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6397 (0.7519)  acc1: 88.8889 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1981 (0.1334 -- 0.3491)  data: 0.0075 (0.0001 -- 0.1429)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.703
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.95%
Epoch: [102]  [  0/160]  eta: 0:22:15  lr: 0.000010  min_lr: 0.000000  loss: 2.3758 (2.3758)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5778 (8.5778)  time: 8.3482 (8.3482 -- 8.3482)  data: 7.8117 (7.8117 -- 7.8117)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:56  lr: 0.000010  min_lr: 0.000000  loss: 1.7928 (1.7840)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3720 (8.8518)  time: 0.9048 (0.5168 -- 4.8441)  data: 0.2565 (0.0003 -- 2.8061)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:12  lr: 0.000010  min_lr: 0.000000  loss: 1.5420 (1.6991)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3331 (9.2491)  time: 0.9484 (0.5249 -- 4.3731)  data: 0.0047 (0.0002 -- 0.0529)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000000  loss: 1.8563 (1.7272)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9813 (8.9828)  time: 0.7585 (0.5273 -- 3.0417)  data: 0.0016 (0.0003 -- 0.0066)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 1.8822 (1.7228)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1001 (8.8811)  time: 0.8869 (0.5127 -- 6.1867)  data: 0.1070 (0.0001 -- 1.9208)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:54  lr: 0.000010  min_lr: 0.000000  loss: 1.7261 (1.7210)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7702 (8.8810)  time: 0.6984 (0.5262 -- 2.7516)  data: 0.1127 (0.0003 -- 1.6002)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.5804 (1.6995)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9668 (8.7778)  time: 0.9855 (0.5276 -- 3.7497)  data: 0.4321 (0.0004 -- 3.2163)  max mem: 16413
[2023-08-30 06:02:49,967] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:02:49,967] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:02:49,967] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:02:49,967] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.4369 (1.6731)  loss_scale: 16384.0000 (9353.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3862 (8.7770)  time: 0.8306 (0.5355 -- 3.5708)  data: 0.2739 (0.0003 -- 3.0389)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7090 (1.6744)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9501 (8.7390)  time: 0.6758 (0.4957 -- 2.1639)  data: 0.1490 (0.0002 -- 1.6269)  max mem: 16413
Epoch: [102] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7090 (1.7265)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9501 (8.7390)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3631 (0.3631)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2464 (2.2464 -- 2.2464)  data: 2.0267 (2.0267 -- 2.0267)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4871 (0.7683)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4233 (0.1990 -- 2.2464)  data: 0.2087 (0.0005 -- 2.0267)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5132 (0.6968)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2270 (0.1708 -- 0.4761)  data: 0.0195 (0.0001 -- 0.2591)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6483 (0.7466)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2104 (0.1328 -- 0.4761)  data: 0.0192 (0.0001 -- 0.2591)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.699
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.95%
Epoch: [103]  [  0/160]  eta: 0:18:22  lr: 0.000010  min_lr: 0.000000  loss: 1.5410 (1.5410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4688 (9.4688)  time: 6.8900 (6.8900 -- 6.8900)  data: 5.1699 (5.1699 -- 5.1699)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:03:09  lr: 0.000010  min_lr: 0.000000  loss: 1.7031 (1.7566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3926 (8.7569)  time: 1.0797 (0.5094 -- 6.1149)  data: 0.0339 (0.0003 -- 0.4205)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:09  lr: 0.000010  min_lr: 0.000000  loss: 1.7333 (1.7567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7654 (8.6074)  time: 0.7939 (0.5146 -- 3.3540)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
[2023-08-30 06:04:22,413] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16532
[2023-08-30 06:04:22,413] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16532
[2023-08-30 06:04:22,413] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:04:22,413] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:04:22,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [103]  [ 60/160]  eta: 0:01:41  lr: 0.000010  min_lr: 0.000000  loss: 1.7726 (1.7780)  loss_scale: 16384.0000 (15175.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9986 (9.0757)  time: 0.8882 (0.5206 -- 3.1181)  data: 0.0013 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 2.0247 (1.8129)  loss_scale: 8192.0000 (13451.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8591 (9.2371)  time: 0.7996 (0.5106 -- 4.4523)  data: 0.0014 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000000  loss: 1.7200 (1.7979)  loss_scale: 8192.0000 (12409.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8865 (9.1822)  time: 0.9225 (0.5297 -- 3.5559)  data: 0.0011 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.8961 (1.7993)  loss_scale: 8192.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6060 (9.2141)  time: 0.7086 (0.5216 -- 3.7227)  data: 0.0071 (0.0005 -- 0.1149)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.6735 (1.7890)  loss_scale: 8192.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7712 (9.2296)  time: 0.9067 (0.5298 -- 2.3353)  data: 0.1843 (0.0005 -- 1.7592)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8892 (1.7919)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9482 (9.0857)  time: 0.7018 (0.4997 -- 2.1135)  data: 0.1589 (0.0001 -- 1.5821)  max mem: 16413
Epoch: [103] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8892 (1.7345)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9482 (9.0857)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3739 (0.3739)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2197 (2.2197 -- 2.2197)  data: 2.0123 (2.0123 -- 2.0123)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4695 (0.7689)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4097 (0.2075 -- 2.2197)  data: 0.1868 (0.0005 -- 2.0123)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5352 (0.6928)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2234 (0.1705 -- 0.3901)  data: 0.0105 (0.0001 -- 0.1636)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6362 (0.7481)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2028 (0.1334 -- 0.3901)  data: 0.0095 (0.0001 -- 0.1636)  max mem: 16413
Val: Total time: 0:00:07 (0.2844 s / it)
* Acc@1 81.950 Acc@5 96.680 loss 0.706
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 81.95%
Epoch: [104]  [  0/160]  eta: 0:24:41  lr: 0.000010  min_lr: 0.000000  loss: 1.6823 (1.6823)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7062 (6.7062)  time: 9.2607 (9.2607 -- 9.2607)  data: 5.8997 (5.8997 -- 5.8997)  max mem: 16413
Epoch: [104]  [ 20/160]  eta: 0:02:50  lr: 0.000010  min_lr: 0.000000  loss: 1.7525 (1.7045)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2279 (8.5989)  time: 0.8172 (0.5254 -- 4.2971)  data: 0.0845 (0.0003 -- 1.6623)  max mem: 16413
[2023-08-30 06:06:22,998] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:06:22,998] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:06:22,998] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:06:22,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [104]  [ 40/160]  eta: 0:02:04  lr: 0.000010  min_lr: 0.000000  loss: 1.8765 (1.8095)  loss_scale: 16384.0000 (12188.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5949 (8.5566)  time: 0.8467 (0.5239 -- 3.3125)  data: 0.0015 (0.0002 -- 0.0035)  max mem: 16413
[2023-08-30 06:06:52,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16695
[2023-08-30 06:06:52,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16695
[2023-08-30 06:06:52,458] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:06:52,458] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:06:52,458] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [104]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000000  loss: 1.4741 (1.7427)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5836 (8.8040)  time: 0.7897 (0.5190 -- 3.8778)  data: 0.0132 (0.0002 -- 0.1477)  max mem: 16413
[2023-08-30 06:07:13,838] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16720
[2023-08-30 06:07:13,838] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 06:07:13,838] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16720
[2023-08-30 06:07:13,839] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 06:07:13,839] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [104]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.8237 (1.7577)  loss_scale: 8192.0000 (11580.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4385 (8.6307)  time: 0.9319 (0.5171 -- 3.7781)  data: 0.0019 (0.0002 -- 0.0076)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.6628 (1.7468)  loss_scale: 4096.0000 (10098.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9379 (8.7725)  time: 0.8547 (0.5410 -- 3.6999)  data: 0.1961 (0.0007 -- 3.1579)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.7335 (1.7529)  loss_scale: 4096.0000 (9105.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9084 (8.8274)  time: 0.8861 (0.5287 -- 4.0939)  data: 0.3414 (0.0002 -- 3.5767)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.6117 (1.7504)  loss_scale: 4096.0000 (8395.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1414 (8.8305)  time: 0.9286 (0.5377 -- 4.1559)  data: 0.3698 (0.0004 -- 3.6149)  max mem: 16413
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9500 (1.7577)  loss_scale: 4096.0000 (7884.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0733 (8.7753)  time: 0.6116 (0.4972 -- 2.0537)  data: 0.0966 (0.0001 -- 1.5361)  max mem: 16413
Epoch: [104] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9500 (1.7498)  loss_scale: 4096.0000 (7884.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0733 (8.7753)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3686 (0.3686)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3777 (2.3777 -- 2.3777)  data: 2.1637 (2.1637 -- 2.1637)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4597 (0.7721)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4223 (0.1949 -- 2.3777)  data: 0.2084 (0.0003 -- 2.1637)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4747 (0.6957)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2200 (0.1690 -- 0.3553)  data: 0.0149 (0.0001 -- 0.1197)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6444 (0.7467)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2061 (0.1325 -- 0.3553)  data: 0.0146 (0.0001 -- 0.1197)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 81.950 Acc@5 96.266 loss 0.701
Accuracy of the network on the 482 val images: 81.95%
[2023-08-30 06:08:26,704] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:08:26,705] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:08:26,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:08:26,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:08:28,001] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:08:28,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.95%
Epoch: [105]  [  0/160]  eta: 0:16:38  lr: 0.000010  min_lr: 0.000000  loss: 2.0332 (2.0332)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2720 (10.2720)  time: 6.2413 (6.2413 -- 6.2413)  data: 5.6732 (5.6732 -- 5.6732)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:35  lr: 0.000010  min_lr: 0.000000  loss: 1.7950 (1.8488)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6554 (8.9042)  time: 0.8511 (0.5274 -- 2.9045)  data: 0.3021 (0.0009 -- 2.3792)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 1.8805 (1.8439)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6211 (8.9587)  time: 0.9540 (0.5265 -- 3.4053)  data: 0.0057 (0.0003 -- 0.0871)  max mem: 16413
[2023-08-30 06:09:17,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:09:17,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 06:09:17,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:09:17,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [105]  [ 60/160]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 1.7003 (1.8052)  loss_scale: 8192.0000 (4901.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2523 (9.2441)  time: 0.7813 (0.5286 -- 2.8376)  data: 0.1619 (0.0004 -- 2.3159)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:13  lr: 0.000009  min_lr: 0.000000  loss: 1.8042 (1.8203)  loss_scale: 8192.0000 (5714.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8888 (9.0863)  time: 0.8194 (0.5216 -- 3.4131)  data: 0.1709 (0.0004 -- 2.0764)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.7124 (1.7866)  loss_scale: 8192.0000 (6204.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0508 (8.8468)  time: 0.9215 (0.5310 -- 2.6682)  data: 0.0925 (0.0003 -- 0.9066)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.5625 (1.7626)  loss_scale: 8192.0000 (6533.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5272 (8.9920)  time: 0.8314 (0.5327 -- 3.3479)  data: 0.0655 (0.0003 -- 0.9945)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.6525 (1.7471)  loss_scale: 8192.0000 (6768.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6798 (9.0792)  time: 0.9013 (0.5368 -- 3.8286)  data: 0.0030 (0.0003 -- 0.0140)  max mem: 16413
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5375 (1.7279)  loss_scale: 8192.0000 (6937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3726 (9.0638)  time: 0.6803 (0.4960 -- 3.1065)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [105] Total time: 0:02:20 (0.8784 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5375 (1.7188)  loss_scale: 8192.0000 (6937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3726 (9.0638)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3772 (0.3772)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3345 (2.3345 -- 2.3345)  data: 2.0939 (2.0939 -- 2.0939)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4848 (0.7738)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4118 (0.2007 -- 2.3345)  data: 0.1927 (0.0007 -- 2.0939)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4911 (0.6966)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1700 -- 0.4314)  data: 0.0119 (0.0001 -- 0.2091)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6263 (0.7490)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2015 (0.1328 -- 0.4314)  data: 0.0116 (0.0001 -- 0.2091)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 82.158 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 82.16%
[2023-08-30 06:10:56,287] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:10:56,288] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:10:56,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:10:56,289] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:10:57,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:10:57,691] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.16%
Epoch: [106]  [  0/160]  eta: 0:20:40  lr: 0.000009  min_lr: 0.000000  loss: 1.6356 (1.6356)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6255 (6.6255)  time: 7.7503 (7.7503 -- 7.7503)  data: 6.4422 (6.4422 -- 6.4422)  max mem: 16413
[2023-08-30 06:11:20,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:11:20,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:11:20,789] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:11:20,789] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [106]  [ 20/160]  eta: 0:02:44  lr: 0.000009  min_lr: 0.000000  loss: 1.7618 (1.6852)  loss_scale: 8192.0000 (9752.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6753 (8.6061)  time: 0.8499 (0.5268 -- 2.5404)  data: 0.0486 (0.0003 -- 0.6567)  max mem: 16413
[2023-08-30 06:11:34,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16994
[2023-08-30 06:11:34,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16994
[2023-08-30 06:11:34,363] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:11:34,363] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:11:34,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 06:11:37,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=94, lr=[2.1839404252172858e-07, 2.1839404252172858e-07, 2.911920566956381e-07, 2.911920566956381e-07, 3.8825607559418415e-07, 3.8825607559418415e-07, 5.176747674589122e-07, 5.176747674589122e-07, 6.902330232785496e-07, 6.902330232785496e-07, 9.203106977047327e-07, 9.203106977047327e-07, 1.227080930272977e-06, 1.227080930272977e-06, 1.636107907030636e-06, 1.636107907030636e-06, 2.1814772093741815e-06, 2.1814772093741815e-06, 2.908636279165575e-06, 2.908636279165575e-06, 3.878181705554101e-06, 3.878181705554101e-06, 5.1709089407388005e-06, 5.1709089407388005e-06, 6.8945452543184e-06, 6.8945452543184e-06, 9.192727005757867e-06, 9.192727005757867e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 06:11:37,095] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=18.047613144271537, CurrSamplesPerSec=22.00448381873061, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000000  loss: 1.6689 (1.7110)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5935 (9.0586)  time: 0.9351 (0.5198 -- 4.0589)  data: 0.3220 (0.0004 -- 3.5145)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:33  lr: 0.000009  min_lr: 0.000000  loss: 1.7482 (1.7580)  loss_scale: 8192.0000 (10475.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4085 (9.3778)  time: 0.6769 (0.5239 -- 2.0924)  data: 0.0796 (0.0002 -- 1.5510)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 1.5722 (1.7129)  loss_scale: 8192.0000 (9911.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9435 (9.2757)  time: 0.9309 (0.5174 -- 2.6175)  data: 0.1466 (0.0002 -- 1.7125)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.8726 (1.7275)  loss_scale: 8192.0000 (9570.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4687 (9.2169)  time: 0.8987 (0.5259 -- 3.7546)  data: 0.0016 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.6595 (1.7228)  loss_scale: 8192.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3515 (9.0722)  time: 0.9810 (0.5234 -- 3.7123)  data: 0.0017 (0.0007 -- 0.0044)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.5728 (1.6984)  loss_scale: 8192.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9444 (8.9889)  time: 0.7974 (0.5180 -- 4.7688)  data: 0.0015 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6468 (1.6976)  loss_scale: 8192.0000 (9062.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3082 (9.0170)  time: 0.7079 (0.4988 -- 2.3234)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [106] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6468 (1.6972)  loss_scale: 8192.0000 (9062.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3082 (9.0170)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3600 (0.3600)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2509 (2.2509 -- 2.2509)  data: 2.0402 (2.0402 -- 2.0402)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4432 (0.7548)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4132 (0.2051 -- 2.2509)  data: 0.1900 (0.0008 -- 2.0402)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4909 (0.6799)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2217 (0.1700 -- 0.2879)  data: 0.0067 (0.0001 -- 0.0433)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6324 (0.7326)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2039 (0.1332 -- 0.2879)  data: 0.0062 (0.0001 -- 0.0433)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.687
Accuracy of the network on the 482 val images: 82.37%
[2023-08-30 06:13:28,154] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:13:28,155] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:13:28,156] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:13:28,156] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:13:29,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:13:29,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [107]  [  0/160]  eta: 0:19:33  lr: 0.000009  min_lr: 0.000000  loss: 1.6193 (1.6193)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0311 (10.0311)  time: 7.3345 (7.3345 -- 7.3345)  data: 6.8164 (6.8164 -- 6.8164)  max mem: 16413
[2023-08-30 06:13:40,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:13:40,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:13:40,129] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:13:40,129] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [107]  [ 20/160]  eta: 0:02:41  lr: 0.000009  min_lr: 0.000000  loss: 1.8123 (1.7894)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1727 (8.1333)  time: 0.8452 (0.5238 -- 2.8171)  data: 0.2989 (0.0004 -- 2.2809)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:10  lr: 0.000009  min_lr: 0.000000  loss: 1.6568 (1.7526)  loss_scale: 16384.0000 (15784.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4886 (8.9905)  time: 1.0144 (0.5141 -- 4.3601)  data: 0.4633 (0.0002 -- 3.8244)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.9994 (1.7952)  loss_scale: 16384.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4170 (8.7243)  time: 0.7628 (0.5222 -- 3.4781)  data: 0.2102 (0.0003 -- 2.9470)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:19  lr: 0.000009  min_lr: 0.000000  loss: 1.9671 (1.8169)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8543 (8.6976)  time: 1.0221 (0.5271 -- 3.7927)  data: 0.4676 (0.0004 -- 3.2630)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.9503 (1.8109)  loss_scale: 16384.0000 (16140.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9163 (8.6391)  time: 0.7238 (0.5267 -- 2.3080)  data: 0.1760 (0.0004 -- 1.7898)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.7233 (1.8015)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5829 (8.6429)  time: 0.9895 (0.5391 -- 2.9799)  data: 0.4385 (0.0005 -- 2.4483)  max mem: 16413
[2023-08-30 06:15:31,861] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:15:31,862] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:15:31,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:15:31,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8143 (1.7952)  loss_scale: 16384.0000 (17371.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5548 (8.5825)  time: 0.7856 (0.5239 -- 3.6230)  data: 0.2399 (0.0002 -- 3.0974)  max mem: 16413
[2023-08-30 06:15:45,096] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17267
[2023-08-30 06:15:45,096] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:15:45,096] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17267
[2023-08-30 06:15:45,096] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:15:45,096] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5773 (1.7777)  loss_scale: 16384.0000 (17868.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5417 (8.5834)  time: 0.6408 (0.4966 -- 1.7996)  data: 0.1121 (0.0002 -- 1.2822)  max mem: 16413
Epoch: [107] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5773 (1.7511)  loss_scale: 16384.0000 (17868.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5417 (8.5834)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3511 (0.3511)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1751 (2.1751 -- 2.1751)  data: 1.9119 (1.9119 -- 1.9119)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4519 (0.7428)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.3998 (0.2008 -- 2.1751)  data: 0.1749 (0.0007 -- 1.9119)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4911 (0.6754)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2273 (0.1689 -- 0.5010)  data: 0.0210 (0.0001 -- 0.2803)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6237 (0.7253)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.2656)  time: 0.2065 (0.1364 -- 0.5010)  data: 0.0206 (0.0001 -- 0.2803)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.681
Accuracy of the network on the 482 val images: 82.99%
[2023-08-30 06:15:59,940] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:15:59,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:15:59,942] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:15:59,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:16:01,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:16:01,493] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.99%
Epoch: [108]  [  0/160]  eta: 0:23:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7843 (1.7843)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2171 (7.2171)  time: 8.6273 (8.6273 -- 8.6273)  data: 8.1259 (8.1259 -- 8.1259)  max mem: 16413
[2023-08-30 06:16:10,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17281
[2023-08-30 06:16:10,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17281
[2023-08-30 06:16:10,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:16:10,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:16:10,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [108]  [ 20/160]  eta: 0:02:48  lr: 0.000009  min_lr: 0.000000  loss: 1.5261 (1.5849)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2550 (9.3788)  time: 0.8301 (0.5154 -- 4.3163)  data: 0.2772 (0.0002 -- 3.7875)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:05  lr: 0.000009  min_lr: 0.000000  loss: 1.7997 (1.6571)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4469 (8.5376)  time: 0.8765 (0.5228 -- 2.9923)  data: 0.2832 (0.0004 -- 2.4353)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:41  lr: 0.000009  min_lr: 0.000000  loss: 1.9098 (1.6985)  loss_scale: 8192.0000 (8326.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5905 (8.5951)  time: 0.9641 (0.5200 -- 3.0633)  data: 0.4247 (0.0004 -- 2.5384)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.8824 (1.7328)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7448 (8.4859)  time: 0.7841 (0.5300 -- 2.6627)  data: 0.2376 (0.0004 -- 2.1377)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000000  loss: 1.8756 (1.7371)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0256 (8.5649)  time: 0.9774 (0.5202 -- 5.2326)  data: 0.4327 (0.0003 -- 4.7262)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.7640 (1.7557)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1155 (8.5932)  time: 0.6945 (0.5313 -- 2.2869)  data: 0.1429 (0.0001 -- 1.7299)  max mem: 16413
[2023-08-30 06:18:02,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:18:02,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:18:02,511] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:18:02,511] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.8697 (1.7593)  loss_scale: 16384.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8046 (8.5684)  time: 0.9579 (0.5315 -- 3.3288)  data: 0.3566 (0.0006 -- 2.7944)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8257 (1.7571)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1224 (8.5469)  time: 0.6676 (0.4981 -- 3.2422)  data: 0.1464 (0.0002 -- 2.7112)  max mem: 16413
Epoch: [108] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8257 (1.7324)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1224 (8.5469)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3475 (0.3475)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2564 (2.2564 -- 2.2564)  data: 2.0611 (2.0611 -- 2.0611)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4470 (0.7601)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4261 (0.2028 -- 2.2564)  data: 0.2077 (0.0008 -- 2.0611)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4807 (0.6835)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2222 (0.1697 -- 0.4664)  data: 0.0117 (0.0001 -- 0.2016)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6291 (0.7352)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2075 (0.1336 -- 0.4664)  data: 0.0108 (0.0001 -- 0.2016)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.687
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [109]  [  0/160]  eta: 0:17:58  lr: 0.000008  min_lr: 0.000000  loss: 2.2456 (2.2456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0207 (9.0207)  time: 6.7395 (6.7395 -- 6.7395)  data: 5.4043 (5.4043 -- 5.4043)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:39  lr: 0.000008  min_lr: 0.000000  loss: 1.9974 (1.9290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8287 (8.4846)  time: 0.8619 (0.5284 -- 2.7940)  data: 0.1527 (0.0005 -- 1.0971)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000000  loss: 1.7876 (1.8638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0837 (8.5778)  time: 0.9106 (0.5185 -- 3.3431)  data: 0.0988 (0.0008 -- 1.1124)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.5182 (1.7650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4854 (8.9396)  time: 0.8628 (0.5260 -- 3.5721)  data: 0.0177 (0.0003 -- 0.2580)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.8655 (1.7734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2924 (8.7977)  time: 0.9074 (0.5252 -- 3.4180)  data: 0.0015 (0.0003 -- 0.0071)  max mem: 16413
[2023-08-30 06:20:05,574] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:20:05,576] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:20:05,616] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:20:05,616] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.7126 (1.7518)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6688 (8.7414)  time: 0.8328 (0.5373 -- 3.8216)  data: 0.0053 (0.0005 -- 0.0686)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9914 (1.7780)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9661 (8.6739)  time: 0.9063 (0.5283 -- 3.2053)  data: 0.0019 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.5838 (1.7531)  loss_scale: 32768.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6512 (8.6247)  time: 0.7994 (0.5362 -- 3.5722)  data: 0.0014 (0.0004 -- 0.0035)  max mem: 16413
[2023-08-30 06:20:51,094] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17593
[2023-08-30 06:20:51,094] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17593
[2023-08-30 06:20:51,094] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:20:51,094] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:20:51,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.5480 (1.7362)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8270 (8.5664)  time: 0.6906 (0.4832 -- 3.9433)  data: 0.0008 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [109] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.5480 (1.7571)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8270 (8.5664)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3706 (0.3706)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4418 (2.4418 -- 2.4418)  data: 2.2125 (2.2125 -- 2.2125)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4598 (0.7517)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4197 (0.1800 -- 2.4418)  data: 0.2054 (0.0003 -- 2.2125)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5179 (0.6802)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2144 (0.1704 -- 0.3076)  data: 0.0075 (0.0001 -- 0.1008)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6134 (0.7301)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2001 (0.1336 -- 0.3076)  data: 0.0073 (0.0001 -- 0.1008)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.685
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [110]  [  0/160]  eta: 0:20:46  lr: 0.000008  min_lr: 0.000000  loss: 1.6294 (1.6294)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9814 (10.9814)  time: 7.7934 (7.7934 -- 7.7934)  data: 7.2615 (7.2615 -- 7.2615)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:39  lr: 0.000008  min_lr: 0.000000  loss: 1.7930 (1.7840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5497 (8.9916)  time: 0.8079 (0.5295 -- 2.1396)  data: 0.1481 (0.0002 -- 0.8870)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000000  loss: 1.6650 (1.7165)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8468 (9.0418)  time: 0.9139 (0.5277 -- 2.4342)  data: 0.1336 (0.0003 -- 1.5876)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.5815 (1.6770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5424 (8.9421)  time: 0.9064 (0.5216 -- 2.3043)  data: 0.1767 (0.0004 -- 1.7798)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 1.7497 (1.6882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4535 (8.7619)  time: 0.7863 (0.5211 -- 3.1061)  data: 0.1745 (0.0003 -- 2.5703)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.8950 (1.7155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4382 (9.0487)  time: 0.8526 (0.5173 -- 2.1130)  data: 0.2325 (0.0003 -- 1.5980)  max mem: 16413
[2023-08-30 06:22:36,153] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17702
[2023-08-30 06:22:36,153] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17702
[2023-08-30 06:22:36,153] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:22:36,153] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:22:36,154] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [110]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.6133 (1.7025)  loss_scale: 8192.0000 (15097.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0343 (8.9690)  time: 0.8744 (0.5280 -- 3.0318)  data: 0.2808 (0.0002 -- 2.4672)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7932 (1.7028)  loss_scale: 8192.0000 (14118.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5209 (8.8321)  time: 0.8941 (0.5330 -- 2.7780)  data: 0.2240 (0.0003 -- 2.2425)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.4871 (1.6748)  loss_scale: 8192.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5381 (8.8431)  time: 0.7556 (0.4959 -- 3.4483)  data: 0.2277 (0.0002 -- 2.9191)  max mem: 16413
Epoch: [110] Total time: 0:02:23 (0.8944 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.4871 (1.6957)  loss_scale: 8192.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5381 (8.8431)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3447 (0.3447)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2663 (2.2663 -- 2.2663)  data: 2.0478 (2.0478 -- 2.0478)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4362 (0.7428)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4259 (0.1934 -- 2.2663)  data: 0.2113 (0.0006 -- 2.0478)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4851 (0.6746)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2214 (0.1690 -- 0.5040)  data: 0.0170 (0.0001 -- 0.2678)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6092 (0.7253)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2073 (0.1330 -- 0.5040)  data: 0.0167 (0.0001 -- 0.2678)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 83.195 Acc@5 96.680 loss 0.681
Accuracy of the network on the 482 val images: 83.20%
[2023-08-30 06:23:32,866] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:23:32,867] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:23:32,867] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:23:32,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:23:34,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:23:34,272] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.20%
Epoch: [111]  [  0/160]  eta: 0:23:44  lr: 0.000008  min_lr: 0.000000  loss: 1.6629 (1.6629)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1428 (9.1428)  time: 8.9035 (8.9035 -- 8.9035)  data: 8.3723 (8.3723 -- 8.3723)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:38  lr: 0.000008  min_lr: 0.000000  loss: 1.7462 (1.7643)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8602 (8.8487)  time: 0.7448 (0.5227 -- 2.5415)  data: 0.1913 (0.0005 -- 2.0211)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:02  lr: 0.000008  min_lr: 0.000000  loss: 1.7322 (1.7146)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6874 (8.8611)  time: 0.9055 (0.5198 -- 3.9601)  data: 0.3562 (0.0003 -- 3.4453)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.6967 (1.6841)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2382 (8.4292)  time: 0.8782 (0.5311 -- 2.7635)  data: 0.2925 (0.0004 -- 2.2405)  max mem: 16413
[2023-08-30 06:24:43,622] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:24:43,622] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:24:43,623] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:24:43,623] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [111]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.8443 (1.7094)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2590 (8.4140)  time: 0.9053 (0.5343 -- 3.8190)  data: 0.2983 (0.0002 -- 3.2722)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.5920 (1.6892)  loss_scale: 16384.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6108 (8.3865)  time: 0.8651 (0.5371 -- 3.2461)  data: 0.3081 (0.0010 -- 2.6972)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.6520 (1.6749)  loss_scale: 16384.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2419 (8.4333)  time: 0.8021 (0.5284 -- 4.0326)  data: 0.2490 (0.0004 -- 3.5253)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7295 (1.6846)  loss_scale: 16384.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7585 (8.5582)  time: 0.8946 (0.5353 -- 2.9357)  data: 0.3399 (0.0007 -- 2.3741)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9287 (1.7095)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2600 (8.5458)  time: 0.7145 (0.4970 -- 2.3986)  data: 0.1967 (0.0002 -- 1.8884)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9287 (1.7196)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2600 (8.5458)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3558 (0.3558)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2709 (2.2709 -- 2.2709)  data: 2.0258 (2.0258 -- 2.0258)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4220 (0.7438)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4065 (0.1976 -- 2.2709)  data: 0.1886 (0.0008 -- 2.0258)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4989 (0.6750)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2248 (0.1695 -- 0.4966)  data: 0.0170 (0.0001 -- 0.2891)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6273 (0.7221)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2095 (0.1325 -- 0.4966)  data: 0.0167 (0.0001 -- 0.2891)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 83.195 Acc@5 97.095 loss 0.677
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 83.20%
Epoch: [112]  [  0/160]  eta: 0:18:01  lr: 0.000008  min_lr: 0.000000  loss: 2.1319 (2.1319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1591 (13.1591)  time: 6.7564 (6.7564 -- 6.7564)  data: 6.2278 (6.2278 -- 6.2278)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000000  loss: 1.6922 (1.6855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1648 (8.8702)  time: 0.8726 (0.5264 -- 3.0591)  data: 0.2781 (0.0009 -- 2.5202)  max mem: 16413
[2023-08-30 06:26:44,518] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:26:44,519] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:26:44,519] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:26:44,519] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [ 40/160]  eta: 0:01:58  lr: 0.000008  min_lr: 0.000000  loss: 1.6357 (1.7032)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9723 (8.4861)  time: 0.8108 (0.5322 -- 3.0193)  data: 0.2455 (0.0004 -- 2.4888)  max mem: 16413
[2023-08-30 06:26:53,183] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17969
[2023-08-30 06:26:53,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:26:53,183] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17969
[2023-08-30 06:26:53,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:26:53,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [112]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6826 (1.7103)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0227 (8.5891)  time: 0.8850 (0.5521 -- 3.1650)  data: 0.2623 (0.0005 -- 2.6194)  max mem: 16413
[2023-08-30 06:27:19,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=99, lr=[1.7671992788051466e-07, 1.7671992788051466e-07, 2.3562657050735288e-07, 2.3562657050735288e-07, 3.141687606764705e-07, 3.141687606764705e-07, 4.1889168090196067e-07, 4.1889168090196067e-07, 5.585222412026142e-07, 5.585222412026142e-07, 7.446963216034856e-07, 7.446963216034856e-07, 9.929284288046475e-07, 9.929284288046475e-07, 1.32390457173953e-06, 1.32390457173953e-06, 1.7652060956527067e-06, 1.7652060956527067e-06, 2.353608127536942e-06, 2.353608127536942e-06, 3.1381441700492562e-06, 3.1381441700492562e-06, 4.184192226732341e-06, 4.184192226732341e-06, 5.578922968976456e-06, 5.578922968976456e-06, 7.438563958635274e-06, 7.438563958635274e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 06:27:19,040] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=18.09669913128621, CurrSamplesPerSec=22.47889875313859, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000000  loss: 1.6849 (1.7165)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8533 (8.6040)  time: 0.8401 (0.5212 -- 2.0537)  data: 0.1477 (0.0003 -- 1.3966)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7513 (1.7186)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8635 (8.6841)  time: 0.9025 (0.5297 -- 2.6975)  data: 0.1404 (0.0006 -- 1.6613)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.5931 (1.7119)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0301 (8.6964)  time: 0.9442 (0.5277 -- 2.2819)  data: 0.1480 (0.0003 -- 1.7508)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7039 (1.7173)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2949 (8.8366)  time: 0.8492 (0.5351 -- 3.9335)  data: 0.2459 (0.0002 -- 3.3687)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6467 (1.7211)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7648 (8.7184)  time: 0.6088 (0.5028 -- 1.2835)  data: 0.0606 (0.0002 -- 0.7777)  max mem: 16413
Epoch: [112] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6467 (1.7281)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7648 (8.7184)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3458 (0.3458)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3685 (2.3685 -- 2.3685)  data: 2.1637 (2.1637 -- 2.1637)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4358 (0.7414)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4101 (0.1993 -- 2.3685)  data: 0.1982 (0.0007 -- 2.1637)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5058 (0.6711)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2139 (0.1702 -- 0.3029)  data: 0.0059 (0.0001 -- 0.0886)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6055 (0.7199)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.1993 (0.1336 -- 0.3029)  data: 0.0053 (0.0001 -- 0.0886)  max mem: 16413
Val: Total time: 0:00:07 (0.2828 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.677
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.20%
Epoch: [113]  [  0/160]  eta: 0:23:37  lr: 0.000007  min_lr: 0.000000  loss: 2.1619 (2.1619)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5124 (8.5124)  time: 8.8603 (8.8603 -- 8.8603)  data: 8.2912 (8.2912 -- 8.2912)  max mem: 16413
[2023-08-30 06:28:55,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:28:55,372] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:28:55,372] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:28:55,373] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [113]  [ 20/160]  eta: 0:02:37  lr: 0.000007  min_lr: 0.000000  loss: 1.7017 (1.6279)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8739 (8.0980)  time: 0.7377 (0.5357 -- 2.6339)  data: 0.1860 (0.0008 -- 2.0792)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000000  loss: 1.9093 (1.7331)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7740 (8.9361)  time: 0.8561 (0.5389 -- 2.7227)  data: 0.2395 (0.0002 -- 2.1554)  max mem: 16413
Epoch: [113]  [ 60/160]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000000  loss: 1.5775 (1.6969)  loss_scale: 32768.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8568 (9.1251)  time: 0.9237 (0.5305 -- 3.5162)  data: 0.1002 (0.0002 -- 0.9795)  max mem: 16413
[2023-08-30 06:29:45,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18156
[2023-08-30 06:29:45,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18156
[2023-08-30 06:29:45,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:29:45,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:29:45,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 80/160]  eta: 0:01:17  lr: 0.000007  min_lr: 0.000000  loss: 2.0763 (1.7658)  loss_scale: 32768.0000 (28115.7531)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7927 (9.3150)  time: 0.9598 (0.5212 -- 4.0777)  data: 0.3985 (0.0004 -- 3.5495)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:57  lr: 0.000007  min_lr: 0.000000  loss: 1.7917 (1.7615)  loss_scale: 16384.0000 (25792.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7650 (9.1254)  time: 0.9072 (0.5237 -- 4.2783)  data: 0.3637 (0.0002 -- 3.7510)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6072 (1.7292)  loss_scale: 16384.0000 (24237.4876)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5362 (9.1285)  time: 0.8083 (0.5158 -- 2.8680)  data: 0.2686 (0.0004 -- 2.3450)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8434 (1.7428)  loss_scale: 16384.0000 (23123.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5388 (9.1079)  time: 0.9206 (0.5191 -- 4.5293)  data: 0.3756 (0.0002 -- 3.9971)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7134 (1.7393)  loss_scale: 16384.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7864 (9.1104)  time: 0.5731 (0.4976 -- 1.3719)  data: 0.0436 (0.0001 -- 0.8587)  max mem: 16413
Epoch: [113] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7134 (1.7147)  loss_scale: 16384.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7864 (9.1104)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3408 (0.3408)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3512 (2.3512 -- 2.3512)  data: 2.1407 (2.1407 -- 2.1407)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4410 (0.7389)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4264 (0.1990 -- 2.3512)  data: 0.2116 (0.0009 -- 2.1407)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5070 (0.6715)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2202 (0.1719 -- 0.4234)  data: 0.0134 (0.0001 -- 0.1769)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6108 (0.7176)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2058 (0.1329 -- 0.4234)  data: 0.0131 (0.0001 -- 0.1769)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 83.610 Acc@5 97.095 loss 0.673
Accuracy of the network on the 482 val images: 83.61%
[2023-08-30 06:31:02,710] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 06:31:02,712] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 06:31:02,712] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 06:31:02,712] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 06:31:04,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 06:31:04,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.61%
Epoch: [114]  [  0/160]  eta: 0:18:48  lr: 0.000007  min_lr: 0.000000  loss: 1.1219 (1.1219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0092 (8.0092)  time: 7.0502 (7.0502 -- 7.0502)  data: 5.8451 (5.8451 -- 5.8451)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:46  lr: 0.000007  min_lr: 0.000000  loss: 1.7045 (1.7188)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9726 (8.2581)  time: 0.8954 (0.5190 -- 2.6521)  data: 0.3494 (0.0011 -- 2.1085)  max mem: 16413
Epoch: [114]  [ 40/160]  eta: 0:02:06  lr: 0.000007  min_lr: 0.000000  loss: 1.7503 (1.7208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6681 (8.9395)  time: 0.9083 (0.5199 -- 3.1111)  data: 0.3628 (0.0004 -- 2.5725)  max mem: 16413
[2023-08-30 06:31:50,419] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:31:50,419] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:31:50,419] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:31:50,419] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [114]  [ 60/160]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000000  loss: 1.4856 (1.6662)  loss_scale: 32768.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4242 (8.9422)  time: 0.8039 (0.5342 -- 2.9076)  data: 0.2511 (0.0005 -- 2.3239)  max mem: 16413
[2023-08-30 06:32:13,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18311
[2023-08-30 06:32:13,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18311
[2023-08-30 06:32:13,386] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:32:13,386] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:32:13,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [114]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6218 (1.6691)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7956 (9.0303)  time: 0.9371 (0.5106 -- 4.5263)  data: 0.3942 (0.0003 -- 4.0030)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7690 (1.6857)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8859 (8.9519)  time: 0.7842 (0.5180 -- 4.0037)  data: 0.2333 (0.0004 -- 3.4646)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6227 (1.6687)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8935 (9.0275)  time: 0.9223 (0.5193 -- 4.3796)  data: 0.2443 (0.0008 -- 2.2529)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.4730 (1.6560)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3182 (8.9848)  time: 0.8474 (0.5230 -- 4.8577)  data: 0.0672 (0.0002 -- 1.3180)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7864 (1.6664)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7668 (8.9798)  time: 0.6210 (0.4972 -- 1.7759)  data: 0.0013 (0.0002 -- 0.0136)  max mem: 16413
Epoch: [114] Total time: 0:02:20 (0.8807 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7864 (1.7188)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7668 (8.9798)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3321 (0.3321)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4552 (2.4552 -- 2.4552)  data: 2.2396 (2.2396 -- 2.2396)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4246 (0.7370)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4234 (0.1984 -- 2.4552)  data: 0.2098 (0.0008 -- 2.2396)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4873 (0.6654)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2190 (0.1700 -- 0.4391)  data: 0.0157 (0.0001 -- 0.2419)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5956 (0.7170)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2036 (0.1335 -- 0.4391)  data: 0.0154 (0.0001 -- 0.2419)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.674
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [115]  [  0/160]  eta: 0:15:39  lr: 0.000007  min_lr: 0.000000  loss: 1.6118 (1.6118)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8784 (9.8784)  time: 5.8701 (5.8701 -- 5.8701)  data: 5.0323 (5.0323 -- 5.0323)  max mem: 16413
Epoch: [115]  [ 20/160]  eta: 0:02:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6587 (1.7442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9688 (9.3695)  time: 0.8744 (0.5325 -- 2.9802)  data: 0.1540 (0.0007 -- 1.3655)  max mem: 16413
[2023-08-30 06:34:12,861] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:34:12,862] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:34:12,864] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:34:12,865] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [ 40/160]  eta: 0:01:56  lr: 0.000007  min_lr: 0.000000  loss: 1.6968 (1.7544)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0266 (9.2767)  time: 0.8275 (0.5388 -- 2.1212)  data: 0.1263 (0.0004 -- 1.0087)  max mem: 16413
[2023-08-30 06:34:14,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18442
[2023-08-30 06:34:14,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18442
[2023-08-30 06:34:14,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:34:14,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:34:14,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 1.7391 (1.7396)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5225 (8.7785)  time: 0.9251 (0.5417 -- 3.4762)  data: 0.0077 (0.0008 -- 0.1156)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:13  lr: 0.000007  min_lr: 0.000000  loss: 1.6861 (1.7537)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6063 (9.2329)  time: 0.8012 (0.5328 -- 2.1874)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.6685 (1.7439)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2382 (9.0876)  time: 1.0322 (0.5377 -- 4.3525)  data: 0.0016 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.8295 (1.7440)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1824 (9.0545)  time: 0.7991 (0.5149 -- 3.5586)  data: 0.0017 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 2.0107 (1.7652)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2720 (8.9548)  time: 0.8636 (0.5374 -- 3.5847)  data: 0.0017 (0.0007 -- 0.0049)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.9645 (1.7831)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4155 (8.8998)  time: 0.6331 (0.4980 -- 2.6281)  data: 0.0009 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [115] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.9645 (1.7811)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4155 (8.8998)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3314 (0.3314)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3359 (2.3359 -- 2.3359)  data: 2.0804 (2.0804 -- 2.0804)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4292 (0.7345)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4245 (0.2027 -- 2.3359)  data: 0.1986 (0.0006 -- 2.0804)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4835 (0.6641)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2220 (0.1704 -- 0.3289)  data: 0.0059 (0.0001 -- 0.0928)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5982 (0.7170)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2036 (0.1330 -- 0.3289)  data: 0.0056 (0.0001 -- 0.0928)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.675
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [116]  [  0/160]  eta: 0:17:16  lr: 0.000006  min_lr: 0.000000  loss: 1.6304 (1.6304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7324 (9.7324)  time: 6.4807 (6.4807 -- 6.4807)  data: 5.9375 (5.9375 -- 5.9375)  max mem: 16413
[2023-08-30 06:36:18,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:36:18,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:36:18,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:36:18,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:36:19,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18573
[2023-08-30 06:36:19,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18573
[2023-08-30 06:36:19,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:36:19,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:36:19,652] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [ 20/160]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 1.6547 (1.6736)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1244 (8.9303)  time: 0.8676 (0.5334 -- 3.0744)  data: 0.2879 (0.0007 -- 2.5383)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:01:59  lr: 0.000006  min_lr: 0.000000  loss: 1.9291 (1.7901)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5507 (8.8802)  time: 0.8468 (0.5281 -- 4.3879)  data: 0.3005 (0.0004 -- 3.8423)  max mem: 16413
[2023-08-30 06:36:50,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18610
[2023-08-30 06:36:50,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18610
[2023-08-30 06:36:50,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:36:50,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:36:50,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [116]  [ 60/160]  eta: 0:01:32  lr: 0.000006  min_lr: 0.000000  loss: 1.5758 (1.7383)  loss_scale: 8192.0000 (15443.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8754 (8.8107)  time: 0.7937 (0.5291 -- 2.3537)  data: 0.2425 (0.0005 -- 1.8023)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:12  lr: 0.000006  min_lr: 0.000000  loss: 1.4760 (1.7231)  loss_scale: 8192.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7414 (8.8339)  time: 0.8467 (0.5337 -- 3.5267)  data: 0.0993 (0.0004 -- 1.7179)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.4867 (1.7080)  loss_scale: 8192.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0247 (8.8030)  time: 0.9691 (0.5126 -- 2.8402)  data: 0.1610 (0.0003 -- 1.4342)  max mem: 16413
[2023-08-30 06:37:36,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18664
[2023-08-30 06:37:36,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18664
[2023-08-30 06:37:36,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 06:37:36,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 06:37:36,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [116]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.5326 (1.7032)  loss_scale: 4096.0000 (11272.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8112 (8.7685)  time: 0.8291 (0.5070 -- 2.6578)  data: 0.0257 (0.0002 -- 0.4809)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.4232 (1.6795)  loss_scale: 4096.0000 (10254.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1699 (8.7819)  time: 0.9072 (0.5337 -- 2.1412)  data: 0.0424 (0.0003 -- 0.8192)  max mem: 16413
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7628 (1.6864)  loss_scale: 4096.0000 (9523.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7242 (8.7646)  time: 0.7048 (0.4966 -- 2.7221)  data: 0.1282 (0.0003 -- 2.2220)  max mem: 16413
Epoch: [116] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7628 (1.6973)  loss_scale: 4096.0000 (9523.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7242 (8.7646)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3095 (0.3095)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2702 (2.2702 -- 2.2702)  data: 2.0444 (2.0444 -- 2.0444)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4117 (0.7379)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4268 (0.1951 -- 2.2702)  data: 0.2058 (0.0004 -- 2.0444)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4707 (0.6649)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2227 (0.1707 -- 0.4016)  data: 0.0112 (0.0001 -- 0.2084)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6010 (0.7208)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2061 (0.1344 -- 0.4016)  data: 0.0109 (0.0001 -- 0.2084)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 81.950 Acc@5 96.680 loss 0.674
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 83.61%
Epoch: [117]  [  0/160]  eta: 0:22:19  lr: 0.000006  min_lr: 0.000000  loss: 0.9441 (0.9441)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9599 (8.9599)  time: 8.3745 (8.3745 -- 8.3745)  data: 5.8928 (5.8928 -- 5.8928)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:44  lr: 0.000006  min_lr: 0.000000  loss: 1.6961 (1.6759)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2248 (8.6195)  time: 0.8177 (0.5123 -- 2.7273)  data: 0.0648 (0.0003 -- 0.6306)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:11  lr: 0.000006  min_lr: 0.000000  loss: 1.7852 (1.7014)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0914 (9.1927)  time: 1.0038 (0.5266 -- 4.0631)  data: 0.0013 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8004 (1.7468)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3021 (9.1828)  time: 0.7241 (0.5264 -- 3.3565)  data: 0.0016 (0.0002 -- 0.0027)  max mem: 16413
[2023-08-30 06:39:43,111] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:39:43,111] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 06:39:43,117] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:39:43,117] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [117]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.5192 (1.7128)  loss_scale: 4096.0000 (4500.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1577 (9.0667)  time: 0.9069 (0.5207 -- 4.1443)  data: 0.0020 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.6160 (1.6915)  loss_scale: 8192.0000 (5231.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5476 (8.8074)  time: 0.7857 (0.5197 -- 2.5930)  data: 0.0018 (0.0007 -- 0.0041)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8513 (1.7219)  loss_scale: 8192.0000 (5720.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6342 (9.0330)  time: 0.9134 (0.5144 -- 2.9986)  data: 0.0149 (0.0004 -- 0.2698)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.8122 (1.7245)  loss_scale: 8192.0000 (6071.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3498 (9.1307)  time: 0.8620 (0.5329 -- 2.3267)  data: 0.0906 (0.0003 -- 1.7864)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6820 (1.7104)  loss_scale: 8192.0000 (6323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2002 (9.5637)  time: 0.6713 (0.4952 -- 2.1547)  data: 0.0717 (0.0002 -- 1.4240)  max mem: 16413
Epoch: [117] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6820 (1.7204)  loss_scale: 8192.0000 (6323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2002 (9.5637)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3179 (0.3179)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3202 (2.3202 -- 2.3202)  data: 2.0711 (2.0711 -- 2.0711)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4173 (0.7327)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4097 (0.2077 -- 2.3202)  data: 0.1940 (0.0008 -- 2.0711)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4357 (0.6612)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2233 (0.1686 -- 0.5299)  data: 0.0207 (0.0001 -- 0.3486)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5974 (0.7177)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.4357)  time: 0.2066 (0.1329 -- 0.5299)  data: 0.0202 (0.0001 -- 0.3486)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 82.158 Acc@5 96.473 loss 0.672
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 83.61%
Epoch: [118]  [  0/160]  eta: 0:22:46  lr: 0.000006  min_lr: 0.000000  loss: 1.5538 (1.5538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5276 (8.5276)  time: 8.5436 (8.5436 -- 8.5436)  data: 5.4889 (5.4889 -- 5.4889)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:03:01  lr: 0.000006  min_lr: 0.000000  loss: 1.8064 (1.7497)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3876 (8.5800)  time: 0.9344 (0.5315 -- 3.7009)  data: 0.1700 (0.0003 -- 2.9819)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000000  loss: 1.7531 (1.7483)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9799 (8.7100)  time: 0.7968 (0.5256 -- 2.9871)  data: 0.0690 (0.0003 -- 1.0144)  max mem: 16413
[2023-08-30 06:41:43,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:41:43,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:41:43,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:41:43,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [118]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.6748 (1.7288)  loss_scale: 16384.0000 (10877.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5775 (8.8718)  time: 0.7893 (0.5332 -- 2.6466)  data: 0.0499 (0.0006 -- 0.6697)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.6508 (1.7248)  loss_scale: 16384.0000 (12237.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1573 (8.6954)  time: 0.9243 (0.5312 -- 3.5344)  data: 0.0717 (0.0004 -- 1.4020)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7295 (1.7228)  loss_scale: 16384.0000 (13058.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8305 (8.8064)  time: 0.8206 (0.5293 -- 1.8957)  data: 0.0880 (0.0005 -- 0.9621)  max mem: 16413
[2023-08-30 06:42:51,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=105, lr=[1.3756335849790365e-07, 1.3756335849790365e-07, 1.8341781133053818e-07, 1.8341781133053818e-07, 2.445570817740509e-07, 2.445570817740509e-07, 3.260761090320679e-07, 3.260761090320679e-07, 4.3476814537609053e-07, 4.3476814537609053e-07, 5.79690860501454e-07, 5.79690860501454e-07, 7.72921147335272e-07, 7.72921147335272e-07, 1.0305615297803628e-06, 1.0305615297803628e-06, 1.3740820397071504e-06, 1.3740820397071504e-06, 1.8321093862762005e-06, 1.8321093862762005e-06, 2.4428125150349337e-06, 2.4428125150349337e-06, 3.257083353379912e-06, 3.257083353379912e-06, 4.342777804506549e-06, 4.342777804506549e-06, 5.790370406008732e-06, 5.790370406008732e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 06:42:51,753] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=18.045928697487216, CurrSamplesPerSec=23.04973184326329, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8486 (1.7281)  loss_scale: 16384.0000 (13608.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1306 (8.7452)  time: 0.9449 (0.5264 -- 4.1876)  data: 0.0852 (0.0005 -- 1.6461)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7195 (1.7272)  loss_scale: 16384.0000 (14001.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1947 (8.7684)  time: 0.7615 (0.5199 -- 3.0053)  data: 0.0015 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7062 (1.7336)  loss_scale: 16384.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4653 (8.8582)  time: 0.8328 (0.4957 -- 4.8931)  data: 0.0005 (0.0002 -- 0.0012)  max mem: 16413
Epoch: [118] Total time: 0:02:24 (0.9008 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7062 (1.7317)  loss_scale: 16384.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4653 (8.8582)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3116 (0.3116)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4409 (2.4409 -- 2.4409)  data: 2.2295 (2.2295 -- 2.2295)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4087 (0.7291)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4333 (0.1977 -- 2.4409)  data: 0.2135 (0.0006 -- 2.2295)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4418 (0.6583)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2201 (0.1714 -- 0.3627)  data: 0.0121 (0.0001 -- 0.1194)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5962 (0.7136)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2038 (0.1331 -- 0.3627)  data: 0.0118 (0.0001 -- 0.1194)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.671
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [119]  [  0/160]  eta: 0:21:21  lr: 0.000006  min_lr: 0.000000  loss: 1.7761 (1.7761)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4715 (7.4715)  time: 8.0089 (8.0089 -- 8.0089)  data: 3.9153 (3.9153 -- 3.9153)  max mem: 16413
[2023-08-30 06:43:46,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:43:46,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:43:46,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:43:46,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [ 20/160]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000000  loss: 1.6943 (1.6532)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2075 (8.1543)  time: 0.8243 (0.5338 -- 2.3101)  data: 0.1234 (0.0006 -- 1.2591)  max mem: 16413
[2023-08-30 06:44:01,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19065
[2023-08-30 06:44:01,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19065
[2023-08-30 06:44:01,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:44:01,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:44:01,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [ 40/160]  eta: 0:02:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7570 (1.7089)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3025 (8.5848)  time: 0.8343 (0.5363 -- 1.9292)  data: 0.0729 (0.0004 -- 0.9728)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000000  loss: 1.7481 (1.7048)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3898 (8.5256)  time: 0.8492 (0.5212 -- 3.8910)  data: 0.1891 (0.0004 -- 3.3797)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.5404 (1.6837)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0866 (8.7090)  time: 0.8871 (0.5070 -- 4.5380)  data: 0.0395 (0.0002 -- 0.7561)  max mem: 16413
[2023-08-30 06:44:50,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19125
[2023-08-30 06:44:50,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19125
[2023-08-30 06:44:50,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:44:50,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:44:50,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [119]  [100/160]  eta: 0:00:54  lr: 0.000006  min_lr: 0.000000  loss: 1.8651 (1.7037)  loss_scale: 8192.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1941 (8.5380)  time: 0.8039 (0.5178 -- 2.2697)  data: 0.1137 (0.0005 -- 1.0615)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.5849 (1.6913)  loss_scale: 8192.0000 (16113.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3805 (8.5422)  time: 0.9329 (0.5168 -- 2.7974)  data: 0.0422 (0.0003 -- 0.8181)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7400 (1.7113)  loss_scale: 8192.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3921 (8.6114)  time: 0.8797 (0.5290 -- 3.0583)  data: 0.0019 (0.0004 -- 0.0090)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5719 (1.6978)  loss_scale: 8192.0000 (14182.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4081 (8.6987)  time: 0.6967 (0.4983 -- 1.8918)  data: 0.0008 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [119] Total time: 0:02:21 (0.8854 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5719 (1.7105)  loss_scale: 8192.0000 (14182.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4081 (8.6987)
[2023-08-30 06:45:53,135] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-08-30 06:45:53,136] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-08-30 06:45:53,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-08-30 06:45:53,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-08-30 06:45:54,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-08-30 06:45:54,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3254 (0.3254)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5249 (2.5249 -- 2.5249)  data: 2.2930 (2.2930 -- 2.2930)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4098 (0.7280)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4284 (0.2060 -- 2.5249)  data: 0.2094 (0.0006 -- 2.2930)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4459 (0.6584)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2122 (0.1691 -- 0.2378)  data: 0.0030 (0.0001 -- 0.0297)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5993 (0.7117)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.4357)  time: 0.1963 (0.1330 -- 0.2349)  data: 0.0027 (0.0001 -- 0.0297)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 82.365 Acc@5 96.473 loss 0.673
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [120]  [  0/160]  eta: 0:19:59  lr: 0.000005  min_lr: 0.000000  loss: 1.9482 (1.9482)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5342 (7.5342)  time: 7.4964 (7.4964 -- 7.4964)  data: 6.0606 (6.0606 -- 6.0606)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:45  lr: 0.000005  min_lr: 0.000000  loss: 1.8662 (1.8004)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7364 (9.3940)  time: 0.8653 (0.5074 -- 3.7842)  data: 0.1424 (0.0004 -- 1.7964)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:05  lr: 0.000005  min_lr: 0.000000  loss: 1.6894 (1.7748)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6870 (9.1764)  time: 0.9030 (0.5337 -- 2.7663)  data: 0.0247 (0.0003 -- 0.4684)  max mem: 16413
[2023-08-30 06:46:55,786] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:46:55,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:46:55,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:46:55,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [120]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000000  loss: 1.7348 (1.7945)  loss_scale: 8192.0000 (9132.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6264 (9.1420)  time: 0.9241 (0.5298 -- 4.1936)  data: 0.0222 (0.0004 -- 0.4158)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.6018 (1.7754)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5374 (8.8165)  time: 0.7397 (0.5186 -- 2.2540)  data: 0.0067 (0.0003 -- 0.1003)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.9061 (1.7963)  loss_scale: 16384.0000 (12004.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9613 (8.8704)  time: 0.9429 (0.5400 -- 3.3350)  data: 0.0635 (0.0004 -- 1.2345)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6264 (1.7697)  loss_scale: 16384.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1152 (8.8210)  time: 0.8393 (0.5421 -- 3.0139)  data: 0.1869 (0.0006 -- 2.4818)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.4577 (1.7335)  loss_scale: 16384.0000 (13246.6383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9677 (8.8470)  time: 0.8584 (0.5316 -- 4.0546)  data: 0.3085 (0.0003 -- 3.4903)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5568 (1.7181)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2112 (8.8166)  time: 0.7098 (0.4981 -- 2.7622)  data: 0.0783 (0.0002 -- 0.5944)  max mem: 16413
Epoch: [120] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5568 (1.7034)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2112 (8.8166)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3039 (0.3039)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4000 (2.4000 -- 2.4000)  data: 2.1858 (2.1858 -- 2.1858)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4149 (0.7229)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4244 (0.1937 -- 2.4000)  data: 0.2052 (0.0007 -- 2.1858)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4352 (0.6506)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2199 (0.1713 -- 0.4023)  data: 0.0149 (0.0001 -- 0.2242)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6018 (0.7061)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2020 (0.1328 -- 0.4023)  data: 0.0146 (0.0001 -- 0.2242)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 81.950 Acc@5 96.473 loss 0.667
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 83.61%
Epoch: [121]  [  0/160]  eta: 0:22:56  lr: 0.000005  min_lr: 0.000000  loss: 1.3180 (1.3180)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.9639 (14.9639)  time: 8.6007 (8.6007 -- 8.6007)  data: 5.6342 (5.6342 -- 5.6342)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000000  loss: 1.8011 (1.8631)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6880 (8.7461)  time: 0.8241 (0.5291 -- 4.7976)  data: 0.0389 (0.0004 -- 0.7490)  max mem: 16413
[2023-08-30 06:48:58,396] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:48:58,397] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 06:48:58,397] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:48:58,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [121]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 1.4869 (1.7315)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9244 (8.2362)  time: 0.8561 (0.5313 -- 3.0287)  data: 0.0323 (0.0002 -- 0.6221)  max mem: 16413
Epoch: [121]  [ 60/160]  eta: 0:01:34  lr: 0.000005  min_lr: 0.000000  loss: 1.8349 (1.7502)  loss_scale: 32768.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5719 (8.1696)  time: 0.7783 (0.5410 -- 2.2308)  data: 0.0662 (0.0003 -- 0.7740)  max mem: 16413
[2023-08-30 06:49:48,144] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19440
[2023-08-30 06:49:48,144] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19440
[2023-08-30 06:49:48,144] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:49:48,144] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 06:49:48,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [121]  [ 80/160]  eta: 0:01:14  lr: 0.000005  min_lr: 0.000000  loss: 1.5156 (1.6936)  loss_scale: 32768.0000 (28115.7531)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3259 (8.4651)  time: 0.9056 (0.5420 -- 3.4019)  data: 0.2851 (0.0004 -- 2.8453)  max mem: 16413
[2023-08-30 06:50:04,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19460
[2023-08-30 06:50:04,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19460
[2023-08-30 06:50:04,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:50:04,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:50:04,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [121]  [100/160]  eta: 0:00:54  lr: 0.000005  min_lr: 0.000000  loss: 1.6040 (1.6830)  loss_scale: 16384.0000 (25711.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5568 (8.7524)  time: 0.8210 (0.5207 -- 2.8847)  data: 0.2091 (0.0002 -- 2.3634)  max mem: 16413
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8552 (1.7028)  loss_scale: 8192.0000 (22815.7355)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5546 (8.8060)  time: 0.8710 (0.5235 -- 3.4366)  data: 0.2289 (0.0004 -- 2.8777)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.5571 (1.6921)  loss_scale: 8192.0000 (20741.4468)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8305 (8.9067)  time: 0.9155 (0.5305 -- 4.1328)  data: 0.3641 (0.0002 -- 3.5900)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.3682 (1.6772)  loss_scale: 8192.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1021 (8.9235)  time: 0.6858 (0.4982 -- 3.7161)  data: 0.1664 (0.0002 -- 3.1980)  max mem: 16413
Epoch: [121] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.3682 (1.6753)  loss_scale: 8192.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1021 (8.9235)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3149 (0.3149)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3873 (2.3873 -- 2.3873)  data: 2.1834 (2.1834 -- 2.1834)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3933 (0.7303)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4158 (0.1954 -- 2.3873)  data: 0.2062 (0.0003 -- 2.1834)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4459 (0.6585)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1697 -- 0.4281)  data: 0.0167 (0.0001 -- 0.2458)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6023 (0.7124)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.4357)  time: 0.2056 (0.1332 -- 0.4281)  data: 0.0164 (0.0001 -- 0.2458)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 82.780 Acc@5 96.266 loss 0.672
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.61%
Epoch: [122]  [  0/160]  eta: 0:21:58  lr: 0.000005  min_lr: 0.000000  loss: 1.4647 (1.4647)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1313 (5.1313)  time: 8.2430 (8.2430 -- 8.2430)  data: 7.7158 (7.7158 -- 7.7158)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:39  lr: 0.000005  min_lr: 0.000000  loss: 1.6842 (1.6363)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5822 (9.2301)  time: 0.7876 (0.5368 -- 3.0302)  data: 0.2141 (0.0002 -- 2.5081)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8857 (1.7236)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5719 (9.3736)  time: 0.8599 (0.5263 -- 3.1879)  data: 0.3148 (0.0002 -- 2.6470)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 1.5499 (1.7156)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7166 (9.1890)  time: 0.9645 (0.5357 -- 3.5606)  data: 0.4115 (0.0006 -- 3.0286)  max mem: 16413
[2023-08-30 06:52:10,091] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:52:10,092] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:52:10,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:52:10,092] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.5322 (1.7054)  loss_scale: 16384.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0213 (9.2301)  time: 0.8290 (0.5231 -- 3.7861)  data: 0.2844 (0.0002 -- 3.2446)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.6264 (1.6876)  loss_scale: 16384.0000 (10787.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6525 (9.3546)  time: 0.9155 (0.5132 -- 4.2857)  data: 0.3770 (0.0002 -- 3.7536)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:35  lr: 0.000005  min_lr: 0.000000  loss: 1.8124 (1.7020)  loss_scale: 16384.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0136 (9.4456)  time: 0.6683 (0.5314 -- 1.4220)  data: 0.0421 (0.0003 -- 0.8007)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7791 (1.7025)  loss_scale: 16384.0000 (12375.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1304 (9.3546)  time: 0.9386 (0.5198 -- 3.1220)  data: 0.0072 (0.0005 -- 0.1053)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5042 (1.7016)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6891 (9.3061)  time: 0.7987 (0.4969 -- 2.8586)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [122] Total time: 0:02:20 (0.8791 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5042 (1.7148)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6891 (9.3061)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3019 (0.3019)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4207 (2.4207 -- 2.4207)  data: 2.1928 (2.1928 -- 2.1928)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4053 (0.7309)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4240 (0.1993 -- 2.4207)  data: 0.2109 (0.0005 -- 2.1928)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4278 (0.6597)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2203 (0.1711 -- 0.3946)  data: 0.0171 (0.0001 -- 0.2115)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6013 (0.7111)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.8506)  time: 0.2069 (0.1331 -- 0.3946)  data: 0.0168 (0.0001 -- 0.2115)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.573 Acc@5 96.680 loss 0.667
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [123]  [  0/160]  eta: 0:19:16  lr: 0.000005  min_lr: 0.000000  loss: 1.6890 (1.6890)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1884 (9.1884)  time: 7.2274 (7.2274 -- 7.2274)  data: 6.6507 (6.6507 -- 6.6507)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:35  lr: 0.000005  min_lr: 0.000000  loss: 1.6971 (1.7774)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8984 (8.9532)  time: 0.8066 (0.5305 -- 2.1694)  data: 0.1627 (0.0004 -- 1.2289)  max mem: 16413
[2023-08-30 06:54:02,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19712
[2023-08-30 06:54:02,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19712
[2023-08-30 06:54:02,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:54:02,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:54:02,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [123]  [ 40/160]  eta: 0:01:59  lr: 0.000005  min_lr: 0.000000  loss: 1.7637 (1.7906)  loss_scale: 16384.0000 (14585.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1225 (8.8757)  time: 0.8713 (0.5333 -- 3.7138)  data: 0.2837 (0.0004 -- 3.2018)  max mem: 16413
Epoch: [123]  [ 60/160]  eta: 0:01:38  lr: 0.000005  min_lr: 0.000000  loss: 1.8607 (1.8096)  loss_scale: 8192.0000 (12489.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2256 (9.1378)  time: 0.9578 (0.5259 -- 3.8405)  data: 0.4022 (0.0004 -- 3.2948)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:13  lr: 0.000005  min_lr: 0.000000  loss: 1.5075 (1.7342)  loss_scale: 8192.0000 (11428.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8040 (9.3714)  time: 0.7148 (0.5380 -- 1.8822)  data: 0.1190 (0.0003 -- 1.2947)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.6865 (1.7332)  loss_scale: 8192.0000 (10787.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6955 (9.1692)  time: 0.9528 (0.5206 -- 3.2293)  data: 0.3533 (0.0007 -- 2.7115)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:35  lr: 0.000005  min_lr: 0.000000  loss: 1.7036 (1.7334)  loss_scale: 8192.0000 (10358.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2949 (9.1413)  time: 0.7165 (0.5271 -- 2.4750)  data: 0.1570 (0.0009 -- 1.9379)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6084 (1.7077)  loss_scale: 8192.0000 (10051.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3228 (9.2036)  time: 1.0489 (0.5169 -- 5.2656)  data: 0.5039 (0.0002 -- 4.7456)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5301 (1.6821)  loss_scale: 8192.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7977 (9.1328)  time: 0.6226 (0.4976 -- 1.8184)  data: 0.0935 (0.0003 -- 1.2674)  max mem: 16413
Epoch: [123] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5301 (1.6902)  loss_scale: 8192.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7977 (9.1328)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3030 (0.3030)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3617 (2.3617 -- 2.3617)  data: 2.1507 (2.1507 -- 2.1507)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3946 (0.7245)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4086 (0.2011 -- 2.3617)  data: 0.1965 (0.0004 -- 2.1507)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4291 (0.6585)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2154 (0.1727 -- 0.3394)  data: 0.0098 (0.0001 -- 0.1400)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5992 (0.7099)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.2656)  time: 0.2036 (0.1334 -- 0.3394)  data: 0.0095 (0.0001 -- 0.1400)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.667
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [124]  [  0/160]  eta: 0:18:29  lr: 0.000005  min_lr: 0.000000  loss: 2.3192 (2.3192)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4649 (6.4649)  time: 6.9321 (6.9321 -- 6.9321)  data: 6.4007 (6.4007 -- 6.4007)  max mem: 16413
[2023-08-30 06:56:06,101] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:56:06,101] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:56:06,101] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:56:06,101] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:56:21,910] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19856
[2023-08-30 06:56:21,910] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19856
[2023-08-30 06:56:21,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:56:21,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 06:56:21,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [124]  [ 20/160]  eta: 0:02:56  lr: 0.000004  min_lr: 0.000000  loss: 1.8161 (1.8490)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2230 (8.1489)  time: 0.9775 (0.5263 -- 4.1288)  data: 0.3463 (0.0002 -- 3.6090)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:15  lr: 0.000004  min_lr: 0.000000  loss: 1.5848 (1.7711)  loss_scale: 8192.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5100 (8.1329)  time: 0.9895 (0.5278 -- 4.1981)  data: 0.2572 (0.0003 -- 3.6805)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000000  loss: 1.5002 (1.7259)  loss_scale: 8192.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4605 (8.3495)  time: 0.7388 (0.5201 -- 2.5197)  data: 0.1914 (0.0002 -- 1.9538)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000000  loss: 1.6677 (1.7174)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5155 (8.5697)  time: 0.8735 (0.5343 -- 3.6682)  data: 0.2778 (0.0004 -- 3.1338)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.4575 (1.6928)  loss_scale: 8192.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0966 (8.4744)  time: 0.7610 (0.5366 -- 1.6862)  data: 0.1513 (0.0009 -- 1.1576)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.7338 (1.7029)  loss_scale: 8192.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0644 (8.5341)  time: 0.9529 (0.5340 -- 3.2903)  data: 0.1181 (0.0005 -- 2.3346)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8546 (1.7185)  loss_scale: 8192.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4275 (8.6137)  time: 0.7986 (0.5281 -- 2.9834)  data: 0.0019 (0.0003 -- 0.0052)  max mem: 16413
[2023-08-30 06:58:12,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:58:12,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 06:58:12,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:58:12,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 06:58:21,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=111, lr=[1.0188849906529907e-07, 1.0188849906529907e-07, 1.3585133208706543e-07, 1.3585133208706543e-07, 1.8113510944942058e-07, 1.8113510944942058e-07, 2.415134792658941e-07, 2.415134792658941e-07, 3.2201797235452545e-07, 3.2201797235452545e-07, 4.293572964727006e-07, 4.293572964727006e-07, 5.724763952969341e-07, 5.724763952969341e-07, 7.633018603959122e-07, 7.633018603959122e-07, 1.0177358138612163e-06, 1.0177358138612163e-06, 1.3569810851482883e-06, 1.3569810851482883e-06, 1.8093081135310512e-06, 1.8093081135310512e-06, 2.4124108180414017e-06, 2.4124108180414017e-06, 3.2165477573885357e-06, 3.2165477573885357e-06, 4.288730343184714e-06, 4.288730343184714e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 06:58:21,433] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=18.03350894297922, CurrSamplesPerSec=24.067365376835834, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.5471 (1.6938)  loss_scale: 16384.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8353 (8.5913)  time: 0.7489 (0.4969 -- 2.0819)  data: 0.0443 (0.0001 -- 0.8687)  max mem: 16413
Epoch: [124] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.5471 (1.7044)  loss_scale: 16384.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8353 (8.5913)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3231 (0.3231)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4105 (2.4105 -- 2.4105)  data: 2.2018 (2.2018 -- 2.2018)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4074 (0.7185)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4218 (0.2025 -- 2.4105)  data: 0.2090 (0.0007 -- 2.2018)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4825 (0.6557)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2139 (0.1691 -- 0.2958)  data: 0.0094 (0.0001 -- 0.0888)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5964 (0.7063)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.1988 (0.1329 -- 0.2958)  data: 0.0091 (0.0001 -- 0.0888)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.666
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [125]  [  0/160]  eta: 0:17:34  lr: 0.000004  min_lr: 0.000000  loss: 0.9760 (0.9760)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0390 (7.0390)  time: 6.5896 (6.5896 -- 6.5896)  data: 6.0307 (6.0307 -- 6.0307)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:29  lr: 0.000004  min_lr: 0.000000  loss: 1.6724 (1.7148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8273 (8.5970)  time: 0.7943 (0.5344 -- 2.8571)  data: 0.2193 (0.0004 -- 2.2712)  max mem: 16413
Epoch: [125]  [ 40/160]  eta: 0:02:04  lr: 0.000004  min_lr: 0.000000  loss: 1.4667 (1.6331)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2199 (8.4101)  time: 0.9996 (0.5385 -- 3.9355)  data: 0.1753 (0.0004 -- 2.0630)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7488 (1.6671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2921 (8.7190)  time: 0.8108 (0.5414 -- 3.2220)  data: 0.0223 (0.0005 -- 0.4034)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.6419 (1.6683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2721 (8.9091)  time: 0.8878 (0.5195 -- 2.9871)  data: 0.1404 (0.0002 -- 1.5414)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.8425 (1.7079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5073 (9.0640)  time: 0.9032 (0.5255 -- 4.2169)  data: 0.0013 (0.0002 -- 0.0045)  max mem: 16413
[2023-08-30 07:00:16,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:00:16,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:00:16,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:00:16,853] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [125]  [120/160]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000000  loss: 1.8331 (1.7261)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2856 (9.0593)  time: 1.0225 (0.5115 -- 4.2973)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
[2023-08-30 07:00:26,411] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20124
[2023-08-30 07:00:26,411] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:00:26,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 07:00:26,411] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20124
[2023-08-30 07:00:26,411] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.9229 (1.7418)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5288 (8.9682)  time: 0.6922 (0.5139 -- 2.1777)  data: 0.0024 (0.0003 -- 0.0160)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7267 (1.7413)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5701 (8.9612)  time: 0.6928 (0.4971 -- 3.5330)  data: 0.0010 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [125] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7267 (1.7468)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5701 (8.9612)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3053 (0.3053)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4795 (2.4795 -- 2.4795)  data: 2.2470 (2.2470 -- 2.2470)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4061 (0.7241)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4250 (0.1979 -- 2.4795)  data: 0.2058 (0.0007 -- 2.2470)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4483 (0.6603)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2084 (0.1706 -- 0.2672)  data: 0.0016 (0.0001 -- 0.0124)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6052 (0.7104)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.1923 (0.1342 -- 0.2444)  data: 0.0013 (0.0001 -- 0.0124)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 82.158 Acc@5 96.680 loss 0.668
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 83.61%
Epoch: [126]  [  0/160]  eta: 0:19:57  lr: 0.000004  min_lr: 0.000000  loss: 1.6694 (1.6694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4812 (7.4812)  time: 7.4820 (7.4820 -- 7.4820)  data: 6.8976 (6.8976 -- 6.8976)  max mem: 16413
Epoch: [126]  [ 20/160]  eta: 0:02:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8080 (1.7458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6567 (8.4433)  time: 0.8055 (0.5170 -- 3.5791)  data: 0.1894 (0.0002 -- 3.0721)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:01:59  lr: 0.000004  min_lr: 0.000000  loss: 1.7908 (1.7600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4942 (8.5785)  time: 0.8702 (0.5243 -- 2.4440)  data: 0.3216 (0.0004 -- 1.8881)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7226 (1.7523)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6795 (8.5440)  time: 0.8986 (0.5233 -- 2.3752)  data: 0.2787 (0.0002 -- 1.8570)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:14  lr: 0.000004  min_lr: 0.000000  loss: 1.7613 (1.7586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4633 (8.4997)  time: 0.8121 (0.5256 -- 2.1691)  data: 0.2658 (0.0002 -- 1.6422)  max mem: 16413
[2023-08-30 07:02:27,746] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:02:27,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:02:27,746] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:02:27,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.5652 (1.7479)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8618 (8.7088)  time: 0.9736 (0.5344 -- 3.2370)  data: 0.1224 (0.0003 -- 1.2406)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 2.0043 (1.7904)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7743 (8.6041)  time: 0.7795 (0.5223 -- 2.4973)  data: 0.1368 (0.0002 -- 1.9830)  max mem: 16413
[2023-08-30 07:02:51,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20282
[2023-08-30 07:02:51,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20282
[2023-08-30 07:02:51,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:02:51,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:02:51,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7226 (1.7851)  loss_scale: 16384.0000 (19753.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3430 (8.7111)  time: 0.9165 (0.5373 -- 4.8601)  data: 0.0016 (0.0004 -- 0.0059)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.5684 (1.7688)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2202 (8.6513)  time: 0.6606 (0.4958 -- 2.1077)  data: 0.0009 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [126] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.5684 (1.7351)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2202 (8.6513)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3044 (0.3044)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4814 (2.4814 -- 2.4814)  data: 2.2729 (2.2729 -- 2.2729)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4029 (0.7255)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4112 (0.1840 -- 2.4814)  data: 0.2074 (0.0004 -- 2.2729)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4612 (0.6589)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2145 (0.1699 -- 0.4380)  data: 0.0125 (0.0001 -- 0.2382)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5936 (0.7094)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (96.2656)  time: 0.2020 (0.1330 -- 0.4380)  data: 0.0123 (0.0001 -- 0.2382)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.666
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [127]  [  0/160]  eta: 0:18:20  lr: 0.000004  min_lr: 0.000000  loss: 1.9831 (1.9831)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5874 (6.5874)  time: 6.8805 (6.8805 -- 6.8805)  data: 5.9818 (5.9818 -- 5.9818)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:41  lr: 0.000004  min_lr: 0.000000  loss: 1.5799 (1.6483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5214 (8.7368)  time: 0.8646 (0.5168 -- 2.6767)  data: 0.2112 (0.0004 -- 2.1525)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:05  lr: 0.000004  min_lr: 0.000000  loss: 1.6657 (1.6571)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6198 (8.9040)  time: 0.9298 (0.5197 -- 3.8938)  data: 0.3906 (0.0002 -- 3.3478)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000000  loss: 1.7737 (1.7111)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7808 (9.0809)  time: 0.9253 (0.5163 -- 3.1124)  data: 0.1768 (0.0003 -- 1.8430)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.8106 (1.7142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1674 (9.0346)  time: 0.7605 (0.5387 -- 2.8982)  data: 0.0482 (0.0002 -- 0.9333)  max mem: 16413
[2023-08-30 07:04:54,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:04:54,268] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:04:54,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:04:54,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8214 (1.7296)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5248 (9.0193)  time: 0.8507 (0.5266 -- 2.4176)  data: 0.0905 (0.0009 -- 0.9104)  max mem: 16413
[2023-08-30 07:05:02,772] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20422
[2023-08-30 07:05:02,772] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20422
[2023-08-30 07:05:02,773] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:05:02,773] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:05:02,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [127]  [120/160]  eta: 0:00:35  lr: 0.000004  min_lr: 0.000000  loss: 1.6870 (1.7174)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9845 (9.0105)  time: 0.7679 (0.5305 -- 2.8915)  data: 0.0733 (0.0004 -- 1.2255)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7193 (1.7077)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0884 (9.0357)  time: 1.0196 (0.5288 -- 3.7615)  data: 0.3087 (0.0006 -- 3.2264)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.3758 (1.7040)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0997 (8.9922)  time: 0.6639 (0.4973 -- 2.3437)  data: 0.1263 (0.0003 -- 1.8520)  max mem: 16413
Epoch: [127] Total time: 0:02:21 (0.8875 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.3758 (1.6874)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0997 (8.9922)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3088 (0.3088)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3404 (2.3404 -- 2.3404)  data: 2.0962 (2.0962 -- 2.0962)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4136 (0.7290)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4233 (0.2069 -- 2.3404)  data: 0.2064 (0.0006 -- 2.0962)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4496 (0.6605)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1696 -- 0.4034)  data: 0.0170 (0.0001 -- 0.1646)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5921 (0.7116)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2070 (0.1334 -- 0.4034)  data: 0.0166 (0.0001 -- 0.1646)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.669
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [128]  [  0/160]  eta: 0:20:21  lr: 0.000004  min_lr: 0.000000  loss: 1.7303 (1.7303)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6380 (8.6380)  time: 7.6315 (7.6315 -- 7.6315)  data: 7.0376 (7.0376 -- 7.0376)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000000  loss: 1.6619 (1.6759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8234 (9.0920)  time: 0.8099 (0.5191 -- 3.0664)  data: 0.2159 (0.0004 -- 2.5432)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:05  lr: 0.000004  min_lr: 0.000000  loss: 1.8494 (1.7739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3095 (9.6085)  time: 0.9473 (0.5193 -- 3.9053)  data: 0.3518 (0.0004 -- 3.3626)  max mem: 16413
Epoch: [128]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000000  loss: 1.5314 (1.6983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9801 (9.0244)  time: 0.9455 (0.5115 -- 5.0570)  data: 0.0639 (0.0003 -- 1.2382)  max mem: 16413
[2023-08-30 07:07:08,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:07:08,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:07:08,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:07:08,681] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.6476 (1.6920)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2627 (9.0223)  time: 0.7474 (0.5199 -- 3.6517)  data: 0.0017 (0.0001 -- 0.0151)  max mem: 16413
[2023-08-30 07:07:19,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20563
[2023-08-30 07:07:19,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20563
[2023-08-30 07:07:19,542] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:07:19,542] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:07:19,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.5192 (1.6719)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5179 (8.8854)  time: 0.9556 (0.5348 -- 3.7284)  data: 0.0018 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6580 (1.6671)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1465 (8.7796)  time: 0.7494 (0.5321 -- 2.8130)  data: 0.0016 (0.0004 -- 0.0036)  max mem: 16413
[2023-08-30 07:07:53,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20607
[2023-08-30 07:07:53,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20607
[2023-08-30 07:07:53,981] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:07:53,981] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:07:53,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.4631 (1.6594)  loss_scale: 8192.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9725 (8.9145)  time: 0.8683 (0.5147 -- 3.2404)  data: 0.0020 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5862 (1.6627)  loss_scale: 8192.0000 (15923.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8018 (8.9811)  time: 0.7427 (0.4936 -- 3.5317)  data: 0.0009 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [128] Total time: 0:02:22 (0.8899 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5862 (1.6669)  loss_scale: 8192.0000 (15923.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8018 (8.9811)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3108 (0.3108)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4149 (2.4149 -- 2.4149)  data: 2.1815 (2.1815 -- 2.1815)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4172 (0.7275)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4200 (0.2017 -- 2.4149)  data: 0.2011 (0.0005 -- 2.1815)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4325 (0.6587)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2195 (0.1703 -- 0.3126)  data: 0.0114 (0.0001 -- 0.1273)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5962 (0.7093)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2015 (0.1326 -- 0.3126)  data: 0.0109 (0.0001 -- 0.1273)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.666
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [129]  [  0/160]  eta: 0:23:07  lr: 0.000003  min_lr: 0.000000  loss: 1.7977 (1.7977)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8145 (9.8145)  time: 8.6738 (8.6738 -- 8.6738)  data: 6.8769 (6.8769 -- 6.8769)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:34  lr: 0.000003  min_lr: 0.000000  loss: 1.9060 (1.7354)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1710 (9.1091)  time: 0.7273 (0.5317 -- 1.5335)  data: 0.0095 (0.0004 -- 0.1617)  max mem: 16413
Epoch: [129]  [ 40/160]  eta: 0:01:59  lr: 0.000003  min_lr: 0.000000  loss: 1.5850 (1.6576)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1049 (8.7765)  time: 0.8856 (0.5358 -- 3.3480)  data: 0.0022 (0.0003 -- 0.0133)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 1.7032 (1.6668)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8098 (8.8659)  time: 0.8703 (0.5298 -- 1.9781)  data: 0.0043 (0.0001 -- 0.0537)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7516 (1.7056)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9832 (8.9499)  time: 0.9066 (0.5251 -- 2.8200)  data: 0.1257 (0.0003 -- 2.2663)  max mem: 16413
[2023-08-30 07:09:57,988] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:09:57,988] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 07:09:57,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:09:57,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [129]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.8191 (1.7304)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7294 (8.8334)  time: 0.8213 (0.5289 -- 2.7960)  data: 0.1026 (0.0003 -- 1.2381)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.5300 (1.7051)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8827 (8.8998)  time: 1.0238 (0.5170 -- 4.1385)  data: 0.4786 (0.0001 -- 3.6118)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7241 (1.7096)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3714 (8.8149)  time: 0.7187 (0.5317 -- 2.8956)  data: 0.1584 (0.0001 -- 2.3311)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6498 (1.7151)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5627 (8.7180)  time: 0.7040 (0.4962 -- 3.5490)  data: 0.1794 (0.0002 -- 3.0128)  max mem: 16413
Epoch: [129] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6498 (1.7193)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5627 (8.7180)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3070 (0.3070)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4016 (2.4016 -- 2.4016)  data: 2.1318 (2.1318 -- 2.1318)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4303 (0.7253)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4195 (0.1916 -- 2.4016)  data: 0.1949 (0.0009 -- 2.1318)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4539 (0.6580)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2178 (0.1693 -- 0.3968)  data: 0.0111 (0.0001 -- 0.2066)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5903 (0.7064)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.1992 (0.1333 -- 0.3968)  data: 0.0107 (0.0001 -- 0.2066)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.665
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.61%
Epoch: [130]  [  0/160]  eta: 0:18:10  lr: 0.000003  min_lr: 0.000000  loss: 1.8749 (1.8749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.3673 (15.3673)  time: 6.8126 (6.8126 -- 6.8126)  data: 6.2477 (6.2477 -- 6.2477)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:41  lr: 0.000003  min_lr: 0.000000  loss: 1.9324 (1.8220)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5230 (9.0729)  time: 0.8728 (0.5203 -- 2.6562)  data: 0.2215 (0.0005 -- 2.1396)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7780 (1.7621)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7649 (9.2010)  time: 0.8391 (0.5267 -- 2.7391)  data: 0.0923 (0.0004 -- 1.4642)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 1.6689 (1.7182)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6516 (8.9179)  time: 0.9404 (0.5362 -- 2.8042)  data: 0.1016 (0.0004 -- 1.8931)  max mem: 16413
[2023-08-30 07:11:59,323] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:11:59,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:11:59,324] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:11:59,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:12:08,785] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20873
[2023-08-30 07:12:08,785] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20873
[2023-08-30 07:12:08,785] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:12:08,785] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:12:08,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [130]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000000  loss: 1.7838 (1.7466)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5747 (8.8991)  time: 0.7744 (0.5308 -- 4.6998)  data: 0.0012 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.9336 (1.7565)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8451 (8.8075)  time: 0.9468 (0.5298 -- 3.7811)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6913 (1.7540)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7960 (8.8940)  time: 0.8643 (0.5241 -- 4.1821)  data: 0.0017 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.4598 (1.7268)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0205 (9.0202)  time: 0.9245 (0.5365 -- 3.4969)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6738 (1.7264)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4262 (9.0960)  time: 0.6697 (0.4963 -- 2.0477)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [130] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6738 (1.6934)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4262 (9.0960)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.3137 (0.3137)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1332 (2.1332 -- 2.1332)  data: 1.8932 (1.8932 -- 1.8932)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4083 (0.7162)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4110 (0.1967 -- 2.1332)  data: 0.1984 (0.0010 -- 1.8932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4414 (0.6513)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2299 (0.1693 -- 0.4811)  data: 0.0286 (0.0001 -- 0.2721)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5912 (0.7003)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2165 (0.1329 -- 0.4811)  data: 0.0281 (0.0001 -- 0.2721)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [131]  [  0/160]  eta: 0:19:20  lr: 0.000003  min_lr: 0.000000  loss: 2.2691 (2.2691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2446 (9.2446)  time: 7.2548 (7.2548 -- 7.2548)  data: 5.9159 (5.9159 -- 5.9159)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:44  lr: 0.000003  min_lr: 0.000000  loss: 1.6091 (1.6424)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7830 (9.1914)  time: 0.8729 (0.5462 -- 2.6388)  data: 0.0691 (0.0003 -- 1.3474)  max mem: 16413
[2023-08-30 07:14:07,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=117, lr=[7.057378302000176e-08, 7.057378302000176e-08, 9.409837736000233e-08, 9.409837736000233e-08, 1.2546450314666978e-07, 1.2546450314666978e-07, 1.672860041955597e-07, 1.672860041955597e-07, 2.2304800559407963e-07, 2.2304800559407963e-07, 2.973973407921062e-07, 2.973973407921062e-07, 3.965297877228082e-07, 3.965297877228082e-07, 5.287063836304109e-07, 5.287063836304109e-07, 7.049418448405479e-07, 7.049418448405479e-07, 9.399224597873972e-07, 9.399224597873972e-07, 1.2532299463831964e-06, 1.2532299463831964e-06, 1.6709732618442617e-06, 1.6709732618442617e-06, 2.2279643491256826e-06, 2.2279643491256826e-06, 2.9706191321675765e-06, 2.9706191321675765e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 07:14:07,756] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.958138137135993, CurrSamplesPerSec=22.733293405708928, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000000  loss: 1.6029 (1.6926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1956 (9.0419)  time: 0.7847 (0.5364 -- 2.9520)  data: 0.0018 (0.0002 -- 0.0033)  max mem: 16413
[2023-08-30 07:14:13,043] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:14:13,044] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:14:13,044] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:14:13,044] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [131]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5832 (1.6880)  loss_scale: 32768.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7391 (8.8605)  time: 0.9223 (0.5298 -- 4.1603)  data: 0.0281 (0.0004 -- 0.5298)  max mem: 16413
Epoch: [131]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7526 (1.6944)  loss_scale: 32768.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0577 (9.4323)  time: 0.8555 (0.5324 -- 1.8923)  data: 0.0991 (0.0002 -- 1.3684)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7054 (1.6980)  loss_scale: 32768.0000 (25954.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4926 (9.4446)  time: 0.8693 (0.5292 -- 3.0641)  data: 0.1687 (0.0002 -- 2.5354)  max mem: 16413
[2023-08-30 07:15:12,026] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21073
[2023-08-30 07:15:12,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21073
[2023-08-30 07:15:12,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:15:12,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:15:12,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6913 (1.6974)  loss_scale: 32768.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8897 (9.3804)  time: 0.8594 (0.5365 -- 3.0781)  data: 0.3025 (0.0003 -- 2.5377)  max mem: 16413
[2023-08-30 07:15:19,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21082
[2023-08-30 07:15:19,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21082
[2023-08-30 07:15:19,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:15:19,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:15:19,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [131]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.6505 (1.6941)  loss_scale: 8192.0000 (23530.2128)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3082 (9.3161)  time: 0.8161 (0.5290 -- 2.4959)  data: 0.1744 (0.0003 -- 1.9767)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6936 (1.6945)  loss_scale: 8192.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3831 (9.2389)  time: 0.7225 (0.4965 -- 2.1799)  data: 0.1651 (0.0001 -- 1.6576)  max mem: 16413
Epoch: [131] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6936 (1.7161)  loss_scale: 8192.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3831 (9.2389)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3120 (0.3120)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4487 (2.4487 -- 2.4487)  data: 2.1597 (2.1597 -- 2.1597)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4081 (0.7184)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4224 (0.2032 -- 2.4487)  data: 0.1975 (0.0006 -- 2.1597)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4446 (0.6538)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2123 (0.1700 -- 0.2452)  data: 0.0036 (0.0001 -- 0.0447)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5919 (0.7024)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.1969 (0.1328 -- 0.2452)  data: 0.0033 (0.0001 -- 0.0447)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.663
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [132]  [  0/160]  eta: 0:22:31  lr: 0.000003  min_lr: 0.000000  loss: 1.7682 (1.7682)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2629 (10.2629)  time: 8.4484 (8.4484 -- 8.4484)  data: 6.2725 (6.2725 -- 6.2725)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:58  lr: 0.000003  min_lr: 0.000000  loss: 1.6486 (1.7047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4897 (9.7640)  time: 0.9146 (0.5286 -- 3.1577)  data: 0.0022 (0.0003 -- 0.0143)  max mem: 16413
Epoch: [132]  [ 40/160]  eta: 0:02:03  lr: 0.000003  min_lr: 0.000000  loss: 1.4910 (1.6107)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9561 (8.7471)  time: 0.7718 (0.5333 -- 2.9344)  data: 0.0019 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [132]  [ 60/160]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 1.6865 (1.6589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8684 (8.7330)  time: 0.8079 (0.5347 -- 3.7183)  data: 0.0343 (0.0007 -- 0.3326)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.5380 (1.6528)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1572 (8.7337)  time: 0.9474 (0.5345 -- 3.5802)  data: 0.3946 (0.0004 -- 3.0535)  max mem: 16413
[2023-08-30 07:17:23,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:17:23,129] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 07:17:23,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:17:23,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7404 (1.6729)  loss_scale: 8192.0000 (9003.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5332 (8.8226)  time: 0.8027 (0.5239 -- 2.4131)  data: 0.2527 (0.0003 -- 1.8814)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.6860 (1.6592)  loss_scale: 16384.0000 (10223.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3941 (8.9271)  time: 0.9555 (0.5207 -- 4.2267)  data: 0.4145 (0.0004 -- 3.7145)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.5578 (1.6509)  loss_scale: 16384.0000 (11096.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1259 (9.0340)  time: 0.7831 (0.5241 -- 2.9915)  data: 0.2254 (0.0002 -- 2.4719)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5498 (1.6478)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1437 (9.0192)  time: 0.6964 (0.4960 -- 2.6428)  data: 0.1442 (0.0002 -- 2.1347)  max mem: 16413
Epoch: [132] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5498 (1.6762)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1437 (9.0192)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3057 (0.3057)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1485 (2.1485 -- 2.1485)  data: 1.9172 (1.9172 -- 1.9172)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4126 (0.7193)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.3943 (0.2071 -- 2.1485)  data: 0.1753 (0.0007 -- 1.9172)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4433 (0.6534)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2237 (0.1695 -- 0.3870)  data: 0.0141 (0.0001 -- 0.1672)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5922 (0.7020)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2057 (0.1328 -- 0.3870)  data: 0.0138 (0.0001 -- 0.1672)  max mem: 16413
Val: Total time: 0:00:07 (0.2817 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.661
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.61%
Epoch: [133]  [  0/160]  eta: 0:18:56  lr: 0.000003  min_lr: 0.000000  loss: 2.1776 (2.1776)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3054 (9.3054)  time: 7.1046 (7.1046 -- 7.1046)  data: 6.0716 (6.0716 -- 6.0716)  max mem: 16413
Epoch: [133]  [ 20/160]  eta: 0:02:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5192 (1.6039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1017 (8.5744)  time: 0.8190 (0.5277 -- 3.8111)  data: 0.1710 (0.0003 -- 2.7708)  max mem: 16413
[2023-08-30 07:18:50,411] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21302
[2023-08-30 07:18:50,411] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21302
[2023-08-30 07:18:50,411] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:18:50,411] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:18:50,411] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [133]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7640 (1.6649)  loss_scale: 8192.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2348 (8.7858)  time: 0.8900 (0.5243 -- 3.5532)  data: 0.2351 (0.0005 -- 3.0280)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.9410 (1.7178)  loss_scale: 8192.0000 (11146.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5657 (8.9246)  time: 0.9050 (0.5277 -- 3.9894)  data: 0.0051 (0.0004 -- 0.0695)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7491 (1.7486)  loss_scale: 8192.0000 (10416.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9098 (9.0068)  time: 0.8748 (0.5233 -- 3.5190)  data: 0.0273 (0.0004 -- 0.3870)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000000  loss: 1.5863 (1.7190)  loss_scale: 8192.0000 (9976.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1988 (9.1140)  time: 0.9946 (0.5143 -- 4.3729)  data: 0.2121 (0.0003 -- 2.2824)  max mem: 16413
Epoch: [133]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7638 (1.7303)  loss_scale: 8192.0000 (9681.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6200 (9.0872)  time: 0.7344 (0.5221 -- 2.6009)  data: 0.0016 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6058 (1.7215)  loss_scale: 8192.0000 (9470.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6037 (8.9756)  time: 0.8841 (0.5332 -- 3.1743)  data: 0.0017 (0.0003 -- 0.0069)  max mem: 16413
[2023-08-30 07:20:44,498] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:20:44,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 07:20:44,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:20:44,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 2.0564 (1.7391)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8363 (9.0021)  time: 0.7604 (0.4944 -- 2.6272)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [133] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 2.0564 (1.7360)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8363 (9.0021)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3032 (0.3032)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3677 (2.3677 -- 2.3677)  data: 2.1509 (2.1509 -- 2.1509)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4173 (0.7217)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4113 (0.1952 -- 2.3677)  data: 0.1980 (0.0009 -- 2.1509)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4375 (0.6553)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2177 (0.1706 -- 0.4329)  data: 0.0137 (0.0001 -- 0.2429)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5921 (0.7046)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2047 (0.1335 -- 0.4329)  data: 0.0132 (0.0001 -- 0.2429)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.663
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [134]  [  0/160]  eta: 0:18:54  lr: 0.000002  min_lr: 0.000000  loss: 2.3736 (2.3736)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.9397 (12.9397)  time: 7.0880 (7.0880 -- 7.0880)  data: 6.5502 (6.5502 -- 6.5502)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 1.7770 (1.7814)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8103 (8.4656)  time: 0.8605 (0.5234 -- 4.6534)  data: 0.3165 (0.0003 -- 4.1157)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 1.4978 (1.7001)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4276 (8.6158)  time: 0.8565 (0.5360 -- 2.6016)  data: 0.3065 (0.0009 -- 2.0504)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6181 (1.7045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9221 (8.6778)  time: 1.0056 (0.5182 -- 3.4272)  data: 0.4610 (0.0002 -- 2.8962)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6576 (1.6934)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4676 (8.7282)  time: 0.8963 (0.5188 -- 4.6155)  data: 0.3490 (0.0004 -- 4.0912)  max mem: 16413
Epoch: [134]  [100/160]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000000  loss: 1.7958 (1.7022)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5647 (8.7637)  time: 0.9348 (0.5122 -- 3.3857)  data: 0.4021 (0.0002 -- 2.8695)  max mem: 16413
[2023-08-30 07:22:47,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:22:47,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:22:47,828] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:22:47,828] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5671 (1.6827)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0973 (8.7176)  time: 0.6984 (0.5155 -- 2.6199)  data: 0.1529 (0.0001 -- 2.0923)  max mem: 16413
[2023-08-30 07:22:53,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21567
[2023-08-30 07:22:53,513] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:22:53,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21567
[2023-08-30 07:22:53,513] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:22:53,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8968 (1.6948)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9978 (8.8573)  time: 0.8338 (0.5298 -- 2.4641)  data: 0.2861 (0.0002 -- 1.9306)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6802 (1.7003)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2679 (8.9086)  time: 0.6541 (0.4968 -- 1.8373)  data: 0.1387 (0.0001 -- 1.2990)  max mem: 16413
Epoch: [134] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6802 (1.7117)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2679 (8.9086)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3015 (0.3015)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4819 (2.4819 -- 2.4819)  data: 2.2505 (2.2505 -- 2.2505)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4133 (0.7201)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4208 (0.1878 -- 2.4819)  data: 0.2062 (0.0011 -- 2.2505)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4464 (0.6538)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2115 (0.1692 -- 0.3065)  data: 0.0070 (0.0001 -- 0.1196)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5938 (0.7029)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.1971 (0.1326 -- 0.3065)  data: 0.0065 (0.0001 -- 0.1196)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [135]  [  0/160]  eta: 0:21:35  lr: 0.000002  min_lr: 0.000000  loss: 1.5654 (1.5654)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3267 (6.3267)  time: 8.0938 (8.0938 -- 8.0938)  data: 6.7658 (6.7658 -- 6.7658)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6258 (1.6607)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8197 (8.6731)  time: 0.7741 (0.5357 -- 2.5695)  data: 0.2205 (0.0007 -- 2.0122)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3902 (1.6197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7746 (8.9921)  time: 0.8809 (0.5306 -- 3.9695)  data: 0.3223 (0.0004 -- 3.4227)  max mem: 16413
Epoch: [135]  [ 60/160]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 1.8703 (1.6662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4513 (8.9470)  time: 0.8479 (0.5309 -- 2.9663)  data: 0.2718 (0.0005 -- 2.4334)  max mem: 16413
[2023-08-30 07:24:30,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21668
[2023-08-30 07:24:30,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:24:30,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21668
[2023-08-30 07:24:30,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 07:24:30,754] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.6747 (1.6727)  loss_scale: 8192.0000 (15069.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8170 (8.9162)  time: 0.9317 (0.5189 -- 2.8492)  data: 0.3895 (0.0004 -- 2.3258)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7213 (1.6859)  loss_scale: 8192.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5536 (8.8955)  time: 0.8585 (0.5244 -- 3.7149)  data: 0.3165 (0.0002 -- 3.2026)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8088 (1.7047)  loss_scale: 8192.0000 (12795.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0365 (8.9622)  time: 0.8052 (0.5197 -- 2.9945)  data: 0.2619 (0.0004 -- 2.4642)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5620 (1.6762)  loss_scale: 8192.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5862 (8.9215)  time: 0.9890 (0.5212 -- 5.4840)  data: 0.4381 (0.0003 -- 4.9568)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6267 (1.6687)  loss_scale: 8192.0000 (11673.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3142 (9.0043)  time: 0.6478 (0.4956 -- 2.5882)  data: 0.1276 (0.0002 -- 2.0522)  max mem: 16413
Epoch: [135] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6267 (1.7003)  loss_scale: 8192.0000 (11673.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3142 (9.0043)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2944 (0.2944)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3652 (2.3652 -- 2.3652)  data: 2.1514 (2.1514 -- 2.1514)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4126 (0.7185)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4250 (0.2080 -- 2.3652)  data: 0.2047 (0.0006 -- 2.1514)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4370 (0.6525)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2197 (0.1721 -- 0.3531)  data: 0.0103 (0.0001 -- 0.0810)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5906 (0.7037)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2042 (0.1330 -- 0.3531)  data: 0.0095 (0.0001 -- 0.0810)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [136]  [  0/160]  eta: 0:17:20  lr: 0.000002  min_lr: 0.000000  loss: 1.2464 (1.2464)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9380 (10.9380)  time: 6.5059 (6.5059 -- 6.5059)  data: 5.9416 (5.9416 -- 5.9416)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000000  loss: 1.6079 (1.5781)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8622 (8.2956)  time: 0.8514 (0.5319 -- 3.1759)  data: 0.0546 (0.0005 -- 0.5751)  max mem: 16413
[2023-08-30 07:26:33,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:26:33,143] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 07:26:33,146] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:26:33,146] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [136]  [ 40/160]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 1.8601 (1.7059)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6073 (8.5667)  time: 0.8977 (0.5232 -- 3.2021)  data: 0.1878 (0.0004 -- 1.8582)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:34  lr: 0.000002  min_lr: 0.000000  loss: 1.8074 (1.7230)  loss_scale: 16384.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2068 (8.6310)  time: 0.7979 (0.5252 -- 2.6818)  data: 0.2440 (0.0004 -- 2.1512)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.5987 (1.6911)  loss_scale: 16384.0000 (12641.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0789 (8.5834)  time: 0.9069 (0.5245 -- 2.7526)  data: 0.1621 (0.0002 -- 1.9905)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7221 (1.6960)  loss_scale: 16384.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5879 (8.4565)  time: 0.8955 (0.5239 -- 3.2094)  data: 0.1969 (0.0002 -- 2.6762)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8368 (1.7005)  loss_scale: 16384.0000 (13879.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4506 (8.7710)  time: 0.8505 (0.5407 -- 2.6016)  data: 0.1481 (0.0005 -- 1.5714)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5783 (1.6951)  loss_scale: 16384.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5492 (8.8585)  time: 0.8508 (0.5235 -- 3.1137)  data: 0.2372 (0.0003 -- 2.5599)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6655 (1.6993)  loss_scale: 16384.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9565 (8.8549)  time: 0.7017 (0.4955 -- 2.5181)  data: 0.1517 (0.0002 -- 1.9812)  max mem: 16413
Epoch: [136] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6655 (1.6847)  loss_scale: 16384.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9565 (8.8549)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2966 (0.2966)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2680 (2.2680 -- 2.2680)  data: 2.0638 (2.0638 -- 2.0638)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4107 (0.7190)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4080 (0.1962 -- 2.2680)  data: 0.2025 (0.0006 -- 2.0638)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4308 (0.6516)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2221 (0.1694 -- 0.4397)  data: 0.0195 (0.0001 -- 0.1816)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5902 (0.7033)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2098 (0.1329 -- 0.4397)  data: 0.0191 (0.0001 -- 0.1816)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.663
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [137]  [  0/160]  eta: 0:20:52  lr: 0.000002  min_lr: 0.000000  loss: 2.3704 (2.3704)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4668 (13.4668)  time: 7.8267 (7.8267 -- 7.8267)  data: 6.7713 (6.7713 -- 6.7713)  max mem: 16413
[2023-08-30 07:28:35,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:28:35,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:28:35,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:28:35,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [137]  [ 20/160]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000000  loss: 1.7831 (1.8240)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4333 (8.3120)  time: 0.8599 (0.5195 -- 3.8242)  data: 0.1176 (0.0008 -- 0.7068)  max mem: 16413
[2023-08-30 07:28:49,528] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21941
[2023-08-30 07:28:49,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:28:49,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 07:28:49,529] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21941
[2023-08-30 07:28:49,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [137]  [ 40/160]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 1.5681 (1.7448)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0414 (8.2954)  time: 0.9294 (0.5115 -- 3.3711)  data: 0.3875 (0.0004 -- 2.8644)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8350 (1.7743)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9275 (8.5473)  time: 0.7511 (0.5242 -- 1.9984)  data: 0.1991 (0.0001 -- 1.4673)  max mem: 16413
[2023-08-30 07:29:37,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=123, lr=[4.4390282598631105e-08, 4.4390282598631105e-08, 5.9187043464841474e-08, 5.9187043464841474e-08, 7.891605795312197e-08, 7.891605795312197e-08, 1.0522141060416263e-07, 1.0522141060416263e-07, 1.402952141388835e-07, 1.402952141388835e-07, 1.8706028551851133e-07, 1.8706028551851133e-07, 2.494137140246818e-07, 2.494137140246818e-07, 3.3255161869957567e-07, 3.3255161869957567e-07, 4.434021582661009e-07, 4.434021582661009e-07, 5.912028776881345e-07, 5.912028776881345e-07, 7.882705035841794e-07, 7.882705035841794e-07, 1.0510273381122393e-06, 1.0510273381122393e-06, 1.4013697841496524e-06, 1.4013697841496524e-06, 1.8684930455328698e-06, 1.8684930455328698e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 07:29:37,564] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=18.00482341587256, CurrSamplesPerSec=21.968429001043173, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8719 (1.7679)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2394 (8.3970)  time: 0.9094 (0.5407 -- 3.2284)  data: 0.3347 (0.0006 -- 2.7026)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8181 (1.7745)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4817 (8.6126)  time: 0.8827 (0.5317 -- 2.8064)  data: 0.3091 (0.0002 -- 2.2533)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8034 (1.7872)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2780 (8.5690)  time: 0.9645 (0.5206 -- 3.1576)  data: 0.4127 (0.0004 -- 2.6164)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7687 (1.7798)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9615 (8.7254)  time: 0.8203 (0.5267 -- 3.4593)  data: 0.2734 (0.0002 -- 2.9303)  max mem: 16413
[2023-08-30 07:30:44,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:30:44,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:30:44,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:30:44,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:30:47,583] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22076
[2023-08-30 07:30:47,583] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22076
[2023-08-30 07:30:47,583] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:30:47,583] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:30:47,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5037 (1.7644)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0350 (8.7210)  time: 0.7720 (0.4864 -- 5.1126)  data: 0.2604 (0.0002 -- 4.6123)  max mem: 16413
Epoch: [137] Total time: 0:02:25 (0.9070 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5037 (1.7279)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0350 (8.7210)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2975 (0.2975)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4093 (2.4093 -- 2.4093)  data: 2.1654 (2.1654 -- 2.1654)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4051 (0.7190)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4155 (0.1907 -- 2.4093)  data: 0.1991 (0.0006 -- 2.1654)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4315 (0.6500)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2154 (0.1693 -- 0.3572)  data: 0.0105 (0.0001 -- 0.1439)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5887 (0.7023)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2008 (0.1330 -- 0.3572)  data: 0.0102 (0.0001 -- 0.1439)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [138]  [  0/160]  eta: 0:21:16  lr: 0.000002  min_lr: 0.000000  loss: 1.9095 (1.9095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1211 (10.1211)  time: 7.9757 (7.9757 -- 7.9757)  data: 7.0448 (7.0448 -- 7.0448)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:34  lr: 0.000002  min_lr: 0.000000  loss: 1.4875 (1.6122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6749 (9.6459)  time: 0.7605 (0.5228 -- 2.9387)  data: 0.2052 (0.0004 -- 2.4011)  max mem: 16413
Epoch: [138]  [ 40/160]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 1.5989 (1.6369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7095 (9.3642)  time: 0.9571 (0.5304 -- 3.6636)  data: 0.4112 (0.0003 -- 3.1285)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6001 (1.6385)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3347 (9.2881)  time: 0.8469 (0.5319 -- 2.6839)  data: 0.0941 (0.0004 -- 1.4317)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.5300 (1.5961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9467 (8.9661)  time: 0.8377 (0.5270 -- 4.1100)  data: 0.0012 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.6651 (1.6092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6703 (9.0976)  time: 0.8717 (0.5243 -- 3.3864)  data: 0.0259 (0.0003 -- 0.4882)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7518 (1.6282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4977 (9.0984)  time: 0.8481 (0.5269 -- 4.8540)  data: 0.0015 (0.0005 -- 0.0035)  max mem: 16413
[2023-08-30 07:32:52,381] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:32:52,381] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:32:52,381] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:32:52,381] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:33:04,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22218
[2023-08-30 07:33:04,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22218
[2023-08-30 07:33:04,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:33:04,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:33:04,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6119 (1.6347)  loss_scale: 32768.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6501 (9.1254)  time: 0.9435 (0.5347 -- 2.8844)  data: 0.0022 (0.0003 -- 0.0093)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5407 (1.6359)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3089 (9.1380)  time: 0.7038 (0.4986 -- 2.4395)  data: 0.0007 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [138] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5407 (1.6580)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3089 (9.1380)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2942 (0.2942)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1414 (2.1414 -- 2.1414)  data: 1.8925 (1.8925 -- 1.8925)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4134 (0.7186)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4050 (0.1931 -- 2.1414)  data: 0.1847 (0.0009 -- 1.8925)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4325 (0.6488)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2266 (0.1703 -- 0.4892)  data: 0.0193 (0.0001 -- 0.2434)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5890 (0.7011)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.2656)  time: 0.2112 (0.1334 -- 0.4892)  data: 0.0186 (0.0001 -- 0.2434)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.660
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.61%
Epoch: [139]  [  0/160]  eta: 0:20:06  lr: 0.000002  min_lr: 0.000000  loss: 1.4836 (1.4836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.6582 (13.6582)  time: 7.5420 (7.5420 -- 7.5420)  data: 6.9983 (6.9983 -- 6.9983)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000000  loss: 1.5643 (1.5542)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7447 (8.7777)  time: 0.8745 (0.5274 -- 2.5460)  data: 0.2960 (0.0005 -- 2.0139)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000000  loss: 1.5457 (1.6003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5648 (8.9287)  time: 0.9551 (0.5210 -- 2.8892)  data: 0.4090 (0.0004 -- 2.3377)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.8019 (1.6385)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5409 (8.7826)  time: 0.8708 (0.5272 -- 3.0295)  data: 0.3275 (0.0005 -- 2.4924)  max mem: 16413
Epoch: [139]  [ 80/160]  eta: 0:01:20  lr: 0.000002  min_lr: 0.000000  loss: 1.7540 (1.6517)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4093 (8.6450)  time: 0.9940 (0.5161 -- 4.8161)  data: 0.4493 (0.0001 -- 4.2842)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6524 (1.6700)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5050 (8.9131)  time: 0.6803 (0.5196 -- 3.2190)  data: 0.1340 (0.0004 -- 2.6585)  max mem: 16413
[2023-08-30 07:35:08,962] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:35:08,962] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:35:08,966] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:35:08,967] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:35:11,197] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22351
[2023-08-30 07:35:11,197] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:35:11,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 07:35:11,197] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22351
[2023-08-30 07:35:11,198] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [139]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5828 (1.6585)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2245 (8.7968)  time: 0.8860 (0.5222 -- 3.2051)  data: 0.3343 (0.0002 -- 2.6789)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6821 (1.6653)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0646 (8.8278)  time: 0.9284 (0.5297 -- 3.1222)  data: 0.3811 (0.0003 -- 2.6022)  max mem: 16413
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7474 (1.6782)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2001 (8.8443)  time: 0.7126 (0.4952 -- 4.3699)  data: 0.1989 (0.0002 -- 3.8485)  max mem: 16413
Epoch: [139] Total time: 0:02:25 (0.9067 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7474 (1.6875)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2001 (8.8443)
[2023-08-30 07:35:52,287] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-08-30 07:35:52,289] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-08-30 07:35:52,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-08-30 07:35:52,289] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-08-30 07:35:53,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-08-30 07:35:53,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2939 (0.2939)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2998 (2.2998 -- 2.2998)  data: 2.0948 (2.0948 -- 2.0948)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4231 (0.7206)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4183 (0.2015 -- 2.2998)  data: 0.2032 (0.0005 -- 2.0948)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4341 (0.6510)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2263 (0.1687 -- 0.4758)  data: 0.0215 (0.0001 -- 0.2857)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5875 (0.7033)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2115 (0.1366 -- 0.4758)  data: 0.0211 (0.0001 -- 0.2857)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.661
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [140]  [  0/160]  eta: 0:16:42  lr: 0.000001  min_lr: 0.000000  loss: 0.9004 (0.9004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8673 (10.8673)  time: 6.2671 (6.2671 -- 6.2671)  data: 5.4987 (5.4987 -- 5.4987)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 1.8296 (1.7293)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2555 (9.3278)  time: 0.9992 (0.5196 -- 4.0613)  data: 0.3174 (0.0003 -- 3.5303)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.5663 (1.6763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5435 (8.8087)  time: 0.8283 (0.5326 -- 2.5458)  data: 0.0020 (0.0003 -- 0.0122)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.6950 (1.6953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3787 (8.9281)  time: 0.8836 (0.5174 -- 3.3135)  data: 0.0014 (0.0004 -- 0.0061)  max mem: 16413
[2023-08-30 07:37:20,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:37:20,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:37:20,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:37:20,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [140]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7666 (1.7050)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1176 (8.9690)  time: 0.9319 (0.5110 -- 3.7425)  data: 0.0014 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.8845 (1.7251)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9336 (8.7914)  time: 0.7628 (0.5371 -- 2.5197)  data: 0.0018 (0.0006 -- 0.0038)  max mem: 16413
[2023-08-30 07:37:46,611] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22512
[2023-08-30 07:37:46,611] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22512
[2023-08-30 07:37:46,611] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:37:46,611] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:37:46,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5848 (1.7095)  loss_scale: 32768.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7248 (8.8111)  time: 0.9835 (0.5240 -- 4.7386)  data: 0.0029 (0.0005 -- 0.0166)  max mem: 16413
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7719 (1.7216)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5190 (8.8339)  time: 0.8850 (0.5122 -- 4.6811)  data: 0.0014 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6106 (1.7181)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7001 (8.7758)  time: 0.5698 (0.4958 -- 1.4997)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [140] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6106 (1.7040)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7001 (8.7758)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2956 (0.2956)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2755 (2.2755 -- 2.2755)  data: 2.0581 (2.0581 -- 2.0581)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4199 (0.7191)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4134 (0.1901 -- 2.2755)  data: 0.2079 (0.0004 -- 2.0581)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4413 (0.6508)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2235 (0.1705 -- 0.4301)  data: 0.0238 (0.0001 -- 0.2179)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5881 (0.7029)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2118 (0.1334 -- 0.4301)  data: 0.0235 (0.0001 -- 0.2179)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.61%
Epoch: [141]  [  0/160]  eta: 0:18:00  lr: 0.000001  min_lr: 0.000000  loss: 1.2391 (1.2391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8816 (7.8816)  time: 6.7502 (6.7502 -- 6.7502)  data: 6.1942 (6.1942 -- 6.1942)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:48  lr: 0.000001  min_lr: 0.000000  loss: 1.7909 (1.7440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6415 (8.9522)  time: 0.9289 (0.5347 -- 3.1775)  data: 0.0529 (0.0004 -- 0.7125)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.8795 (1.7562)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2329 (9.1168)  time: 0.8795 (0.5219 -- 4.3956)  data: 0.1209 (0.0002 -- 1.4424)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5385 (1.7198)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2201 (9.7196)  time: 0.8236 (0.5254 -- 2.7942)  data: 0.0456 (0.0005 -- 0.8205)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.5519 (1.6814)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0895 (9.7372)  time: 0.9104 (0.5317 -- 3.2339)  data: 0.1029 (0.0003 -- 1.3001)  max mem: 16413
[2023-08-30 07:39:49,733] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:39:49,734] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:39:49,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:39:49,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9055 (1.7176)  loss_scale: 32768.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3870 (9.5275)  time: 0.8678 (0.5303 -- 3.4229)  data: 0.0016 (0.0003 -- 0.0067)  max mem: 16413
[2023-08-30 07:40:12,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22668
[2023-08-30 07:40:12,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22668
[2023-08-30 07:40:12,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:40:12,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:40:12,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.2810 (1.6869)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7306 (9.4207)  time: 0.8758 (0.5198 -- 2.6317)  data: 0.1799 (0.0004 -- 2.0981)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8368 (1.7123)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4897 (9.3904)  time: 0.7804 (0.5242 -- 3.1196)  data: 0.2300 (0.0003 -- 2.6017)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6287 (1.7106)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2304 (9.2913)  time: 0.7274 (0.4955 -- 4.0767)  data: 0.2070 (0.0001 -- 3.5621)  max mem: 16413
Epoch: [141] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6287 (1.7509)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2304 (9.2913)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2937 (0.2937)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3284 (2.3284 -- 2.3284)  data: 2.1154 (2.1154 -- 2.1154)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4174 (0.7198)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4166 (0.1991 -- 2.3284)  data: 0.2029 (0.0004 -- 2.1154)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4260 (0.6497)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2213 (0.1699 -- 0.3448)  data: 0.0106 (0.0001 -- 0.1049)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5899 (0.7031)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2074 (0.1328 -- 0.3448)  data: 0.0104 (0.0001 -- 0.1049)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [142]  [  0/160]  eta: 0:19:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4475 (1.4475)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1920 (10.1920)  time: 7.3522 (7.3522 -- 7.3522)  data: 6.6610 (6.6610 -- 6.6610)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 1.8249 (1.7620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2345 (9.3526)  time: 0.8899 (0.5324 -- 3.6816)  data: 0.2783 (0.0003 -- 2.6826)  max mem: 16413
Epoch: [142]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.7093 (1.7374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2375 (9.2711)  time: 0.8114 (0.5191 -- 2.1796)  data: 0.2362 (0.0002 -- 1.6003)  max mem: 16413
Epoch: [142]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.6319 (1.7089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9355 (9.0992)  time: 0.9517 (0.5377 -- 3.6681)  data: 0.0018 (0.0008 -- 0.0040)  max mem: 16413
[2023-08-30 07:42:14,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:42:14,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:42:14,421] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:42:14,422] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [142]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9543 (1.7375)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5921 (8.9599)  time: 0.7714 (0.5335 -- 1.8763)  data: 0.1886 (0.0006 -- 1.3344)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.4703 (1.7042)  loss_scale: 32768.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9737 (8.8720)  time: 0.9288 (0.5129 -- 3.7590)  data: 0.3154 (0.0002 -- 3.2256)  max mem: 16413
[2023-08-30 07:42:44,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22831
[2023-08-30 07:42:44,829] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22831
[2023-08-30 07:42:44,829] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:42:44,829] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:42:44,829] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [142]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8104 (1.7263)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6246 (8.8770)  time: 0.8998 (0.5166 -- 3.2765)  data: 0.3480 (0.0002 -- 2.7230)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6436 (1.7402)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5867 (8.9171)  time: 0.8700 (0.5286 -- 4.3406)  data: 0.2913 (0.0003 -- 3.7968)  max mem: 16413
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7990 (1.7531)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9174 (8.9711)  time: 0.6109 (0.4981 -- 2.3411)  data: 0.0907 (0.0002 -- 1.8028)  max mem: 16413
Epoch: [142] Total time: 0:02:21 (0.8845 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7990 (1.7332)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9174 (8.9711)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2896 (0.2896)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4341 (2.4341 -- 2.4341)  data: 2.1813 (2.1813 -- 2.1813)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4149 (0.7200)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4193 (0.1968 -- 2.4341)  data: 0.1993 (0.0006 -- 2.1813)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4207 (0.6486)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2148 (0.1722 -- 0.3068)  data: 0.0108 (0.0001 -- 0.1173)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5898 (0.7022)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.1971 (0.1329 -- 0.3068)  data: 0.0105 (0.0001 -- 0.1173)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.660
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [143]  [  0/160]  eta: 0:22:19  lr: 0.000001  min_lr: 0.000000  loss: 1.0720 (1.0720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.4145 (20.4145)  time: 8.3746 (8.3746 -- 8.3746)  data: 7.8602 (7.8602 -- 7.8602)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 1.6216 (1.6465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5285 (9.3544)  time: 0.8072 (0.5417 -- 3.5154)  data: 0.2575 (0.0006 -- 3.0002)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.6415 (1.6433)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3877 (9.0435)  time: 0.9371 (0.5269 -- 2.7933)  data: 0.2640 (0.0003 -- 2.2489)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.5179 (1.6227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0570 (8.9510)  time: 0.8338 (0.5208 -- 3.8480)  data: 0.2865 (0.0003 -- 3.3116)  max mem: 16413
[2023-08-30 07:44:48,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:44:48,268] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:44:48,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:44:48,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [143]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.8585 (1.6582)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9819 (9.0649)  time: 0.8770 (0.5296 -- 3.2075)  data: 0.2688 (0.0008 -- 2.6663)  max mem: 16413
[2023-08-30 07:44:48,827] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22961
[2023-08-30 07:44:48,827] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22961
[2023-08-30 07:44:48,827] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:44:48,827] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:44:48,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [143]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.8022 (1.6786)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3212 (8.9551)  time: 0.8417 (0.5309 -- 3.2476)  data: 0.2084 (0.0004 -- 2.6811)  max mem: 16413
[2023-08-30 07:45:20,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=130, lr=[2.3982722479617303e-08, 2.3982722479617303e-08, 3.1976963306156406e-08, 3.1976963306156406e-08, 4.2635951074875204e-08, 4.2635951074875204e-08, 5.684793476650028e-08, 5.684793476650028e-08, 7.57972463553337e-08, 7.57972463553337e-08, 1.0106299514044494e-07, 1.0106299514044494e-07, 1.347506601872599e-07, 1.347506601872599e-07, 1.7966754691634654e-07, 1.7966754691634654e-07, 2.3955672922179537e-07, 2.3955672922179537e-07, 3.194089722957272e-07, 3.194089722957272e-07, 4.2587862972763624e-07, 4.2587862972763624e-07, 5.678381729701817e-07, 5.678381729701817e-07, 7.571175639602423e-07, 7.571175639602423e-07, 1.009490085280323e-06, 1.009490085280323e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 07:45:20,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=18.014758476805135, CurrSamplesPerSec=21.00026411150262, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5395 (1.6644)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3519 (9.0662)  time: 0.9542 (0.5267 -- 3.3724)  data: 0.3939 (0.0005 -- 2.8611)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.9370 (1.6881)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3765 (9.1128)  time: 0.7872 (0.5258 -- 2.2381)  data: 0.1105 (0.0003 -- 1.6823)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8928 (1.7065)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1834 (9.1516)  time: 0.7507 (0.4953 -- 1.8787)  data: 0.1585 (0.0002 -- 1.2387)  max mem: 16413
Epoch: [143] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8928 (1.7220)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1834 (9.1516)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2920 (0.2920)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2795 (2.2795 -- 2.2795)  data: 2.0663 (2.0663 -- 2.0663)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4156 (0.7189)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4152 (0.2052 -- 2.2795)  data: 0.2015 (0.0005 -- 2.0663)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4172 (0.6477)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2269 (0.1695 -- 0.3797)  data: 0.0228 (0.0001 -- 0.1520)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5897 (0.7013)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2121 (0.1334 -- 0.3797)  data: 0.0222 (0.0001 -- 0.1520)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.660
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [144]  [  0/160]  eta: 0:24:46  lr: 0.000001  min_lr: 0.000000  loss: 0.9298 (0.9298)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7756 (13.7756)  time: 9.2928 (9.2928 -- 9.2928)  data: 4.8733 (4.8733 -- 4.8733)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 1.6104 (1.6399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4957 (8.6415)  time: 0.8348 (0.5169 -- 3.1552)  data: 0.1779 (0.0006 -- 2.6316)  max mem: 16413
Epoch: [144]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.7057 (1.6500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2091 (8.9282)  time: 0.8382 (0.5327 -- 3.2697)  data: 0.0404 (0.0004 -- 0.7567)  max mem: 16413
[2023-08-30 07:46:50,629] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:46:50,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:46:50,629] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:46:50,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:46:51,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23091
[2023-08-30 07:46:51,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23091
[2023-08-30 07:46:51,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:46:51,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:46:51,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [144]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6473 (1.6628)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9793 (9.0931)  time: 0.7971 (0.5109 -- 2.6945)  data: 0.0667 (0.0004 -- 1.3036)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7529 (1.7047)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6483 (9.0497)  time: 1.0392 (0.5203 -- 4.7507)  data: 0.2799 (0.0002 -- 2.1329)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 2.0799 (1.7532)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5837 (9.0688)  time: 0.8197 (0.5233 -- 3.8126)  data: 0.1947 (0.0003 -- 2.0461)  max mem: 16413
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6314 (1.7376)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7507 (9.1121)  time: 0.8293 (0.5262 -- 3.4114)  data: 0.2791 (0.0002 -- 2.8720)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7222 (1.7388)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8628 (9.0961)  time: 0.8057 (0.5247 -- 3.5664)  data: 0.2522 (0.0004 -- 3.0292)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6549 (1.7264)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6846 (9.0634)  time: 0.7057 (0.4951 -- 3.8192)  data: 0.1829 (0.0002 -- 3.2515)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6549 (1.7028)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6846 (9.0634)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2929 (0.2929)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4872 (2.4872 -- 2.4872)  data: 2.2734 (2.2734 -- 2.2734)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4171 (0.7185)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4182 (0.1969 -- 2.4872)  data: 0.2075 (0.0007 -- 2.2734)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4171 (0.6474)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2146 (0.1709 -- 0.2810)  data: 0.0087 (0.0001 -- 0.0981)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5899 (0.7011)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2005 (0.1337 -- 0.2810)  data: 0.0084 (0.0001 -- 0.0981)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.660
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [145]  [  0/160]  eta: 0:16:59  lr: 0.000001  min_lr: 0.000000  loss: 2.1085 (2.1085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7350 (11.7350)  time: 6.3743 (6.3743 -- 6.3743)  data: 4.4784 (4.4784 -- 4.4784)  max mem: 16413
[2023-08-30 07:48:56,348] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:48:56,348] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:48:56,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:48:56,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [145]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 1.6555 (1.6413)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1422 (9.4693)  time: 0.9598 (0.5332 -- 4.1120)  data: 0.1730 (0.0007 -- 1.9199)  max mem: 16413
[2023-08-30 07:49:09,573] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23234
[2023-08-30 07:49:09,573] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23234
[2023-08-30 07:49:09,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:49:09,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:49:09,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.7213 (1.6802)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1125 (8.9344)  time: 0.8926 (0.5226 -- 2.9671)  data: 0.0019 (0.0002 -- 0.0076)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 1.8075 (1.6789)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1747 (8.6484)  time: 0.9268 (0.5246 -- 3.8213)  data: 0.0016 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.8208 (1.7012)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0807 (8.8876)  time: 0.7653 (0.5316 -- 2.1149)  data: 0.0015 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [145]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7611 (1.6998)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7242 (8.9943)  time: 0.9150 (0.5294 -- 3.9637)  data: 0.0017 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5600 (1.6852)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6461 (9.1812)  time: 0.7883 (0.5148 -- 4.7051)  data: 0.0019 (0.0003 -- 0.0127)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5879 (1.6674)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8747 (9.1897)  time: 1.0098 (0.5251 -- 4.3361)  data: 0.0023 (0.0003 -- 0.0206)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6443 (1.6720)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9820 (9.1484)  time: 0.6886 (0.4958 -- 3.1985)  data: 0.0006 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [145] Total time: 0:02:22 (0.8883 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6443 (1.6869)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9820 (9.1484)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2920 (0.2920)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3081 (2.3081 -- 2.3081)  data: 2.0858 (2.0858 -- 2.0858)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4167 (0.7179)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4084 (0.1907 -- 2.3081)  data: 0.2028 (0.0004 -- 2.0858)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4210 (0.6468)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2244 (0.1705 -- 0.5003)  data: 0.0247 (0.0001 -- 0.3180)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5880 (0.7003)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2145 (0.1325 -- 0.5003)  data: 0.0245 (0.0001 -- 0.3180)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.659
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [146]  [  0/160]  eta: 0:20:15  lr: 0.000001  min_lr: 0.000000  loss: 0.7891 (0.7891)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9411 (8.9411)  time: 7.5954 (7.5954 -- 7.5954)  data: 7.0438 (7.0438 -- 7.0438)  max mem: 16413
[2023-08-30 07:51:10,659] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:51:10,660] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:51:10,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:51:10,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:51:12,362] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23366
[2023-08-30 07:51:12,362] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23366
[2023-08-30 07:51:12,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:51:12,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:51:12,362] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [146]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.6227 (1.5525)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0986 (9.0070)  time: 0.8606 (0.5213 -- 3.1099)  data: 0.2459 (0.0005 -- 2.5852)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.8633 (1.6798)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0760 (8.9706)  time: 0.9110 (0.5221 -- 2.4365)  data: 0.1994 (0.0002 -- 1.9211)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000001  min_lr: 0.000000  loss: 1.9568 (1.7618)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9975 (9.1139)  time: 0.9150 (0.5258 -- 2.9005)  data: 0.0891 (0.0002 -- 1.7249)  max mem: 16413
Epoch: [146]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.7089 (1.7511)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6815 (9.3163)  time: 0.8451 (0.5360 -- 2.7667)  data: 0.1899 (0.0005 -- 2.2404)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.5945 (1.7399)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3560 (9.2561)  time: 0.9091 (0.5298 -- 3.3293)  data: 0.0388 (0.0001 -- 0.7592)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6289 (1.7116)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9308 (9.0630)  time: 0.7732 (0.5146 -- 3.8551)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
[2023-08-30 07:53:04,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:53:04,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:53:04,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:53:04,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:53:08,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23499
[2023-08-30 07:53:08,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:53:08,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23499
[2023-08-30 07:53:08,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:53:08,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6373 (1.6998)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7876 (9.0964)  time: 0.8301 (0.5185 -- 3.9385)  data: 0.0018 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7399 (1.6980)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5150 (9.0965)  time: 0.6573 (0.4960 -- 2.1707)  data: 0.0009 (0.0003 -- 0.0018)  max mem: 16413
Epoch: [146] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7399 (1.7105)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5150 (9.0965)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2924 (0.2924)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3431 (2.3431 -- 2.3431)  data: 2.1247 (2.1247 -- 2.1247)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4180 (0.7169)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4245 (0.1991 -- 2.3431)  data: 0.2035 (0.0008 -- 2.1247)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4208 (0.6469)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2246 (0.1702 -- 0.4193)  data: 0.0176 (0.0001 -- 0.2363)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5881 (0.6999)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2084 (0.1334 -- 0.4193)  data: 0.0173 (0.0001 -- 0.2363)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.658
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [147]  [  0/160]  eta: 0:23:31  lr: 0.000001  min_lr: 0.000000  loss: 1.5924 (1.5924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8053 (10.8053)  time: 8.8214 (8.8214 -- 8.8214)  data: 4.4864 (4.4864 -- 4.4864)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:48  lr: 0.000001  min_lr: 0.000000  loss: 1.4890 (1.5392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7259 (9.0405)  time: 0.8254 (0.5246 -- 3.1214)  data: 0.0631 (0.0005 -- 1.2325)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:11  lr: 0.000001  min_lr: 0.000000  loss: 1.8557 (1.6672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3022 (8.7244)  time: 0.9876 (0.5207 -- 3.8316)  data: 0.0024 (0.0006 -- 0.0171)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.4330 (1.6472)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4643 (9.2128)  time: 0.7512 (0.5277 -- 2.8642)  data: 0.0018 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [147]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.4407 (1.6277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8544 (9.2055)  time: 0.9243 (0.5323 -- 3.5258)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6558 (1.6390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9719 (9.1373)  time: 0.7774 (0.5225 -- 2.8892)  data: 0.0015 (0.0004 -- 0.0026)  max mem: 16413
[2023-08-30 07:55:11,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:55:11,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:55:11,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:55:11,889] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:55:13,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23631
[2023-08-30 07:55:13,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23631
[2023-08-30 07:55:13,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:55:13,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:55:13,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7257 (1.6654)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6206 (9.1632)  time: 0.9594 (0.5337 -- 4.2025)  data: 0.0015 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6491 (1.6682)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1771 (9.0551)  time: 0.7971 (0.5275 -- 2.8493)  data: 0.0014 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7044 (1.6735)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4379 (9.0940)  time: 0.6503 (0.5005 -- 1.4048)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [147] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7044 (1.7060)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4379 (9.0940)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2917 (0.2917)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4065 (2.4065 -- 2.4065)  data: 2.1861 (2.1861 -- 2.1861)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4155 (0.7157)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4263 (0.2078 -- 2.4065)  data: 0.2005 (0.0006 -- 2.1861)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4261 (0.6461)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2195 (0.1740 -- 0.2908)  data: 0.0063 (0.0001 -- 0.1039)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5871 (0.6991)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2003 (0.1327 -- 0.2908)  data: 0.0059 (0.0001 -- 0.1039)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.658
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [148]  [  0/160]  eta: 0:21:07  lr: 0.000001  min_lr: 0.000000  loss: 1.7536 (1.7536)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9710 (9.9710)  time: 7.9191 (7.9191 -- 7.9191)  data: 7.3786 (7.3786 -- 7.3786)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:52  lr: 0.000001  min_lr: 0.000000  loss: 1.6438 (1.5682)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5430 (9.1644)  time: 0.8961 (0.5271 -- 4.1409)  data: 0.0649 (0.0007 -- 1.2492)  max mem: 16413
Epoch: [148]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.7212 (1.6211)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9238 (9.1439)  time: 0.9803 (0.5242 -- 4.0567)  data: 0.0022 (0.0002 -- 0.0134)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7698 (1.6511)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9461 (8.9633)  time: 0.6932 (0.5359 -- 2.0885)  data: 0.0016 (0.0003 -- 0.0029)  max mem: 16413
[2023-08-30 07:57:14,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:57:14,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 07:57:14,630] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 07:57:14,630] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [148]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7482 (1.6841)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3621 (8.7939)  time: 0.8042 (0.5229 -- 3.7672)  data: 0.0900 (0.0004 -- 1.7716)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7361 (1.6939)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7185 (8.5817)  time: 1.0109 (0.5132 -- 3.8200)  data: 0.4263 (0.0003 -- 3.3083)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:35  lr: 0.000001  min_lr: 0.000000  loss: 1.7351 (1.7027)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2108 (8.6052)  time: 0.6481 (0.5271 -- 2.3731)  data: 0.0991 (0.0003 -- 1.8276)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6094 (1.7010)  loss_scale: 32768.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2558 (8.6805)  time: 0.9379 (0.5296 -- 3.1297)  data: 0.2306 (0.0006 -- 2.5885)  max mem: 16413
[2023-08-30 07:58:15,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23832
[2023-08-30 07:58:15,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23832
[2023-08-30 07:58:15,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:58:15,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 07:58:15,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7338 (1.6909)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0632 (8.8891)  time: 0.7240 (0.4996 -- 2.3203)  data: 0.1002 (0.0002 -- 0.9883)  max mem: 16413
Epoch: [148] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7338 (1.7057)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0632 (8.8891)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2917 (0.2917)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3578 (2.3578 -- 2.3578)  data: 2.1427 (2.1427 -- 2.1427)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4149 (0.7161)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4235 (0.1926 -- 2.3578)  data: 0.2131 (0.0006 -- 2.1427)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4265 (0.6462)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2234 (0.1693 -- 0.3880)  data: 0.0201 (0.0001 -- 0.1986)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5873 (0.6991)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2084 (0.1327 -- 0.3880)  data: 0.0192 (0.0001 -- 0.1986)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.658
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [149]  [  0/160]  eta: 0:22:33  lr: 0.000000  min_lr: 0.000000  loss: 0.9338 (0.9338)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3656 (5.3656)  time: 8.4592 (8.4592 -- 8.4592)  data: 7.3076 (7.3076 -- 7.3076)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.5097 (1.5177)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1904 (7.9176)  time: 0.8720 (0.5169 -- 4.4146)  data: 0.2945 (0.0002 -- 3.8822)  max mem: 16413
Epoch: [149]  [ 40/160]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 1.5800 (1.5610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5165 (8.3855)  time: 0.7400 (0.5119 -- 2.1077)  data: 0.1879 (0.0002 -- 1.5550)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5267 (1.5672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0188 (8.4004)  time: 0.8978 (0.5394 -- 3.2942)  data: 0.3456 (0.0009 -- 2.7635)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 1.5785 (1.5691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2002 (8.4836)  time: 0.8091 (0.5326 -- 2.3459)  data: 0.2562 (0.0010 -- 1.7418)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8657 (1.6193)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4548 (8.7583)  time: 0.9724 (0.5345 -- 4.0461)  data: 0.4192 (0.0003 -- 3.5017)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6746 (1.6364)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8228 (8.8333)  time: 0.8098 (0.5416 -- 2.8687)  data: 0.2454 (0.0002 -- 2.3261)  max mem: 16413
[2023-08-30 08:00:20,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:00:20,099] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:00:20,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:00:20,100] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:00:29,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23975
[2023-08-30 08:00:29,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23975
[2023-08-30 08:00:29,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:00:29,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:00:29,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8765 (1.6516)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3900 (8.8417)  time: 0.8948 (0.5217 -- 3.0107)  data: 0.1935 (0.0003 -- 2.2616)  max mem: 16413
[2023-08-30 08:00:49,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=137, lr=[9.85360452189916e-09, 9.85360452189916e-09, 1.3138139362532214e-08, 1.3138139362532214e-08, 1.751751915004295e-08, 1.751751915004295e-08, 2.335669220005727e-08, 2.335669220005727e-08, 3.114225626674303e-08, 3.114225626674303e-08, 4.1523008355657365e-08, 4.1523008355657365e-08, 5.536401114087649e-08, 5.536401114087649e-08, 7.381868152116865e-08, 7.381868152116865e-08, 9.842490869489154e-08, 9.842490869489154e-08, 1.312332115931887e-07, 1.312332115931887e-07, 1.7497761545758496e-07, 1.7497761545758496e-07, 2.3330348727677993e-07, 2.3330348727677993e-07, 3.110713163690399e-07, 3.110713163690399e-07, 4.147617551587199e-07, 4.147617551587199e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 08:00:49,534] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.95967876687878, CurrSamplesPerSec=24.657147056229977, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6069 (1.6610)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1842 (8.9877)  time: 0.6664 (0.4978 -- 2.6952)  data: 0.0510 (0.0002 -- 0.8172)  max mem: 16413
Epoch: [149] Total time: 0:02:21 (0.8824 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6069 (1.6980)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1842 (8.9877)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2897 (0.2897)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4468 (2.4468 -- 2.4468)  data: 2.1981 (2.1981 -- 2.1981)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4158 (0.7158)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4207 (0.1997 -- 2.4468)  data: 0.2009 (0.0006 -- 2.1981)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4267 (0.6464)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2158 (0.1696 -- 0.3609)  data: 0.0092 (0.0001 -- 0.1688)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5871 (0.6989)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.1968 (0.1333 -- 0.3609)  data: 0.0089 (0.0001 -- 0.1688)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.658
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [150]  [  0/160]  eta: 0:19:57  lr: 0.000000  min_lr: 0.000000  loss: 1.4227 (1.4227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3318 (9.3318)  time: 7.4833 (7.4833 -- 7.4833)  data: 6.9456 (6.9456 -- 6.9456)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 1.7048 (1.7155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4811 (8.6380)  time: 0.8227 (0.5102 -- 2.9497)  data: 0.2159 (0.0008 -- 2.3917)  max mem: 16413
Epoch: [150]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.6276 (1.6935)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8296 (9.0221)  time: 0.8892 (0.5394 -- 2.4434)  data: 0.2559 (0.0002 -- 1.9095)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6899 (1.6969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6226 (9.0702)  time: 0.8565 (0.5347 -- 3.3048)  data: 0.2819 (0.0004 -- 2.7733)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8724 (1.7255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9108 (8.9841)  time: 0.8465 (0.5219 -- 2.9438)  data: 0.0639 (0.0003 -- 1.2524)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7287 (1.7315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0887 (8.9794)  time: 0.9860 (0.5254 -- 4.1085)  data: 0.0425 (0.0002 -- 0.4989)  max mem: 16413
[2023-08-30 08:02:34,979] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:02:34,979] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:02:34,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:02:34,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5971 (1.6972)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9625 (8.9728)  time: 0.7810 (0.5140 -- 4.1115)  data: 0.0014 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8202 (1.7060)  loss_scale: 32768.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3132 (8.8337)  time: 0.8877 (0.5242 -- 3.0093)  data: 0.0495 (0.0004 -- 0.5541)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8365 (1.7093)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8465 (8.8323)  time: 0.6923 (0.4988 -- 2.7898)  data: 0.0487 (0.0002 -- 0.6431)  max mem: 16413
Epoch: [150] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8365 (1.6939)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8465 (8.8323)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2880 (0.2880)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4480 (2.4480 -- 2.4480)  data: 2.1963 (2.1963 -- 2.1963)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4138 (0.7153)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4348 (0.1995 -- 2.4480)  data: 0.2189 (0.0006 -- 2.1963)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4274 (0.6463)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2156 (0.1705 -- 0.4183)  data: 0.0108 (0.0001 -- 0.1883)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5866 (0.6987)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2020 (0.1329 -- 0.4183)  data: 0.0098 (0.0001 -- 0.1883)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.657
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [151]  [  0/160]  eta: 0:18:47  lr: 0.000000  min_lr: 0.000000  loss: 1.5138 (1.5138)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2324 (8.2324)  time: 7.0465 (7.0465 -- 7.0465)  data: 6.5196 (6.5196 -- 6.5196)  max mem: 16413
[2023-08-30 08:03:36,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24163
[2023-08-30 08:03:36,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:03:36,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24163
[2023-08-30 08:03:36,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:03:36,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.5976 (1.6571)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1289 (9.4774)  time: 0.8745 (0.5236 -- 2.9378)  data: 0.2981 (0.0004 -- 2.3970)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 1.7627 (1.6841)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8175 (9.1493)  time: 0.8138 (0.5274 -- 4.0462)  data: 0.2459 (0.0004 -- 3.4988)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.8010 (1.7213)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7195 (9.4439)  time: 0.9794 (0.5263 -- 2.6242)  data: 0.4366 (0.0003 -- 2.1180)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5104 (1.7033)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4438 (9.2139)  time: 0.8700 (0.5263 -- 2.3038)  data: 0.3079 (0.0004 -- 1.7364)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8640 (1.7153)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3264 (9.2920)  time: 0.8028 (0.5267 -- 2.5789)  data: 0.2555 (0.0001 -- 2.0576)  max mem: 16413
[2023-08-30 08:05:03,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24264
[2023-08-30 08:05:03,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24264
[2023-08-30 08:05:03,954] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:05:03,954] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:05:03,954] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7240 (1.7289)  loss_scale: 8192.0000 (15639.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9618 (9.1517)  time: 0.8635 (0.5327 -- 3.6133)  data: 0.3124 (0.0006 -- 3.0870)  max mem: 16413
Epoch: [151]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7745 (1.7293)  loss_scale: 8192.0000 (14582.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0808 (9.2315)  time: 0.9577 (0.5188 -- 3.5466)  data: 0.4119 (0.0003 -- 3.0373)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7063 (1.7430)  loss_scale: 8192.0000 (13824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1572 (9.2494)  time: 0.6656 (0.4952 -- 2.6061)  data: 0.1475 (0.0002 -- 2.0911)  max mem: 16413
Epoch: [151] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7063 (1.7236)  loss_scale: 8192.0000 (13824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1572 (9.2494)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2881 (0.2881)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3380 (2.3380 -- 2.3380)  data: 2.1162 (2.1162 -- 2.1162)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4137 (0.7152)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4091 (0.1985 -- 2.3380)  data: 0.1940 (0.0008 -- 2.1162)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4265 (0.6463)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2171 (0.1690 -- 0.3171)  data: 0.0097 (0.0001 -- 0.0963)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5865 (0.6987)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2017 (0.1331 -- 0.3171)  data: 0.0094 (0.0001 -- 0.0963)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.657
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [152]  [  0/160]  eta: 0:20:38  lr: 0.000000  min_lr: 0.000000  loss: 1.2107 (1.2107)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5886 (11.5886)  time: 7.7396 (7.7396 -- 7.7396)  data: 7.2059 (7.2059 -- 7.2059)  max mem: 16413
Epoch: [152]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.8093 (1.6970)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8882 (8.8133)  time: 0.8233 (0.5224 -- 2.6819)  data: 0.2351 (0.0003 -- 2.1512)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.7661 (1.7489)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9228 (9.5149)  time: 0.9142 (0.5319 -- 3.2584)  data: 0.3003 (0.0005 -- 2.7495)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6083 (1.7066)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8675 (9.1476)  time: 0.8812 (0.5274 -- 3.9163)  data: 0.3335 (0.0003 -- 3.3648)  max mem: 16413
[2023-08-30 08:07:10,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:07:10,645] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 08:07:10,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:07:10,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [152]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.7803 (1.7446)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9413 (9.1669)  time: 0.8994 (0.5326 -- 3.9107)  data: 0.3495 (0.0005 -- 3.3862)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.4586 (1.7059)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9028 (9.1747)  time: 0.8132 (0.5352 -- 2.9541)  data: 0.1954 (0.0004 -- 2.4402)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7911 (1.7067)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4608 (9.1252)  time: 0.8807 (0.5429 -- 2.0880)  data: 0.1978 (0.0003 -- 1.5739)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8230 (1.7231)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8122 (9.1536)  time: 0.8797 (0.5220 -- 3.8305)  data: 0.2629 (0.0003 -- 3.2780)  max mem: 16413
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8308 (1.7361)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5961 (9.1381)  time: 0.6777 (0.4957 -- 3.7095)  data: 0.1586 (0.0002 -- 3.1591)  max mem: 16413
Epoch: [152] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8308 (1.6863)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5961 (9.1381)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2868 (0.2868)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1622 (2.1622 -- 2.1622)  data: 1.9557 (1.9557 -- 1.9557)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4132 (0.7148)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4301 (0.2171 -- 2.1622)  data: 0.2086 (0.0007 -- 1.9557)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4261 (0.6463)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2291 (0.1710 -- 0.4841)  data: 0.0193 (0.0001 -- 0.2304)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5856 (0.6987)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2085 (0.1332 -- 0.4841)  data: 0.0142 (0.0001 -- 0.2304)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [153]  [  0/160]  eta: 0:21:46  lr: 0.000000  min_lr: 0.000000  loss: 1.8195 (1.8195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9378 (10.9378)  time: 8.1672 (8.1672 -- 8.1672)  data: 7.6127 (7.6127 -- 7.6127)  max mem: 16413
Epoch: [153]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.5330 (1.6345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2532 (9.2821)  time: 0.7946 (0.5222 -- 2.0040)  data: 0.1055 (0.0007 -- 1.1033)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7431 (1.6485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9404 (9.5212)  time: 0.8609 (0.5207 -- 3.2909)  data: 0.1197 (0.0005 -- 1.6534)  max mem: 16413
[2023-08-30 08:09:12,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:09:12,978] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:09:12,979] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:09:12,979] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:09:21,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24533
[2023-08-30 08:09:21,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:09:21,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24533
[2023-08-30 08:09:21,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:09:21,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6128 (1.6704)  loss_scale: 32768.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0800 (9.5221)  time: 0.9873 (0.5225 -- 4.8354)  data: 0.0013 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [153]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7765 (1.7101)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9895 (9.4981)  time: 0.7655 (0.5268 -- 3.1102)  data: 0.0023 (0.0002 -- 0.0148)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7803 (1.7083)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5093 (9.2949)  time: 0.9748 (0.5147 -- 4.5524)  data: 0.0013 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6674 (1.7122)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2011 (9.2948)  time: 0.7155 (0.5336 -- 2.6912)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6402 (1.7029)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3913 (9.2719)  time: 1.0059 (0.5250 -- 4.2682)  data: 0.0020 (0.0001 -- 0.0067)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7505 (1.7080)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0049 (9.1356)  time: 0.6293 (0.4958 -- 2.1472)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [153] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7505 (1.7095)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0049 (9.1356)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2871 (0.2871)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3515 (2.3515 -- 2.3515)  data: 2.1234 (2.1234 -- 2.1234)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4136 (0.7148)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4111 (0.1981 -- 2.3515)  data: 0.1945 (0.0007 -- 2.1234)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4243 (0.6462)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2153 (0.1701 -- 0.3575)  data: 0.0089 (0.0001 -- 0.1138)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5858 (0.6987)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.1986 (0.1326 -- 0.3575)  data: 0.0086 (0.0001 -- 0.1138)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [154]  [  0/160]  eta: 0:20:12  lr: 0.000000  min_lr: 0.000000  loss: 2.1539 (2.1539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4093 (4.4093)  time: 7.5787 (7.5787 -- 7.5787)  data: 7.0207 (7.0207 -- 7.0207)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.6715 (1.6488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0687 (9.2087)  time: 0.8500 (0.5276 -- 2.9499)  data: 0.0587 (0.0004 -- 0.9764)  max mem: 16413
[2023-08-30 08:11:24,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:11:24,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:11:24,484] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:11:24,484] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:11:29,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24667
[2023-08-30 08:11:29,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24667
[2023-08-30 08:11:29,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:11:29,871] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:11:29,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [154]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.6351 (1.6422)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0370 (9.2067)  time: 0.9011 (0.5163 -- 2.5314)  data: 0.2215 (0.0002 -- 1.9845)  max mem: 16413
Epoch: [154]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7428 (1.6668)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6330 (8.9883)  time: 0.8737 (0.5119 -- 3.0332)  data: 0.2758 (0.0006 -- 2.0687)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7290 (1.6876)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6690 (9.2729)  time: 0.7975 (0.5245 -- 2.9986)  data: 0.2481 (0.0002 -- 2.4708)  max mem: 16413
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.5747 (1.6737)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4490 (9.2753)  time: 0.8640 (0.5351 -- 2.9002)  data: 0.3170 (0.0004 -- 2.3772)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.4511 (1.6620)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4340 (9.0486)  time: 0.9796 (0.5274 -- 3.4398)  data: 0.4368 (0.0002 -- 2.8985)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8482 (1.6936)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9324 (8.9838)  time: 0.8556 (0.5234 -- 2.2121)  data: 0.3080 (0.0005 -- 1.5862)  max mem: 16413
[2023-08-30 08:13:10,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24783
[2023-08-30 08:13:10,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24783
[2023-08-30 08:13:10,338] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:13:10,338] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:13:10,338] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5964 (1.6939)  loss_scale: 8192.0000 (16025.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1090 (9.0337)  time: 0.6684 (0.4958 -- 1.5229)  data: 0.0510 (0.0002 -- 0.9715)  max mem: 16413
Epoch: [154] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5964 (1.6857)  loss_scale: 8192.0000 (16025.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1090 (9.0337)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2871 (0.2871)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2373 (2.2373 -- 2.2373)  data: 2.0269 (2.0269 -- 2.0269)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4135 (0.7147)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4177 (0.2001 -- 2.2373)  data: 0.2017 (0.0006 -- 2.0269)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4239 (0.6462)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2236 (0.1700 -- 0.3975)  data: 0.0185 (0.0001 -- 0.1751)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5860 (0.6987)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2087 (0.1332 -- 0.3975)  data: 0.0183 (0.0001 -- 0.1751)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [155]  [  0/160]  eta: 0:21:51  lr: 0.000000  min_lr: 0.000000  loss: 0.9698 (0.9698)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4381 (8.4381)  time: 8.2000 (8.2000 -- 8.2000)  data: 7.0427 (7.0427 -- 7.0427)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.7792 (1.7114)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0207 (10.1806)  time: 0.8177 (0.5223 -- 2.2510)  data: 0.2386 (0.0006 -- 1.7039)  max mem: 16413
Epoch: [155]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.6387 (1.6747)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5397 (9.5442)  time: 0.8712 (0.5342 -- 2.5582)  data: 0.1990 (0.0003 -- 2.0260)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8196 (1.7183)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7423 (9.4420)  time: 0.8593 (0.5317 -- 1.7964)  data: 0.2664 (0.0002 -- 1.2511)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.6723 (1.6962)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4363 (9.6597)  time: 0.8470 (0.5250 -- 2.1238)  data: 0.3067 (0.0002 -- 1.5772)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8058 (1.7277)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0586 (9.8616)  time: 0.9787 (0.5300 -- 3.3449)  data: 0.4352 (0.0005 -- 2.8289)  max mem: 16413
[2023-08-30 08:15:13,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:15:13,326] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 08:15:13,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:15:13,327] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [155]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6667 (1.7215)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1283 (9.5761)  time: 0.8603 (0.5090 -- 5.0869)  data: 0.3236 (0.0002 -- 4.5812)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7473 (1.7309)  loss_scale: 16384.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8859 (9.4631)  time: 0.8799 (0.5166 -- 3.7756)  data: 0.3330 (0.0005 -- 3.2530)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8482 (1.7444)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9930 (9.3365)  time: 0.6503 (0.4987 -- 2.5355)  data: 0.1328 (0.0002 -- 2.0285)  max mem: 16413
Epoch: [155] Total time: 0:02:22 (0.8935 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8482 (1.7059)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9930 (9.3365)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2869 (0.2869)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3787 (2.3787 -- 2.3787)  data: 2.1558 (2.1558 -- 2.1558)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4133 (0.7142)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4152 (0.1995 -- 2.3787)  data: 0.1970 (0.0007 -- 2.1558)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4254 (0.6459)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2209 (0.1693 -- 0.4657)  data: 0.0144 (0.0001 -- 0.2742)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5857 (0.6984)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2056 (0.1332 -- 0.4657)  data: 0.0141 (0.0001 -- 0.2742)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [156]  [  0/160]  eta: 0:18:18  lr: 0.000000  min_lr: 0.000000  loss: 2.1085 (2.1085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4207 (8.4207)  time: 6.8640 (6.8640 -- 6.8640)  data: 5.9841 (5.9841 -- 5.9841)  max mem: 16413
Epoch: [156]  [ 20/160]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7557 (1.7642)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7247 (9.0989)  time: 0.8458 (0.5412 -- 3.0647)  data: 0.1990 (0.0009 -- 1.5732)  max mem: 16413
[2023-08-30 08:16:40,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=142, lr=[2.3508345014499765e-09, 2.3508345014499765e-09, 3.134446001933302e-09, 3.134446001933302e-09, 4.179261335911069e-09, 4.179261335911069e-09, 5.5723484478814254e-09, 5.5723484478814254e-09, 7.429797930508567e-09, 7.429797930508567e-09, 9.90639724067809e-09, 9.90639724067809e-09, 1.3208529654237454e-08, 1.3208529654237454e-08, 1.7611372872316604e-08, 1.7611372872316604e-08, 2.3481830496422138e-08, 2.3481830496422138e-08, 3.130910732856285e-08, 3.130910732856285e-08, 4.17454764380838e-08, 4.17454764380838e-08, 5.56606352507784e-08, 5.56606352507784e-08, 7.42141803343712e-08, 7.42141803343712e-08, 9.895224044582827e-08, 9.895224044582827e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 08:16:40,447] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.99732268535265, CurrSamplesPerSec=23.05533832933287, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.4581 (1.6166)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0015 (9.4165)  time: 0.9880 (0.5343 -- 4.2387)  data: 0.1116 (0.0003 -- 1.2168)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6143 (1.6513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0754 (9.3047)  time: 0.8189 (0.5327 -- 2.5373)  data: 0.1857 (0.0001 -- 2.0066)  max mem: 16413
[2023-08-30 08:17:15,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:17:15,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:17:15,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:17:15,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [156]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6357 (1.6681)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6845 (9.2745)  time: 0.7784 (0.5256 -- 2.6811)  data: 0.2045 (0.0003 -- 2.1344)  max mem: 16413
[2023-08-30 08:17:21,220] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25047
[2023-08-30 08:17:21,220] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:17:21,220] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25047
[2023-08-30 08:17:21,220] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:17:21,220] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [156]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6772 (1.6654)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4067 (9.1310)  time: 0.9611 (0.5278 -- 3.5001)  data: 0.3775 (0.0007 -- 2.9634)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.9314 (1.6751)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3231 (9.0841)  time: 0.7755 (0.5277 -- 2.9082)  data: 0.2215 (0.0004 -- 2.4032)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9862 (1.7031)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3135 (9.2745)  time: 0.9528 (0.5255 -- 4.6091)  data: 0.4044 (0.0003 -- 4.0923)  max mem: 16413
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7657 (1.7034)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6788 (9.4026)  time: 0.6881 (0.4979 -- 3.1930)  data: 0.1639 (0.0002 -- 2.6662)  max mem: 16413
Epoch: [156] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7657 (1.7044)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6788 (9.4026)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2871 (0.2871)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5215 (2.5215 -- 2.5215)  data: 2.2924 (2.2924 -- 2.2924)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4129 (0.7145)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4254 (0.2067 -- 2.5215)  data: 0.2131 (0.0006 -- 2.2924)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4255 (0.6462)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2168 (0.1695 -- 0.4224)  data: 0.0145 (0.0001 -- 0.2347)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5850 (0.6987)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2015 (0.1332 -- 0.4224)  data: 0.0142 (0.0001 -- 0.2347)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.658
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [157]  [  0/160]  eta: 0:19:31  lr: 0.000000  min_lr: 0.000000  loss: 2.0247 (2.0247)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9593 (9.9593)  time: 7.3205 (7.3205 -- 7.3205)  data: 4.8536 (4.8536 -- 4.8536)  max mem: 16413
Epoch: [157]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.8563 (1.8022)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7953 (9.4039)  time: 0.8490 (0.5200 -- 2.6198)  data: 0.0993 (0.0004 -- 1.4555)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.7322 (1.7565)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7490 (9.0356)  time: 0.9537 (0.5188 -- 2.7625)  data: 0.1372 (0.0008 -- 1.9447)  max mem: 16413
[2023-08-30 08:19:21,571] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25170
[2023-08-30 08:19:21,571] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:19:21,571] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25170
[2023-08-30 08:19:21,572] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 08:19:21,572] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [157]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6588 (1.7319)  loss_scale: 8192.0000 (14906.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5421 (8.7145)  time: 0.8360 (0.5158 -- 3.1689)  data: 0.2400 (0.0004 -- 1.8559)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5325 (1.7257)  loss_scale: 8192.0000 (13248.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6674 (8.6925)  time: 0.8792 (0.5275 -- 3.3192)  data: 0.2711 (0.0002 -- 2.7786)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7328 (1.7320)  loss_scale: 8192.0000 (12247.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7254 (8.7735)  time: 0.8843 (0.5263 -- 2.6348)  data: 0.2796 (0.0003 -- 2.1065)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8947 (1.7453)  loss_scale: 8192.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6670 (8.9657)  time: 0.7668 (0.5130 -- 2.7410)  data: 0.1003 (0.0002 -- 1.9824)  max mem: 16413
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6766 (1.7385)  loss_scale: 8192.0000 (11096.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1592 (9.0947)  time: 0.8843 (0.5300 -- 3.5473)  data: 0.0217 (0.0002 -- 0.3936)  max mem: 16413
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4161 (1.7134)  loss_scale: 8192.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7882 (9.0598)  time: 0.7229 (0.4964 -- 3.3949)  data: 0.0544 (0.0002 -- 0.5909)  max mem: 16413
Epoch: [157] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4161 (1.7012)  loss_scale: 8192.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7882 (9.0598)
Val:  [ 0/27]  eta: 0:00:55  loss: 0.2872 (0.2872)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0687 (2.0687 -- 2.0687)  data: 1.8491 (1.8491 -- 1.8491)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4129 (0.7144)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4254 (0.1870 -- 2.0687)  data: 0.2153 (0.0008 -- 1.8491)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4258 (0.6461)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2337 (0.1707 -- 0.6929)  data: 0.0299 (0.0001 -- 0.4776)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5855 (0.6985)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2184 (0.1337 -- 0.6929)  data: 0.0280 (0.0001 -- 0.4776)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [158]  [  0/160]  eta: 0:18:10  lr: 0.000000  min_lr: 0.000000  loss: 0.8127 (0.8127)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0479 (10.0479)  time: 6.8132 (6.8132 -- 6.8132)  data: 6.2417 (6.2417 -- 6.2417)  max mem: 16413
[2023-08-30 08:21:24,200] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:21:24,200] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 08:21:24,201] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:21:24,201] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [158]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.8705 (1.8007)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5160 (9.0324)  time: 0.8785 (0.5212 -- 3.9291)  data: 0.1617 (0.0002 -- 2.1347)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.8991 (1.8221)  loss_scale: 16384.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0756 (9.2046)  time: 0.9151 (0.5298 -- 3.3732)  data: 0.2511 (0.0003 -- 2.6476)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 1.6750 (1.7877)  loss_scale: 16384.0000 (13832.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9282 (9.1199)  time: 0.7384 (0.5331 -- 3.3091)  data: 0.1765 (0.0002 -- 2.6504)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.8076 (1.7678)  loss_scale: 16384.0000 (14462.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1132 (9.1467)  time: 0.9804 (0.5260 -- 3.9519)  data: 0.3298 (0.0005 -- 2.5518)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 1.4352 (1.7139)  loss_scale: 16384.0000 (14842.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2516 (9.1370)  time: 0.7699 (0.5312 -- 2.8341)  data: 0.1751 (0.0006 -- 2.2913)  max mem: 16413
Epoch: [158]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7379 (1.7148)  loss_scale: 16384.0000 (15097.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6519 (9.0008)  time: 1.0562 (0.5266 -- 4.7798)  data: 0.5045 (0.0007 -- 4.2565)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6548 (1.7232)  loss_scale: 16384.0000 (15280.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4859 (9.0048)  time: 0.7308 (0.5136 -- 2.8759)  data: 0.1770 (0.0002 -- 2.3327)  max mem: 16413
[2023-08-30 08:23:16,638] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:23:16,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:23:16,638] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:23:16,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5166 (1.7133)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8993 (8.9939)  time: 0.7344 (0.4974 -- 4.8702)  data: 0.2183 (0.0002 -- 4.3573)  max mem: 16413
Epoch: [158] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5166 (1.6942)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8993 (8.9939)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2873 (0.2873)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3534 (2.3534 -- 2.3534)  data: 2.1252 (2.1252 -- 2.1252)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4133 (0.7143)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4294 (0.1927 -- 2.3534)  data: 0.2152 (0.0008 -- 2.1252)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4261 (0.6461)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2253 (0.1727 -- 0.4419)  data: 0.0178 (0.0001 -- 0.2327)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5855 (0.6985)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2109 (0.1332 -- 0.4419)  data: 0.0175 (0.0001 -- 0.2327)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Epoch: [159]  [  0/160]  eta: 0:19:50  lr: 0.000000  min_lr: 0.000000  loss: 2.1089 (2.1089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4673 (10.4673)  time: 7.4411 (7.4411 -- 7.4411)  data: 5.6688 (5.6688 -- 5.6688)  max mem: 16413
[2023-08-30 08:23:38,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25441
[2023-08-30 08:23:38,841] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25441
[2023-08-30 08:23:38,841] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:23:38,841] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:23:38,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [ 20/160]  eta: 0:02:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7339 (1.8047)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1932 (9.6946)  time: 0.8080 (0.5144 -- 3.0092)  data: 0.1601 (0.0004 -- 1.0542)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.8297 (1.8010)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7679 (9.4026)  time: 0.9144 (0.5217 -- 2.2728)  data: 0.3587 (0.0003 -- 1.7532)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8435 (1.8151)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9274 (9.0550)  time: 0.8572 (0.5181 -- 3.4606)  data: 0.2987 (0.0007 -- 2.9268)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.4240 (1.7345)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8008 (9.1250)  time: 0.8931 (0.5225 -- 2.8516)  data: 0.2892 (0.0002 -- 2.3311)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7153 (1.7144)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7973 (9.0843)  time: 0.8187 (0.5355 -- 3.9497)  data: 0.2652 (0.0005 -- 3.4128)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8322 (1.7148)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4007 (9.1482)  time: 0.9287 (0.5228 -- 4.0581)  data: 0.3820 (0.0003 -- 3.5340)  max mem: 16413
[2023-08-30 08:25:29,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:25:29,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 08:25:29,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 08:25:29,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [140/160]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 1.7067 (1.7104)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6316 (9.2471)  time: 0.7359 (0.5228 -- 2.4575)  data: 0.1361 (0.0003 -- 1.9223)  max mem: 16413
[2023-08-30 08:25:45,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25588
[2023-08-30 08:25:45,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25588
[2023-08-30 08:25:45,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:25:45,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 08:25:45,505] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7067 (1.7102)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6936 (9.1061)  time: 0.7872 (0.4989 -- 3.1586)  data: 0.0684 (0.0002 -- 1.3552)  max mem: 16413
Epoch: [159] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7067 (1.7119)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6936 (9.1061)
[2023-08-30 08:25:52,362] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-08-30 08:25:52,363] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-08-30 08:25:52,364] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-08-30 08:25:52,364] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-08-30 08:25:53,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-08-30 08:25:53,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2871 (0.2871)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5716 (2.5716 -- 2.5716)  data: 2.3771 (2.3771 -- 2.3771)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4131 (0.7142)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4377 (0.1955 -- 2.5716)  data: 0.2248 (0.0007 -- 2.3771)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4269 (0.6461)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2125 (0.1755 -- 0.3261)  data: 0.0049 (0.0001 -- 0.0846)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5853 (0.6985)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.1980 (0.1343 -- 0.3261)  data: 0.0047 (0.0001 -- 0.0846)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.657
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.61%
Test:  [  0/603]  eta: 1:15:16  loss: 0.4468 (0.4468)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 7.4905 (7.4905 -- 7.4905)  data: 7.2514 (7.2514 -- 7.2514)  max mem: 16413
Test:  [ 10/603]  eta: 0:11:31  loss: 0.4849 (0.6308)  acc1: 83.3333 (80.3030)  acc5: 100.0000 (98.4848)  time: 1.1665 (0.1331 -- 7.4905)  data: 0.9663 (0.0004 -- 7.2514)  max mem: 16413
Test:  [ 20/603]  eta: 0:16:04  loss: 0.7746 (0.9323)  acc1: 66.6667 (70.6349)  acc5: 100.0000 (94.4444)  time: 1.3626 (0.1331 -- 13.7849)  data: 1.1724 (0.0004 -- 13.6027)  max mem: 16413
Test:  [ 30/603]  eta: 0:11:37  loss: 0.6560 (0.8006)  acc1: 83.3333 (76.3441)  acc5: 100.0000 (96.2366)  time: 1.2447 (0.1351 -- 13.7849)  data: 1.0047 (0.0002 -- 13.6027)  max mem: 16413
Test:  [ 40/603]  eta: 0:14:02  loss: 0.4520 (0.7791)  acc1: 83.3333 (76.8293)  acc5: 100.0000 (96.7480)  time: 1.3293 (0.1351 -- 13.7574)  data: 1.0212 (0.0002 -- 13.5978)  max mem: 16413
Test:  [ 50/603]  eta: 0:12:59  loss: 0.5719 (0.7540)  acc1: 83.3333 (77.1242)  acc5: 100.0000 (97.0588)  time: 1.7093 (0.1356 -- 13.7574)  data: 1.4189 (0.0006 -- 13.5978)  max mem: 16413
Test:  [ 60/603]  eta: 0:26:20  loss: 0.7219 (0.8368)  acc1: 83.3333 (74.3169)  acc5: 100.0000 (95.9016)  time: 5.8117 (0.1325 -- 53.3268)  data: 5.5665 (0.0003 -- 53.1907)  max mem: 16413
Test:  [ 70/603]  eta: 0:22:47  loss: 0.5601 (0.7917)  acc1: 83.3333 (76.5258)  acc5: 100.0000 (96.4789)  time: 5.5112 (0.1135 -- 53.3268)  data: 5.3299 (0.0002 -- 53.1907)  max mem: 16413
Test:  [ 80/603]  eta: 0:23:17  loss: 0.4849 (0.7835)  acc1: 83.3333 (76.5432)  acc5: 100.0000 (96.7078)  time: 1.9448 (0.1135 -- 18.3959)  data: 1.7797 (0.0002 -- 18.2167)  max mem: 16413
Test:  [ 90/603]  eta: 0:23:18  loss: 0.5106 (0.7689)  acc1: 83.3333 (76.7399)  acc5: 100.0000 (96.8864)  time: 3.3002 (0.1226 -- 19.0878)  data: 2.9479 (0.0006 -- 18.2167)  max mem: 16413
Test:  [100/603]  eta: 0:27:14  loss: 0.7234 (0.8159)  acc1: 83.3333 (75.2475)  acc5: 100.0000 (96.2046)  time: 5.5870 (0.1805 -- 38.0461)  data: 5.0024 (0.0006 -- 37.1692)  max mem: 16413
Test:  [110/603]  eta: 0:24:47  loss: 0.5532 (0.7904)  acc1: 83.3333 (76.4264)  acc5: 100.0000 (96.5465)  time: 4.3425 (0.1214 -- 38.0461)  data: 3.8832 (0.0002 -- 37.1692)  max mem: 16413
Test:  [120/603]  eta: 0:25:23  loss: 0.4824 (0.7842)  acc1: 83.3333 (76.4463)  acc5: 100.0000 (96.6942)  time: 2.6783 (0.1214 -- 43.0055)  data: 2.3012 (0.0002 -- 41.7677)  max mem: 16413
Test:  [130/603]  eta: 0:24:48  loss: 0.5097 (0.7699)  acc1: 83.3333 (76.7176)  acc5: 100.0000 (96.9466)  time: 3.8686 (0.1605 -- 43.0055)  data: 3.3572 (0.0005 -- 41.7677)  max mem: 16413
Test:  [140/603]  eta: 0:28:11  loss: 0.5097 (0.7625)  acc1: 83.3333 (77.4232)  acc5: 100.0000 (96.8085)  time: 6.6624 (0.1425 -- 53.6088)  data: 6.2265 (0.0002 -- 53.2718)  max mem: 16413
Test:  [150/603]  eta: 0:26:05  loss: 0.4414 (0.7402)  acc1: 83.3333 (78.0353)  acc5: 100.0000 (96.9095)  time: 5.4653 (0.1425 -- 53.6088)  data: 4.9618 (0.0002 -- 53.2718)  max mem: 16413
Test:  [160/603]  eta: 0:27:07  loss: 0.5267 (0.7380)  acc1: 83.3333 (78.1574)  acc5: 100.0000 (96.8944)  time: 3.8218 (0.1627 -- 31.1093)  data: 3.1455 (0.0008 -- 30.9050)  max mem: 16413
Test:  [170/603]  eta: 0:25:56  loss: 0.5485 (0.7293)  acc1: 83.3333 (78.2651)  acc5: 100.0000 (97.0760)  time: 4.6461 (0.3175 -- 31.1093)  data: 3.9174 (0.0008 -- 30.9050)  max mem: 16413
Test:  [180/603]  eta: 0:27:47  loss: 0.4703 (0.7262)  acc1: 83.3333 (78.7293)  acc5: 100.0000 (96.9613)  time: 6.1025 (0.3305 -- 49.2274)  data: 5.4301 (0.0012 -- 49.0539)  max mem: 16413
Test:  [190/603]  eta: 0:26:41  loss: 0.4047 (0.7106)  acc1: 83.3333 (79.1449)  acc5: 100.0000 (97.0332)  time: 6.3023 (0.3729 -- 49.2274)  data: 5.5472 (0.0017 -- 49.0539)  max mem: 16413
Test:  [200/603]  eta: 0:26:52  loss: 0.4919 (0.7095)  acc1: 83.3333 (79.1874)  acc5: 100.0000 (97.0149)  time: 4.5470 (0.1422 -- 53.3006)  data: 3.8275 (0.0006 -- 53.0501)  max mem: 16413
Test:  [210/603]  eta: 0:26:04  loss: 0.5565 (0.7041)  acc1: 83.3333 (79.2259)  acc5: 100.0000 (97.1564)  time: 4.9792 (0.1422 -- 53.3006)  data: 4.3863 (0.0006 -- 53.0501)  max mem: 16413
Test:  [220/603]  eta: 0:26:34  loss: 0.5487 (0.7004)  acc1: 83.3333 (79.6380)  acc5: 100.0000 (97.1342)  time: 5.7812 (0.1246 -- 45.5431)  data: 5.3288 (0.0006 -- 45.3057)  max mem: 16413
Test:  [230/603]  eta: 0:25:08  loss: 0.4802 (0.6912)  acc1: 83.3333 (79.8701)  acc5: 100.0000 (97.1140)  time: 4.7125 (0.1246 -- 45.5431)  data: 4.4104 (0.0006 -- 45.3057)  max mem: 16413
Test:  [240/603]  eta: 0:25:03  loss: 0.4802 (0.6911)  acc1: 83.3333 (79.8755)  acc5: 100.0000 (97.0954)  time: 3.9004 (0.1268 -- 44.4979)  data: 3.4796 (0.0008 -- 44.2093)  max mem: 16413
Test:  [250/603]  eta: 0:24:28  loss: 0.4930 (0.6926)  acc1: 66.6667 (79.6813)  acc5: 100.0000 (97.2112)  time: 5.4676 (0.1320 -- 44.4979)  data: 4.9187 (0.0007 -- 44.2093)  max mem: 16413
Test:  [260/603]  eta: 0:23:50  loss: 0.6538 (0.6980)  acc1: 66.6667 (79.5019)  acc5: 100.0000 (97.0626)  time: 4.5133 (0.1320 -- 24.6195)  data: 3.9766 (0.0002 -- 23.9639)  max mem: 16413
Test:  [270/603]  eta: 0:23:15  loss: 0.3776 (0.6903)  acc1: 83.3333 (79.8893)  acc5: 100.0000 (97.1095)  time: 4.5934 (0.1175 -- 41.3480)  data: 4.3005 (0.0002 -- 41.2142)  max mem: 16413
Test:  [280/603]  eta: 0:22:37  loss: 0.4013 (0.6984)  acc1: 83.3333 (79.5374)  acc5: 100.0000 (97.0344)  time: 4.6209 (0.1175 -- 41.3480)  data: 4.3416 (0.0003 -- 41.2142)  max mem: 16413
Test:  [290/603]  eta: 0:22:01  loss: 0.6513 (0.6994)  acc1: 66.6667 (79.3814)  acc5: 100.0000 (97.1363)  time: 4.6293 (0.1268 -- 42.9637)  data: 4.3308 (0.0004 -- 42.5994)  max mem: 16413
Test:  [300/603]  eta: 0:21:39  loss: 0.4759 (0.7018)  acc1: 83.3333 (79.2913)  acc5: 100.0000 (97.0653)  time: 5.4852 (0.1310 -- 44.3932)  data: 5.2441 (0.0002 -- 44.1861)  max mem: 16413
Test:  [310/603]  eta: 0:21:16  loss: 0.3696 (0.6970)  acc1: 83.3333 (79.5820)  acc5: 100.0000 (97.0525)  time: 6.3505 (0.1146 -- 44.3932)  data: 6.1319 (0.0002 -- 44.1861)  max mem: 16413
Test:  [320/603]  eta: 0:20:29  loss: 0.4414 (0.7023)  acc1: 83.3333 (79.3354)  acc5: 100.0000 (97.0405)  time: 5.2042 (0.1146 -- 43.4604)  data: 4.8737 (0.0002 -- 43.3133)  max mem: 16413
Test:  [330/603]  eta: 0:19:38  loss: 0.7840 (0.7045)  acc1: 66.6667 (79.1541)  acc5: 100.0000 (97.0796)  time: 3.7062 (0.1377 -- 30.6674)  data: 3.2043 (0.0005 -- 29.8127)  max mem: 16413
Test:  [340/603]  eta: 0:18:55  loss: 0.6802 (0.7065)  acc1: 83.3333 (79.0811)  acc5: 100.0000 (97.0186)  time: 3.8570 (0.1377 -- 20.6296)  data: 3.3898 (0.0005 -- 20.2529)  max mem: 16413
Test:  [350/603]  eta: 0:18:25  loss: 0.4493 (0.7021)  acc1: 83.3333 (79.3447)  acc5: 100.0000 (97.0085)  time: 5.2174 (0.1340 -- 54.6675)  data: 4.8810 (0.0006 -- 54.5154)  max mem: 16413
Test:  [360/603]  eta: 0:17:49  loss: 0.6508 (0.7066)  acc1: 83.3333 (79.1320)  acc5: 100.0000 (96.9991)  time: 5.8551 (0.1340 -- 54.6675)  data: 5.3860 (0.0006 -- 54.5154)  max mem: 16413
Test:  [370/603]  eta: 0:16:51  loss: 0.8595 (0.7094)  acc1: 83.3333 (79.0656)  acc5: 100.0000 (96.9901)  time: 3.8807 (0.1265 -- 15.7804)  data: 3.4098 (0.0005 -- 14.9599)  max mem: 16413
Test:  [380/603]  eta: 0:16:18  loss: 0.4856 (0.7111)  acc1: 83.3333 (79.0464)  acc5: 100.0000 (96.8941)  time: 4.1349 (0.1265 -- 29.5184)  data: 3.8377 (0.0005 -- 29.1244)  max mem: 16413
Test:  [390/603]  eta: 0:15:23  loss: 0.3906 (0.7067)  acc1: 83.3333 (79.3265)  acc5: 100.0000 (96.8457)  time: 4.2191 (0.1396 -- 29.5184)  data: 3.8644 (0.0006 -- 29.1244)  max mem: 16413
Test:  [400/603]  eta: 0:15:02  loss: 0.5313 (0.7077)  acc1: 83.3333 (79.1771)  acc5: 100.0000 (96.7997)  time: 5.5686 (0.1580 -- 50.0797)  data: 5.2382 (0.0008 -- 49.9341)  max mem: 16413
Test:  [410/603]  eta: 0:14:12  loss: 0.5680 (0.7083)  acc1: 83.3333 (79.1971)  acc5: 100.0000 (96.8370)  time: 6.0220 (0.1398 -- 50.0797)  data: 5.5944 (0.0006 -- 49.9341)  max mem: 16413
Test:  [420/603]  eta: 0:13:32  loss: 0.5467 (0.7099)  acc1: 83.3333 (79.1766)  acc5: 100.0000 (96.7538)  time: 4.3686 (0.1398 -- 43.8036)  data: 3.8437 (0.0006 -- 43.3618)  max mem: 16413
Test:  [430/603]  eta: 0:12:47  loss: 0.4788 (0.7065)  acc1: 83.3333 (79.3890)  acc5: 100.0000 (96.7131)  time: 4.7747 (0.2521 -- 43.8036)  data: 4.1867 (0.0012 -- 43.3618)  max mem: 16413
Test:  [440/603]  eta: 0:12:16  loss: 0.4854 (0.7072)  acc1: 83.3333 (79.2517)  acc5: 100.0000 (96.6742)  time: 6.1752 (0.2927 -- 42.5863)  data: 5.3779 (0.0012 -- 41.6664)  max mem: 16413
Test:  [450/603]  eta: 0:11:32  loss: 0.5884 (0.7078)  acc1: 83.3333 (79.2683)  acc5: 100.0000 (96.7110)  time: 6.4639 (0.3986 -- 42.5863)  data: 5.6634 (0.0018 -- 41.6664)  max mem: 16413
Test:  [460/603]  eta: 0:10:51  loss: 0.5524 (0.7094)  acc1: 83.3333 (79.2480)  acc5: 100.0000 (96.6377)  time: 5.3948 (0.3346 -- 37.8610)  data: 4.6957 (0.0016 -- 36.3558)  max mem: 16413
Test:  [470/603]  eta: 0:10:09  loss: 0.4125 (0.7062)  acc1: 83.3333 (79.4409)  acc5: 100.0000 (96.6030)  time: 5.9362 (0.2009 -- 34.8317)  data: 5.2306 (0.0016 -- 34.3923)  max mem: 16413
Test:  [480/603]  eta: 0:09:24  loss: 0.3297 (0.7068)  acc1: 83.3333 (79.3139)  acc5: 100.0000 (96.5696)  time: 5.2813 (0.2009 -- 41.7946)  data: 4.6398 (0.0012 -- 41.4544)  max mem: 16413
Test:  [490/603]  eta: 0:08:35  loss: 0.6569 (0.7073)  acc1: 83.3333 (79.2261)  acc5: 100.0000 (96.5716)  time: 4.0895 (0.2798 -- 41.7946)  data: 3.3439 (0.0012 -- 41.4544)  max mem: 16413
Test:  [500/603]  eta: 0:07:50  loss: 0.7328 (0.7156)  acc1: 83.3333 (79.0752)  acc5: 100.0000 (96.3407)  time: 3.9947 (0.1215 -- 17.4145)  data: 3.4167 (0.0011 -- 17.1717)  max mem: 16413
Test:  [510/603]  eta: 0:07:09  loss: 0.4681 (0.7131)  acc1: 83.3333 (79.1911)  acc5: 100.0000 (96.3470)  time: 5.8036 (0.1215 -- 48.7014)  data: 5.4408 (0.0011 -- 48.4538)  max mem: 16413
Test:  [520/603]  eta: 0:06:21  loss: 0.5594 (0.7162)  acc1: 83.3333 (78.9827)  acc5: 100.0000 (96.3852)  time: 5.4965 (0.1250 -- 48.7014)  data: 5.0533 (0.0006 -- 48.4538)  max mem: 16413
Test:  [530/603]  eta: 0:05:37  loss: 0.8775 (0.7176)  acc1: 66.6667 (78.8763)  acc5: 100.0000 (96.3591)  time: 5.0359 (0.1250 -- 26.7686)  data: 4.4012 (0.0006 -- 26.5757)  max mem: 16413
Test:  [540/603]  eta: 0:04:53  loss: 0.7730 (0.7251)  acc1: 83.3333 (78.7431)  acc5: 100.0000 (96.1491)  time: 6.1061 (0.1359 -- 38.4216)  data: 5.4555 (0.0010 -- 38.2589)  max mem: 16413
Test:  [550/603]  eta: 0:04:09  loss: 0.4564 (0.7227)  acc1: 83.3333 (78.8566)  acc5: 100.0000 (96.1585)  time: 6.6183 (0.1129 -- 57.8551)  data: 6.3178 (0.0001 -- 57.7312)  max mem: 16413
Test:  [560/603]  eta: 0:03:21  loss: 0.4564 (0.7253)  acc1: 83.3333 (78.6690)  acc5: 100.0000 (96.1973)  time: 5.2596 (0.1129 -- 57.8551)  data: 5.0229 (0.0001 -- 57.7312)  max mem: 16413
Test:  [570/603]  eta: 0:02:34  loss: 0.7974 (0.7265)  acc1: 66.6667 (78.5756)  acc5: 100.0000 (96.1763)  time: 4.1979 (0.1372 -- 47.6407)  data: 3.8433 (0.0006 -- 46.4354)  max mem: 16413
Test:  [580/603]  eta: 0:01:48  loss: 0.7807 (0.7334)  acc1: 66.6667 (78.4567)  acc5: 100.0000 (95.9839)  time: 5.9304 (0.1372 -- 65.8257)  data: 5.5884 (0.0006 -- 65.2877)  max mem: 16413
Test:  [590/603]  eta: 0:01:01  loss: 0.4688 (0.7309)  acc1: 83.3333 (78.5674)  acc5: 100.0000 (95.9955)  time: 6.9983 (0.1412 -- 65.8257)  data: 6.7083 (0.0006 -- 65.2877)  max mem: 16413
Test:  [600/603]  eta: 0:00:14  loss: 0.5715 (0.7332)  acc1: 83.3333 (78.3971)  acc5: 100.0000 (96.0344)  time: 5.0818 (0.1342 -- 53.4404)  data: 4.8549 (0.0003 -- 53.1966)  max mem: 16413
Test:  [602/603]  eta: 0:00:04  loss: 0.5715 (0.7337)  acc1: 83.3333 (78.4232)  acc5: 100.0000 (96.0166)  time: 2.3990 (0.1254 -- 26.3591)  data: 2.1949 (0.0003 -- 26.2002)  max mem: 16413
Test: Total time: 0:47:26 (4.7200 s / it)
* Acc@1 79.710 Acc@5 95.934 loss 0.714
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 7230 test videos: Top-1: 86.72%, Top-5: 98.96%
Training time 7:40:22
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
