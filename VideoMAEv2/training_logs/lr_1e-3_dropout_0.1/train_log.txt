[2023-08-31 22:10:07,748] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 22:10:07,762] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.1, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9026376970>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.1, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.1, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-31 22:10:13,393] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-31 22:10:13,393] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-31 22:10:13,405] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-31 22:10:13,405] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-31 22:10:13,518] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4143795967102051 seconds
[2023-08-31 22:10:14,481] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-31 22:10:14,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-31 22:10:14,487] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-31 22:10:14,502] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-31 22:10:14,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-31 22:10:14,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-31 22:10:14,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 22:10:14,503] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 22:10:14,503] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 22:10:14,503] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8fceab3400>
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 22:10:14,504] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 22:10:14,505] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 22:10:14,505] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:30:54  lr: 0.000000  min_lr: 0.000000  loss: 2.7733 (2.7733)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.5922 (11.5922 -- 11.5922)  data: 6.4639 (6.4639 -- 6.4639)  max mem: 16735
Epoch: [0]  [ 20/160]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 2.7732 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3556 (1.3456)  time: 0.6342 (0.5019 -- 2.2870)  data: 0.0020 (0.0006 -- 0.0149)  max mem: 16735
Epoch: [0]  [ 40/160]  eta: 0:02:11  lr: 0.000002  min_lr: 0.000000  loss: 2.7724 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2455 (1.3095)  time: 1.0263 (0.5117 -- 4.1324)  data: 0.0257 (0.0002 -- 0.3134)  max mem: 16735
Epoch: [0]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 2.7726 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2071 (1.3040)  time: 0.7692 (0.5062 -- 3.2332)  data: 0.0022 (0.0004 -- 0.0174)  max mem: 16735
Epoch: [0]  [ 80/160]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000000  loss: 2.7725 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2648 (1.2939)  time: 0.9920 (0.5040 -- 5.4648)  data: 0.0017 (0.0006 -- 0.0059)  max mem: 16735
Epoch: [0]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2808 (1.2891)  time: 0.6877 (0.5123 -- 3.0639)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16735
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 2.7722 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2721 (1.2883)  time: 0.9655 (0.5003 -- 4.2771)  data: 0.0305 (0.0002 -- 0.5765)  max mem: 16735
[2023-08-31 22:12:13,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:12:13,416] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:12:13,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-31 22:12:13,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 2.7722 (2.7725)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.2147 (1.2794)  time: 0.7744 (0.5183 -- 3.2118)  data: 0.0015 (0.0007 -- 0.0050)  max mem: 16735
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7714 (2.7724)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1760 (1.2687)  time: 0.7052 (0.4905 -- 2.5466)  data: 0.0006 (0.0001 -- 0.0019)  max mem: 16735
Epoch: [0] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7714 (2.7724)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1760 (1.2687)
Val:  [ 0/27]  eta: 0:01:17  loss: 2.7697 (2.7697)  acc1: 0.0000 (0.0000)  acc5: 55.5556 (55.5556)  time: 2.8842 (2.8842 -- 2.8842)  data: 2.4958 (2.4958 -- 2.4958)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 2.7693 (2.7692)  acc1: 22.2222 (22.2222)  acc5: 66.6667 (61.6162)  time: 0.4613 (0.1963 -- 2.8842)  data: 0.2278 (0.0006 -- 2.4958)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.7689 (2.7690)  acc1: 22.2222 (23.8095)  acc5: 66.6667 (62.9630)  time: 0.2207 (0.1684 -- 0.3862)  data: 0.0106 (0.0001 -- 0.1995)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.7689 (2.7691)  acc1: 22.2222 (22.8216)  acc5: 66.6667 (61.4108)  time: 0.2052 (0.1659 -- 0.3862)  data: 0.0104 (0.0001 -- 0.1995)  max mem: 16735
Val: Total time: 0:00:08 (0.3075 s / it)
* Acc@1 22.614 Acc@5 62.656 loss 2.769
Accuracy of the network on the 482 val images: 22.61%
[2023-08-31 22:12:45,090] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-31 22:12:45,095] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:12:45,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:12:45,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:12:46,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:12:46,010] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 22.61%
Epoch: [1]  [  0/160]  eta: 0:19:16  lr: 0.000009  min_lr: 0.000000  loss: 2.7707 (2.7707)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5604 (1.5604)  time: 7.2304 (7.2304 -- 7.2304)  data: 6.6895 (6.6895 -- 6.6895)  max mem: 16735
Epoch: [1]  [ 20/160]  eta: 0:02:34  lr: 0.000011  min_lr: 0.000000  loss: 2.7708 (2.7705)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3215 (1.3545)  time: 0.7982 (0.5127 -- 2.3588)  data: 0.2597 (0.0003 -- 1.8148)  max mem: 16735
Epoch: [1]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 2.7695 (2.7702)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4373 (1.3926)  time: 0.9376 (0.5162 -- 2.9790)  data: 0.1539 (0.0008 -- 1.7009)  max mem: 16735
Epoch: [1]  [ 60/160]  eta: 0:01:38  lr: 0.000013  min_lr: 0.000000  loss: 2.7681 (2.7695)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5058 (1.4328)  time: 0.8976 (0.5199 -- 4.0996)  data: 0.0895 (0.0004 -- 1.1993)  max mem: 16735
Epoch: [1]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 2.7640 (2.7683)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4957 (1.4340)  time: 0.8664 (0.5194 -- 3.9911)  data: 0.0298 (0.0003 -- 0.5716)  max mem: 16735
[2023-08-31 22:14:17,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:14:17,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:14:17,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-31 22:14:17,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 2.7613 (2.7664)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5486 (1.4521)  time: 0.8051 (0.5295 -- 3.1218)  data: 0.0018 (0.0006 -- 0.0049)  max mem: 16735
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.7513 (2.7639)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4767 (1.4582)  time: 0.8882 (0.5350 -- 3.0426)  data: 0.2123 (0.0004 -- 2.5179)  max mem: 16735
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.7393 (2.7603)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5521 (1.4707)  time: 0.8053 (0.5234 -- 1.7749)  data: 0.1670 (0.0004 -- 1.2392)  max mem: 16735
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.7338 (2.7573)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5567 (1.4869)  time: 0.6682 (0.4998 -- 1.4536)  data: 0.0465 (0.0003 -- 0.9110)  max mem: 16735
Epoch: [1] Total time: 0:02:20 (0.8754 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.7338 (2.7580)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5567 (1.4869)
Val:  [ 0/27]  eta: 0:01:00  loss: 2.6882 (2.6882)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.2352 (2.2352 -- 2.2352)  data: 2.0217 (2.0217 -- 2.0217)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 2.7116 (2.7079)  acc1: 22.2222 (35.3535)  acc5: 88.8889 (80.8081)  time: 0.4154 (0.1942 -- 2.2352)  data: 0.2014 (0.0008 -- 2.0217)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.6997 (2.7021)  acc1: 22.2222 (37.5661)  acc5: 88.8889 (82.5397)  time: 0.2229 (0.1679 -- 0.4495)  data: 0.0160 (0.0001 -- 0.1811)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.7023 (2.7052)  acc1: 33.3333 (36.9295)  acc5: 77.7778 (80.9129)  time: 0.2086 (0.1326 -- 0.4495)  data: 0.0155 (0.0001 -- 0.1811)  max mem: 16735
Val: Total time: 0:00:07 (0.2844 s / it)
* Acc@1 37.137 Acc@5 81.743 loss 2.705
Accuracy of the network on the 482 val images: 37.14%
[2023-08-31 22:15:13,750] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:15:13,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:15:13,752] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:15:13,752] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:15:15,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:15:15,220] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.14%
Epoch: [2]  [  0/160]  eta: 0:21:22  lr: 0.000019  min_lr: 0.000000  loss: 2.7530 (2.7530)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4587 (1.4587)  time: 8.0127 (8.0127 -- 8.0127)  data: 7.4759 (7.4759 -- 7.4759)  max mem: 16735
Epoch: [2]  [ 20/160]  eta: 0:02:40  lr: 0.000020  min_lr: 0.000000  loss: 2.7301 (2.7303)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5326 (1.5432)  time: 0.8016 (0.5322 -- 4.7746)  data: 0.2537 (0.0003 -- 4.2563)  max mem: 16735
Epoch: [2]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000001  loss: 2.7152 (2.7256)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5967 (1.5730)  time: 0.9852 (0.5165 -- 4.6102)  data: 0.4399 (0.0003 -- 4.0871)  max mem: 16735
Epoch: [2]  [ 60/160]  eta: 0:01:35  lr: 0.000022  min_lr: 0.000001  loss: 2.6970 (2.7172)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6354 (1.5933)  time: 0.7170 (0.5174 -- 2.4440)  data: 0.1711 (0.0005 -- 1.9404)  max mem: 16735
[2023-08-31 22:16:17,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:16:17,381] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-31 22:16:17,388] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:16:17,389] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.7052 (2.7145)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6076 (1.5950)  time: 0.8751 (0.5298 -- 3.7670)  data: 0.3288 (0.0004 -- 3.2360)  max mem: 16735
Epoch: [2]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000001  loss: 2.6805 (2.7103)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6105 (1.6135)  time: 0.8984 (0.5262 -- 2.6757)  data: 0.1195 (0.0003 -- 1.4915)  max mem: 16735
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 2.6764 (2.7057)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5978 (1.6328)  time: 0.8685 (0.5286 -- 2.0086)  data: 0.2455 (0.0004 -- 1.4767)  max mem: 16735
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 2.6634 (2.7002)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8276 (1.6652)  time: 0.8354 (0.5241 -- 2.4855)  data: 0.2853 (0.0005 -- 1.9286)  max mem: 16735
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.6629 (2.6943)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7572 (1.6758)  time: 0.7145 (0.4984 -- 2.2239)  data: 0.1477 (0.0002 -- 1.6783)  max mem: 16735
Epoch: [2] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.6629 (2.6949)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7572 (1.6758)
Val:  [ 0/27]  eta: 0:01:02  loss: 2.5082 (2.5082)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3130 (2.3130 -- 2.3130)  data: 2.0631 (2.0631 -- 2.0631)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 2.5679 (2.5611)  acc1: 33.3333 (33.3333)  acc5: 88.8889 (83.8384)  time: 0.4143 (0.2097 -- 2.3130)  data: 0.1935 (0.0006 -- 2.0631)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.5393 (2.5454)  acc1: 33.3333 (34.9206)  acc5: 88.8889 (84.6561)  time: 0.2200 (0.1682 -- 0.3847)  data: 0.0135 (0.0001 -- 0.2021)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.5412 (2.5542)  acc1: 33.3333 (34.4398)  acc5: 77.7778 (81.3278)  time: 0.2038 (0.1325 -- 0.3847)  data: 0.0131 (0.0001 -- 0.2021)  max mem: 16735
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 36.515 Acc@5 83.195 loss 2.554
Accuracy of the network on the 482 val images: 36.51%
Max accuracy: 37.14%
Epoch: [3]  [  0/160]  eta: 0:15:53  lr: 0.000028  min_lr: 0.000001  loss: 2.6677 (2.6677)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9495 (1.9495)  time: 5.9576 (5.9576 -- 5.9576)  data: 5.4177 (5.4177 -- 5.4177)  max mem: 16735
Epoch: [3]  [ 20/160]  eta: 0:02:36  lr: 0.000029  min_lr: 0.000001  loss: 2.6785 (2.6758)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8140 (1.9361)  time: 0.8749 (0.5251 -- 3.0192)  data: 0.3032 (0.0003 -- 2.4815)  max mem: 16735
[2023-08-31 22:18:20,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:18:20,548] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-31 22:18:20,549] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:18:20,549] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:03  lr: 0.000031  min_lr: 0.000001  loss: 2.6572 (2.6631)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7291 (1.8465)  time: 0.9285 (0.5253 -- 4.3994)  data: 0.3702 (0.0003 -- 3.8708)  max mem: 16735
Epoch: [3]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000001  loss: 2.6390 (2.6523)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8282 (1.8973)  time: 0.8685 (0.5268 -- 3.5750)  data: 0.2684 (0.0002 -- 3.0383)  max mem: 16735
Epoch: [3]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000001  loss: 2.6112 (2.6449)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0159 (1.9358)  time: 0.8607 (0.5326 -- 2.8144)  data: 0.3079 (0.0003 -- 2.2879)  max mem: 16735
Epoch: [3]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000001  loss: 2.6502 (2.6435)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8100 (1.9460)  time: 0.8811 (0.5299 -- 3.6301)  data: 0.0619 (0.0003 -- 0.7802)  max mem: 16735
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000001  loss: 2.5927 (2.6348)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1487 (1.9961)  time: 0.8144 (0.5326 -- 2.4742)  data: 0.0843 (0.0003 -- 1.6552)  max mem: 16735
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 2.6119 (2.6302)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0972 (2.0401)  time: 0.8914 (0.5255 -- 3.6717)  data: 0.3432 (0.0004 -- 3.1261)  max mem: 16735
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.6022 (2.6259)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1370 (2.0668)  time: 0.6967 (0.4983 -- 1.8288)  data: 0.1248 (0.0002 -- 1.3238)  max mem: 16735
Epoch: [3] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.6022 (2.6218)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1370 (2.0668)
Val:  [ 0/27]  eta: 0:00:59  loss: 2.3380 (2.3380)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.2147 (2.2147 -- 2.2147)  data: 2.0164 (2.0164 -- 2.0164)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 2.4094 (2.4073)  acc1: 44.4444 (40.4040)  acc5: 88.8889 (85.8586)  time: 0.4133 (0.1927 -- 2.2147)  data: 0.1847 (0.0005 -- 2.0164)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.3466 (2.3801)  acc1: 44.4444 (39.6825)  acc5: 88.8889 (85.7143)  time: 0.2231 (0.1690 -- 0.3699)  data: 0.0085 (0.0001 -- 0.1504)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.3496 (2.3945)  acc1: 22.2222 (38.1743)  acc5: 77.7778 (83.4025)  time: 0.2044 (0.1324 -- 0.3699)  data: 0.0079 (0.0001 -- 0.1504)  max mem: 16735
Val: Total time: 0:00:07 (0.2839 s / it)
* Acc@1 38.382 Acc@5 83.610 loss 2.394
Accuracy of the network on the 482 val images: 38.38%
[2023-08-31 22:20:13,845] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:20:13,846] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:20:13,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:20:13,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:20:15,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:20:15,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.38%
[2023-08-31 22:20:20,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:20:20,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-31 22:20:20,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:20:20,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:49  lr: 0.000038  min_lr: 0.000001  loss: 2.6218 (2.6218)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9148 (1.9148)  time: 5.5591 (5.5591 -- 5.5591)  data: 4.5467 (4.5467 -- 4.5467)  max mem: 16735
Epoch: [4]  [ 20/160]  eta: 0:02:45  lr: 0.000039  min_lr: 0.000001  loss: 2.5569 (2.5435)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2393 (2.2276)  time: 0.9614 (0.5260 -- 3.4717)  data: 0.3084 (0.0004 -- 2.9214)  max mem: 16735
Epoch: [4]  [ 40/160]  eta: 0:01:59  lr: 0.000040  min_lr: 0.000001  loss: 2.5956 (2.5622)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4164 (2.4489)  time: 0.8086 (0.5217 -- 3.6411)  data: 0.2604 (0.0003 -- 3.1084)  max mem: 16735
Epoch: [4]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000001  loss: 2.5233 (2.5476)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3333 (2.5830)  time: 0.9968 (0.5208 -- 3.8103)  data: 0.4493 (0.0006 -- 3.2666)  max mem: 16735
Epoch: [4]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000001  loss: 2.5371 (2.5405)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6475 (2.6734)  time: 0.7702 (0.5258 -- 3.5238)  data: 0.2234 (0.0005 -- 2.9891)  max mem: 16735
Epoch: [4]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000001  loss: 2.5799 (2.5440)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5933 (2.6671)  time: 0.9036 (0.5305 -- 4.1042)  data: 0.3534 (0.0004 -- 3.5553)  max mem: 16735
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.5088 (2.5336)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6119 (2.6594)  time: 0.7916 (0.5188 -- 3.9787)  data: 0.2393 (0.0003 -- 3.4253)  max mem: 16735
[2023-08-31 22:22:14,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:22:14,158] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-31 22:22:14,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:22:14,159] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.6122 (2.5398)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8854 (2.7357)  time: 0.9842 (0.5244 -- 4.8690)  data: 0.4399 (0.0003 -- 4.3441)  max mem: 16735
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5689 (2.5439)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9012 (2.7761)  time: 0.6147 (0.5013 -- 1.7447)  data: 0.0830 (0.0002 -- 1.2129)  max mem: 16735
Epoch: [4] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5689 (2.5556)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9012 (2.7761)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.1964 (2.1964)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3335 (2.3335 -- 2.3335)  data: 2.1266 (2.1266 -- 2.1266)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 2.2548 (2.2413)  acc1: 33.3333 (39.3939)  acc5: 100.0000 (92.9293)  time: 0.4084 (0.1893 -- 2.3335)  data: 0.1996 (0.0005 -- 2.1266)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.1733 (2.2063)  acc1: 33.3333 (41.2698)  acc5: 88.8889 (91.0053)  time: 0.2168 (0.1699 -- 0.3464)  data: 0.0129 (0.0001 -- 0.1430)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.2067 (2.2285)  acc1: 33.3333 (40.6639)  acc5: 88.8889 (87.9668)  time: 0.2046 (0.1330 -- 0.3464)  data: 0.0126 (0.0001 -- 0.1430)  max mem: 16735
Val: Total time: 0:00:07 (0.2838 s / it)
* Acc@1 41.909 Acc@5 88.589 loss 2.218
Accuracy of the network on the 482 val images: 41.91%
[2023-08-31 22:22:44,580] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:22:44,581] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:22:44,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:22:44,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:22:45,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:22:45,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.91%
Epoch: [5]  [  0/160]  eta: 0:17:23  lr: 0.000047  min_lr: 0.000001  loss: 2.4117 (2.4117)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2690 (2.2690)  time: 6.5231 (6.5231 -- 6.5231)  data: 5.6310 (5.6310 -- 5.6310)  max mem: 16735
Epoch: [5]  [ 20/160]  eta: 0:02:34  lr: 0.000047  min_lr: 0.000001  loss: 2.5110 (2.4882)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8833 (3.2720)  time: 0.8343 (0.5296 -- 3.1003)  data: 0.1875 (0.0007 -- 2.3281)  max mem: 16735
Epoch: [5]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000001  loss: 2.4484 (2.4711)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8838 (3.1096)  time: 0.8829 (0.5314 -- 3.5734)  data: 0.0187 (0.0002 -- 0.3427)  max mem: 16735
Epoch: [5]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000001  loss: 2.5735 (2.4900)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1048 (3.1769)  time: 1.0344 (0.5171 -- 4.2122)  data: 0.0012 (0.0004 -- 0.0026)  max mem: 16735
Epoch: [5]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.4224 (2.4797)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9944 (3.2395)  time: 0.7418 (0.5315 -- 3.0271)  data: 0.0017 (0.0003 -- 0.0055)  max mem: 16735
[2023-08-31 22:24:14,973] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:24:14,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-31 22:24:14,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:24:14,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.4751 (2.4799)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4282 (3.3780)  time: 0.8254 (0.5393 -- 2.4441)  data: 0.0022 (0.0003 -- 0.0086)  max mem: 16735
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.4839 (2.4790)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6230 (3.4389)  time: 0.8417 (0.5208 -- 3.4412)  data: 0.0028 (0.0004 -- 0.0229)  max mem: 16735
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.5081 (2.4808)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7542 (3.4951)  time: 0.9401 (0.5361 -- 5.3763)  data: 0.0534 (0.0006 -- 0.9273)  max mem: 16735
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3954 (2.4778)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5963 (3.5993)  time: 0.6565 (0.5002 -- 2.5418)  data: 0.0008 (0.0001 -- 0.0027)  max mem: 16735
Epoch: [5] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3954 (2.4856)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5963 (3.5993)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.0460 (2.0460)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3555 (2.3555 -- 2.3555)  data: 2.1415 (2.1415 -- 2.1415)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 2.1200 (2.1033)  acc1: 44.4444 (42.4242)  acc5: 100.0000 (94.9495)  time: 0.4053 (0.1928 -- 2.3555)  data: 0.1983 (0.0005 -- 2.1415)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 2.0213 (2.0564)  acc1: 44.4444 (42.3280)  acc5: 88.8889 (92.5926)  time: 0.2216 (0.1702 -- 0.3756)  data: 0.0193 (0.0001 -- 0.1941)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 2.0443 (2.0823)  acc1: 44.4444 (41.4938)  acc5: 88.8889 (89.6266)  time: 0.2096 (0.1329 -- 0.3756)  data: 0.0189 (0.0001 -- 0.1941)  max mem: 16735
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 44.191 Acc@5 89.834 loss 2.069
Accuracy of the network on the 482 val images: 44.19%
[2023-08-31 22:25:14,868] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:25:14,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:25:14,870] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:25:14,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:25:16,155] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:25:16,155] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.19%
Epoch: [6]  [  0/160]  eta: 0:16:55  lr: 0.000047  min_lr: 0.000001  loss: 2.6901 (2.6901)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4100 (6.4100)  time: 6.3447 (6.3447 -- 6.3447)  data: 5.6647 (5.6647 -- 5.6647)  max mem: 16735
Epoch: [6]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000001  loss: 2.5332 (2.5514)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1500 (4.2871)  time: 0.9233 (0.5387 -- 3.3336)  data: 0.2933 (0.0007 -- 2.8185)  max mem: 16735
[2023-08-31 22:25:58,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1135100837097188e-06, 1.1135100837097188e-06, 1.4846801116129583e-06, 1.4846801116129583e-06, 1.979573482150611e-06, 1.979573482150611e-06, 2.639431309534148e-06, 2.639431309534148e-06, 3.5192417460455307e-06, 3.5192417460455307e-06, 4.692322328060707e-06, 4.692322328060707e-06, 6.25642977074761e-06, 6.25642977074761e-06, 8.341906360996815e-06, 8.341906360996815e-06, 1.1122541814662419e-05, 1.1122541814662419e-05, 1.4830055752883225e-05, 1.4830055752883225e-05, 1.9773407670510965e-05, 1.9773407670510965e-05, 2.6364543560681288e-05, 2.6364543560681288e-05, 3.515272474757505e-05, 3.515272474757505e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 22:25:58,531] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.960979752989452, CurrSamplesPerSec=22.0455130987583, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [6]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000001  loss: 2.4441 (2.4928)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9845 (4.5396)  time: 0.9067 (0.5184 -- 5.0396)  data: 0.3673 (0.0004 -- 4.5044)  max mem: 16735
Epoch: [6]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000001  loss: 2.4187 (2.4697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7523 (4.3737)  time: 0.9787 (0.5305 -- 4.0663)  data: 0.4348 (0.0004 -- 3.5486)  max mem: 16735
[2023-08-31 22:26:20,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:26:20,892] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-31 22:26:20,894] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:26:20,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.3911 (2.4623)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7628 (4.2995)  time: 0.7019 (0.5223 -- 3.4312)  data: 0.1518 (0.0003 -- 2.8948)  max mem: 16735
Epoch: [6]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.3955 (2.4618)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3526 (4.3435)  time: 0.8838 (0.5291 -- 4.1401)  data: 0.3290 (0.0005 -- 3.6044)  max mem: 16735
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3982 (2.4510)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7863 (4.3125)  time: 0.7852 (0.5404 -- 3.0039)  data: 0.2318 (0.0004 -- 2.4667)  max mem: 16735
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.4027 (2.4461)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4394 (4.4087)  time: 0.9102 (0.5365 -- 3.1687)  data: 0.3552 (0.0010 -- 2.6288)  max mem: 16735
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4340 (2.4433)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3204 (4.4085)  time: 0.7428 (0.5008 -- 3.9628)  data: 0.2202 (0.0002 -- 3.4342)  max mem: 16735
Epoch: [6] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4340 (2.4452)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3204 (4.4085)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.9229 (1.9229)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4191 (2.4191 -- 2.4191)  data: 2.1913 (2.1913 -- 2.1913)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.9805 (1.9921)  acc1: 44.4444 (43.4343)  acc5: 100.0000 (92.9293)  time: 0.4372 (0.2003 -- 2.4191)  data: 0.2165 (0.0008 -- 2.1913)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.9327 (1.9531)  acc1: 44.4444 (43.3862)  acc5: 88.8889 (92.0635)  time: 0.2219 (0.1680 -- 0.3925)  data: 0.0198 (0.0001 -- 0.2041)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.9538 (1.9805)  acc1: 44.4444 (42.3237)  acc5: 88.8889 (90.8714)  time: 0.2048 (0.1328 -- 0.3925)  data: 0.0183 (0.0001 -- 0.2041)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 46.473 Acc@5 90.249 loss 1.962
Accuracy of the network on the 482 val images: 46.47%
[2023-08-31 22:27:46,478] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:27:46,480] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:27:46,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:27:46,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:27:47,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:27:47,840] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.47%
Epoch: [7]  [  0/160]  eta: 0:18:51  lr: 0.000047  min_lr: 0.000001  loss: 2.0173 (2.0173)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4615 (3.4615)  time: 7.0692 (7.0692 -- 7.0692)  data: 6.5368 (6.5368 -- 6.5368)  max mem: 16735
Epoch: [7]  [ 20/160]  eta: 0:02:53  lr: 0.000047  min_lr: 0.000001  loss: 2.4912 (2.4533)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1752 (4.3555)  time: 0.9443 (0.5221 -- 3.2778)  data: 0.1268 (0.0004 -- 2.4241)  max mem: 16735
[2023-08-31 22:28:22,438] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:28:22,439] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-08-31 22:28:22,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:28:22,441] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000001  loss: 2.3754 (2.4390)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0173 (4.1525)  time: 0.7497 (0.5205 -- 2.6262)  data: 0.0015 (0.0002 -- 0.0038)  max mem: 16735
Epoch: [7]  [ 60/160]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000001  loss: 2.4968 (2.4518)  loss_scale: 65536.0000 (48346.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2852 (4.2900)  time: 0.8473 (0.5235 -- 3.5010)  data: 0.0019 (0.0005 -- 0.0086)  max mem: 16735
Epoch: [7]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.4495 (2.4560)  loss_scale: 65536.0000 (52590.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0933 (4.3115)  time: 0.9307 (0.5356 -- 3.5114)  data: 0.0728 (0.0003 -- 0.8072)  max mem: 16735
Epoch: [7]  [100/160]  eta: 0:00:54  lr: 0.000047  min_lr: 0.000001  loss: 2.3677 (2.4349)  loss_scale: 65536.0000 (55154.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2791 (4.4090)  time: 0.8014 (0.5318 -- 3.0202)  data: 0.0016 (0.0005 -- 0.0046)  max mem: 16735
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.4772 (2.4352)  loss_scale: 65536.0000 (56870.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8471 (4.5546)  time: 0.8548 (0.5312 -- 3.6054)  data: 0.0027 (0.0004 -- 0.0160)  max mem: 16735
Epoch: [7]  [140/160]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000001  loss: 2.4413 (2.4365)  loss_scale: 65536.0000 (58099.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7400 (4.6950)  time: 0.8331 (0.5218 -- 2.2892)  data: 0.0478 (0.0002 -- 0.9257)  max mem: 16735
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4312 (2.4398)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1701 (4.7500)  time: 0.6764 (0.5022 -- 2.2989)  data: 0.0574 (0.0002 -- 1.1139)  max mem: 16735
Epoch: [7] Total time: 0:02:19 (0.8708 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4312 (2.4311)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1701 (4.7500)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.8609 (1.8609)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.5132 (2.5132 -- 2.5132)  data: 2.2459 (2.2459 -- 2.2459)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.9368 (1.9154)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (92.9293)  time: 0.4253 (0.2036 -- 2.5132)  data: 0.2063 (0.0007 -- 2.2459)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.8614 (1.8800)  acc1: 44.4444 (45.5026)  acc5: 88.8889 (91.5344)  time: 0.2137 (0.1695 -- 0.3776)  data: 0.0118 (0.0001 -- 0.1907)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.8705 (1.9137)  acc1: 44.4444 (44.3983)  acc5: 88.8889 (89.2116)  time: 0.1982 (0.1329 -- 0.3776)  data: 0.0114 (0.0001 -- 0.1907)  max mem: 16735
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 49.170 Acc@5 90.041 loss 1.888
Accuracy of the network on the 482 val images: 49.17%
[2023-08-31 22:30:14,942] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:30:14,944] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:30:14,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:30:14,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:30:16,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:30:16,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 49.17%
[2023-08-31 22:30:23,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:30:23,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2023-08-31 22:30:23,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:30:23,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [8]  [  0/160]  eta: 0:18:47  lr: 0.000047  min_lr: 0.000001  loss: 2.4324 (2.4324)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7877 (4.7877)  time: 7.0462 (7.0462 -- 7.0462)  data: 6.3652 (6.3652 -- 6.3652)  max mem: 16735
[2023-08-31 22:30:32,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1293
[2023-08-31 22:30:32,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1293
[2023-08-31 22:30:32,851] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-08-31 22:30:32,851] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-08-31 22:30:32,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072, reducing to 65536.0
Epoch: [8]  [ 20/160]  eta: 0:02:43  lr: 0.000047  min_lr: 0.000001  loss: 2.3677 (2.3973)  loss_scale: 131072.0000 (106105.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3066 (4.6965)  time: 0.8748 (0.5235 -- 2.7214)  data: 0.1270 (0.0003 -- 1.2957)  max mem: 16735
Epoch: [8]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000001  loss: 2.3471 (2.3715)  loss_scale: 65536.0000 (86315.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9956 (5.1330)  time: 0.9432 (0.5182 -- 5.5837)  data: 0.0187 (0.0001 -- 0.3542)  max mem: 16735
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000001  loss: 2.3931 (2.3814)  loss_scale: 65536.0000 (79502.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6281 (5.2647)  time: 0.8863 (0.5222 -- 3.7487)  data: 0.0015 (0.0001 -- 0.0058)  max mem: 16735
Epoch: [8]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000001  loss: 2.4083 (2.3812)  loss_scale: 65536.0000 (76054.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9288 (5.1559)  time: 0.8559 (0.5168 -- 4.4721)  data: 0.0013 (0.0004 -- 0.0029)  max mem: 16735
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.5003 (2.3948)  loss_scale: 65536.0000 (73971.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8716 (5.1194)  time: 0.8489 (0.5365 -- 4.8513)  data: 0.0086 (0.0006 -- 0.1399)  max mem: 16735
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.4287 (2.4014)  loss_scale: 65536.0000 (72577.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5681 (5.0668)  time: 0.7602 (0.5461 -- 2.7113)  data: 0.0019 (0.0004 -- 0.0047)  max mem: 16735
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.4870 (2.4092)  loss_scale: 65536.0000 (71578.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9891 (5.0842)  time: 0.9076 (0.5203 -- 2.9689)  data: 0.0016 (0.0003 -- 0.0055)  max mem: 16735
[2023-08-31 22:32:25,902] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:32:25,903] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-31 22:32:25,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:32:25,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-31 22:32:26,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1423
[2023-08-31 22:32:26,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-31 22:32:26,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1423
[2023-08-31 22:32:26,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-31 22:32:26,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4431 (2.4098)  loss_scale: 65536.0000 (71270.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0318 (5.0548)  time: 0.6616 (0.5003 -- 1.9563)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16735
Epoch: [8] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4431 (2.4016)  loss_scale: 65536.0000 (71270.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0318 (5.0548)
Val:  [ 0/27]  eta: 0:01:08  loss: 1.7747 (1.7747)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.5390 (2.5390 -- 2.5390)  data: 2.2844 (2.2844 -- 2.2844)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.8176 (1.8183)  acc1: 55.5556 (46.4646)  acc5: 100.0000 (92.9293)  time: 0.4325 (0.1996 -- 2.5390)  data: 0.2105 (0.0007 -- 2.2844)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.7845 (1.7873)  acc1: 55.5556 (47.0899)  acc5: 88.8889 (92.0635)  time: 0.2110 (0.1690 -- 0.2542)  data: 0.0041 (0.0001 -- 0.0470)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.8212 (1.8197)  acc1: 55.5556 (46.0581)  acc5: 88.8889 (91.2863)  time: 0.1955 (0.1331 -- 0.2542)  data: 0.0032 (0.0001 -- 0.0470)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 50.622 Acc@5 91.701 loss 1.794
Accuracy of the network on the 482 val images: 50.62%
[2023-08-31 22:32:45,109] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:32:45,111] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:32:45,111] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:32:45,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:32:46,437] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:32:46,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.62%
Epoch: [9]  [  0/160]  eta: 0:16:28  lr: 0.000047  min_lr: 0.000001  loss: 2.3138 (2.3138)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9755 (5.9755)  time: 6.1761 (6.1761 -- 6.1761)  data: 5.3656 (5.3656 -- 5.3656)  max mem: 16735
[2023-08-31 22:32:57,884] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1443
[2023-08-31 22:32:57,884] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1443
[2023-08-31 22:32:57,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:32:57,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:32:57,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 20/160]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000001  loss: 2.3720 (2.3597)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0920 (5.4209)  time: 1.0673 (0.5273 -- 4.9523)  data: 0.0017 (0.0003 -- 0.0061)  max mem: 16735
Epoch: [9]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000001  loss: 2.3820 (2.3792)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5455 (5.5409)  time: 0.8048 (0.5196 -- 3.4862)  data: 0.0013 (0.0002 -- 0.0034)  max mem: 16735
Epoch: [9]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000001  loss: 2.3685 (2.3604)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7582 (5.7797)  time: 0.9484 (0.5212 -- 4.9618)  data: 0.0014 (0.0002 -- 0.0039)  max mem: 16735
Epoch: [9]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.4289 (2.3667)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7079 (5.7749)  time: 0.7091 (0.5210 -- 2.4696)  data: 0.0015 (0.0008 -- 0.0043)  max mem: 16735
Epoch: [9]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.4068 (2.3684)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0758 (5.6572)  time: 0.9336 (0.5201 -- 4.0574)  data: 0.0017 (0.0008 -- 0.0044)  max mem: 16735
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3320 (2.3668)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9114 (5.6770)  time: 0.7633 (0.5167 -- 3.2696)  data: 0.0016 (0.0003 -- 0.0079)  max mem: 16735
[2023-08-31 22:34:49,160] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:34:49,160] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:34:49,161] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:34:49,161] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3094 (2.3558)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2246 (5.7085)  time: 0.9910 (0.5238 -- 4.0373)  data: 0.0014 (0.0003 -- 0.0048)  max mem: 16735
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3006 (2.3532)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5206 (5.7328)  time: 0.6322 (0.5023 -- 1.9991)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16735
Epoch: [9] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3006 (2.3565)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5206 (5.7328)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.6986 (1.6986)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4646 (2.4646 -- 2.4646)  data: 2.2480 (2.2480 -- 2.2480)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.7444 (1.7523)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (93.9394)  time: 0.4238 (0.1920 -- 2.4646)  data: 0.2127 (0.0005 -- 2.2480)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.6876 (1.7180)  acc1: 55.5556 (50.2646)  acc5: 100.0000 (92.5926)  time: 0.2181 (0.1693 -- 0.3586)  data: 0.0135 (0.0001 -- 0.1735)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.7444 (1.7518)  acc1: 55.5556 (48.9627)  acc5: 88.8889 (92.1162)  time: 0.2049 (0.1323 -- 0.3586)  data: 0.0133 (0.0001 -- 0.1735)  max mem: 16735
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 51.452 Acc@5 91.286 loss 1.731
Accuracy of the network on the 482 val images: 51.45%
[2023-08-31 22:35:16,941] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:35:16,943] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:35:16,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:35:16,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:35:18,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:35:18,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.45%
Epoch: [10]  [  0/160]  eta: 0:23:14  lr: 0.000047  min_lr: 0.000001  loss: 2.5247 (2.5247)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9553 (4.9553)  time: 8.7141 (8.7141 -- 8.7141)  data: 8.1824 (8.1824 -- 8.1824)  max mem: 16735
Epoch: [10]  [ 20/160]  eta: 0:02:52  lr: 0.000047  min_lr: 0.000001  loss: 2.3157 (2.3159)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2237 (5.3033)  time: 0.8545 (0.5348 -- 3.8833)  data: 0.2691 (0.0006 -- 3.3633)  max mem: 16735
Epoch: [10]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000001  loss: 2.3599 (2.3545)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8187 (5.6023)  time: 0.8622 (0.5221 -- 4.0663)  data: 0.1152 (0.0004 -- 1.3714)  max mem: 16735
Epoch: [10]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000001  loss: 2.2443 (2.3058)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7476 (5.6369)  time: 0.7940 (0.5402 -- 3.0002)  data: 0.0023 (0.0003 -- 0.0102)  max mem: 16735
Epoch: [10]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000001  loss: 2.1988 (2.2801)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4318 (5.5779)  time: 0.9342 (0.5284 -- 4.6166)  data: 0.0015 (0.0003 -- 0.0037)  max mem: 16735
[2023-08-31 22:36:52,631] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:36:52,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-08-31 22:36:52,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:36:52,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.3996 (2.2965)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3139 (5.5663)  time: 0.8315 (0.5293 -- 4.0808)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16735
[2023-08-31 22:36:53,165] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1701
[2023-08-31 22:36:53,165] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1701
[2023-08-31 22:36:53,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-08-31 22:36:53,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2023-08-31 22:36:53,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3422 (2.2956)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9453 (5.5198)  time: 0.7708 (0.5211 -- 1.7300)  data: 0.0013 (0.0001 -- 0.0029)  max mem: 16735
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3590 (2.3101)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2014 (5.5182)  time: 0.9269 (0.5355 -- 2.9344)  data: 0.2151 (0.0005 -- 2.4183)  max mem: 16735
[2023-08-31 22:37:38,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1757
[2023-08-31 22:37:38,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1757
[2023-08-31 22:37:38,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:37:38,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:37:38,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3852 (2.3236)  loss_scale: 65536.0000 (65331.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8086 (5.6359)  time: 0.6635 (0.4887 -- 1.9377)  data: 0.0878 (0.0002 -- 1.4000)  max mem: 16735
Epoch: [10] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3852 (2.3345)  loss_scale: 65536.0000 (65331.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8086 (5.6359)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.6536 (1.6536)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.2309 (2.2309 -- 2.2309)  data: 2.0239 (2.0239 -- 2.0239)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.6342 (1.6590)  acc1: 55.5556 (49.4949)  acc5: 100.0000 (93.9394)  time: 0.4233 (0.1977 -- 2.2309)  data: 0.2059 (0.0004 -- 2.0239)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.6057 (1.6288)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (93.1217)  time: 0.2262 (0.1692 -- 0.4666)  data: 0.0205 (0.0001 -- 0.2297)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.6281 (1.6648)  acc1: 55.5556 (51.8672)  acc5: 88.8889 (92.9461)  time: 0.2116 (0.1329 -- 0.4666)  data: 0.0202 (0.0001 -- 0.2297)  max mem: 16735
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 54.772 Acc@5 92.739 loss 1.642
Accuracy of the network on the 482 val images: 54.77%
[2023-08-31 22:37:47,086] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:37:47,088] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:37:47,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:37:47,088] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:37:48,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:37:48,484] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 54.77%
Epoch: [11]  [  0/160]  eta: 0:20:16  lr: 0.000047  min_lr: 0.000001  loss: 2.4867 (2.4867)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2780 (4.2780)  time: 7.6034 (7.6034 -- 7.6034)  data: 6.1060 (6.1060 -- 6.1060)  max mem: 16735
Epoch: [11]  [ 20/160]  eta: 0:02:33  lr: 0.000047  min_lr: 0.000001  loss: 2.4107 (2.4168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9404 (5.1679)  time: 0.7720 (0.5412 -- 2.5805)  data: 0.1909 (0.0007 -- 2.0562)  max mem: 16735
Epoch: [11]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000001  loss: 2.2986 (2.3633)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2727 (5.3898)  time: 0.8897 (0.5302 -- 3.0798)  data: 0.0508 (0.0002 -- 0.8529)  max mem: 16735
Epoch: [11]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000001  loss: 2.3844 (2.3696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7248 (5.3477)  time: 0.9694 (0.5295 -- 3.0511)  data: 0.2090 (0.0005 -- 1.2813)  max mem: 16735
Epoch: [11]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.2525 (2.3619)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0051 (5.6247)  time: 0.7968 (0.5187 -- 2.7583)  data: 0.0615 (0.0004 -- 1.1944)  max mem: 16735
Epoch: [11]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.3499 (2.3510)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6667 (5.6812)  time: 0.9305 (0.5255 -- 4.1518)  data: 0.3857 (0.0002 -- 3.5999)  max mem: 16735
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.2959 (2.3420)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2072 (5.7930)  time: 0.8446 (0.5226 -- 3.6988)  data: 0.2867 (0.0003 -- 3.1621)  max mem: 16735
[2023-08-31 22:39:43,970] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:39:43,971] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:39:43,970] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:39:43,971] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.2387 (2.3268)  loss_scale: 65536.0000 (36253.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9527 (5.8754)  time: 0.8105 (0.5328 -- 2.9432)  data: 0.2360 (0.0003 -- 1.8983)  max mem: 16735
[2023-08-31 22:40:08,852] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1916
[2023-08-31 22:40:08,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:40:08,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 22:40:08,852] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1916
[2023-08-31 22:40:08,853] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2432 (2.3217)  loss_scale: 65536.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5887 (5.9315)  time: 0.7268 (0.4914 -- 2.2256)  data: 0.1734 (0.0002 -- 1.6371)  max mem: 16735
Epoch: [11] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2432 (2.3118)  loss_scale: 65536.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5887 (5.9315)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.6012 (1.6012)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4219 (2.4219 -- 2.4219)  data: 2.1992 (2.1992 -- 2.1992)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.5669 (1.6157)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (93.9394)  time: 0.4344 (0.1957 -- 2.4219)  data: 0.2148 (0.0006 -- 2.1992)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.5346 (1.5793)  acc1: 55.5556 (49.2063)  acc5: 100.0000 (94.1799)  time: 0.2221 (0.1681 -- 0.4202)  data: 0.0152 (0.0001 -- 0.1545)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.5685 (1.6351)  acc1: 55.5556 (48.1328)  acc5: 88.8889 (93.3610)  time: 0.2066 (0.1320 -- 0.4202)  data: 0.0149 (0.0001 -- 0.1545)  max mem: 16735
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 51.452 Acc@5 92.116 loss 1.606
Accuracy of the network on the 482 val images: 51.45%
Max accuracy: 54.77%
Epoch: [12]  [  0/160]  eta: 0:18:28  lr: 0.000047  min_lr: 0.000001  loss: 1.9435 (1.9435)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6454 (6.6454)  time: 6.9309 (6.9309 -- 6.9309)  data: 4.3731 (4.3731 -- 4.3731)  max mem: 16735
Epoch: [12]  [ 20/160]  eta: 0:02:32  lr: 0.000047  min_lr: 0.000001  loss: 2.2241 (2.1874)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9572 (6.8646)  time: 0.7982 (0.5388 -- 2.0683)  data: 0.1493 (0.0006 -- 1.3127)  max mem: 16735
Epoch: [12]  [ 40/160]  eta: 0:01:55  lr: 0.000047  min_lr: 0.000001  loss: 2.2499 (2.2293)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9152 (6.6767)  time: 0.8261 (0.5293 -- 2.5384)  data: 0.1312 (0.0003 -- 1.8970)  max mem: 16735
Epoch: [12]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000001  loss: 2.2447 (2.2375)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2982 (6.5763)  time: 1.0574 (0.5329 -- 3.5874)  data: 0.3816 (0.0003 -- 3.0672)  max mem: 16735
[2023-08-31 22:41:25,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1990
[2023-08-31 22:41:25,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1990
[2023-08-31 22:41:25,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 22:41:25,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 22:41:25,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 22:41:32,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=7, lr=[1.1095727809336618e-06, 1.1095727809336618e-06, 1.4794303745782159e-06, 1.4794303745782159e-06, 1.9725738327709545e-06, 1.9725738327709545e-06, 2.630098443694606e-06, 2.630098443694606e-06, 3.5067979249261413e-06, 3.5067979249261413e-06, 4.675730566568188e-06, 4.675730566568188e-06, 6.234307422090918e-06, 6.234307422090918e-06, 8.312409896121224e-06, 8.312409896121224e-06, 1.1083213194828298e-05, 1.1083213194828298e-05, 1.4777617593104396e-05, 1.4777617593104396e-05, 1.9703490124139195e-05, 1.9703490124139195e-05, 2.6271320165518927e-05, 2.6271320165518927e-05, 3.502842688735857e-05, 3.502842688735857e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 22:41:32,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.233074840870636, CurrSamplesPerSec=22.6673596205447, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [12]  [ 80/160]  eta: 0:01:13  lr: 0.000047  min_lr: 0.000001  loss: 2.2631 (2.2387)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4332 (6.5403)  time: 0.7110 (0.5422 -- 2.0882)  data: 0.1530 (0.0007 -- 1.5567)  max mem: 16735
Epoch: [12]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.4208 (2.2741)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8401 (6.4968)  time: 0.8968 (0.5286 -- 2.8639)  data: 0.1870 (0.0004 -- 2.3408)  max mem: 16735
Epoch: [12]  [120/160]  eta: 0:00:35  lr: 0.000047  min_lr: 0.000001  loss: 2.4567 (2.2918)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7926 (6.4782)  time: 0.8081 (0.5339 -- 2.5390)  data: 0.2335 (0.0005 -- 1.9697)  max mem: 16735
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.1446 (2.2740)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0341 (6.3978)  time: 0.9268 (0.5395 -- 3.4288)  data: 0.2636 (0.0010 -- 2.3125)  max mem: 16735
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3368 (2.2827)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1646 (6.3329)  time: 0.6943 (0.5024 -- 3.5360)  data: 0.0679 (0.0002 -- 1.1487)  max mem: 16735
Epoch: [12] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3368 (2.2896)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1646 (6.3329)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.5193 (1.5193)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4906 (2.4906 -- 2.4906)  data: 2.2633 (2.2633 -- 2.2633)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.5387 (1.5960)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (91.9192)  time: 0.4276 (0.2018 -- 2.4906)  data: 0.2066 (0.0007 -- 2.2633)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.5047 (1.5593)  acc1: 55.5556 (53.9683)  acc5: 88.8889 (91.5344)  time: 0.2152 (0.1686 -- 0.3271)  data: 0.0080 (0.0001 -- 0.1476)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.5592 (1.6090)  acc1: 55.5556 (51.4523)  acc5: 88.8889 (91.2863)  time: 0.1973 (0.1334 -- 0.3271)  data: 0.0078 (0.0001 -- 0.1476)  max mem: 16735
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 54.357 Acc@5 90.664 loss 1.578
Accuracy of the network on the 482 val images: 54.36%
Max accuracy: 54.77%
Epoch: [13]  [  0/160]  eta: 0:21:05  lr: 0.000047  min_lr: 0.000001  loss: 2.0645 (2.0645)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7520 (4.7520)  time: 7.9099 (7.9099 -- 7.9099)  data: 5.2371 (5.2371 -- 5.2371)  max mem: 16735
Epoch: [13]  [ 20/160]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000001  loss: 2.3585 (2.3157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5165 (5.7065)  time: 0.8045 (0.5299 -- 3.6142)  data: 0.0024 (0.0004 -- 0.0167)  max mem: 16735
[2023-08-31 22:43:26,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:43:26,410] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 22:43:26,410] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:43:26,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [ 40/160]  eta: 0:02:01  lr: 0.000047  min_lr: 0.000001  loss: 2.3123 (2.3318)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0132 (5.8365)  time: 0.8786 (0.5273 -- 3.6300)  data: 0.0022 (0.0004 -- 0.0081)  max mem: 16735
Epoch: [13]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3533 (2.3290)  loss_scale: 32768.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3010 (6.0675)  time: 0.8516 (0.5259 -- 3.2007)  data: 0.1078 (0.0006 -- 1.4739)  max mem: 16735
Epoch: [13]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000001  loss: 2.3081 (2.3281)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9508 (6.0322)  time: 0.9906 (0.5334 -- 3.1548)  data: 0.2688 (0.0004 -- 2.2630)  max mem: 16735
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.1130 (2.3095)  loss_scale: 32768.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8554 (6.0044)  time: 0.7229 (0.5440 -- 2.5552)  data: 0.0611 (0.0004 -- 1.1904)  max mem: 16735
Epoch: [13]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3050 (2.2997)  loss_scale: 32768.0000 (27487.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4039 (6.0627)  time: 0.8700 (0.5188 -- 2.5626)  data: 0.2148 (0.0008 -- 2.0139)  max mem: 16735
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3067 (2.2791)  loss_scale: 32768.0000 (28236.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9708 (6.1942)  time: 0.9475 (0.5240 -- 2.6809)  data: 0.0013 (0.0004 -- 0.0041)  max mem: 16735
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1966 (2.2653)  loss_scale: 32768.0000 (28774.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7418 (6.2671)  time: 0.7124 (0.5017 -- 2.3292)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [13] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1966 (2.2684)  loss_scale: 32768.0000 (28774.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7418 (6.2671)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.4652 (1.4652)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4316 (2.4316 -- 2.4316)  data: 2.2269 (2.2269 -- 2.2269)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.4401 (1.5317)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (91.9192)  time: 0.4203 (0.2031 -- 2.4316)  data: 0.2056 (0.0009 -- 2.2269)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.4174 (1.4928)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (93.1217)  time: 0.2138 (0.1695 -- 0.2990)  data: 0.0086 (0.0001 -- 0.0946)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.4882 (1.5446)  acc1: 55.5556 (53.5270)  acc5: 88.8889 (92.5311)  time: 0.1975 (0.1326 -- 0.2990)  data: 0.0082 (0.0001 -- 0.0946)  max mem: 16735
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 55.809 Acc@5 91.909 loss 1.510
Accuracy of the network on the 482 val images: 55.81%
[2023-08-31 22:45:15,658] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:45:15,659] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:45:15,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:45:15,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:45:16,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:45:16,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.81%
Epoch: [14]  [  0/160]  eta: 0:22:42  lr: 0.000047  min_lr: 0.000001  loss: 2.1494 (2.1494)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0580 (7.0580)  time: 8.5175 (8.5175 -- 8.5175)  data: 7.9681 (7.9681 -- 7.9681)  max mem: 16735
[2023-08-31 22:45:29,124] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:45:29,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:45:29,124] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:45:29,125] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [ 20/160]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000001  loss: 2.2686 (2.2476)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6334 (6.7729)  time: 0.7615 (0.5248 -- 2.8603)  data: 0.1396 (0.0004 -- 2.3429)  max mem: 16735
[2023-08-31 22:45:42,158] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2263
[2023-08-31 22:45:42,158] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2263
[2023-08-31 22:45:42,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:45:42,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:45:42,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [ 40/160]  eta: 0:02:04  lr: 0.000047  min_lr: 0.000001  loss: 2.2192 (2.2334)  loss_scale: 32768.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3196 (6.6963)  time: 0.9393 (0.5117 -- 4.5377)  data: 0.0835 (0.0003 -- 0.7564)  max mem: 16735
Epoch: [14]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000001  loss: 2.2490 (2.2492)  loss_scale: 32768.0000 (41362.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8792 (6.6417)  time: 0.8403 (0.5289 -- 3.8309)  data: 0.2853 (0.0003 -- 3.3085)  max mem: 16735
Epoch: [14]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000001  loss: 2.2669 (2.2588)  loss_scale: 32768.0000 (39240.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3478 (6.5621)  time: 0.9121 (0.5344 -- 2.4343)  data: 0.1708 (0.0004 -- 1.3284)  max mem: 16735
Epoch: [14]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.2792 (2.2565)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3770 (6.5849)  time: 0.8435 (0.5343 -- 4.0054)  data: 0.0016 (0.0002 -- 0.0089)  max mem: 16735
Epoch: [14]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.1687 (2.2420)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6113 (6.5297)  time: 0.8518 (0.5287 -- 3.0018)  data: 0.0024 (0.0007 -- 0.0133)  max mem: 16735
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.2879 (2.2429)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1887 (6.4933)  time: 0.8170 (0.5313 -- 2.8741)  data: 0.0020 (0.0004 -- 0.0083)  max mem: 16735
[2023-08-31 22:47:33,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:47:33,691] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:47:33,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:47:33,691] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1579 (2.2386)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3542 (6.5103)  time: 0.6598 (0.5030 -- 1.7177)  data: 0.0485 (0.0002 -- 0.9503)  max mem: 16735
Epoch: [14] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1579 (2.2638)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3542 (6.5103)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.4275 (1.4275)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2773 (2.2773 -- 2.2773)  data: 2.0304 (2.0304 -- 2.0304)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.4456 (1.5242)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (92.9293)  time: 0.4184 (0.2123 -- 2.2773)  data: 0.1906 (0.0006 -- 2.0304)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.4057 (1.4814)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (93.1217)  time: 0.2286 (0.1685 -- 0.5497)  data: 0.0218 (0.0001 -- 0.3657)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.4724 (1.5388)  acc1: 55.5556 (55.6017)  acc5: 88.8889 (92.5311)  time: 0.2074 (0.1328 -- 0.5497)  data: 0.0212 (0.0001 -- 0.3657)  max mem: 16735
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 57.054 Acc@5 91.701 loss 1.504
Accuracy of the network on the 482 val images: 57.05%
[2023-08-31 22:47:45,090] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:47:45,092] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:47:45,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:47:45,092] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:47:46,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:47:46,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.05%
Epoch: [15]  [  0/160]  eta: 0:16:14  lr: 0.000047  min_lr: 0.000001  loss: 2.1606 (2.1606)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1875 (7.1875)  time: 6.0935 (6.0935 -- 6.0935)  data: 5.5352 (5.5352 -- 5.5352)  max mem: 16735
[2023-08-31 22:48:03,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2412
[2023-08-31 22:48:03,397] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2412
[2023-08-31 22:48:03,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:48:03,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:48:03,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 20/160]  eta: 0:02:52  lr: 0.000047  min_lr: 0.000001  loss: 2.3004 (2.2625)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7951 (7.6261)  time: 0.9869 (0.5243 -- 2.9228)  data: 0.3657 (0.0006 -- 2.3611)  max mem: 16735
Epoch: [15]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000001  loss: 2.2161 (2.2470)  loss_scale: 32768.0000 (42358.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7900 (7.3328)  time: 0.7449 (0.5274 -- 2.0301)  data: 0.1986 (0.0004 -- 1.4732)  max mem: 16735
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000001  loss: 2.2951 (2.2581)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6972 (7.2188)  time: 0.9042 (0.5254 -- 3.7062)  data: 0.3569 (0.0003 -- 3.1483)  max mem: 16735
Epoch: [15]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000001  loss: 2.0959 (2.2406)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4268 (7.2177)  time: 0.9393 (0.5228 -- 3.8151)  data: 0.3906 (0.0004 -- 3.2837)  max mem: 16735
Epoch: [15]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.2558 (2.2464)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8773 (7.2049)  time: 0.8063 (0.5284 -- 3.6187)  data: 0.2569 (0.0002 -- 3.0741)  max mem: 16735
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 2.1258 (2.2387)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8659 (7.2260)  time: 0.9852 (0.5287 -- 4.3596)  data: 0.4150 (0.0006 -- 3.8348)  max mem: 16735
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.4069 (2.2539)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6288 (7.1675)  time: 0.8330 (0.5216 -- 4.3996)  data: 0.2820 (0.0004 -- 3.8784)  max mem: 16735
[2023-08-31 22:49:58,957] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:49:58,957] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:49:58,958] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:49:58,958] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:50:07,141] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2555
[2023-08-31 22:50:07,141] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2555
[2023-08-31 22:50:07,141] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:50:07,141] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:50:07,141] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3580 (2.2688)  loss_scale: 65536.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6312 (7.0237)  time: 0.6570 (0.4937 -- 2.3790)  data: 0.1262 (0.0002 -- 1.8165)  max mem: 16735
Epoch: [15] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3580 (2.2416)  loss_scale: 65536.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6312 (7.0237)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.4224 (1.4224)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4667 (2.4667 -- 2.4667)  data: 2.2487 (2.2487 -- 2.2487)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.4224 (1.4559)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (93.9394)  time: 0.4337 (0.1984 -- 2.4667)  data: 0.2204 (0.0008 -- 2.2487)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.3482 (1.4295)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (93.1217)  time: 0.2209 (0.1703 -- 0.3902)  data: 0.0165 (0.0001 -- 0.1659)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.4648 (1.4790)  acc1: 55.5556 (57.6763)  acc5: 88.8889 (92.9461)  time: 0.2055 (0.1333 -- 0.3902)  data: 0.0162 (0.0001 -- 0.1659)  max mem: 16735
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 59.129 Acc@5 91.701 loss 1.448
Accuracy of the network on the 482 val images: 59.13%
[2023-08-31 22:50:17,030] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:50:17,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:50:17,032] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:50:17,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:50:18,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:50:18,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.13%
Epoch: [16]  [  0/160]  eta: 0:21:30  lr: 0.000047  min_lr: 0.000001  loss: 2.1577 (2.1577)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4955 (8.4955)  time: 8.0670 (8.0670 -- 8.0670)  data: 7.5158 (7.5158 -- 7.5158)  max mem: 16735
Epoch: [16]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000001  loss: 2.3537 (2.3358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6541 (7.1752)  time: 0.8073 (0.5354 -- 3.2812)  data: 0.2582 (0.0003 -- 2.7477)  max mem: 16735
Epoch: [16]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000001  loss: 2.2921 (2.2927)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8843 (7.1071)  time: 0.9177 (0.5302 -- 3.0694)  data: 0.3685 (0.0002 -- 2.5261)  max mem: 16735
Epoch: [16]  [ 60/160]  eta: 0:01:34  lr: 0.000046  min_lr: 0.000001  loss: 2.2408 (2.2825)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8760 (7.2251)  time: 0.7519 (0.5225 -- 2.7035)  data: 0.2022 (0.0004 -- 2.1883)  max mem: 16735
Epoch: [16]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.3056 (2.2907)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1917 (7.1775)  time: 0.9437 (0.5295 -- 4.1204)  data: 0.2972 (0.0009 -- 3.5568)  max mem: 16735
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.1813 (2.2736)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9813 (7.2461)  time: 0.8102 (0.5193 -- 2.9865)  data: 0.2289 (0.0004 -- 2.4664)  max mem: 16735
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.3118 (2.2803)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4307 (7.2065)  time: 0.8478 (0.5385 -- 2.8943)  data: 0.1540 (0.0004 -- 1.1591)  max mem: 16735
[2023-08-31 22:52:11,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:52:11,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:52:11,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:52:11,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.3617 (2.2863)  loss_scale: 65536.0000 (36718.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0029 (7.2017)  time: 0.9102 (0.5276 -- 2.8629)  data: 0.3479 (0.0003 -- 2.3264)  max mem: 16735
[2023-08-31 22:52:31,857] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2708
[2023-08-31 22:52:31,858] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:52:31,858] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2708
[2023-08-31 22:52:31,858] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:52:31,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2982 (2.2894)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0155 (7.0655)  time: 0.6999 (0.5015 -- 2.4851)  data: 0.0445 (0.0002 -- 0.4547)  max mem: 16735
Epoch: [16] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2982 (2.2729)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0155 (7.0655)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.3494 (1.3494)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3527 (2.3527 -- 2.3527)  data: 2.1469 (2.1469 -- 2.1469)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.3494 (1.4490)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (91.9192)  time: 0.4204 (0.2004 -- 2.3527)  data: 0.2042 (0.0003 -- 2.1469)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.3342 (1.4106)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (93.6508)  time: 0.2228 (0.1679 -- 0.4634)  data: 0.0193 (0.0001 -- 0.2852)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.4269 (1.4584)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (92.9461)  time: 0.2088 (0.1322 -- 0.4634)  data: 0.0183 (0.0001 -- 0.2852)  max mem: 16735
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 59.336 Acc@5 92.324 loss 1.430
Accuracy of the network on the 482 val images: 59.34%
[2023-08-31 22:52:47,671] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:52:47,672] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:52:47,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:52:47,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:52:48,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:52:48,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.34%
Epoch: [17]  [  0/160]  eta: 0:21:12  lr: 0.000046  min_lr: 0.000001  loss: 2.5976 (2.5976)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9252 (7.9252)  time: 7.9528 (7.9528 -- 7.9528)  data: 4.5753 (4.5753 -- 4.5753)  max mem: 16735
Epoch: [17]  [ 20/160]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000001  loss: 2.1101 (2.1489)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5292 (6.9967)  time: 0.8764 (0.5292 -- 2.9539)  data: 0.1934 (0.0003 -- 2.4039)  max mem: 16735
Epoch: [17]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000001  loss: 2.1525 (2.1587)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4092 (6.8154)  time: 0.8389 (0.5318 -- 3.6723)  data: 0.2840 (0.0003 -- 3.1057)  max mem: 16735
Epoch: [17]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000001  loss: 2.1470 (2.1848)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5300 (6.9275)  time: 0.8948 (0.5167 -- 3.9430)  data: 0.3495 (0.0003 -- 3.4019)  max mem: 16735
Epoch: [17]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.2841 (2.1881)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7138 (7.1208)  time: 0.8194 (0.5303 -- 3.8660)  data: 0.2673 (0.0002 -- 3.3216)  max mem: 16735
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.2951 (2.1979)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6793 (7.1075)  time: 0.8455 (0.5253 -- 4.3311)  data: 0.2971 (0.0003 -- 3.8073)  max mem: 16735
[2023-08-31 22:54:39,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:54:39,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:54:39,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:54:39,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.1639 (2.1919)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7663 (7.0190)  time: 0.9480 (0.5248 -- 3.5170)  data: 0.0905 (0.0002 -- 1.5474)  max mem: 16735
[2023-08-31 22:54:54,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2856
[2023-08-31 22:54:54,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:54:54,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2856
[2023-08-31 22:54:54,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:54:54,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.1005 (2.1795)  loss_scale: 65536.0000 (37183.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6471 (6.9928)  time: 0.8409 (0.5291 -- 3.4152)  data: 0.0014 (0.0002 -- 0.0034)  max mem: 16735
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0857 (2.1698)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9565 (7.0294)  time: 0.6263 (0.5027 -- 1.5428)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16735
Epoch: [17] Total time: 0:02:20 (0.8770 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0857 (2.2028)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9565 (7.0294)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.2986 (1.2986)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3548 (2.3548 -- 2.3548)  data: 2.1078 (2.1078 -- 2.1078)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.3143 (1.4099)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (93.9394)  time: 0.4374 (0.2138 -- 2.3548)  data: 0.2156 (0.0007 -- 2.1078)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.3142 (1.3660)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (95.2381)  time: 0.2295 (0.1700 -- 0.4593)  data: 0.0198 (0.0001 -- 0.2548)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.3766 (1.4207)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (94.1909)  time: 0.2129 (0.1332 -- 0.4593)  data: 0.0196 (0.0001 -- 0.2548)  max mem: 16735
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 58.506 Acc@5 93.361 loss 1.392
Accuracy of the network on the 482 val images: 58.51%
Max accuracy: 59.34%
Epoch: [18]  [  0/160]  eta: 0:20:54  lr: 0.000046  min_lr: 0.000001  loss: 2.5033 (2.5033)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2610 (6.2610)  time: 7.8422 (7.8422 -- 7.8422)  data: 7.2476 (7.2476 -- 7.2476)  max mem: 16735
Epoch: [18]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000001  loss: 2.0903 (2.1924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0107 (8.1497)  time: 0.8635 (0.5389 -- 4.2439)  data: 0.3102 (0.0004 -- 3.6926)  max mem: 16735
Epoch: [18]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000001  loss: 2.2168 (2.1861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5761 (7.7231)  time: 0.8614 (0.5360 -- 3.9601)  data: 0.2497 (0.0004 -- 3.4198)  max mem: 16735
Epoch: [18]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 2.3628 (2.2397)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8149 (7.3031)  time: 0.9235 (0.5193 -- 3.9219)  data: 0.3145 (0.0003 -- 3.3616)  max mem: 16735
Epoch: [18]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.1764 (2.2241)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7521 (7.3363)  time: 0.7671 (0.5289 -- 3.0327)  data: 0.2214 (0.0002 -- 2.5012)  max mem: 16735
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.2409 (2.2346)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6468 (7.1781)  time: 0.8850 (0.5300 -- 4.3823)  data: 0.3379 (0.0002 -- 3.8243)  max mem: 16735
[2023-08-31 22:56:55,765] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:56:55,765] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:56:55,766] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:56:55,766] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:57:06,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[1.1000414583904577e-06, 1.1000414583904577e-06, 1.4667219445206102e-06, 1.4667219445206102e-06, 1.9556292593608135e-06, 1.9556292593608135e-06, 2.6075056791477513e-06, 2.6075056791477513e-06, 3.4766742388636687e-06, 3.4766742388636687e-06, 4.635565651818225e-06, 4.635565651818225e-06, 6.1807542024243e-06, 6.1807542024243e-06, 8.2410056032324e-06, 8.2410056032324e-06, 1.0988007470976532e-05, 1.0988007470976532e-05, 1.465067662796871e-05, 1.465067662796871e-05, 1.953423550395828e-05, 1.953423550395828e-05, 2.604564733861104e-05, 2.604564733861104e-05, 3.472752978481472e-05, 3.472752978481472e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 22:57:06,810] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.720900427182784, CurrSamplesPerSec=22.792415577311218, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [18]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2925 (2.2453)  loss_scale: 65536.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4986 (7.1215)  time: 0.8190 (0.5415 -- 3.7380)  data: 0.2623 (0.0003 -- 3.1927)  max mem: 16735
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.1769 (2.2423)  loss_scale: 65536.0000 (41134.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4421 (7.1015)  time: 0.8774 (0.5288 -- 3.8433)  data: 0.3268 (0.0003 -- 3.2771)  max mem: 16735
[2023-08-31 22:57:34,802] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3032
[2023-08-31 22:57:34,802] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3032
[2023-08-31 22:57:34,803] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:57:34,803] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 22:57:34,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1588 (2.2330)  loss_scale: 65536.0000 (42393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9132 (6.9984)  time: 0.6981 (0.4901 -- 2.0588)  data: 0.1778 (0.0002 -- 1.5525)  max mem: 16735
Epoch: [18] Total time: 0:02:21 (0.8826 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1588 (2.2256)  loss_scale: 65536.0000 (42393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9132 (6.9984)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2640 (1.2640)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3300 (2.3300 -- 2.3300)  data: 2.1099 (2.1099 -- 2.1099)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.3000 (1.3826)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (92.9293)  time: 0.4215 (0.2001 -- 2.3300)  data: 0.2146 (0.0008 -- 2.1099)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.2868 (1.3497)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (94.1799)  time: 0.2245 (0.1688 -- 0.4528)  data: 0.0252 (0.0001 -- 0.2504)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.3920 (1.4041)  acc1: 55.5556 (56.4315)  acc5: 100.0000 (93.7759)  time: 0.2118 (0.1325 -- 0.4528)  data: 0.0246 (0.0001 -- 0.2504)  max mem: 16735
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 59.544 Acc@5 92.739 loss 1.375
Accuracy of the network on the 482 val images: 59.54%
[2023-08-31 22:57:46,134] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 22:57:46,135] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 22:57:46,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 22:57:46,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 22:57:47,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 22:57:47,537] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.54%
Epoch: [19]  [  0/160]  eta: 0:17:55  lr: 0.000046  min_lr: 0.000001  loss: 2.0437 (2.0437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4520 (4.4520)  time: 6.7214 (6.7214 -- 6.7214)  data: 6.1659 (6.1659 -- 6.1659)  max mem: 16735
Epoch: [19]  [ 20/160]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000001  loss: 2.0583 (2.0933)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2747 (6.4359)  time: 0.8465 (0.5281 -- 3.0419)  data: 0.2113 (0.0006 -- 2.5140)  max mem: 16735
Epoch: [19]  [ 40/160]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000001  loss: 2.2671 (2.1646)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3183 (6.8033)  time: 0.8309 (0.5296 -- 4.2339)  data: 0.1875 (0.0002 -- 1.8469)  max mem: 16735
Epoch: [19]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2530 (2.2099)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4429 (7.0923)  time: 0.9166 (0.5284 -- 4.0361)  data: 0.3622 (0.0008 -- 3.4954)  max mem: 16735
Epoch: [19]  [ 80/160]  eta: 0:01:13  lr: 0.000046  min_lr: 0.000001  loss: 2.1187 (2.1852)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0624 (7.0308)  time: 0.7786 (0.5346 -- 2.9847)  data: 0.1438 (0.0005 -- 1.5765)  max mem: 16735
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.1775 (2.1894)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9448 (7.1049)  time: 0.9597 (0.5219 -- 2.7829)  data: 0.2602 (0.0004 -- 2.2659)  max mem: 16735
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2305 (2.1945)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0766 (7.1112)  time: 0.8771 (0.5284 -- 5.0147)  data: 0.3315 (0.0004 -- 4.5013)  max mem: 16735
[2023-08-31 22:59:39,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:59:39,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 22:59:39,877] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 22:59:39,877] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.3224 (2.1977)  loss_scale: 65536.0000 (37415.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8556 (7.0266)  time: 0.8355 (0.5331 -- 4.5966)  data: 0.2788 (0.0003 -- 4.0475)  max mem: 16735
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2179 (2.1913)  loss_scale: 65536.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5366 (6.9986)  time: 0.6531 (0.4991 -- 2.9637)  data: 0.1253 (0.0001 -- 2.4446)  max mem: 16735
Epoch: [19] Total time: 0:02:20 (0.8760 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2179 (2.2067)  loss_scale: 65536.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5366 (6.9986)
[2023-08-31 23:00:07,699] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-31 23:00:07,701] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-31 23:00:07,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-31 23:00:07,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-31 23:00:08,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-31 23:00:08,680] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 1.2872 (1.2872)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3617 (2.3617 -- 2.3617)  data: 2.1512 (2.1512 -- 2.1512)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.3090 (1.3957)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (91.9192)  time: 0.4138 (0.1974 -- 2.3617)  data: 0.2010 (0.0008 -- 2.1512)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.2976 (1.3583)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (94.1799)  time: 0.2273 (0.1698 -- 0.5029)  data: 0.0222 (0.0001 -- 0.3020)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.3325 (1.4126)  acc1: 44.4444 (56.0166)  acc5: 100.0000 (93.7759)  time: 0.2141 (0.1322 -- 0.5029)  data: 0.0217 (0.0001 -- 0.3020)  max mem: 16735
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 59.336 Acc@5 92.946 loss 1.379
Accuracy of the network on the 482 val images: 59.34%
Max accuracy: 59.54%
Epoch: [20]  [  0/160]  eta: 0:18:40  lr: 0.000046  min_lr: 0.000001  loss: 2.3746 (2.3746)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5700 (4.5700)  time: 7.0035 (7.0035 -- 7.0035)  data: 4.6465 (4.6465 -- 4.6465)  max mem: 16735
[2023-08-31 23:00:25,245] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3201
[2023-08-31 23:00:25,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:00:25,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:00:25,245] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3201
[2023-08-31 23:00:25,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [20]  [ 20/160]  eta: 0:02:40  lr: 0.000046  min_lr: 0.000001  loss: 2.0844 (2.1804)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7204 (6.8070)  time: 0.8512 (0.5317 -- 3.5689)  data: 0.0640 (0.0002 -- 1.2504)  max mem: 16735
Epoch: [20]  [ 40/160]  eta: 0:02:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2416 (2.2283)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5533 (6.8481)  time: 0.8619 (0.5333 -- 4.1834)  data: 0.0238 (0.0004 -- 0.4505)  max mem: 16735
Epoch: [20]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000001  loss: 2.2136 (2.2223)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6637 (6.9870)  time: 0.9389 (0.5322 -- 3.0807)  data: 0.1355 (0.0005 -- 1.9729)  max mem: 16735
Epoch: [20]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000001  loss: 2.2405 (2.2198)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8631 (7.0147)  time: 0.8587 (0.5301 -- 4.6993)  data: 0.3070 (0.0003 -- 4.1732)  max mem: 16735
Epoch: [20]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.1357 (2.2103)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3629 (6.9932)  time: 0.8099 (0.5356 -- 2.3589)  data: 0.2076 (0.0005 -- 1.8054)  max mem: 16735
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2329 (2.2081)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7628 (6.9741)  time: 0.8953 (0.5223 -- 4.7011)  data: 0.1163 (0.0002 -- 1.3934)  max mem: 16735
[2023-08-31 23:02:16,506] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:02:16,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:02:16,507] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:02:16,507] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0979 (2.1969)  loss_scale: 65536.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4010 (7.1190)  time: 0.8226 (0.5279 -- 2.6961)  data: 0.1094 (0.0003 -- 2.1492)  max mem: 16735
[2023-08-31 23:02:26,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3344
[2023-08-31 23:02:26,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3344
[2023-08-31 23:02:26,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:02:26,568] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:02:26,568] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2907 (2.2081)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7583 (7.1422)  time: 0.6942 (0.5017 -- 2.9435)  data: 0.1656 (0.0002 -- 2.3997)  max mem: 16735
Epoch: [20] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2907 (2.2047)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7583 (7.1422)
Val:  [ 0/27]  eta: 0:01:10  loss: 1.1854 (1.1854)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.6272 (2.6272 -- 2.6272)  data: 2.3723 (2.3723 -- 2.3723)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.2270 (1.3361)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (91.9192)  time: 0.4363 (0.1991 -- 2.6272)  data: 0.2171 (0.0008 -- 2.3723)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.2366 (1.3125)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (93.1217)  time: 0.2123 (0.1690 -- 0.3120)  data: 0.0064 (0.0001 -- 0.1097)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.3295 (1.3611)  acc1: 55.5556 (57.6763)  acc5: 100.0000 (92.9461)  time: 0.1969 (0.1327 -- 0.3120)  data: 0.0059 (0.0001 -- 0.1097)  max mem: 16735
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 60.996 Acc@5 92.324 loss 1.332
Accuracy of the network on the 482 val images: 61.00%
[2023-08-31 23:02:45,571] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:02:45,572] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:02:45,572] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:02:45,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:02:46,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:02:46,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.00%
Epoch: [21]  [  0/160]  eta: 0:24:56  lr: 0.000046  min_lr: 0.000001  loss: 2.3706 (2.3706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8783 (7.8783)  time: 9.3513 (9.3513 -- 9.3513)  data: 8.8272 (8.8272 -- 8.8272)  max mem: 16735
Epoch: [21]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000001  loss: 2.2901 (2.2507)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5124 (6.8185)  time: 0.7891 (0.5310 -- 3.4058)  data: 0.2368 (0.0005 -- 2.8607)  max mem: 16735
Epoch: [21]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000001  loss: 2.2680 (2.2222)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4868 (6.7749)  time: 0.8905 (0.5316 -- 3.5961)  data: 0.3373 (0.0001 -- 3.0626)  max mem: 16735
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000001  loss: 2.3836 (2.2266)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3108 (6.7405)  time: 0.7784 (0.5397 -- 2.6608)  data: 0.1227 (0.0005 -- 0.9534)  max mem: 16735
Epoch: [21]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.1164 (2.2048)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2239 (6.7031)  time: 0.9055 (0.5258 -- 2.3972)  data: 0.2905 (0.0003 -- 1.8715)  max mem: 16735
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.1515 (2.1955)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3460 (6.8663)  time: 0.8501 (0.5382 -- 2.6869)  data: 0.2825 (0.0004 -- 2.1462)  max mem: 16735
[2023-08-31 23:04:31,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:04:31,020] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:04:31,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:04:31,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [21]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1613 (2.1924)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5182 (6.8338)  time: 0.8322 (0.5325 -- 1.7140)  data: 0.1631 (0.0002 -- 1.1761)  max mem: 16735
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0259 (2.1764)  loss_scale: 65536.0000 (39275.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4624 (6.9519)  time: 0.8810 (0.5306 -- 3.1688)  data: 0.1102 (0.0004 -- 1.6834)  max mem: 16735
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0955 (2.1773)  loss_scale: 65536.0000 (42393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0300 (6.9805)  time: 0.6850 (0.5022 -- 3.6080)  data: 0.0008 (0.0002 -- 0.0047)  max mem: 16735
Epoch: [21] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0955 (2.1740)  loss_scale: 65536.0000 (42393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0300 (6.9805)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1341 (1.1341)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4838 (2.4838 -- 2.4838)  data: 2.2828 (2.2828 -- 2.2828)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.2340 (1.2775)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4279 (0.2059 -- 2.4838)  data: 0.2084 (0.0005 -- 2.2828)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.2190 (1.2479)  acc1: 66.6667 (60.3175)  acc5: 100.0000 (96.2963)  time: 0.2143 (0.1689 -- 0.2732)  data: 0.0066 (0.0001 -- 0.0626)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2340 (1.3043)  acc1: 66.6667 (59.3361)  acc5: 100.0000 (96.2656)  time: 0.1963 (0.1327 -- 0.2732)  data: 0.0064 (0.0001 -- 0.0626)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 62.656 Acc@5 93.776 loss 1.277
Accuracy of the network on the 482 val images: 62.66%
[2023-08-31 23:05:15,774] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:05:15,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:05:15,776] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:05:15,776] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:05:17,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:05:17,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.66%
Epoch: [22]  [  0/160]  eta: 0:20:42  lr: 0.000046  min_lr: 0.000001  loss: 1.8575 (1.8575)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8088 (6.8088)  time: 7.7686 (7.7686 -- 7.7686)  data: 7.2499 (7.2499 -- 7.2499)  max mem: 16735
Epoch: [22]  [ 20/160]  eta: 0:02:48  lr: 0.000046  min_lr: 0.000001  loss: 2.1855 (2.0996)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0148 (7.2632)  time: 0.8768 (0.5264 -- 2.6124)  data: 0.2066 (0.0003 -- 2.0814)  max mem: 16735
[2023-08-31 23:05:43,656] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3542
[2023-08-31 23:05:43,656] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:05:43,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3542
[2023-08-31 23:05:43,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:05:43,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 40/160]  eta: 0:01:59  lr: 0.000046  min_lr: 0.000001  loss: 2.3243 (2.1861)  loss_scale: 32768.0000 (50350.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3156 (7.2982)  time: 0.7835 (0.5277 -- 3.6871)  data: 0.0354 (0.0004 -- 0.6727)  max mem: 16735
Epoch: [22]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000001  loss: 2.3528 (2.2382)  loss_scale: 32768.0000 (44585.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1267 (7.3571)  time: 0.9579 (0.5255 -- 2.3802)  data: 0.1153 (0.0002 -- 1.3520)  max mem: 16735
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.2138 (2.2165)  loss_scale: 32768.0000 (41667.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6821 (7.2539)  time: 0.8326 (0.5378 -- 2.1761)  data: 0.1346 (0.0005 -- 1.0190)  max mem: 16735
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.2106 (2.2045)  loss_scale: 32768.0000 (39905.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1819 (7.1186)  time: 0.8632 (0.5260 -- 3.0374)  data: 0.3044 (0.0002 -- 2.5062)  max mem: 16735
Epoch: [22]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1273 (2.1984)  loss_scale: 32768.0000 (38725.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3574 (7.1640)  time: 0.8330 (0.5366 -- 2.5246)  data: 0.2385 (0.0010 -- 1.9641)  max mem: 16735
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.2318 (2.2004)  loss_scale: 32768.0000 (37880.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6000 (7.3059)  time: 1.0060 (0.5380 -- 4.1000)  data: 0.4368 (0.0004 -- 3.5827)  max mem: 16735
[2023-08-31 23:07:36,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:07:36,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:07:36,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:07:36,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:07:40,488] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3679
[2023-08-31 23:07:40,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:07:40,488] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3679
[2023-08-31 23:07:40,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:07:40,489] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2425 (2.2000)  loss_scale: 32768.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8244 (7.2786)  time: 0.6460 (0.4977 -- 2.9724)  data: 0.1232 (0.0002 -- 2.4492)  max mem: 16735
Epoch: [22] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2425 (2.1887)  loss_scale: 32768.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8244 (7.2786)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.1440 (1.1440)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4055 (2.4055 -- 2.4055)  data: 2.1933 (2.1933 -- 2.1933)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.1811 (1.2661)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4208 (0.2068 -- 2.4055)  data: 0.2037 (0.0004 -- 2.1933)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.1734 (1.2356)  acc1: 66.6667 (60.8466)  acc5: 100.0000 (96.2963)  time: 0.2195 (0.1696 -- 0.4214)  data: 0.0143 (0.0001 -- 0.2357)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2665 (1.2907)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.8506)  time: 0.2021 (0.1335 -- 0.4214)  data: 0.0140 (0.0001 -- 0.2357)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 62.863 Acc@5 93.983 loss 1.267
Accuracy of the network on the 482 val images: 62.86%
[2023-08-31 23:07:48,278] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:07:48,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:07:48,280] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:07:48,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:07:49,454] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:07:49,454] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [23]  [  0/160]  eta: 0:21:11  lr: 0.000046  min_lr: 0.000001  loss: 2.2552 (2.2552)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6726 (12.6726)  time: 7.9486 (7.9486 -- 7.9486)  data: 7.3824 (7.3824 -- 7.3824)  max mem: 16735
Epoch: [23]  [ 20/160]  eta: 0:02:58  lr: 0.000046  min_lr: 0.000001  loss: 2.1346 (2.1588)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6038 (7.9136)  time: 0.9385 (0.5203 -- 3.9371)  data: 0.3941 (0.0004 -- 3.3955)  max mem: 16735
Epoch: [23]  [ 40/160]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000001  loss: 2.2648 (2.1822)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4277 (8.2833)  time: 0.8568 (0.5154 -- 4.4193)  data: 0.3107 (0.0003 -- 3.8852)  max mem: 16735
Epoch: [23]  [ 60/160]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000001  loss: 2.4066 (2.2287)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3802 (7.9476)  time: 1.0163 (0.5118 -- 4.3185)  data: 0.4738 (0.0003 -- 3.8091)  max mem: 16735
Epoch: [23]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000001  loss: 2.2544 (2.2249)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4601 (7.7354)  time: 0.7749 (0.5053 -- 3.2228)  data: 0.2344 (0.0001 -- 2.6876)  max mem: 16735
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000001  loss: 2.1916 (2.2300)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1646 (7.6952)  time: 0.8972 (0.5280 -- 3.1600)  data: 0.3513 (0.0004 -- 2.6136)  max mem: 16735
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2547 (2.2303)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3390 (7.7033)  time: 0.7012 (0.5255 -- 2.2513)  data: 0.1459 (0.0004 -- 1.7151)  max mem: 16735
[2023-08-31 23:09:47,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:09:47,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:09:47,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:09:47,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:09:51,313] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3812
[2023-08-31 23:09:51,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:09:51,314] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3812
[2023-08-31 23:09:51,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:09:51,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.1907 (2.2244)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3861 (7.6244)  time: 0.9159 (0.5298 -- 2.5033)  data: 0.2071 (0.0004 -- 1.3305)  max mem: 16735
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2157 (2.2233)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2742 (7.5767)  time: 0.7097 (0.5018 -- 3.7996)  data: 0.0236 (0.0002 -- 0.4583)  max mem: 16735
Epoch: [23] Total time: 0:02:23 (0.8978 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2157 (2.2203)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2742 (7.5767)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.1798 (1.1798)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3777 (2.3777 -- 2.3777)  data: 2.1092 (2.1092 -- 2.1092)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 1.1878 (1.2589)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4087 (0.1973 -- 2.3777)  data: 0.1933 (0.0007 -- 2.1092)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.1781 (1.2257)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2233 (0.1701 -- 0.4082)  data: 0.0192 (0.0001 -- 0.2112)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1905 (1.2754)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (95.8506)  time: 0.2084 (0.1333 -- 0.4082)  data: 0.0189 (0.0001 -- 0.2112)  max mem: 16735
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 63.900 Acc@5 94.191 loss 1.263
Accuracy of the network on the 482 val images: 63.90%
[2023-08-31 23:10:20,935] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:10:20,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:10:20,937] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:10:20,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:10:22,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:10:22,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.90%
Epoch: [24]  [  0/160]  eta: 0:19:07  lr: 0.000046  min_lr: 0.000001  loss: 1.8475 (1.8475)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8391 (5.8391)  time: 7.1715 (7.1715 -- 7.1715)  data: 5.8573 (5.8573 -- 5.8573)  max mem: 16735
Epoch: [24]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000001  loss: 2.2046 (2.1529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6453 (7.9375)  time: 0.8501 (0.5345 -- 3.4272)  data: 0.0885 (0.0003 -- 0.8323)  max mem: 16735
Epoch: [24]  [ 40/160]  eta: 0:01:58  lr: 0.000046  min_lr: 0.000001  loss: 2.1696 (2.1420)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5439 (7.8368)  time: 0.8109 (0.5350 -- 1.7507)  data: 0.1466 (0.0003 -- 1.2022)  max mem: 16735
Epoch: [24]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000001  loss: 2.0937 (2.1301)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1160 (7.7326)  time: 1.0536 (0.5431 -- 3.4968)  data: 0.2846 (0.0003 -- 2.9844)  max mem: 16735
Epoch: [24]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000001  loss: 2.2712 (2.1526)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6931 (7.7757)  time: 0.8426 (0.5225 -- 3.5797)  data: 0.2924 (0.0001 -- 3.0535)  max mem: 16735
Epoch: [24]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000001  loss: 1.9588 (2.1101)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7494 (7.8911)  time: 0.8626 (0.5359 -- 3.4990)  data: 0.3103 (0.0005 -- 2.9695)  max mem: 16735
[2023-08-31 23:11:58,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:11:58,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:11:58,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:11:58,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:11:59,026] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3942
[2023-08-31 23:11:59,026] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:11:59,026] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:11:59,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3942
[2023-08-31 23:11:59,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [24]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1933 (2.1175)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1616 (7.8767)  time: 0.7408 (0.5149 -- 2.6492)  data: 0.1898 (0.0003 -- 2.1117)  max mem: 16735
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.3349 (2.1413)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2819 (7.8330)  time: 0.9098 (0.5269 -- 3.4916)  data: 0.3605 (0.0003 -- 2.9526)  max mem: 16735
[2023-08-31 23:12:43,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[1.0850126715216115e-06, 1.0850126715216115e-06, 1.4466835620288151e-06, 1.4466835620288151e-06, 1.9289114160384203e-06, 1.9289114160384203e-06, 2.571881888051227e-06, 2.571881888051227e-06, 3.4291758507349696e-06, 3.4291758507349696e-06, 4.572234467646626e-06, 4.572234467646626e-06, 6.096312623528834e-06, 6.096312623528834e-06, 8.12841683137178e-06, 8.12841683137178e-06, 1.0837889108495706e-05, 1.0837889108495706e-05, 1.4450518811327607e-05, 1.4450518811327607e-05, 1.9267358415103478e-05, 1.9267358415103478e-05, 2.568981122013797e-05, 2.568981122013797e-05, 3.425308162685062e-05, 3.425308162685062e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 23:12:43,654] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=18.126816290524783, CurrSamplesPerSec=24.35499742570348, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1501 (2.1351)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7520 (7.7580)  time: 0.6610 (0.5028 -- 3.2597)  data: 0.1374 (0.0002 -- 2.7328)  max mem: 16735
Epoch: [24] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1501 (2.1414)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7520 (7.7580)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.1343 (1.1343)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3112 (2.3112 -- 2.3112)  data: 2.0762 (2.0762 -- 2.0762)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.1343 (1.2184)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4210 (0.2009 -- 2.3112)  data: 0.2019 (0.0006 -- 2.0762)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0847 (1.1937)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2207 (0.1698 -- 0.4026)  data: 0.0121 (0.0001 -- 0.1350)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2072 (1.2460)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.8506)  time: 0.2069 (0.1324 -- 0.4026)  data: 0.0118 (0.0001 -- 0.1350)  max mem: 16735
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 63.900 Acc@5 94.398 loss 1.233
Accuracy of the network on the 482 val images: 63.90%
Max accuracy: 63.90%
Epoch: [25]  [  0/160]  eta: 0:20:04  lr: 0.000046  min_lr: 0.000001  loss: 2.6217 (2.6217)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9193 (6.9193)  time: 7.5276 (7.5276 -- 7.5276)  data: 6.9637 (6.9637 -- 6.9637)  max mem: 16735
Epoch: [25]  [ 20/160]  eta: 0:03:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1343 (2.2450)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0846 (7.4586)  time: 0.9752 (0.5197 -- 3.4249)  data: 0.2395 (0.0006 -- 2.2972)  max mem: 16735
Epoch: [25]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000001  loss: 2.1669 (2.1873)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1264 (7.2464)  time: 0.7904 (0.5253 -- 3.0431)  data: 0.0015 (0.0001 -- 0.0034)  max mem: 16735
Epoch: [25]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 2.2993 (2.2163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8792 (7.4267)  time: 0.8856 (0.5318 -- 3.8563)  data: 0.0016 (0.0004 -- 0.0063)  max mem: 16735
[2023-08-31 23:13:59,959] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:13:59,959] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:13:59,963] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:13:59,963] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 2.0107 (2.1854)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9382 (7.3678)  time: 0.7814 (0.5291 -- 3.2926)  data: 0.0017 (0.0002 -- 0.0071)  max mem: 16735
[2023-08-31 23:14:14,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4086
[2023-08-31 23:14:14,286] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:14:14,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4086
[2023-08-31 23:14:14,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:14:14,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [25]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000001  loss: 2.0900 (2.1743)  loss_scale: 32768.0000 (37634.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0128 (7.3117)  time: 0.9181 (0.5323 -- 3.9115)  data: 0.0018 (0.0004 -- 0.0066)  max mem: 16735
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2055 (2.1679)  loss_scale: 32768.0000 (36830.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4906 (7.4237)  time: 0.7851 (0.5378 -- 2.9056)  data: 0.0070 (0.0003 -- 0.0960)  max mem: 16735
Epoch: [25]  [140/160]  eta: 0:00:17  lr: 0.000046  min_lr: 0.000001  loss: 2.0823 (2.1691)  loss_scale: 32768.0000 (36253.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8489 (7.4577)  time: 0.8125 (0.5379 -- 2.4606)  data: 0.0018 (0.0004 -- 0.0052)  max mem: 16735
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2278 (2.1668)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7328 (7.4302)  time: 0.7488 (0.5021 -- 3.7813)  data: 0.0009 (0.0002 -- 0.0027)  max mem: 16735
Epoch: [25] Total time: 0:02:20 (0.8810 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2278 (2.1751)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7328 (7.4302)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.0797 (1.0797)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5176 (2.5176 -- 2.5176)  data: 2.2663 (2.2663 -- 2.2663)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.1106 (1.2527)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (94.9495)  time: 0.4265 (0.2007 -- 2.5176)  data: 0.2083 (0.0009 -- 2.2663)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.1178 (1.2117)  acc1: 66.6667 (60.8466)  acc5: 100.0000 (95.2381)  time: 0.2131 (0.1695 -- 0.3061)  data: 0.0076 (0.0001 -- 0.1233)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2577 (1.2713)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (95.0207)  time: 0.1966 (0.1325 -- 0.3061)  data: 0.0066 (0.0001 -- 0.1233)  max mem: 16735
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 61.826 Acc@5 93.776 loss 1.251
Accuracy of the network on the 482 val images: 61.83%
Max accuracy: 63.90%
Epoch: [26]  [  0/160]  eta: 0:21:20  lr: 0.000046  min_lr: 0.000001  loss: 1.4250 (1.4250)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0670 (9.0671)  time: 8.0012 (8.0012 -- 8.0012)  data: 5.7169 (5.7169 -- 5.7169)  max mem: 16735
Epoch: [26]  [ 20/160]  eta: 0:02:31  lr: 0.000046  min_lr: 0.000001  loss: 2.2965 (2.2303)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4202 (6.5281)  time: 0.7379 (0.5441 -- 2.0206)  data: 0.0273 (0.0009 -- 0.5117)  max mem: 16735
Epoch: [26]  [ 40/160]  eta: 0:02:06  lr: 0.000046  min_lr: 0.000001  loss: 2.1654 (2.1969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0194 (6.9267)  time: 1.0149 (0.5233 -- 2.9846)  data: 0.0054 (0.0004 -- 0.0866)  max mem: 16735
[2023-08-31 23:16:15,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:16:15,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:16:15,655] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:16:15,656] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [26]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1766 (2.1933)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1639 (6.7446)  time: 0.8954 (0.5297 -- 2.4601)  data: 0.0015 (0.0002 -- 0.0046)  max mem: 16735
[2023-08-31 23:16:21,762] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4221
[2023-08-31 23:16:21,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:16:21,762] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4221
[2023-08-31 23:16:21,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:16:21,762] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000001  loss: 2.1394 (2.1887)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4920 (6.8663)  time: 0.8757 (0.5239 -- 2.5750)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16735
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 2.2945 (2.1925)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3457 (6.9924)  time: 0.8144 (0.5395 -- 2.7163)  data: 0.0025 (0.0004 -- 0.0179)  max mem: 16735
[2023-08-31 23:17:01,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4268
[2023-08-31 23:17:01,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 23:17:01,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4268
[2023-08-31 23:17:01,841] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 23:17:01,842] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 2.2261 (2.1874)  loss_scale: 16384.0000 (32632.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6680 (7.0894)  time: 0.9296 (0.5515 -- 3.3528)  data: 0.0019 (0.0003 -- 0.0040)  max mem: 16735
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.2546 (2.1820)  loss_scale: 16384.0000 (30327.8298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0585 (7.0939)  time: 0.7069 (0.5398 -- 1.9142)  data: 0.0018 (0.0006 -- 0.0061)  max mem: 16735
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1779 (2.1836)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1077 (7.1228)  time: 0.7292 (0.5007 -- 3.0750)  data: 0.0281 (0.0001 -- 0.5465)  max mem: 16735
Epoch: [26] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1779 (2.1737)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1077 (7.1228)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0701 (1.0701)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4224 (2.4224 -- 2.4224)  data: 2.2165 (2.2165 -- 2.2165)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0807 (1.2015)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4410 (0.2063 -- 2.4224)  data: 0.2254 (0.0007 -- 2.2165)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0807 (1.1604)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2270 (0.1733 -- 0.4691)  data: 0.0183 (0.0001 -- 0.2543)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1566 (1.2112)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (96.2656)  time: 0.2115 (0.1326 -- 0.4691)  data: 0.0180 (0.0001 -- 0.2543)  max mem: 16735
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 66.598 Acc@5 93.983 loss 1.197
Accuracy of the network on the 482 val images: 66.60%
[2023-08-31 23:17:49,743] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:17:49,745] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:17:49,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:17:49,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:17:51,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:17:51,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.60%
Epoch: [27]  [  0/160]  eta: 0:21:55  lr: 0.000045  min_lr: 0.000001  loss: 2.2839 (2.2839)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7220 (6.7220)  time: 8.2232 (8.2232 -- 8.2232)  data: 4.7124 (4.7124 -- 4.7124)  max mem: 16735
Epoch: [27]  [ 20/160]  eta: 0:02:52  lr: 0.000045  min_lr: 0.000001  loss: 2.0760 (2.1129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7142 (7.5180)  time: 0.8805 (0.5247 -- 4.2584)  data: 0.0116 (0.0002 -- 0.1323)  max mem: 16735
Epoch: [27]  [ 40/160]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000001  loss: 2.0692 (2.1014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9162 (7.5022)  time: 0.8496 (0.5159 -- 3.2909)  data: 0.0626 (0.0003 -- 1.2159)  max mem: 16735
Epoch: [27]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1866 (2.1120)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5642 (8.0165)  time: 0.8403 (0.5314 -- 2.7041)  data: 0.0959 (0.0003 -- 0.9315)  max mem: 16735
[2023-08-31 23:19:04,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:19:04,477] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 23:19:04,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:19:04,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000001  loss: 2.1480 (2.1199)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4420 (7.9825)  time: 0.8944 (0.5386 -- 2.4306)  data: 0.0319 (0.0002 -- 0.5912)  max mem: 16735
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000001  loss: 2.3470 (2.1473)  loss_scale: 32768.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7013 (7.9287)  time: 0.8290 (0.5341 -- 2.4681)  data: 0.0854 (0.0004 -- 0.8136)  max mem: 16735
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.2312 (2.1555)  loss_scale: 32768.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7030 (7.9636)  time: 0.8289 (0.5359 -- 3.3219)  data: 0.2028 (0.0004 -- 2.2723)  max mem: 16735
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.2594 (2.1588)  loss_scale: 32768.0000 (23820.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0299 (7.8863)  time: 0.8944 (0.5233 -- 3.0975)  data: 0.2409 (0.0005 -- 2.5525)  max mem: 16735
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1689 (2.1600)  loss_scale: 32768.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4036 (7.8111)  time: 0.6993 (0.5015 -- 3.3957)  data: 0.1441 (0.0002 -- 2.8685)  max mem: 16735
Epoch: [27] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1689 (2.1645)  loss_scale: 32768.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4036 (7.8111)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.0221 (1.0221)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4060 (2.4060 -- 2.4060)  data: 2.1889 (2.1889 -- 2.1889)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0853 (1.2424)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (93.9394)  time: 0.4322 (0.2019 -- 2.4060)  data: 0.2066 (0.0006 -- 2.1889)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0801 (1.1962)  acc1: 66.6667 (60.3175)  acc5: 100.0000 (95.2381)  time: 0.2171 (0.1691 -- 0.3080)  data: 0.0086 (0.0001 -- 0.0836)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2334 (1.2616)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (95.0207)  time: 0.1937 (0.1328 -- 0.2795)  data: 0.0083 (0.0001 -- 0.0836)  max mem: 16735
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 61.411 Acc@5 93.983 loss 1.236
Accuracy of the network on the 482 val images: 61.41%
Max accuracy: 66.60%
Epoch: [28]  [  0/160]  eta: 0:15:33  lr: 0.000045  min_lr: 0.000001  loss: 2.2766 (2.2766)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6611 (6.6611)  time: 5.8332 (5.8332 -- 5.8332)  data: 5.2838 (5.2838 -- 5.2838)  max mem: 16735
Epoch: [28]  [ 20/160]  eta: 0:02:41  lr: 0.000045  min_lr: 0.000001  loss: 2.0813 (2.0711)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7642 (7.8829)  time: 0.9173 (0.5381 -- 3.1523)  data: 0.3048 (0.0010 -- 2.6204)  max mem: 16735
Epoch: [28]  [ 40/160]  eta: 0:02:02  lr: 0.000045  min_lr: 0.000001  loss: 2.2935 (2.1147)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1855 (7.7957)  time: 0.8861 (0.5322 -- 2.8087)  data: 0.0828 (0.0005 -- 0.5376)  max mem: 16735
[2023-08-31 23:21:06,328] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:21:06,328] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:21:06,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:21:06,330] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:21:17,994] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4537
[2023-08-31 23:21:17,994] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:21:17,994] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4537
[2023-08-31 23:21:17,994] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:21:17,994] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000001  loss: 2.0589 (2.1034)  loss_scale: 65536.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1574 (7.5844)  time: 0.8391 (0.5203 -- 2.4789)  data: 0.1352 (0.0002 -- 1.7380)  max mem: 16735
Epoch: [28]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000001  loss: 2.1851 (2.1139)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5482 (7.7400)  time: 0.8842 (0.5252 -- 3.4436)  data: 0.3412 (0.0004 -- 2.9130)  max mem: 16735
Epoch: [28]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000001  loss: 2.2303 (2.1415)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3087 (7.8053)  time: 0.8387 (0.5261 -- 2.4027)  data: 0.2081 (0.0004 -- 1.8729)  max mem: 16735
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.1588 (2.1441)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9333 (7.8001)  time: 0.8005 (0.5415 -- 2.4783)  data: 0.2424 (0.0005 -- 1.9506)  max mem: 16735
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.0614 (2.1359)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3985 (7.7826)  time: 0.8987 (0.5359 -- 2.3310)  data: 0.1555 (0.0003 -- 1.2071)  max mem: 16735
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1344 (2.1375)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2066 (7.7279)  time: 0.6935 (0.5025 -- 1.4839)  data: 0.0964 (0.0002 -- 0.9867)  max mem: 16735
Epoch: [28] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1344 (2.1534)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2066 (7.7279)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.9923 (0.9923)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4636 (2.4636 -- 2.4636)  data: 2.2204 (2.2204 -- 2.2204)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.1177 (1.2114)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4338 (0.2034 -- 2.4636)  data: 0.2083 (0.0005 -- 2.2204)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0764 (1.1724)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (94.7090)  time: 0.2166 (0.1704 -- 0.2672)  data: 0.0077 (0.0001 -- 0.0586)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1871 (1.2303)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (94.6058)  time: 0.1993 (0.1327 -- 0.2672)  data: 0.0074 (0.0001 -- 0.0586)  max mem: 16735
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 65.353 Acc@5 93.568 loss 1.209
Accuracy of the network on the 482 val images: 65.35%
Max accuracy: 66.60%
Epoch: [29]  [  0/160]  eta: 0:20:27  lr: 0.000045  min_lr: 0.000001  loss: 2.6713 (2.6713)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9486 (5.9486)  time: 7.6747 (7.6747 -- 7.6747)  data: 7.1228 (7.1228 -- 7.1228)  max mem: 16735
Epoch: [29]  [ 20/160]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000001  loss: 2.3121 (2.2723)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5315 (8.1942)  time: 0.8035 (0.5332 -- 2.4290)  data: 0.2274 (0.0008 -- 1.8763)  max mem: 16735
[2023-08-31 23:23:19,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:23:19,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:23:19,377] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:23:19,378] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 40/160]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000001  loss: 2.0945 (2.1735)  loss_scale: 65536.0000 (44756.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8262 (7.6194)  time: 0.9842 (0.5189 -- 3.6327)  data: 0.4343 (0.0003 -- 3.1064)  max mem: 16735
[2023-08-31 23:23:34,960] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4684
[2023-08-31 23:23:34,960] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:23:34,960] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:23:34,960] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4684
[2023-08-31 23:23:34,962] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [29]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000001  loss: 2.3021 (2.2126)  loss_scale: 32768.0000 (42437.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7688 (7.7429)  time: 0.7854 (0.5215 -- 2.6218)  data: 0.1682 (0.0009 -- 2.0559)  max mem: 16735
Epoch: [29]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000001  loss: 2.1894 (2.1918)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2134 (7.8907)  time: 0.8276 (0.5310 -- 2.0675)  data: 0.2741 (0.0005 -- 1.5227)  max mem: 16735
Epoch: [29]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000001  loss: 2.0977 (2.1720)  loss_scale: 32768.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3193 (7.8256)  time: 0.8754 (0.5383 -- 3.1872)  data: 0.3201 (0.0006 -- 2.6631)  max mem: 16735
Epoch: [29]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 1.9808 (2.1569)  loss_scale: 32768.0000 (37642.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8427 (7.7418)  time: 0.8499 (0.5319 -- 2.7265)  data: 0.2979 (0.0003 -- 2.1807)  max mem: 16735
Epoch: [29]  [140/160]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000001  loss: 2.1073 (2.1464)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5364 (7.7478)  time: 0.8116 (0.5263 -- 3.0487)  data: 0.0705 (0.0006 -- 0.7735)  max mem: 16735
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1578 (2.1481)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (7.7161)  time: 0.8050 (0.5015 -- 2.9871)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [29] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1578 (2.1362)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (7.7161)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.0147 (1.0147)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3450 (2.3450 -- 2.3450)  data: 2.1324 (2.1324 -- 2.1324)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0147 (1.1790)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4291 (0.1998 -- 2.3450)  data: 0.2121 (0.0007 -- 2.1324)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0312 (1.1448)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (95.7672)  time: 0.2237 (0.1701 -- 0.4223)  data: 0.0183 (0.0001 -- 0.1914)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.2335 (1.2024)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (95.4357)  time: 0.2061 (0.1328 -- 0.4223)  data: 0.0180 (0.0001 -- 0.1914)  max mem: 16735
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 66.183 Acc@5 94.191 loss 1.187
Accuracy of the network on the 482 val images: 66.18%
Max accuracy: 66.60%
Epoch: [30]  [  0/160]  eta: 0:16:48  lr: 0.000045  min_lr: 0.000001  loss: 1.9545 (1.9545)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9716 (6.9716)  time: 6.3053 (6.3053 -- 6.3053)  data: 5.7592 (5.7592 -- 5.7592)  max mem: 16735
[2023-08-31 23:25:38,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:25:38,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:25:38,667] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:25:38,668] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:25:39,225] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4814
[2023-08-31 23:25:39,226] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:25:39,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:25:39,226] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4814
[2023-08-31 23:25:39,227] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [30]  [ 20/160]  eta: 0:03:04  lr: 0.000045  min_lr: 0.000001  loss: 2.0499 (2.0935)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2609 (7.4522)  time: 1.0661 (0.5217 -- 4.8200)  data: 0.5003 (0.0003 -- 4.2999)  max mem: 16735
Epoch: [30]  [ 40/160]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000001  loss: 2.1449 (2.1093)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5748 (7.6368)  time: 0.7995 (0.5100 -- 4.2487)  data: 0.2559 (0.0002 -- 3.7421)  max mem: 16735
Epoch: [30]  [ 60/160]  eta: 0:01:43  lr: 0.000045  min_lr: 0.000001  loss: 2.0878 (2.0866)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1277 (7.5189)  time: 0.9787 (0.5097 -- 4.6761)  data: 0.1762 (0.0003 -- 2.4053)  max mem: 16735
Epoch: [30]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000001  loss: 1.8907 (2.0528)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2747 (7.6959)  time: 0.7453 (0.5194 -- 2.9581)  data: 0.0012 (0.0003 -- 0.0035)  max mem: 16735
Epoch: [30]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 2.0683 (2.0642)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0909 (7.6710)  time: 0.8865 (0.5348 -- 4.0964)  data: 0.1042 (0.0005 -- 1.2942)  max mem: 16735
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1407 (2.0692)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1458 (7.6258)  time: 0.9086 (0.5269 -- 3.3008)  data: 0.0012 (0.0001 -- 0.0022)  max mem: 16735
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.2322 (2.0896)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0991 (7.5905)  time: 0.7710 (0.5306 -- 2.0397)  data: 0.0016 (0.0001 -- 0.0055)  max mem: 16735
[2023-08-31 23:27:31,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:27:31,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:27:31,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:27:31,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:27:36,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4949
[2023-08-31 23:27:36,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:27:36,501] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4949
[2023-08-31 23:27:36,502] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:27:36,502] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1276 (2.1023)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5292 (7.5790)  time: 0.7367 (0.5006 -- 2.0567)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16735
Epoch: [30] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1276 (2.1068)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5292 (7.5790)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.9901 (0.9901)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.6236 (2.6236 -- 2.6236)  data: 2.3956 (2.3956 -- 2.3956)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0440 (1.2155)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4423 (0.2087 -- 2.6236)  data: 0.2197 (0.0007 -- 2.3956)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0334 (1.1585)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (95.7672)  time: 0.2182 (0.1686 -- 0.4034)  data: 0.0121 (0.0001 -- 0.2186)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1471 (1.2166)  acc1: 55.5556 (63.9004)  acc5: 100.0000 (95.0207)  time: 0.1994 (0.1323 -- 0.4034)  data: 0.0119 (0.0001 -- 0.2186)  max mem: 16735
Val: Total time: 0:00:07 (0.2952 s / it)
* Acc@1 66.805 Acc@5 93.983 loss 1.200
Accuracy of the network on the 482 val images: 66.80%
[2023-08-31 23:27:49,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:27:49,635] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:27:49,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:27:49,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:27:51,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:27:51,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.80%
Epoch: [31]  [  0/160]  eta: 0:22:31  lr: 0.000045  min_lr: 0.000001  loss: 1.4795 (1.4795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1486 (8.1486)  time: 8.4439 (8.4439 -- 8.4439)  data: 7.8979 (7.8979 -- 7.8979)  max mem: 16735
Epoch: [31]  [ 20/160]  eta: 0:02:43  lr: 0.000045  min_lr: 0.000001  loss: 2.2049 (2.1202)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8319 (8.4388)  time: 0.8027 (0.5410 -- 2.6651)  data: 0.1261 (0.0002 -- 2.1319)  max mem: 16735
[2023-08-31 23:28:30,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=26, lr=[1.0646386668957208e-06, 1.0646386668957208e-06, 1.4195182225276278e-06, 1.4195182225276278e-06, 1.8926909633701704e-06, 1.8926909633701704e-06, 2.523587951160227e-06, 2.523587951160227e-06, 3.364783934880303e-06, 3.364783934880303e-06, 4.486378579840404e-06, 4.486378579840404e-06, 5.9818381064538724e-06, 5.9818381064538724e-06, 7.975784141938495e-06, 7.975784141938495e-06, 1.0634378855917995e-05, 1.0634378855917995e-05, 1.4179171807890659e-05, 1.4179171807890659e-05, 1.890556241052088e-05, 1.890556241052088e-05, 2.5207416547361173e-05, 2.5207416547361173e-05, 3.3609888729814896e-05, 3.3609888729814896e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 23:28:30,491] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.83131598046088, CurrSamplesPerSec=22.90036007851295, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [31]  [ 40/160]  eta: 0:01:58  lr: 0.000045  min_lr: 0.000001  loss: 2.2305 (2.1715)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3900 (8.0864)  time: 0.8009 (0.5304 -- 2.0586)  data: 0.0946 (0.0002 -- 0.9477)  max mem: 16735
Epoch: [31]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1326 (2.1510)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2876 (8.0318)  time: 0.9502 (0.5360 -- 2.3766)  data: 0.1919 (0.0004 -- 1.4680)  max mem: 16735
Epoch: [31]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000001  loss: 2.2341 (2.1414)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5156 (7.8349)  time: 0.7889 (0.5198 -- 3.2677)  data: 0.0092 (0.0003 -- 0.1543)  max mem: 16735
Epoch: [31]  [100/160]  eta: 0:00:54  lr: 0.000045  min_lr: 0.000001  loss: 2.3654 (2.1589)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7803 (7.9001)  time: 0.7915 (0.5355 -- 2.5469)  data: 0.0047 (0.0003 -- 0.0581)  max mem: 16735
[2023-08-31 23:29:40,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:29:40,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:29:40,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:29:40,459] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.1757 (2.1605)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3373 (7.8426)  time: 0.9672 (0.5344 -- 3.9518)  data: 0.0017 (0.0003 -- 0.0060)  max mem: 16735
[2023-08-31 23:29:48,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5089
[2023-08-31 23:29:48,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5089
[2023-08-31 23:29:48,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:29:48,494] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:29:48,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [31]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9455 (2.1380)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4607 (7.8077)  time: 0.8319 (0.5302 -- 2.8589)  data: 0.0016 (0.0001 -- 0.0040)  max mem: 16735
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1308 (2.1340)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3550 (7.7608)  time: 0.7059 (0.5017 -- 2.9309)  data: 0.0017 (0.0002 -- 0.0163)  max mem: 16735
Epoch: [31] Total time: 0:02:20 (0.8797 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1308 (2.1398)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3550 (7.7608)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.0506 (1.0506)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3458 (2.3458 -- 2.3458)  data: 2.1367 (2.1367 -- 2.1367)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 1.0506 (1.1916)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4106 (0.1985 -- 2.3458)  data: 0.1955 (0.0005 -- 2.1367)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9916 (1.1492)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1698 -- 0.5200)  data: 0.0158 (0.0001 -- 0.2983)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1162 (1.2027)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (96.2656)  time: 0.2073 (0.1331 -- 0.5200)  data: 0.0154 (0.0001 -- 0.2983)  max mem: 16735
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 64.523 Acc@5 94.813 loss 1.181
Accuracy of the network on the 482 val images: 64.52%
Max accuracy: 66.80%
Epoch: [32]  [  0/160]  eta: 0:21:01  lr: 0.000045  min_lr: 0.000001  loss: 2.3638 (2.3638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2567 (8.2567)  time: 7.8837 (7.8837 -- 7.8837)  data: 7.1662 (7.1662 -- 7.1662)  max mem: 16735
Epoch: [32]  [ 20/160]  eta: 0:02:51  lr: 0.000045  min_lr: 0.000001  loss: 2.0911 (2.1060)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8564 (7.1902)  time: 0.8942 (0.5172 -- 4.3597)  data: 0.2592 (0.0004 -- 2.6022)  max mem: 16735
Epoch: [32]  [ 40/160]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000001  loss: 2.1907 (2.1616)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6940 (7.5082)  time: 0.8462 (0.5305 -- 4.0657)  data: 0.1606 (0.0001 -- 1.7815)  max mem: 16735
Epoch: [32]  [ 60/160]  eta: 0:01:39  lr: 0.000045  min_lr: 0.000001  loss: 1.9391 (2.1440)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8359 (7.4655)  time: 0.9024 (0.5317 -- 5.1153)  data: 0.1220 (0.0004 -- 2.3076)  max mem: 16735
Epoch: [32]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000001  loss: 2.1857 (2.1435)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0734 (7.3830)  time: 0.7877 (0.5386 -- 2.1705)  data: 0.0812 (0.0005 -- 1.5849)  max mem: 16735
[2023-08-31 23:31:54,614] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:31:54,615] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:31:54,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:31:54,616] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [32]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000001  loss: 2.2639 (2.1509)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6155 (7.2752)  time: 0.9826 (0.5272 -- 4.4228)  data: 0.4309 (0.0003 -- 3.9142)  max mem: 16735
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.2549 (2.1724)  loss_scale: 65536.0000 (38996.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8056 (7.3905)  time: 0.7713 (0.5309 -- 3.2350)  data: 0.2194 (0.0001 -- 2.6987)  max mem: 16735
[2023-08-31 23:32:14,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5243
[2023-08-31 23:32:14,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:32:14,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5243
[2023-08-31 23:32:14,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:32:14,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.0689 (2.1686)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0679 (7.3133)  time: 0.8765 (0.5459 -- 3.0801)  data: 0.3239 (0.0003 -- 2.5688)  max mem: 16735
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.2150 (2.1641)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3139 (7.4106)  time: 0.7223 (0.5018 -- 2.4944)  data: 0.1955 (0.0003 -- 1.9891)  max mem: 16735
Epoch: [32] Total time: 0:02:23 (0.8939 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.2150 (2.1312)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3139 (7.4106)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9338 (0.9338)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3596 (2.3596 -- 2.3596)  data: 2.1526 (2.1526 -- 2.1526)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9929 (1.1775)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4362 (0.1962 -- 2.3596)  data: 0.2216 (0.0008 -- 2.1526)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0019 (1.1260)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2206 (0.1689 -- 0.4737)  data: 0.0143 (0.0001 -- 0.2709)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1131 (1.1824)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.8506)  time: 0.2027 (0.1325 -- 0.4737)  data: 0.0139 (0.0001 -- 0.2709)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 64.315 Acc@5 94.398 loss 1.158
Accuracy of the network on the 482 val images: 64.32%
Max accuracy: 66.80%
Epoch: [33]  [  0/160]  eta: 0:20:23  lr: 0.000045  min_lr: 0.000001  loss: 2.3714 (2.3714)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1160 (6.1160)  time: 7.6487 (7.6487 -- 7.6487)  data: 5.6786 (5.6786 -- 5.6786)  max mem: 16735
Epoch: [33]  [ 20/160]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000001  loss: 2.1699 (2.1837)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1624 (7.3446)  time: 0.8194 (0.5317 -- 3.1419)  data: 0.0371 (0.0008 -- 0.6673)  max mem: 16735
Epoch: [33]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1647 (2.1533)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1270 (7.4321)  time: 0.8506 (0.5455 -- 2.2815)  data: 0.1456 (0.0006 -- 1.0279)  max mem: 16735
Epoch: [33]  [ 60/160]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000001  loss: 2.2881 (2.1706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1622 (7.3827)  time: 0.9328 (0.5304 -- 2.3606)  data: 0.3246 (0.0007 -- 1.7828)  max mem: 16735
Epoch: [33]  [ 80/160]  eta: 0:01:14  lr: 0.000044  min_lr: 0.000001  loss: 2.0977 (2.1495)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8888 (7.3666)  time: 0.7847 (0.5230 -- 3.6541)  data: 0.2174 (0.0002 -- 3.1230)  max mem: 16735
[2023-08-31 23:34:17,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:34:17,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:34:17,248] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:34:17,248] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000001  loss: 2.0580 (2.1414)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5544 (7.3750)  time: 0.9057 (0.5186 -- 4.6919)  data: 0.3630 (0.0003 -- 4.1533)  max mem: 16735
Epoch: [33]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.1481 (2.1351)  loss_scale: 65536.0000 (40621.4876)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3898 (7.3741)  time: 0.8059 (0.5270 -- 2.8452)  data: 0.0354 (0.0003 -- 0.6746)  max mem: 16735
[2023-08-31 23:34:48,914] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5410
[2023-08-31 23:34:48,914] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5410
[2023-08-31 23:34:48,915] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:34:48,915] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:34:48,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000001  loss: 2.0314 (2.1182)  loss_scale: 32768.0000 (41599.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8896 (7.3800)  time: 0.8193 (0.5274 -- 2.5738)  data: 0.1043 (0.0004 -- 2.0458)  max mem: 16735
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.0350 (2.1088)  loss_scale: 32768.0000 (40550.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1556 (7.3655)  time: 0.7478 (0.5012 -- 3.3629)  data: 0.2228 (0.0002 -- 2.8245)  max mem: 16735
Epoch: [33] Total time: 0:02:20 (0.8779 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.0350 (2.1037)  loss_scale: 32768.0000 (40550.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1556 (7.3655)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9655 (0.9655)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3402 (2.3402 -- 2.3402)  data: 2.1114 (2.1114 -- 2.1114)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9663 (1.1903)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4196 (0.2021 -- 2.3402)  data: 0.1946 (0.0008 -- 2.1114)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9643 (1.1282)  acc1: 77.7778 (65.6085)  acc5: 100.0000 (96.2963)  time: 0.2250 (0.1692 -- 0.4147)  data: 0.0118 (0.0001 -- 0.2041)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0955 (1.1817)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.4357)  time: 0.2043 (0.1322 -- 0.4147)  data: 0.0112 (0.0001 -- 0.2041)  max mem: 16735
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 65.975 Acc@5 94.398 loss 1.164
Accuracy of the network on the 482 val images: 65.98%
Max accuracy: 66.80%
Epoch: [34]  [  0/160]  eta: 0:17:15  lr: 0.000044  min_lr: 0.000001  loss: 1.9408 (1.9408)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3845 (8.3845)  time: 6.4746 (6.4746 -- 6.4746)  data: 4.9481 (4.9481 -- 4.9481)  max mem: 16735
Epoch: [34]  [ 20/160]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000001  loss: 2.2247 (2.1322)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2854 (7.9674)  time: 0.9530 (0.5216 -- 3.1693)  data: 0.0702 (0.0004 -- 1.0590)  max mem: 16735
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000001  loss: 2.1213 (2.1228)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7156 (7.8530)  time: 0.9197 (0.5185 -- 5.1731)  data: 0.0020 (0.0003 -- 0.0153)  max mem: 16735
Epoch: [34]  [ 60/160]  eta: 0:01:36  lr: 0.000044  min_lr: 0.000001  loss: 2.1631 (2.0996)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1958 (7.9131)  time: 0.7407 (0.5150 -- 2.7932)  data: 0.0016 (0.0003 -- 0.0063)  max mem: 16735
Epoch: [34]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000001  loss: 2.2150 (2.1138)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2203 (7.8573)  time: 0.9180 (0.5274 -- 3.3744)  data: 0.0251 (0.0003 -- 0.4771)  max mem: 16735
[2023-08-31 23:36:52,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:36:52,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:36:52,442] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:36:52,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 2.1358 (2.1118)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1581 (7.7308)  time: 0.8586 (0.5346 -- 3.0943)  data: 0.1162 (0.0001 -- 1.9991)  max mem: 16735
[2023-08-31 23:36:54,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5542
[2023-08-31 23:36:54,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:36:54,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:36:54,088] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5542
[2023-08-31 23:36:54,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 1.9009 (2.0903)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8632 (7.8374)  time: 0.7606 (0.5275 -- 1.9634)  data: 0.1297 (0.0002 -- 1.4398)  max mem: 16735
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 2.0776 (2.0846)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8233 (7.8097)  time: 0.9186 (0.5203 -- 2.7248)  data: 0.2911 (0.0003 -- 2.1957)  max mem: 16735
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1464 (2.0886)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1817 (7.8264)  time: 0.7406 (0.5005 -- 2.6201)  data: 0.1680 (0.0002 -- 2.1180)  max mem: 16735
Epoch: [34] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.1464 (2.1136)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1817 (7.8264)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8937 (0.8937)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2577 (2.2577 -- 2.2577)  data: 2.0462 (2.0462 -- 2.0462)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.9377 (1.1686)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (94.9495)  time: 0.4036 (0.1922 -- 2.2577)  data: 0.1873 (0.0005 -- 2.0462)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9503 (1.1049)  acc1: 66.6667 (67.7249)  acc5: 100.0000 (95.7672)  time: 0.2186 (0.1694 -- 0.3949)  data: 0.0101 (0.0001 -- 0.1793)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1032 (1.1584)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (95.0207)  time: 0.2036 (0.1327 -- 0.3949)  data: 0.0096 (0.0001 -- 0.1793)  max mem: 16735
Val: Total time: 0:00:07 (0.2819 s / it)
* Acc@1 68.050 Acc@5 94.606 loss 1.136
Accuracy of the network on the 482 val images: 68.05%
[2023-08-31 23:37:47,836] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:37:47,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:37:47,838] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:37:47,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:37:49,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:37:49,101] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.05%
Epoch: [35]  [  0/160]  eta: 0:20:29  lr: 0.000044  min_lr: 0.000001  loss: 2.2180 (2.2180)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4848 (8.4848)  time: 7.6833 (7.6833 -- 7.6833)  data: 7.1573 (7.1573 -- 7.1573)  max mem: 16735
Epoch: [35]  [ 20/160]  eta: 0:02:35  lr: 0.000044  min_lr: 0.000001  loss: 2.0355 (2.0133)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5946 (8.6152)  time: 0.7835 (0.5373 -- 4.1255)  data: 0.1394 (0.0005 -- 2.1382)  max mem: 16735
Epoch: [35]  [ 40/160]  eta: 0:02:02  lr: 0.000044  min_lr: 0.000001  loss: 2.2286 (2.1336)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0527 (8.3261)  time: 0.9284 (0.5326 -- 3.7600)  data: 0.1577 (0.0003 -- 2.6702)  max mem: 16735
Epoch: [35]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000001  loss: 2.1037 (2.1272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2556 (8.0682)  time: 0.9897 (0.5261 -- 3.2803)  data: 0.0694 (0.0004 -- 1.3336)  max mem: 16735
[2023-08-31 23:38:58,098] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:38:58,098] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:38:58,100] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:38:58,101] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [35]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000001  loss: 1.9845 (2.1039)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4269 (8.0018)  time: 0.7598 (0.5335 -- 2.1210)  data: 0.0016 (0.0004 -- 0.0043)  max mem: 16735
[2023-08-31 23:39:12,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5687
[2023-08-31 23:39:12,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5687
[2023-08-31 23:39:12,935] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:39:12,935] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:39:12,935] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [35]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 2.1692 (2.1112)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9325 (8.0183)  time: 0.9210 (0.5363 -- 2.6126)  data: 0.0013 (0.0004 -- 0.0034)  max mem: 16735
Epoch: [35]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.1648 (2.1057)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3799 (7.8732)  time: 0.8234 (0.5316 -- 2.9972)  data: 0.0030 (0.0003 -- 0.0171)  max mem: 16735
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 2.1029 (2.1106)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8386 (7.7013)  time: 0.8920 (0.5338 -- 3.6822)  data: 0.0020 (0.0003 -- 0.0153)  max mem: 16735
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.2012 (2.1143)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4933 (7.7321)  time: 0.5822 (0.5013 -- 1.2286)  data: 0.0008 (0.0002 -- 0.0040)  max mem: 16735
Epoch: [35] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.2012 (2.1059)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4933 (7.7321)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8485 (0.8485)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2489 (2.2489 -- 2.2489)  data: 2.0064 (2.0064 -- 2.0064)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0323 (1.1643)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4297 (0.1944 -- 2.2489)  data: 0.2094 (0.0005 -- 2.0064)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9738 (1.0982)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.2381)  time: 0.2284 (0.1709 -- 0.4995)  data: 0.0200 (0.0001 -- 0.2903)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0604 (1.1492)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (95.0207)  time: 0.2109 (0.1334 -- 0.4995)  data: 0.0198 (0.0001 -- 0.2903)  max mem: 16735
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 68.672 Acc@5 94.606 loss 1.127
Accuracy of the network on the 482 val images: 68.67%
[2023-08-31 23:40:17,681] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:40:17,682] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:40:17,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:40:17,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:40:18,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:40:18,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.67%
Epoch: [36]  [  0/160]  eta: 0:19:03  lr: 0.000044  min_lr: 0.000001  loss: 2.2909 (2.2909)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9443 (6.9443)  time: 7.1457 (7.1457 -- 7.1457)  data: 6.5925 (6.5925 -- 6.5925)  max mem: 16735
Epoch: [36]  [ 20/160]  eta: 0:02:40  lr: 0.000044  min_lr: 0.000001  loss: 1.9192 (1.9222)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5046 (7.6312)  time: 0.8493 (0.5310 -- 2.5348)  data: 0.1789 (0.0002 -- 1.9699)  max mem: 16735
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1431 (2.0409)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0449 (8.0372)  time: 0.8483 (0.5331 -- 3.3381)  data: 0.2145 (0.0002 -- 1.4602)  max mem: 16735
[2023-08-31 23:41:13,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:41:13,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:41:13,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:41:13,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [36]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000001  loss: 1.9883 (2.0432)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4216 (8.0541)  time: 0.9403 (0.5228 -- 4.1178)  data: 0.2411 (0.0004 -- 2.4316)  max mem: 16735
Epoch: [36]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000001  loss: 2.2341 (2.0785)  loss_scale: 65536.0000 (42881.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7681 (7.9567)  time: 0.8536 (0.5208 -- 3.7664)  data: 0.1740 (0.0004 -- 1.9762)  max mem: 16735
[2023-08-31 23:41:38,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5841
[2023-08-31 23:41:38,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5841
[2023-08-31 23:41:38,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:41:38,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:41:38,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [36]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000001  loss: 2.2190 (2.0996)  loss_scale: 32768.0000 (40878.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5931 (8.0007)  time: 0.9589 (0.5199 -- 4.2570)  data: 0.0014 (0.0002 -- 0.0037)  max mem: 16735
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.2274 (2.1246)  loss_scale: 32768.0000 (39538.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9020 (7.9150)  time: 0.7122 (0.5300 -- 2.9926)  data: 0.1144 (0.0004 -- 2.2646)  max mem: 16735
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.9998 (2.1155)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1303 (7.8662)  time: 0.9385 (0.5306 -- 3.5512)  data: 0.3672 (0.0003 -- 3.0188)  max mem: 16735
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1947 (2.1240)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5628 (7.8552)  time: 0.6570 (0.5023 -- 2.8736)  data: 0.0844 (0.0002 -- 1.6564)  max mem: 16735
Epoch: [36] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.1947 (2.1064)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5628 (7.8552)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.8286 (0.8286)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2207 (2.2207 -- 2.2207)  data: 2.0082 (2.0082 -- 2.0082)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9882 (1.1815)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4169 (0.2023 -- 2.2207)  data: 0.2032 (0.0007 -- 2.0082)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9592 (1.1060)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.7672)  time: 0.2298 (0.1689 -- 0.4999)  data: 0.0275 (0.0001 -- 0.3209)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1210 (1.1652)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (95.0207)  time: 0.2134 (0.1323 -- 0.4999)  data: 0.0273 (0.0001 -- 0.3209)  max mem: 16735
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 67.012 Acc@5 94.398 loss 1.145
Accuracy of the network on the 482 val images: 67.01%
Max accuracy: 68.67%
Epoch: [37]  [  0/160]  eta: 0:19:15  lr: 0.000044  min_lr: 0.000001  loss: 2.5983 (2.5983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1586 (6.1586)  time: 7.2199 (7.2199 -- 7.2199)  data: 6.2908 (6.2908 -- 6.2908)  max mem: 16735
Epoch: [37]  [ 20/160]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000001  loss: 2.1398 (2.1909)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9368 (7.5915)  time: 0.9182 (0.5383 -- 2.9834)  data: 0.2007 (0.0005 -- 2.4588)  max mem: 16735
Epoch: [37]  [ 40/160]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000001  loss: 2.0698 (2.1243)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5180 (7.5771)  time: 0.8364 (0.5235 -- 2.1656)  data: 0.1919 (0.0002 -- 1.4884)  max mem: 16735
[2023-08-31 23:43:41,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:43:41,106] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:43:41,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:43:41,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000001  loss: 2.1570 (2.1307)  loss_scale: 65536.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3481 (7.5034)  time: 0.9219 (0.5309 -- 3.1533)  data: 0.3759 (0.0004 -- 2.6236)  max mem: 16735
[2023-08-31 23:43:56,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5986
[2023-08-31 23:43:56,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:43:56,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5986
[2023-08-31 23:43:56,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:43:56,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:44:06,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=33, lr=[1.0391258399005154e-06, 1.0391258399005154e-06, 1.3855011198673537e-06, 1.3855011198673537e-06, 1.847334826489805e-06, 1.847334826489805e-06, 2.463113101986407e-06, 2.463113101986407e-06, 3.2841508026485425e-06, 3.2841508026485425e-06, 4.378867736864724e-06, 4.378867736864724e-06, 5.838490315819631e-06, 5.838490315819631e-06, 7.784653754426174e-06, 7.784653754426174e-06, 1.03795383392349e-05, 1.03795383392349e-05, 1.38393844523132e-05, 1.38393844523132e-05, 1.8452512603084266e-05, 1.8452512603084266e-05, 2.4603350137445687e-05, 2.4603350137445687e-05, 3.2804466849927585e-05, 3.2804466849927585e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 23:44:06,068] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.743638764915502, CurrSamplesPerSec=23.112391978476285, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [37]  [ 80/160]  eta: 0:01:17  lr: 0.000044  min_lr: 0.000001  loss: 2.0823 (2.1151)  loss_scale: 32768.0000 (39240.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3834 (7.4085)  time: 0.8723 (0.5200 -- 4.9783)  data: 0.3269 (0.0002 -- 4.4508)  max mem: 16735
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000001  loss: 2.0915 (2.1256)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3630 (7.4469)  time: 0.8942 (0.5320 -- 3.2884)  data: 0.3389 (0.0005 -- 2.7512)  max mem: 16735
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000001  loss: 2.1456 (2.1181)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0501 (7.5319)  time: 0.7975 (0.5317 -- 2.3520)  data: 0.2457 (0.0003 -- 1.8370)  max mem: 16735
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 2.0555 (2.1041)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6006 (7.4342)  time: 0.8810 (0.5408 -- 3.5418)  data: 0.3267 (0.0009 -- 3.0064)  max mem: 16735
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1035 (2.1065)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0774 (7.4265)  time: 0.7000 (0.5028 -- 1.9421)  data: 0.1702 (0.0002 -- 1.4089)  max mem: 16735
Epoch: [37] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.1035 (2.1067)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0774 (7.4265)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.8398 (0.8398)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3653 (2.3653 -- 2.3653)  data: 2.1627 (2.1627 -- 2.1627)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9146 (1.1253)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (93.9394)  time: 0.4161 (0.2032 -- 2.3653)  data: 0.1975 (0.0009 -- 2.1627)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9146 (1.0560)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (94.7090)  time: 0.2191 (0.1723 -- 0.3381)  data: 0.0103 (0.0001 -- 0.1241)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0745 (1.1207)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (93.3610)  time: 0.2027 (0.1325 -- 0.3381)  data: 0.0101 (0.0001 -- 0.1241)  max mem: 16735
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 67.012 Acc@5 93.568 loss 1.106
Accuracy of the network on the 482 val images: 67.01%
Max accuracy: 68.67%
Epoch: [38]  [  0/160]  eta: 0:15:24  lr: 0.000044  min_lr: 0.000001  loss: 2.0161 (2.0161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5171 (13.5171)  time: 5.7775 (5.7775 -- 5.7775)  data: 5.2415 (5.2415 -- 5.2415)  max mem: 16735
Epoch: [38]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000001  loss: 2.1396 (2.0822)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7240 (7.4105)  time: 0.9074 (0.5322 -- 2.3785)  data: 0.3266 (0.0009 -- 1.7457)  max mem: 16735
[2023-08-31 23:45:57,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:45:57,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:45:57,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:45:57,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1131 (2.0928)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0082 (7.8589)  time: 0.8636 (0.5352 -- 3.2751)  data: 0.3081 (0.0005 -- 2.7342)  max mem: 16735
[2023-08-31 23:46:01,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6122
[2023-08-31 23:46:01,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6122
[2023-08-31 23:46:01,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:46:01,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:46:01,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 60/160]  eta: 0:01:36  lr: 0.000044  min_lr: 0.000001  loss: 2.0155 (2.0685)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5418 (7.8552)  time: 0.8851 (0.5254 -- 5.7334)  data: 0.3353 (0.0004 -- 5.2221)  max mem: 16735
Epoch: [38]  [ 80/160]  eta: 0:01:14  lr: 0.000044  min_lr: 0.000001  loss: 2.1264 (2.0892)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9101 (7.9177)  time: 0.8332 (0.5288 -- 3.1205)  data: 0.2497 (0.0006 -- 2.4653)  max mem: 16735
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 2.1984 (2.1162)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1455 (7.8392)  time: 0.9439 (0.5332 -- 3.5838)  data: 0.3845 (0.0005 -- 3.0503)  max mem: 16735
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 2.1713 (2.1220)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1079 (7.8609)  time: 0.8938 (0.5325 -- 4.8851)  data: 0.3495 (0.0004 -- 4.3723)  max mem: 16735
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.9475 (2.1134)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8741 (7.8929)  time: 0.8722 (0.5348 -- 3.5218)  data: 0.3239 (0.0003 -- 2.9943)  max mem: 16735
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.1300 (2.1098)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1105 (7.8390)  time: 0.6375 (0.5017 -- 2.7621)  data: 0.1170 (0.0002 -- 2.2425)  max mem: 16735
Epoch: [38] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.1300 (2.1007)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1105 (7.8390)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7964 (0.7964)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3533 (2.3533 -- 2.3533)  data: 2.1516 (2.1516 -- 2.1516)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9155 (1.0977)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (97.9798)  time: 0.4139 (0.1977 -- 2.3533)  data: 0.2002 (0.0005 -- 2.1516)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9155 (1.0585)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1706 -- 0.4647)  data: 0.0177 (0.0001 -- 0.2723)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1138 (1.1157)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (96.2656)  time: 0.2084 (0.1329 -- 0.4647)  data: 0.0173 (0.0001 -- 0.2723)  max mem: 16735
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 71.784 Acc@5 95.228 loss 1.092
Accuracy of the network on the 482 val images: 71.78%
[2023-08-31 23:47:49,095] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:47:49,097] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:47:49,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:47:49,097] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:47:50,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:47:50,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.78%
Epoch: [39]  [  0/160]  eta: 0:17:18  lr: 0.000043  min_lr: 0.000001  loss: 2.3046 (2.3046)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7910 (6.7910)  time: 6.4917 (6.4917 -- 6.4917)  data: 4.2249 (4.2249 -- 4.2249)  max mem: 16735
[2023-08-31 23:48:05,987] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:48:05,987] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:48:05,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:48:05,990] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [ 20/160]  eta: 0:02:39  lr: 0.000043  min_lr: 0.000001  loss: 2.2091 (2.1265)  loss_scale: 32768.0000 (48371.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2869 (7.5254)  time: 0.8718 (0.5414 -- 2.8156)  data: 0.1552 (0.0002 -- 2.2897)  max mem: 16735
[2023-08-31 23:48:22,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6270
[2023-08-31 23:48:22,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:48:22,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6270
[2023-08-31 23:48:22,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:48:22,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 40/160]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000001  loss: 2.1323 (2.1362)  loss_scale: 32768.0000 (47953.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3688 (7.3863)  time: 0.8769 (0.5252 -- 4.0762)  data: 0.2872 (0.0004 -- 3.5397)  max mem: 16735
Epoch: [39]  [ 60/160]  eta: 0:01:34  lr: 0.000043  min_lr: 0.000001  loss: 2.2285 (2.1529)  loss_scale: 32768.0000 (42974.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4632 (7.5727)  time: 0.8181 (0.5277 -- 2.7429)  data: 0.2680 (0.0007 -- 2.1961)  max mem: 16735
Epoch: [39]  [ 80/160]  eta: 0:01:13  lr: 0.000043  min_lr: 0.000001  loss: 2.2919 (2.1826)  loss_scale: 32768.0000 (40454.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5098 (7.5594)  time: 0.8308 (0.5287 -- 3.1460)  data: 0.0704 (0.0003 -- 1.2421)  max mem: 16735
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000001  loss: 2.1148 (2.1700)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5368 (7.6471)  time: 0.9280 (0.5251 -- 3.1716)  data: 0.2517 (0.0004 -- 2.2422)  max mem: 16735
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 2.0870 (2.1498)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8538 (7.6554)  time: 0.8257 (0.5322 -- 3.6110)  data: 0.2784 (0.0004 -- 3.0780)  max mem: 16735
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 2.1170 (2.1488)  loss_scale: 32768.0000 (37183.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3608 (7.6336)  time: 1.0306 (0.5244 -- 5.4194)  data: 0.4896 (0.0004 -- 4.9068)  max mem: 16735
[2023-08-31 23:50:12,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:50:12,376] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:50:12,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:50:12,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.1697 (2.1383)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7903 (7.6691)  time: 0.6246 (0.4999 -- 2.2224)  data: 0.0997 (0.0002 -- 1.6925)  max mem: 16735
Epoch: [39] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.1697 (2.1214)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7903 (7.6691)
[2023-08-31 23:50:12,390] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-31 23:50:12,392] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-31 23:50:12,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-31 23:50:12,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-31 23:50:13,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-31 23:50:13,427] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7862 (0.7862)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3761 (2.3761 -- 2.3761)  data: 2.1595 (2.1595 -- 2.1595)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 1.0085 (1.1560)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (94.9495)  time: 0.4194 (0.2016 -- 2.3761)  data: 0.1976 (0.0006 -- 2.1595)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 1.0085 (1.0822)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (95.2381)  time: 0.2180 (0.1707 -- 0.3375)  data: 0.0079 (0.0001 -- 0.1412)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.1059 (1.1464)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (94.1909)  time: 0.2007 (0.1355 -- 0.3375)  data: 0.0076 (0.0001 -- 0.1412)  max mem: 16735
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 69.502 Acc@5 93.983 loss 1.129
Accuracy of the network on the 482 val images: 69.50%
Max accuracy: 71.78%
Epoch: [40]  [  0/160]  eta: 0:16:34  lr: 0.000043  min_lr: 0.000001  loss: 1.3412 (1.3412)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1522 (4.1522)  time: 6.2181 (6.2181 -- 6.2181)  data: 4.1332 (4.1332 -- 4.1332)  max mem: 16735
Epoch: [40]  [ 20/160]  eta: 0:02:36  lr: 0.000043  min_lr: 0.000001  loss: 2.0787 (2.0775)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9935 (7.2310)  time: 0.8638 (0.5203 -- 3.7704)  data: 0.0705 (0.0005 -- 1.3772)  max mem: 16735
[2023-08-31 23:51:00,735] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6436
[2023-08-31 23:51:00,736] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:51:00,735] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6436
[2023-08-31 23:51:00,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 23:51:00,736] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [40]  [ 40/160]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000001  loss: 2.1856 (2.1019)  loss_scale: 65536.0000 (61539.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1162 (7.3955)  time: 0.9094 (0.5127 -- 3.1255)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16735
Epoch: [40]  [ 60/160]  eta: 0:01:37  lr: 0.000043  min_lr: 0.000001  loss: 2.1307 (2.0885)  loss_scale: 32768.0000 (52106.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9107 (7.5318)  time: 0.8992 (0.5310 -- 4.0829)  data: 0.0016 (0.0004 -- 0.0039)  max mem: 16735
Epoch: [40]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000001  loss: 2.1070 (2.0727)  loss_scale: 32768.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4908 (7.6011)  time: 0.8048 (0.5285 -- 3.1254)  data: 0.0093 (0.0004 -- 0.0852)  max mem: 16735
Epoch: [40]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000001  loss: 2.2027 (2.0777)  loss_scale: 32768.0000 (44447.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6183 (7.7696)  time: 0.8546 (0.5314 -- 2.1777)  data: 0.0027 (0.0002 -- 0.0201)  max mem: 16735
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 2.1729 (2.0892)  loss_scale: 32768.0000 (42517.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6074 (7.7606)  time: 0.8362 (0.5275 -- 3.3476)  data: 0.1247 (0.0002 -- 1.0382)  max mem: 16735
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 2.2373 (2.1068)  loss_scale: 32768.0000 (41134.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1809 (7.7029)  time: 0.8697 (0.5190 -- 3.6737)  data: 0.0282 (0.0003 -- 0.5199)  max mem: 16735
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.0174 (2.0887)  loss_scale: 32768.0000 (40140.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5947 (7.7179)  time: 0.7352 (0.5006 -- 3.1877)  data: 0.1241 (0.0002 -- 2.1396)  max mem: 16735
Epoch: [40] Total time: 0:02:21 (0.8826 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.0174 (2.0903)  loss_scale: 32768.0000 (40140.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5947 (7.7179)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7327 (0.7327)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3594 (2.3594 -- 2.3594)  data: 2.1357 (2.1357 -- 2.1357)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9290 (1.1353)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4183 (0.2062 -- 2.3594)  data: 0.1973 (0.0008 -- 2.1357)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9290 (1.0599)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (95.2381)  time: 0.2180 (0.1705 -- 0.3174)  data: 0.0082 (0.0001 -- 0.1254)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0655 (1.1222)  acc1: 71.4286 (68.8797)  acc5: 100.0000 (93.7759)  time: 0.2008 (0.1333 -- 0.3174)  data: 0.0072 (0.0001 -- 0.1254)  max mem: 16735
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 71.784 Acc@5 93.776 loss 1.107
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 71.78%
Epoch: [41]  [  0/160]  eta: 0:22:46  lr: 0.000043  min_lr: 0.000001  loss: 2.5620 (2.5620)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6297 (9.6297)  time: 8.5380 (8.5380 -- 8.5380)  data: 8.0039 (8.0039 -- 8.0039)  max mem: 16735
[2023-08-31 23:53:01,558] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:53:01,558] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:53:01,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:53:01,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:53:12,822] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6579
[2023-08-31 23:53:12,822] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6579
[2023-08-31 23:53:12,823] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:53:12,823] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:53:12,823] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [41]  [ 20/160]  eta: 0:02:34  lr: 0.000043  min_lr: 0.000001  loss: 2.1221 (2.1279)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9150 (7.4387)  time: 0.7292 (0.5233 -- 2.5207)  data: 0.1137 (0.0006 -- 1.8114)  max mem: 16735
Epoch: [41]  [ 40/160]  eta: 0:02:04  lr: 0.000043  min_lr: 0.000001  loss: 2.1877 (2.1295)  loss_scale: 32768.0000 (43957.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4827 (7.4075)  time: 0.9738 (0.5300 -- 4.0245)  data: 0.4187 (0.0003 -- 3.4908)  max mem: 16735
Epoch: [41]  [ 60/160]  eta: 0:01:33  lr: 0.000043  min_lr: 0.000001  loss: 2.1121 (2.1200)  loss_scale: 32768.0000 (40288.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1191 (7.4465)  time: 0.7173 (0.5206 -- 1.6822)  data: 0.1482 (0.0002 -- 1.1327)  max mem: 16735
Epoch: [41]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000001  loss: 2.1823 (2.1105)  loss_scale: 32768.0000 (38431.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5248 (7.5248)  time: 0.9090 (0.5281 -- 2.5429)  data: 0.2022 (0.0006 -- 1.7024)  max mem: 16735
Epoch: [41]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000001  loss: 2.0550 (2.0936)  loss_scale: 32768.0000 (37310.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1045 (7.6298)  time: 0.8919 (0.5217 -- 3.8728)  data: 0.0411 (0.0001 -- 0.4053)  max mem: 16735
Epoch: [41]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 2.0242 (2.0732)  loss_scale: 32768.0000 (36559.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4832 (7.6374)  time: 0.8009 (0.5301 -- 3.1264)  data: 0.0404 (0.0003 -- 0.7753)  max mem: 16735
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.9960 (2.0696)  loss_scale: 32768.0000 (36021.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8452 (7.6748)  time: 0.9627 (0.5394 -- 2.4734)  data: 0.1666 (0.0003 -- 1.4117)  max mem: 16735
[2023-08-31 23:55:04,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:55:04,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:55:04,545] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:55:04,545] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.2332 (2.0849)  loss_scale: 65536.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3919 (7.6845)  time: 0.6771 (0.5008 -- 1.6825)  data: 0.0750 (0.0002 -- 0.7204)  max mem: 16735
Epoch: [41] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.2332 (2.0840)  loss_scale: 65536.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3919 (7.6845)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7424 (0.7424)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4371 (2.4371 -- 2.4371)  data: 2.2135 (2.2135 -- 2.2135)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9427 (1.0957)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (98.9899)  time: 0.4211 (0.2056 -- 2.4371)  data: 0.2026 (0.0009 -- 2.2135)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9394 (1.0385)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.8254)  time: 0.2198 (0.1712 -- 0.4608)  data: 0.0148 (0.0001 -- 0.2789)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0245 (1.0904)  acc1: 77.7778 (70.9544)  acc5: 100.0000 (95.8506)  time: 0.2016 (0.1325 -- 0.4608)  data: 0.0143 (0.0001 -- 0.2789)  max mem: 16735
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 73.651 Acc@5 95.436 loss 1.070
Accuracy of the network on the 482 val images: 73.65%
[2023-08-31 23:55:19,327] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 23:55:19,329] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 23:55:19,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 23:55:19,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 23:55:20,725] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 23:55:20,725] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.65%
Epoch: [42]  [  0/160]  eta: 0:18:21  lr: 0.000043  min_lr: 0.000001  loss: 2.3445 (2.3445)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9863 (8.9863)  time: 6.8822 (6.8822 -- 6.8822)  data: 6.3410 (6.3410 -- 6.3410)  max mem: 16735
Epoch: [42]  [ 20/160]  eta: 0:02:41  lr: 0.000043  min_lr: 0.000001  loss: 2.1048 (2.1321)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3067 (7.6130)  time: 0.8676 (0.5375 -- 2.5620)  data: 0.1066 (0.0008 -- 1.5170)  max mem: 16735
Epoch: [42]  [ 40/160]  eta: 0:02:04  lr: 0.000043  min_lr: 0.000001  loss: 2.1059 (2.0954)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0996 (7.4827)  time: 0.9122 (0.5326 -- 3.5114)  data: 0.0010 (0.0003 -- 0.0027)  max mem: 16735
[2023-08-31 23:56:09,330] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6768
[2023-08-31 23:56:09,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6768
[2023-08-31 23:56:09,331] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:56:09,331] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:56:09,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [42]  [ 60/160]  eta: 0:01:37  lr: 0.000043  min_lr: 0.000001  loss: 2.1818 (2.0932)  loss_scale: 32768.0000 (58552.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0674 (7.2923)  time: 0.8539 (0.5375 -- 2.9723)  data: 0.0072 (0.0002 -- 0.1118)  max mem: 16735
Epoch: [42]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000001  loss: 2.1890 (2.1037)  loss_scale: 32768.0000 (52186.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8172 (7.2233)  time: 0.7968 (0.5192 -- 3.4374)  data: 0.0450 (0.0003 -- 0.8685)  max mem: 16735
Epoch: [42]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000001  loss: 2.0310 (2.0914)  loss_scale: 32768.0000 (48340.9109)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1836 (7.1965)  time: 0.9475 (0.5376 -- 3.6896)  data: 0.1315 (0.0007 -- 1.4228)  max mem: 16735
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 2.0034 (2.0779)  loss_scale: 32768.0000 (45766.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9410 (7.3449)  time: 0.7688 (0.5349 -- 2.6161)  data: 0.0014 (0.0004 -- 0.0045)  max mem: 16735
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 2.0651 (2.0721)  loss_scale: 32768.0000 (43923.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5832 (7.4396)  time: 0.9947 (0.5247 -- 4.3720)  data: 0.0351 (0.0003 -- 0.6765)  max mem: 16735
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9773 (2.0673)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0148 (7.5333)  time: 0.6737 (0.5004 -- 1.9982)  data: 0.1126 (0.0003 -- 1.4915)  max mem: 16735
Epoch: [42] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9773 (2.0747)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0148 (7.5333)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.7114 (0.7114)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1580 (2.1580 -- 2.1580)  data: 1.9526 (1.9526 -- 1.9526)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.9361 (1.1175)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4086 (0.2029 -- 2.1580)  data: 0.1955 (0.0003 -- 1.9526)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9361 (1.0415)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (95.2381)  time: 0.2315 (0.1709 -- 0.4820)  data: 0.0276 (0.0001 -- 0.2900)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0143 (1.0988)  acc1: 77.7778 (70.9544)  acc5: 100.0000 (94.1909)  time: 0.2172 (0.1328 -- 0.4820)  data: 0.0273 (0.0001 -- 0.2900)  max mem: 16735
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 72.614 Acc@5 94.398 loss 1.081
Accuracy of the network on the 482 val images: 72.61%
Max accuracy: 73.65%
Epoch: [43]  [  0/160]  eta: 0:17:24  lr: 0.000043  min_lr: 0.000001  loss: 2.3118 (2.3118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5573 (6.5573)  time: 6.5301 (6.5301 -- 6.5301)  data: 4.8618 (4.8618 -- 4.8618)  max mem: 16735
[2023-08-31 23:58:13,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:58:13,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:58:13,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 23:58:13,872] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 23:58:15,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6900
[2023-08-31 23:58:15,547] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:58:15,548] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6900
[2023-08-31 23:58:15,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 23:58:15,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [43]  [ 20/160]  eta: 0:02:42  lr: 0.000043  min_lr: 0.000001  loss: 2.1643 (2.0922)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1961 (7.1674)  time: 0.8923 (0.5249 -- 2.8139)  data: 0.2443 (0.0003 -- 2.0965)  max mem: 16735
Epoch: [43]  [ 40/160]  eta: 0:02:00  lr: 0.000043  min_lr: 0.000001  loss: 2.1090 (2.0438)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4977 (7.5550)  time: 0.8372 (0.5158 -- 3.7064)  data: 0.2044 (0.0005 -- 2.4205)  max mem: 16735
Epoch: [43]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000001  loss: 2.0628 (2.0716)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1512 (7.5934)  time: 0.9562 (0.5280 -- 4.7612)  data: 0.1193 (0.0003 -- 2.3496)  max mem: 16735
Epoch: [43]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000001  loss: 2.0321 (2.0526)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1382 (7.8204)  time: 0.7508 (0.5175 -- 2.6016)  data: 0.0025 (0.0007 -- 0.0146)  max mem: 16735
Epoch: [43]  [100/160]  eta: 0:00:54  lr: 0.000042  min_lr: 0.000001  loss: 2.1653 (2.0695)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0494 (7.8080)  time: 0.8258 (0.5401 -- 2.5227)  data: 0.0019 (0.0007 -- 0.0053)  max mem: 16735
[2023-08-31 23:59:39,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=39, lr=[1.0087326438894925e-06, 1.0087326438894925e-06, 1.3449768585193233e-06, 1.3449768585193233e-06, 1.7933024780257643e-06, 1.7933024780257643e-06, 2.3910699707010193e-06, 2.3910699707010193e-06, 3.1880932942680254e-06, 3.1880932942680254e-06, 4.2507910590240344e-06, 4.2507910590240344e-06, 5.667721412032045e-06, 5.667721412032045e-06, 7.556961882709394e-06, 7.556961882709394e-06, 1.0075949176945859e-05, 1.0075949176945859e-05, 1.3434598902594478e-05, 1.3434598902594478e-05, 1.7912798536792636e-05, 1.7912798536792636e-05, 2.3883731382390183e-05, 2.3883731382390183e-05, 3.1844975176520246e-05, 3.1844975176520246e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 23:59:39,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.7582084256494, CurrSamplesPerSec=22.404502654577325, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 1.9258 (2.0485)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8407 (7.8675)  time: 0.8607 (0.5431 -- 3.6723)  data: 0.2919 (0.0004 -- 2.9364)  max mem: 16735
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 2.0487 (2.0510)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3447 (7.8241)  time: 1.0307 (0.5283 -- 4.6418)  data: 0.4879 (0.0004 -- 4.1373)  max mem: 16735
[2023-09-01 00:00:06,637] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:00:06,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:00:06,638] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:00:06,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:00:11,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7036
[2023-09-01 00:00:11,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7036
[2023-09-01 00:00:11,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:00:11,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:00:11,038] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.1046 (2.0510)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1294 (7.7924)  time: 0.6141 (0.4971 -- 1.4082)  data: 0.0863 (0.0002 -- 0.8706)  max mem: 16735
Epoch: [43] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.1046 (2.0759)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1294 (7.7924)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7112 (0.7112)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2859 (2.2859 -- 2.2859)  data: 2.0263 (2.0263 -- 2.0263)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9096 (1.0914)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4152 (0.1917 -- 2.2859)  data: 0.1947 (0.0006 -- 2.0263)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9070 (1.0377)  acc1: 66.6667 (73.0159)  acc5: 100.0000 (95.7672)  time: 0.2241 (0.1692 -- 0.3382)  data: 0.0155 (0.0001 -- 0.1013)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0270 (1.0927)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.0207)  time: 0.2099 (0.1328 -- 0.3382)  data: 0.0152 (0.0001 -- 0.1013)  max mem: 16735
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 72.822 Acc@5 94.191 loss 1.070
Accuracy of the network on the 482 val images: 72.82%
Max accuracy: 73.65%
Epoch: [44]  [  0/160]  eta: 0:20:42  lr: 0.000042  min_lr: 0.000001  loss: 1.9252 (1.9252)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8128 (8.8128)  time: 7.7672 (7.7672 -- 7.7672)  data: 7.2302 (7.2302 -- 7.2302)  max mem: 16735
Epoch: [44]  [ 20/160]  eta: 0:02:47  lr: 0.000042  min_lr: 0.000001  loss: 2.0476 (2.0541)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7515 (7.5156)  time: 0.8666 (0.5311 -- 2.2736)  data: 0.0998 (0.0003 -- 1.2062)  max mem: 16735
Epoch: [44]  [ 40/160]  eta: 0:02:03  lr: 0.000042  min_lr: 0.000001  loss: 1.9183 (1.9971)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2763 (7.5046)  time: 0.8559 (0.5251 -- 2.9694)  data: 0.1329 (0.0003 -- 1.3638)  max mem: 16735
Epoch: [44]  [ 60/160]  eta: 0:01:38  lr: 0.000042  min_lr: 0.000001  loss: 2.1479 (2.0378)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3307 (7.4927)  time: 0.8869 (0.5292 -- 3.1274)  data: 0.0669 (0.0004 -- 1.1365)  max mem: 16735
Epoch: [44]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000001  loss: 2.0844 (2.0472)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3738 (7.6966)  time: 0.8227 (0.5341 -- 2.5681)  data: 0.1338 (0.0002 -- 2.0190)  max mem: 16735
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 2.0005 (2.0280)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6873 (7.5714)  time: 0.9641 (0.5331 -- 3.4310)  data: 0.0256 (0.0003 -- 0.4827)  max mem: 16735
Epoch: [44]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000001  loss: 2.1040 (2.0424)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1925 (7.6159)  time: 0.8147 (0.5191 -- 4.0540)  data: 0.0011 (0.0005 -- 0.0019)  max mem: 16735
[2023-09-01 00:02:19,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:02:19,643] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:02:19,647] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:02:19,647] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.9525 (2.0502)  loss_scale: 65536.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0520 (7.6503)  time: 0.9637 (0.5149 -- 5.1172)  data: 0.0009 (0.0003 -- 0.0028)  max mem: 16735
[2023-09-01 00:02:41,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7196
[2023-09-01 00:02:41,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7196
[2023-09-01 00:02:41,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:02:41,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:02:41,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.1707 (2.0572)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2039 (7.6212)  time: 0.5959 (0.4885 -- 1.2633)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16735
Epoch: [44] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.1707 (2.0750)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2039 (7.6212)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.6569 (0.6569)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2116 (2.2116 -- 2.2116)  data: 1.9990 (1.9990 -- 1.9990)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.8521 (1.0640)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (96.9697)  time: 0.4071 (0.2020 -- 2.2116)  data: 0.1905 (0.0005 -- 1.9990)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8521 (1.0217)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.2963)  time: 0.2269 (0.1690 -- 0.3779)  data: 0.0158 (0.0001 -- 0.1926)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0291 (1.0830)  acc1: 71.4286 (69.2946)  acc5: 100.0000 (95.4357)  time: 0.2130 (0.1338 -- 0.3779)  data: 0.0155 (0.0001 -- 0.1926)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 71.992 Acc@5 94.813 loss 1.056
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 73.65%
Epoch: [45]  [  0/160]  eta: 0:19:33  lr: 0.000042  min_lr: 0.000001  loss: 2.1487 (2.1487)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5025 (8.5025)  time: 7.3373 (7.3373 -- 7.3373)  data: 5.3723 (5.3723 -- 5.3723)  max mem: 16735
Epoch: [45]  [ 20/160]  eta: 0:02:39  lr: 0.000042  min_lr: 0.000001  loss: 2.1670 (2.0643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7624 (7.4761)  time: 0.8331 (0.5388 -- 2.4129)  data: 0.1460 (0.0005 -- 1.6218)  max mem: 16735
Epoch: [45]  [ 40/160]  eta: 0:01:54  lr: 0.000042  min_lr: 0.000001  loss: 2.2216 (2.1113)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7304 (7.8261)  time: 0.7527 (0.5294 -- 2.9381)  data: 0.1740 (0.0005 -- 2.3939)  max mem: 16735
Epoch: [45]  [ 60/160]  eta: 0:01:33  lr: 0.000042  min_lr: 0.000001  loss: 2.0873 (2.0891)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2801 (7.9047)  time: 0.8933 (0.5369 -- 2.9190)  data: 0.0933 (0.0007 -- 0.7814)  max mem: 16735
Epoch: [45]  [ 80/160]  eta: 0:01:13  lr: 0.000042  min_lr: 0.000001  loss: 2.0426 (2.0672)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1589 (7.9442)  time: 0.8622 (0.5232 -- 2.7738)  data: 0.2276 (0.0006 -- 2.2528)  max mem: 16735
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 2.2203 (2.0917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4316 (8.0773)  time: 1.0585 (0.5210 -- 5.0903)  data: 0.0232 (0.0003 -- 0.4414)  max mem: 16735
Epoch: [45]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 2.0400 (2.0836)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0153 (8.0948)  time: 0.8180 (0.5290 -- 3.4729)  data: 0.0012 (0.0002 -- 0.0066)  max mem: 16735
[2023-09-01 00:04:48,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:04:48,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:04:48,460] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:04:48,461] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 2.0756 (2.0789)  loss_scale: 65536.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4952 (8.0738)  time: 0.7883 (0.5262 -- 3.7046)  data: 0.0014 (0.0003 -- 0.0043)  max mem: 16735
[2023-09-01 00:05:04,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7348
[2023-09-01 00:05:04,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:05:04,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7348
[2023-09-01 00:05:04,782] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:05:04,782] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.0862 (2.0788)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8162 (8.0811)  time: 0.6659 (0.5037 -- 2.5937)  data: 0.0010 (0.0002 -- 0.0042)  max mem: 16735
Epoch: [45] Total time: 0:02:20 (0.8770 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.0862 (2.0427)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8162 (8.0811)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6621 (0.6621)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3936 (2.3936 -- 2.3936)  data: 2.1812 (2.1812 -- 2.1812)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8286 (1.0341)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4185 (0.2047 -- 2.3936)  data: 0.2003 (0.0006 -- 2.1812)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8286 (0.9994)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (95.7672)  time: 0.2189 (0.1689 -- 0.3247)  data: 0.0117 (0.0001 -- 0.1259)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0155 (1.0583)  acc1: 77.7778 (72.1992)  acc5: 100.0000 (94.1909)  time: 0.2014 (0.1323 -- 0.3247)  data: 0.0111 (0.0001 -- 0.1259)  max mem: 16735
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 73.237 Acc@5 94.606 loss 1.037
Accuracy of the network on the 482 val images: 73.24%
Max accuracy: 73.65%
Epoch: [46]  [  0/160]  eta: 0:17:05  lr: 0.000042  min_lr: 0.000001  loss: 2.2988 (2.2988)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3451 (6.3451)  time: 6.4072 (6.4072 -- 6.4072)  data: 5.8336 (5.8336 -- 5.8336)  max mem: 16735
Epoch: [46]  [ 20/160]  eta: 0:02:57  lr: 0.000042  min_lr: 0.000001  loss: 2.0768 (2.0533)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8240 (7.4669)  time: 1.0112 (0.5195 -- 4.0030)  data: 0.4691 (0.0001 -- 3.4757)  max mem: 16735
Epoch: [46]  [ 40/160]  eta: 0:02:06  lr: 0.000042  min_lr: 0.000001  loss: 2.0212 (2.0646)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3269 (7.4912)  time: 0.8306 (0.5200 -- 4.2004)  data: 0.2856 (0.0003 -- 3.6688)  max mem: 16735
Epoch: [46]  [ 60/160]  eta: 0:01:41  lr: 0.000042  min_lr: 0.000001  loss: 2.1863 (2.0980)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9324 (7.3602)  time: 0.9191 (0.5313 -- 3.5950)  data: 0.3521 (0.0002 -- 3.0745)  max mem: 16735
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000001  loss: 1.9966 (2.0690)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7014 (7.2828)  time: 0.7487 (0.5345 -- 3.4251)  data: 0.1980 (0.0001 -- 2.8970)  max mem: 16735
Epoch: [46]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 1.9816 (2.0539)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5317 (7.4228)  time: 0.8953 (0.5304 -- 3.6498)  data: 0.3393 (0.0008 -- 3.1218)  max mem: 16735
[2023-09-01 00:07:08,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:07:08,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:07:08,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:07:08,679] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 2.2820 (2.0854)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1317 (7.3789)  time: 0.8485 (0.5383 -- 3.5047)  data: 0.2964 (0.0005 -- 2.9842)  max mem: 16735
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 2.0615 (2.0858)  loss_scale: 65536.0000 (38345.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0730 (7.3325)  time: 0.9482 (0.5218 -- 3.4761)  data: 0.4068 (0.0001 -- 2.9367)  max mem: 16735
[2023-09-01 00:07:41,562] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7519
[2023-09-01 00:07:41,562] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7519
[2023-09-01 00:07:41,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:07:41,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:07:41,562] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.1422 (2.0862)  loss_scale: 65536.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8903 (7.3668)  time: 0.6354 (0.4914 -- 2.7655)  data: 0.1131 (0.0002 -- 2.2485)  max mem: 16735
Epoch: [46] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.1422 (2.0799)  loss_scale: 65536.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8903 (7.3668)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7407 (0.7407)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4382 (2.4382 -- 2.4382)  data: 2.2014 (2.2014 -- 2.2014)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.9094 (1.0654)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (96.9697)  time: 0.4311 (0.2017 -- 2.4382)  data: 0.2099 (0.0005 -- 2.2014)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.9094 (1.0114)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (96.2963)  time: 0.2171 (0.1704 -- 0.3030)  data: 0.0114 (0.0001 -- 0.1173)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0033 (1.0650)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (95.4357)  time: 0.2014 (0.1334 -- 0.3030)  data: 0.0110 (0.0001 -- 0.1173)  max mem: 16735
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 73.859 Acc@5 95.436 loss 1.051
Accuracy of the network on the 482 val images: 73.86%
[2023-09-01 00:07:49,377] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:07:49,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:07:49,379] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:07:49,379] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:07:50,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:07:50,760] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.86%
Epoch: [47]  [  0/160]  eta: 0:19:27  lr: 0.000042  min_lr: 0.000001  loss: 1.8272 (1.8272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9342 (5.9342)  time: 7.2996 (7.2996 -- 7.2996)  data: 5.7429 (5.7429 -- 5.7429)  max mem: 16735
Epoch: [47]  [ 20/160]  eta: 0:02:41  lr: 0.000042  min_lr: 0.000001  loss: 1.9915 (2.0532)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6006 (7.6842)  time: 0.8428 (0.5243 -- 4.0089)  data: 0.2904 (0.0003 -- 3.4580)  max mem: 16735
Epoch: [47]  [ 40/160]  eta: 0:02:03  lr: 0.000042  min_lr: 0.000001  loss: 2.0006 (2.0116)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4687 (7.8142)  time: 0.9039 (0.5200 -- 3.2846)  data: 0.3309 (0.0008 -- 2.7511)  max mem: 16735
Epoch: [47]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000001  loss: 2.0156 (2.0139)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6329 (7.6905)  time: 0.9345 (0.5218 -- 4.1677)  data: 0.0740 (0.0002 -- 1.0660)  max mem: 16735
Epoch: [47]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000001  loss: 1.9897 (2.0077)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9950 (7.6504)  time: 0.8019 (0.5266 -- 2.6533)  data: 0.0727 (0.0001 -- 1.4102)  max mem: 16735
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 1.9847 (2.0025)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9589 (7.7643)  time: 0.8878 (0.5146 -- 3.1332)  data: 0.0791 (0.0003 -- 1.5487)  max mem: 16735
Epoch: [47]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 2.2084 (2.0287)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9495 (7.8649)  time: 0.8478 (0.5331 -- 3.1497)  data: 0.2926 (0.0004 -- 2.5980)  max mem: 16735
[2023-09-01 00:09:48,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:09:48,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:09:48,416] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:09:48,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 2.1609 (2.0465)  loss_scale: 65536.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0657 (7.8088)  time: 0.8703 (0.5453 -- 3.3580)  data: 0.1126 (0.0007 -- 1.4296)  max mem: 16735
[2023-09-01 00:10:02,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7664
[2023-09-01 00:10:02,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7664
[2023-09-01 00:10:02,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:10:02,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:10:02,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0490 (2.0386)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7817 (7.8614)  time: 0.6416 (0.5039 -- 1.6643)  data: 0.0016 (0.0002 -- 0.0143)  max mem: 16735
Epoch: [47] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0490 (2.0582)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7817 (7.8614)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.6657 (0.6657)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5149 (2.5149 -- 2.5149)  data: 2.2564 (2.2564 -- 2.2564)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8769 (1.0796)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (93.9394)  time: 0.4384 (0.1916 -- 2.5149)  data: 0.2208 (0.0007 -- 2.2564)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8453 (1.0092)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (95.2381)  time: 0.2161 (0.1704 -- 0.3901)  data: 0.0127 (0.0001 -- 0.1515)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9425 (1.0680)  acc1: 77.7778 (69.2946)  acc5: 100.0000 (94.6058)  time: 0.2004 (0.1328 -- 0.3901)  data: 0.0121 (0.0001 -- 0.1515)  max mem: 16735
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 71.784 Acc@5 94.398 loss 1.047
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 73.86%
Epoch: [48]  [  0/160]  eta: 0:18:40  lr: 0.000041  min_lr: 0.000001  loss: 1.8088 (1.8088)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0397 (7.0397)  time: 7.0034 (7.0034 -- 7.0034)  data: 5.2832 (5.2832 -- 5.2832)  max mem: 16735
Epoch: [48]  [ 20/160]  eta: 0:02:38  lr: 0.000041  min_lr: 0.000001  loss: 2.0920 (2.1117)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8793 (7.1781)  time: 0.8395 (0.5248 -- 2.1463)  data: 0.2524 (0.0004 -- 1.6003)  max mem: 16735
[2023-09-01 00:11:00,970] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7720
[2023-09-01 00:11:00,971] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7720
[2023-09-01 00:11:00,971] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 00:11:00,971] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 00:11:00,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [ 40/160]  eta: 0:01:59  lr: 0.000041  min_lr: 0.000001  loss: 1.9802 (2.0333)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8831 (7.3204)  time: 0.8592 (0.5165 -- 3.6863)  data: 0.2525 (0.0004 -- 3.1599)  max mem: 16735
Epoch: [48]  [ 60/160]  eta: 0:01:40  lr: 0.000041  min_lr: 0.000001  loss: 2.0682 (2.0410)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8952 (7.3116)  time: 1.0264 (0.5280 -- 3.8266)  data: 0.0717 (0.0005 -- 1.4054)  max mem: 16735
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000001  loss: 2.0457 (2.0492)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8509 (7.2672)  time: 0.7844 (0.5276 -- 2.8057)  data: 0.0015 (0.0002 -- 0.0039)  max mem: 16735
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000001  loss: 1.9780 (2.0402)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4454 (7.3919)  time: 0.8101 (0.5286 -- 2.2985)  data: 0.1055 (0.0003 -- 1.1258)  max mem: 16735
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 2.1686 (2.0492)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2227 (7.4893)  time: 0.8294 (0.5345 -- 3.2101)  data: 0.0011 (0.0003 -- 0.0020)  max mem: 16735
Epoch: [48]  [140/160]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000001  loss: 2.0866 (2.0566)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4551 (7.5208)  time: 0.8108 (0.5410 -- 2.0536)  data: 0.0237 (0.0006 -- 0.3320)  max mem: 16735
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.1418 (2.0449)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9634 (7.5288)  time: 0.8444 (0.5015 -- 2.6200)  data: 0.0048 (0.0001 -- 0.0787)  max mem: 16735
Epoch: [48] Total time: 0:02:21 (0.8826 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.1418 (2.0362)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9634 (7.5288)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6431 (0.6431)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3364 (2.3364 -- 2.3364)  data: 2.0996 (2.0996 -- 2.0996)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.8913 (1.0375)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4073 (0.1997 -- 2.3364)  data: 0.1923 (0.0006 -- 2.0996)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8375 (0.9917)  acc1: 66.6667 (71.9577)  acc5: 100.0000 (95.7672)  time: 0.2160 (0.1697 -- 0.3516)  data: 0.0105 (0.0001 -- 0.1402)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9999 (1.0515)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (94.6058)  time: 0.2004 (0.1337 -- 0.3516)  data: 0.0100 (0.0001 -- 0.1402)  max mem: 16735
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 71.992 Acc@5 95.228 loss 1.026
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 73.86%
Epoch: [49]  [  0/160]  eta: 0:19:05  lr: 0.000041  min_lr: 0.000001  loss: 2.5732 (2.5732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4013 (12.4013)  time: 7.1610 (7.1610 -- 7.1610)  data: 6.6044 (6.6044 -- 6.6044)  max mem: 16735
[2023-09-01 00:13:02,959] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:13:02,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 00:13:02,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:13:02,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [ 20/160]  eta: 0:02:50  lr: 0.000041  min_lr: 0.000001  loss: 1.9021 (2.0450)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0930 (8.4571)  time: 0.9234 (0.5279 -- 3.1391)  data: 0.3841 (0.0003 -- 2.6308)  max mem: 16735
Epoch: [49]  [ 40/160]  eta: 0:02:09  lr: 0.000041  min_lr: 0.000001  loss: 2.1111 (2.0854)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0532 (8.0301)  time: 0.9261 (0.5252 -- 3.5426)  data: 0.3807 (0.0001 -- 2.9798)  max mem: 16735
Epoch: [49]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000001  loss: 1.9659 (2.0667)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8683 (8.0125)  time: 0.8391 (0.5200 -- 3.4532)  data: 0.2461 (0.0004 -- 2.9401)  max mem: 16735
Epoch: [49]  [ 80/160]  eta: 0:01:18  lr: 0.000041  min_lr: 0.000001  loss: 2.1513 (2.0784)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0405 (8.1173)  time: 0.9295 (0.5144 -- 3.5995)  data: 0.3561 (0.0004 -- 3.0658)  max mem: 16735
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000001  loss: 2.1785 (2.0895)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9968 (8.0769)  time: 0.8909 (0.5216 -- 4.5375)  data: 0.3528 (0.0002 -- 4.0246)  max mem: 16735
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000001  loss: 2.1029 (2.0814)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8004 (8.0647)  time: 0.8336 (0.5335 -- 3.8253)  data: 0.2853 (0.0002 -- 3.2982)  max mem: 16735
[2023-09-01 00:14:56,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:14:56,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:14:56,859] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:14:56,859] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 2.0191 (2.0659)  loss_scale: 32768.0000 (32651.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4621 (8.0549)  time: 0.7745 (0.5331 -- 4.2464)  data: 0.2255 (0.0002 -- 3.7235)  max mem: 16735
[2023-09-01 00:15:10,195] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7994
[2023-09-01 00:15:10,195] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:15:10,195] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 00:15:10,195] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7994
[2023-09-01 00:15:10,195] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:15:12,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=46, lr=[9.737669719641908e-07, 9.737669719641908e-07, 1.2983559626189209e-06, 1.2983559626189209e-06, 1.7311412834918947e-06, 1.7311412834918947e-06, 2.308188377989193e-06, 2.308188377989193e-06, 3.0775845039855904e-06, 3.0775845039855904e-06, 4.103446005314121e-06, 4.103446005314121e-06, 5.471261340418828e-06, 5.471261340418828e-06, 7.295015120558436e-06, 7.295015120558436e-06, 9.726686827411249e-06, 9.726686827411249e-06, 1.2968915769881664e-05, 1.2968915769881664e-05, 1.7291887693175552e-05, 1.7291887693175552e-05, 2.3055850257567405e-05, 2.3055850257567405e-05, 3.0741133676756536e-05, 3.0741133676756536e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 00:15:12,723] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.795467312239428, CurrSamplesPerSec=24.217642742523353, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0992 (2.0591)  loss_scale: 65536.0000 (35328.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7190 (7.9921)  time: 0.7355 (0.4906 -- 4.5857)  data: 0.2137 (0.0003 -- 4.0453)  max mem: 16735
Epoch: [49] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0992 (2.0643)  loss_scale: 65536.0000 (35328.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7190 (7.9921)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6558 (0.6558)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3152 (2.3152 -- 2.3152)  data: 2.0845 (2.0845 -- 2.0845)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.8685 (1.0414)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4074 (0.2032 -- 2.3152)  data: 0.1911 (0.0009 -- 2.0845)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8533 (0.9870)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2235 (0.1692 -- 0.3739)  data: 0.0182 (0.0001 -- 0.1819)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9649 (1.0442)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.0207)  time: 0.2077 (0.1327 -- 0.3739)  data: 0.0175 (0.0001 -- 0.1819)  max mem: 16735
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 73.651 Acc@5 94.606 loss 1.030
Accuracy of the network on the 482 val images: 73.65%
Max accuracy: 73.86%
Epoch: [50]  [  0/160]  eta: 0:19:35  lr: 0.000041  min_lr: 0.000001  loss: 2.4975 (2.4975)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8480 (8.8480)  time: 7.3463 (7.3463 -- 7.3463)  data: 5.5266 (5.5266 -- 5.5266)  max mem: 16735
Epoch: [50]  [ 20/160]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000001  loss: 1.8306 (1.9025)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1851 (7.3357)  time: 0.8728 (0.5290 -- 2.9041)  data: 0.2679 (0.0003 -- 2.3245)  max mem: 16735
Epoch: [50]  [ 40/160]  eta: 0:02:03  lr: 0.000041  min_lr: 0.000001  loss: 1.8933 (1.9378)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9307 (7.7175)  time: 0.8640 (0.5264 -- 3.7155)  data: 0.3075 (0.0004 -- 3.1624)  max mem: 16735
Epoch: [50]  [ 60/160]  eta: 0:01:38  lr: 0.000041  min_lr: 0.000001  loss: 2.0351 (1.9430)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0579 (7.6962)  time: 0.8896 (0.5237 -- 3.6283)  data: 0.2948 (0.0003 -- 3.0851)  max mem: 16735
Epoch: [50]  [ 80/160]  eta: 0:01:14  lr: 0.000041  min_lr: 0.000001  loss: 2.2977 (1.9824)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0879 (7.8906)  time: 0.7869 (0.5380 -- 1.5627)  data: 0.1319 (0.0003 -- 1.0341)  max mem: 16735
Epoch: [50]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000001  loss: 2.1440 (1.9929)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7035 (7.9753)  time: 0.9183 (0.5327 -- 5.4618)  data: 0.3652 (0.0003 -- 4.9414)  max mem: 16735
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 1.9781 (1.9918)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7479 (8.0252)  time: 0.8626 (0.5209 -- 4.3398)  data: 0.3167 (0.0003 -- 3.8113)  max mem: 16735
[2023-09-01 00:17:13,357] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:17:13,358] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:17:13,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:17:13,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:17:20,473] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8131
[2023-09-01 00:17:20,474] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:17:20,473] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8131
[2023-09-01 00:17:20,475] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:17:20,476] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 2.1678 (2.0123)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4370 (7.9756)  time: 0.8932 (0.5181 -- 4.6589)  data: 0.3433 (0.0006 -- 4.0915)  max mem: 16735
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0535 (2.0182)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9403 (8.0086)  time: 0.6735 (0.5028 -- 3.5151)  data: 0.1493 (0.0003 -- 2.9701)  max mem: 16735
Epoch: [50] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0535 (2.0319)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9403 (8.0086)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5973 (0.5973)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3106 (2.3106 -- 2.3106)  data: 2.1010 (2.1010 -- 2.1010)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8181 (1.0406)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4284 (0.2036 -- 2.3106)  data: 0.2118 (0.0006 -- 2.1010)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8181 (0.9809)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (95.2381)  time: 0.2283 (0.1696 -- 0.4685)  data: 0.0182 (0.0001 -- 0.2196)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 1.0267 (1.0425)  acc1: 66.6667 (70.1245)  acc5: 100.0000 (94.6058)  time: 0.2126 (0.1329 -- 0.4685)  data: 0.0179 (0.0001 -- 0.2196)  max mem: 16735
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 72.614 Acc@5 94.606 loss 1.025
Accuracy of the network on the 482 val images: 72.61%
Max accuracy: 73.86%
Epoch: [51]  [  0/160]  eta: 0:17:18  lr: 0.000041  min_lr: 0.000001  loss: 2.0629 (2.0629)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3621 (6.3621)  time: 6.4923 (6.4923 -- 6.4923)  data: 5.9202 (5.9202 -- 5.9202)  max mem: 16735
Epoch: [51]  [ 20/160]  eta: 0:02:50  lr: 0.000041  min_lr: 0.000001  loss: 2.0668 (2.0132)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3521 (7.5952)  time: 0.9505 (0.5316 -- 2.7901)  data: 0.0955 (0.0004 -- 1.8789)  max mem: 16735
Epoch: [51]  [ 40/160]  eta: 0:01:58  lr: 0.000041  min_lr: 0.000001  loss: 2.0606 (2.0088)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7930 (7.8622)  time: 0.7537 (0.5301 -- 2.6409)  data: 0.0145 (0.0004 -- 0.2579)  max mem: 16735
Epoch: [51]  [ 60/160]  eta: 0:01:33  lr: 0.000041  min_lr: 0.000001  loss: 1.9901 (2.0047)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4469 (8.1145)  time: 0.8333 (0.5244 -- 2.1153)  data: 0.0842 (0.0005 -- 1.5775)  max mem: 16735
Epoch: [51]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000001  loss: 2.0055 (2.0044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6365 (7.9620)  time: 0.9784 (0.5278 -- 3.1922)  data: 0.4315 (0.0003 -- 2.6505)  max mem: 16735
[2023-09-01 00:19:23,926] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:19:23,927] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:19:23,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:19:23,930] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000001  loss: 2.0450 (2.0070)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4115 (7.9256)  time: 0.8362 (0.5348 -- 4.1900)  data: 0.2794 (0.0002 -- 3.6175)  max mem: 16735
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 2.1282 (2.0224)  loss_scale: 65536.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1563 (7.8055)  time: 0.8631 (0.5409 -- 3.6715)  data: 0.3088 (0.0005 -- 3.1176)  max mem: 16735
[2023-09-01 00:19:47,144] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8287
[2023-09-01 00:19:47,144] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:19:47,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 00:19:47,145] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8287
[2023-09-01 00:19:47,145] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000001  loss: 1.9878 (2.0321)  loss_scale: 32768.0000 (39042.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3675 (7.8049)  time: 0.7477 (0.5343 -- 2.6113)  data: 0.1931 (0.0007 -- 2.0594)  max mem: 16735
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 2.1397 (2.0353)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7861 (7.7512)  time: 0.7912 (0.5010 -- 3.0512)  data: 0.2605 (0.0002 -- 2.5086)  max mem: 16735
Epoch: [51] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 2.1397 (2.0507)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7861 (7.7512)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.5867 (0.5867)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1159 (2.1159 -- 2.1159)  data: 1.8879 (1.8879 -- 1.8879)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.7877 (1.0022)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4009 (0.1973 -- 2.1159)  data: 0.1829 (0.0009 -- 1.8879)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7877 (0.9523)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.7672)  time: 0.2275 (0.1716 -- 0.4889)  data: 0.0213 (0.0001 -- 0.2925)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9093 (1.0102)  acc1: 66.6667 (72.1992)  acc5: 100.0000 (95.4357)  time: 0.2113 (0.1325 -- 0.4889)  data: 0.0206 (0.0001 -- 0.2925)  max mem: 16735
Val: Total time: 0:00:07 (0.2839 s / it)
* Acc@1 74.689 Acc@5 95.021 loss 0.991
Accuracy of the network on the 482 val images: 74.69%
[2023-09-01 00:20:19,114] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:20:19,116] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:20:19,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:20:19,116] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:20:20,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:20:20,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.69%
Epoch: [52]  [  0/160]  eta: 0:19:18  lr: 0.000040  min_lr: 0.000001  loss: 2.2695 (2.2695)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9001 (8.9001)  time: 7.2400 (7.2400 -- 7.2400)  data: 6.6925 (6.6925 -- 6.6925)  max mem: 16735
Epoch: [52]  [ 20/160]  eta: 0:02:34  lr: 0.000040  min_lr: 0.000001  loss: 1.8844 (1.9523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0322 (8.1778)  time: 0.7944 (0.5466 -- 1.9958)  data: 0.1268 (0.0006 -- 1.4648)  max mem: 16735
Epoch: [52]  [ 40/160]  eta: 0:01:59  lr: 0.000040  min_lr: 0.000001  loss: 1.9873 (1.9764)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0451 (7.6899)  time: 0.8816 (0.5243 -- 2.9981)  data: 0.0765 (0.0005 -- 0.7040)  max mem: 16735
Epoch: [52]  [ 60/160]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000001  loss: 2.2057 (2.0359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6720 (7.9198)  time: 0.8664 (0.5223 -- 1.9288)  data: 0.1201 (0.0007 -- 0.9170)  max mem: 16735
Epoch: [52]  [ 80/160]  eta: 0:01:14  lr: 0.000040  min_lr: 0.000001  loss: 2.1587 (2.0426)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3636 (7.9148)  time: 0.8716 (0.5301 -- 3.9929)  data: 0.2274 (0.0004 -- 3.4682)  max mem: 16735
[2023-09-01 00:21:49,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:21:49,681] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:21:49,682] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:21:49,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [52]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000001  loss: 2.0563 (2.0439)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4725 (7.8489)  time: 0.9299 (0.5385 -- 3.1510)  data: 0.0102 (0.0005 -- 0.1684)  max mem: 16735
Epoch: [52]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000001  loss: 2.0807 (2.0460)  loss_scale: 65536.0000 (39538.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3160 (7.7402)  time: 0.8350 (0.5268 -- 3.7865)  data: 0.0212 (0.0004 -- 0.3805)  max mem: 16735
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 2.0746 (2.0518)  loss_scale: 65536.0000 (43225.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5400 (7.7079)  time: 0.9566 (0.5308 -- 3.7978)  data: 0.0646 (0.0002 -- 1.2694)  max mem: 16735
[2023-09-01 00:22:32,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8464
[2023-09-01 00:22:32,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8464
[2023-09-01 00:22:32,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:22:32,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:22:32,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 2.0369 (2.0471)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9983 (7.6712)  time: 0.6269 (0.4997 -- 2.4520)  data: 0.0128 (0.0002 -- 0.2415)  max mem: 16735
Epoch: [52] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 2.0369 (2.0627)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9983 (7.6712)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.5732 (0.5732)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5822 (2.5822 -- 2.5822)  data: 2.3642 (2.3642 -- 2.3642)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8412 (1.0171)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (96.9697)  time: 0.4306 (0.1944 -- 2.5822)  data: 0.2216 (0.0003 -- 2.3642)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8412 (0.9784)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2181 (0.1738 -- 0.4482)  data: 0.0169 (0.0001 -- 0.2609)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9647 (1.0405)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (95.8506)  time: 0.2050 (0.1368 -- 0.4482)  data: 0.0166 (0.0000 -- 0.2609)  max mem: 16735
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 74.066 Acc@5 95.436 loss 1.020
Accuracy of the network on the 482 val images: 74.07%
Max accuracy: 74.69%
Epoch: [53]  [  0/160]  eta: 0:22:51  lr: 0.000040  min_lr: 0.000001  loss: 2.2800 (2.2800)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2811 (5.2811)  time: 8.5699 (8.5699 -- 8.5699)  data: 8.0155 (8.0155 -- 8.0155)  max mem: 16735
Epoch: [53]  [ 20/160]  eta: 0:02:43  lr: 0.000040  min_lr: 0.000001  loss: 2.0573 (2.0248)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3544 (8.2769)  time: 0.7983 (0.5372 -- 2.9512)  data: 0.2298 (0.0005 -- 2.4178)  max mem: 16735
Epoch: [53]  [ 40/160]  eta: 0:02:10  lr: 0.000040  min_lr: 0.000001  loss: 1.9363 (1.9900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0720 (8.0635)  time: 0.9999 (0.5242 -- 4.2386)  data: 0.4500 (0.0003 -- 3.6855)  max mem: 16735
Epoch: [53]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000001  loss: 2.2284 (2.0807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4031 (8.1692)  time: 0.7664 (0.5344 -- 3.3359)  data: 0.2130 (0.0003 -- 2.8014)  max mem: 16735
Epoch: [53]  [ 80/160]  eta: 0:01:14  lr: 0.000040  min_lr: 0.000001  loss: 1.9445 (2.0560)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8863 (8.0517)  time: 0.7609 (0.5215 -- 2.4793)  data: 0.1830 (0.0003 -- 1.9390)  max mem: 16735
Epoch: [53]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000001  loss: 2.1198 (2.0692)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7361 (7.9401)  time: 0.9877 (0.5210 -- 3.9193)  data: 0.1410 (0.0006 -- 1.4097)  max mem: 16735
[2023-09-01 00:24:35,718] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:24:35,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:24:35,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:24:35,723] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000001  loss: 2.1321 (2.0616)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4723 (7.7213)  time: 0.8003 (0.5291 -- 3.8724)  data: 0.0015 (0.0004 -- 0.0026)  max mem: 16735
[2023-09-01 00:24:51,718] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8612
[2023-09-01 00:24:51,718] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:24:51,719] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8612
[2023-09-01 00:24:51,719] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:24:51,720] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 1.9181 (2.0499)  loss_scale: 65536.0000 (37183.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2885 (7.6819)  time: 0.8111 (0.5417 -- 3.9034)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16735
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 2.0856 (2.0528)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6019 (7.7243)  time: 0.7626 (0.5010 -- 4.1576)  data: 0.0480 (0.0002 -- 0.6634)  max mem: 16735
Epoch: [53] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 2.0856 (2.0230)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6019 (7.7243)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5562 (0.5562)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4072 (2.4072 -- 2.4072)  data: 2.1921 (2.1921 -- 2.1921)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8420 (1.0257)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (93.9394)  time: 0.4318 (0.1999 -- 2.4072)  data: 0.2189 (0.0004 -- 2.1921)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8100 (0.9574)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (94.7090)  time: 0.2175 (0.1724 -- 0.4106)  data: 0.0110 (0.0001 -- 0.1942)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9530 (1.0209)  acc1: 77.7778 (71.3693)  acc5: 100.0000 (93.7759)  time: 0.2016 (0.1329 -- 0.4106)  data: 0.0101 (0.0001 -- 0.1942)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 72.822 Acc@5 93.776 loss 1.015
Accuracy of the network on the 482 val images: 72.82%
Max accuracy: 74.69%
Epoch: [54]  [  0/160]  eta: 0:19:36  lr: 0.000040  min_lr: 0.000001  loss: 2.5616 (2.5616)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1380 (6.1380)  time: 7.3514 (7.3514 -- 7.3514)  data: 6.3842 (6.3842 -- 6.3842)  max mem: 16735
Epoch: [54]  [ 20/160]  eta: 0:02:42  lr: 0.000040  min_lr: 0.000001  loss: 1.9475 (1.9771)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2528 (7.8369)  time: 0.8526 (0.5309 -- 3.5281)  data: 0.2562 (0.0004 -- 2.4775)  max mem: 16735
Epoch: [54]  [ 40/160]  eta: 0:02:03  lr: 0.000040  min_lr: 0.000001  loss: 2.1237 (2.0286)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6198 (8.0021)  time: 0.8844 (0.5325 -- 3.6572)  data: 0.2638 (0.0003 -- 3.1365)  max mem: 16735
Epoch: [54]  [ 60/160]  eta: 0:01:40  lr: 0.000040  min_lr: 0.000001  loss: 2.0783 (2.0144)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7099 (8.2564)  time: 0.9635 (0.5292 -- 5.2305)  data: 0.4192 (0.0003 -- 4.6770)  max mem: 16735
Epoch: [54]  [ 80/160]  eta: 0:01:17  lr: 0.000040  min_lr: 0.000001  loss: 2.1228 (2.0446)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5744 (8.1921)  time: 0.8330 (0.5230 -- 3.2792)  data: 0.2851 (0.0003 -- 2.7422)  max mem: 16735
Epoch: [54]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000001  loss: 2.0942 (2.0495)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8638 (8.2865)  time: 0.9066 (0.5169 -- 4.3121)  data: 0.3592 (0.0003 -- 3.7974)  max mem: 16735
[2023-09-01 00:26:56,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:26:56,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:26:56,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:26:56,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 2.0419 (2.0447)  loss_scale: 65536.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2847 (8.1873)  time: 0.8680 (0.5295 -- 3.3866)  data: 0.3170 (0.0003 -- 2.8522)  max mem: 16735
[2023-09-01 00:27:29,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8780
[2023-09-01 00:27:29,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8780
[2023-09-01 00:27:29,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:27:29,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:27:29,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 2.0699 (2.0460)  loss_scale: 65536.0000 (41831.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5286 (8.1315)  time: 0.7768 (0.5298 -- 2.7686)  data: 0.2212 (0.0003 -- 2.2292)  max mem: 16735
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 2.0324 (2.0386)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2009 (8.0530)  time: 0.6818 (0.5033 -- 3.3084)  data: 0.1616 (0.0003 -- 2.7600)  max mem: 16735
Epoch: [54] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 2.0324 (2.0342)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2009 (8.0530)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.5993 (0.5993)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4870 (2.4870 -- 2.4870)  data: 2.2699 (2.2699 -- 2.2699)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8423 (1.0089)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (93.9394)  time: 0.4226 (0.2042 -- 2.4870)  data: 0.2094 (0.0007 -- 2.2699)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7927 (0.9514)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.2381)  time: 0.2145 (0.1684 -- 0.3680)  data: 0.0110 (0.0001 -- 0.1832)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9182 (1.0134)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (93.7759)  time: 0.2001 (0.1324 -- 0.3680)  data: 0.0106 (0.0001 -- 0.1832)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 75.311 Acc@5 93.776 loss 1.003
Accuracy of the network on the 482 val images: 75.31%
[2023-09-01 00:27:49,976] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:27:49,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:27:49,978] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:27:49,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:27:51,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:27:51,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.31%
Epoch: [55]  [  0/160]  eta: 0:21:32  lr: 0.000040  min_lr: 0.000001  loss: 1.7775 (1.7775)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7029 (8.7029)  time: 8.0774 (8.0774 -- 8.0774)  data: 6.3048 (6.3048 -- 6.3048)  max mem: 16735
Epoch: [55]  [ 20/160]  eta: 0:02:38  lr: 0.000040  min_lr: 0.000001  loss: 2.2041 (2.1160)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7241 (8.2051)  time: 0.7857 (0.5186 -- 3.1451)  data: 0.2216 (0.0003 -- 2.2385)  max mem: 16735
Epoch: [55]  [ 40/160]  eta: 0:02:00  lr: 0.000040  min_lr: 0.000001  loss: 2.1431 (2.1476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7232 (7.8035)  time: 0.8696 (0.5289 -- 3.9613)  data: 0.3270 (0.0003 -- 3.4287)  max mem: 16735
Epoch: [55]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000001  loss: 2.0803 (2.1246)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7911 (7.9414)  time: 0.9334 (0.5370 -- 2.6256)  data: 0.3639 (0.0010 -- 2.0817)  max mem: 16735
Epoch: [55]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000001  loss: 2.0526 (2.0858)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1254 (7.9866)  time: 0.8588 (0.5394 -- 2.9533)  data: 0.2299 (0.0003 -- 2.3776)  max mem: 16735
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000001  loss: 2.0542 (2.0772)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8583 (8.0181)  time: 0.8244 (0.5342 -- 2.4833)  data: 0.0974 (0.0004 -- 1.0707)  max mem: 16735
[2023-09-01 00:29:33,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:29:33,341] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:29:33,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:29:33,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000001  loss: 2.0540 (2.0717)  loss_scale: 65536.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8721 (7.9233)  time: 0.9007 (0.5131 -- 3.2566)  data: 0.1107 (0.0003 -- 1.4927)  max mem: 16735
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 2.2508 (2.0808)  loss_scale: 65536.0000 (40204.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1198 (7.8385)  time: 0.8462 (0.5230 -- 3.8433)  data: 0.0021 (0.0004 -- 0.0082)  max mem: 16735
[2023-09-01 00:30:12,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-09-01 00:30:12,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-09-01 00:30:12,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:30:12,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:30:12,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 2.0246 (2.0811)  loss_scale: 65536.0000 (42803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5886 (7.8458)  time: 0.6946 (0.4890 -- 2.1214)  data: 0.0092 (0.0001 -- 0.1380)  max mem: 16735
Epoch: [55] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 2.0246 (2.0608)  loss_scale: 65536.0000 (42803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5886 (7.8458)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.5636 (0.5636)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6735 (2.6735 -- 2.6735)  data: 2.4292 (2.4292 -- 2.4292)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8647 (1.0250)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (93.9394)  time: 0.4404 (0.2042 -- 2.6735)  data: 0.2226 (0.0006 -- 2.4292)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8266 (0.9647)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (94.7090)  time: 0.2126 (0.1705 -- 0.3026)  data: 0.0053 (0.0001 -- 0.0832)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9319 (1.0303)  acc1: 71.4286 (74.2739)  acc5: 100.0000 (93.7759)  time: 0.1980 (0.1326 -- 0.3026)  data: 0.0048 (0.0001 -- 0.0832)  max mem: 16735
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 74.481 Acc@5 94.606 loss 1.018
Accuracy of the network on the 482 val images: 74.48%
Max accuracy: 75.31%
Epoch: [56]  [  0/160]  eta: 0:17:31  lr: 0.000039  min_lr: 0.000001  loss: 2.3560 (2.3560)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7551 (5.7551)  time: 6.5747 (6.5747 -- 6.5747)  data: 6.0009 (6.0009 -- 6.0009)  max mem: 16735
Epoch: [56]  [ 20/160]  eta: 0:02:38  lr: 0.000039  min_lr: 0.000001  loss: 2.1300 (2.0944)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9752 (8.4872)  time: 0.8568 (0.5417 -- 2.4902)  data: 0.2118 (0.0003 -- 1.9221)  max mem: 16735
[2023-09-01 00:31:00,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=52, lr=[9.345830379155042e-07, 9.345830379155042e-07, 1.2461107172206725e-06, 1.2461107172206725e-06, 1.6614809562942299e-06, 1.6614809562942299e-06, 2.2153079417256397e-06, 2.2153079417256397e-06, 2.953743922300853e-06, 2.953743922300853e-06, 3.938325229734471e-06, 3.938325229734471e-06, 5.251100306312628e-06, 5.251100306312628e-06, 7.001467075083503e-06, 7.001467075083503e-06, 9.33528943344467e-06, 9.33528943344467e-06, 1.2447052577926228e-05, 1.2447052577926228e-05, 1.659607010390164e-05, 1.659607010390164e-05, 2.212809347186885e-05, 2.212809347186885e-05, 2.950412462915847e-05, 2.950412462915847e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 00:31:00,707] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.911828491521877, CurrSamplesPerSec=22.9179198331641, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [56]  [ 40/160]  eta: 0:01:57  lr: 0.000039  min_lr: 0.000001  loss: 2.0061 (2.0248)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8017 (8.6498)  time: 0.8158 (0.5364 -- 2.7086)  data: 0.2523 (0.0003 -- 2.1576)  max mem: 16735
Epoch: [56]  [ 60/160]  eta: 0:01:36  lr: 0.000039  min_lr: 0.000001  loss: 2.0346 (2.0193)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5184 (8.5529)  time: 0.9432 (0.5299 -- 3.0679)  data: 0.1748 (0.0004 -- 2.5107)  max mem: 16735
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000039  min_lr: 0.000001  loss: 2.0232 (2.0363)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1492 (8.5668)  time: 0.8581 (0.5250 -- 3.5936)  data: 0.2864 (0.0003 -- 3.0554)  max mem: 16735
Epoch: [56]  [100/160]  eta: 0:00:54  lr: 0.000039  min_lr: 0.000001  loss: 1.9005 (1.9962)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8874 (8.4695)  time: 0.8257 (0.5378 -- 2.7046)  data: 0.1387 (0.0005 -- 2.1824)  max mem: 16735
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000001  loss: 2.1625 (2.0202)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9893 (8.3804)  time: 0.8623 (0.5333 -- 2.7817)  data: 0.0016 (0.0003 -- 0.0039)  max mem: 16735
[2023-09-01 00:32:18,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:32:18,563] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:32:18,564] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:32:18,564] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 1.9548 (2.0147)  loss_scale: 65536.0000 (36021.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8976 (8.3040)  time: 0.9075 (0.5280 -- 4.0711)  data: 0.0055 (0.0004 -- 0.0851)  max mem: 16735
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 2.0054 (2.0192)  loss_scale: 65536.0000 (39526.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0963 (8.3261)  time: 0.6969 (0.5025 -- 3.0171)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16735
Epoch: [56] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 2.0054 (2.0096)  loss_scale: 65536.0000 (39526.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0963 (8.3261)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5382 (0.5382)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3074 (2.3074 -- 2.3074)  data: 2.0893 (2.0893 -- 2.0893)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7799 (0.9821)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (97.9798)  time: 0.4321 (0.1906 -- 2.3074)  data: 0.2144 (0.0009 -- 2.0893)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7799 (0.9247)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.8254)  time: 0.2228 (0.1697 -- 0.4974)  data: 0.0156 (0.0001 -- 0.2486)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8770 (0.9953)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.8506)  time: 0.2075 (0.1325 -- 0.4974)  data: 0.0148 (0.0001 -- 0.2486)  max mem: 16735
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 75.519 Acc@5 95.228 loss 0.981
Accuracy of the network on the 482 val images: 75.52%
[2023-09-01 00:32:50,357] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:32:50,358] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:32:50,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:32:50,358] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:32:51,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:32:51,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.52%
Epoch: [57]  [  0/160]  eta: 0:19:21  lr: 0.000039  min_lr: 0.000001  loss: 2.1671 (2.1671)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6088 (7.6088)  time: 7.2569 (7.2569 -- 7.2569)  data: 6.7205 (6.7205 -- 6.7205)  max mem: 16735
[2023-09-01 00:33:01,583] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9125
[2023-09-01 00:33:01,584] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:33:01,584] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9125
[2023-09-01 00:33:01,584] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:33:01,584] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [57]  [ 20/160]  eta: 0:02:41  lr: 0.000039  min_lr: 0.000001  loss: 2.2385 (2.1433)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8080 (7.4535)  time: 0.8501 (0.5308 -- 2.1267)  data: 0.3036 (0.0005 -- 1.5762)  max mem: 16735
Epoch: [57]  [ 40/160]  eta: 0:02:13  lr: 0.000039  min_lr: 0.000001  loss: 1.7739 (2.0031)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8087 (7.7829)  time: 1.0701 (0.5234 -- 4.4603)  data: 0.5223 (0.0001 -- 3.9250)  max mem: 16735
Epoch: [57]  [ 60/160]  eta: 0:01:35  lr: 0.000039  min_lr: 0.000001  loss: 1.9417 (1.9832)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1477 (7.7515)  time: 0.6375 (0.5242 -- 1.4788)  data: 0.0915 (0.0004 -- 0.9284)  max mem: 16735
Epoch: [57]  [ 80/160]  eta: 0:01:16  lr: 0.000039  min_lr: 0.000001  loss: 1.9663 (1.9836)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1732 (7.9318)  time: 0.9760 (0.5368 -- 4.3084)  data: 0.4265 (0.0003 -- 3.7852)  max mem: 16735
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000001  loss: 2.1302 (2.0003)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0720 (7.9932)  time: 0.8280 (0.5256 -- 3.4035)  data: 0.2833 (0.0003 -- 2.8731)  max mem: 16735
Epoch: [57]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000001  loss: 2.0135 (1.9896)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0805 (8.0313)  time: 0.9833 (0.5294 -- 4.1687)  data: 0.4381 (0.0003 -- 3.6450)  max mem: 16735
[2023-09-01 00:34:55,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:34:55,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:34:55,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:34:55,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 2.1807 (2.0103)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3703 (8.0888)  time: 0.6861 (0.5390 -- 1.9586)  data: 0.0638 (0.0003 -- 0.7421)  max mem: 16735
[2023-09-01 00:35:06,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9267
[2023-09-01 00:35:06,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:35:06,943] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9267
[2023-09-01 00:35:06,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:35:06,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 2.1734 (2.0125)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8431 (8.0990)  time: 0.7851 (0.5013 -- 4.2552)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16735
Epoch: [57] Total time: 0:02:23 (0.8942 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 2.1734 (2.0431)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8431 (8.0990)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4925 (0.4925)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3112 (2.3112 -- 2.3112)  data: 2.1013 (2.1013 -- 2.1013)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8009 (0.9788)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (94.9495)  time: 0.4132 (0.2030 -- 2.3112)  data: 0.1977 (0.0009 -- 2.1013)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7974 (0.9153)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.7672)  time: 0.2223 (0.1698 -- 0.4019)  data: 0.0141 (0.0001 -- 0.2017)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8165 (0.9847)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.0207)  time: 0.2068 (0.1332 -- 0.4019)  data: 0.0138 (0.0001 -- 0.2017)  max mem: 16735
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 74.481 Acc@5 95.021 loss 0.969
Accuracy of the network on the 482 val images: 74.48%
Max accuracy: 75.52%
Epoch: [58]  [  0/160]  eta: 0:22:10  lr: 0.000039  min_lr: 0.000001  loss: 1.8491 (1.8491)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7309 (7.7309)  time: 8.3180 (8.3180 -- 8.3180)  data: 7.0552 (7.0552 -- 7.0552)  max mem: 16735
Epoch: [58]  [ 20/160]  eta: 0:02:38  lr: 0.000039  min_lr: 0.000001  loss: 1.7650 (1.8687)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6012 (7.6049)  time: 0.7707 (0.5289 -- 3.2255)  data: 0.0016 (0.0005 -- 0.0036)  max mem: 16735
Epoch: [58]  [ 40/160]  eta: 0:02:05  lr: 0.000039  min_lr: 0.000001  loss: 2.0661 (1.9552)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8502 (7.7361)  time: 0.9624 (0.5253 -- 4.0925)  data: 0.3517 (0.0007 -- 2.4689)  max mem: 16735
Epoch: [58]  [ 60/160]  eta: 0:01:38  lr: 0.000039  min_lr: 0.000001  loss: 1.8798 (1.9522)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8988 (7.9475)  time: 0.8663 (0.5256 -- 4.8176)  data: 0.1767 (0.0002 -- 1.8594)  max mem: 16735
Epoch: [58]  [ 80/160]  eta: 0:01:17  lr: 0.000039  min_lr: 0.000001  loss: 2.0099 (1.9639)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4883 (8.1175)  time: 0.9198 (0.5332 -- 3.6655)  data: 0.0898 (0.0006 -- 0.8468)  max mem: 16735
Epoch: [58]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000001  loss: 2.2775 (2.0096)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7377 (8.1003)  time: 0.8571 (0.5173 -- 4.4645)  data: 0.0012 (0.0003 -- 0.0044)  max mem: 16735
[2023-09-01 00:37:10,451] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:37:10,451] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:37:10,453] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:37:10,453] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [58]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000001  loss: 2.1994 (2.0374)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3258 (8.0462)  time: 0.7769 (0.5283 -- 2.5376)  data: 0.0019 (0.0002 -- 0.0099)  max mem: 16735
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 2.1823 (2.0481)  loss_scale: 65536.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1529 (7.9718)  time: 0.7953 (0.5392 -- 3.2142)  data: 0.0030 (0.0002 -- 0.0107)  max mem: 16735
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.9797 (2.0427)  loss_scale: 65536.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2790 (7.9555)  time: 0.7022 (0.5017 -- 2.9985)  data: 0.0010 (0.0002 -- 0.0042)  max mem: 16735
Epoch: [58] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.9797 (2.0532)  loss_scale: 65536.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2790 (7.9555)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5143 (0.5143)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4345 (2.4345 -- 2.4345)  data: 2.1946 (2.1946 -- 2.1946)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7905 (0.9863)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4285 (0.2022 -- 2.4345)  data: 0.2071 (0.0004 -- 2.1946)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7956 (0.9349)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2166 (0.1716 -- 0.3172)  data: 0.0076 (0.0001 -- 0.0751)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9113 (0.9949)  acc1: 71.4286 (74.6888)  acc5: 100.0000 (96.2656)  time: 0.2003 (0.1332 -- 0.3172)  data: 0.0074 (0.0001 -- 0.0751)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 75.934 Acc@5 95.643 loss 0.980
Accuracy of the network on the 482 val images: 75.93%
[2023-09-01 00:37:50,938] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:37:50,940] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:37:50,940] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:37:50,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:37:52,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:37:52,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.93%
Epoch: [59]  [  0/160]  eta: 0:19:45  lr: 0.000039  min_lr: 0.000001  loss: 1.7373 (1.7373)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5667 (7.5667)  time: 7.4067 (7.4067 -- 7.4067)  data: 6.8661 (6.8661 -- 6.8661)  max mem: 16735
[2023-09-01 00:38:08,400] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9451
[2023-09-01 00:38:08,400] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9451
[2023-09-01 00:38:08,400] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:38:08,400] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:38:08,401] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [59]  [ 20/160]  eta: 0:02:44  lr: 0.000039  min_lr: 0.000001  loss: 1.9586 (1.9988)  loss_scale: 32768.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8032 (7.9714)  time: 0.8603 (0.5304 -- 3.0673)  data: 0.2932 (0.0004 -- 2.2009)  max mem: 16735
Epoch: [59]  [ 40/160]  eta: 0:02:07  lr: 0.000038  min_lr: 0.000001  loss: 2.1204 (2.0505)  loss_scale: 32768.0000 (41559.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5621 (7.8893)  time: 0.9447 (0.5454 -- 3.7958)  data: 0.3894 (0.0003 -- 3.2673)  max mem: 16735
Epoch: [59]  [ 60/160]  eta: 0:01:35  lr: 0.000038  min_lr: 0.000001  loss: 2.0691 (2.0549)  loss_scale: 32768.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3344 (7.7879)  time: 0.7291 (0.5398 -- 2.6637)  data: 0.1440 (0.0005 -- 2.1251)  max mem: 16735
Epoch: [59]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000001  loss: 1.8089 (2.0017)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7126 (7.8812)  time: 0.8991 (0.5321 -- 3.7255)  data: 0.2188 (0.0003 -- 1.6975)  max mem: 16735
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000001  loss: 2.0970 (2.0319)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0638 (7.9818)  time: 0.9682 (0.5217 -- 4.3622)  data: 0.3635 (0.0003 -- 3.8432)  max mem: 16735
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000001  loss: 1.9325 (2.0193)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0931 (8.1184)  time: 0.7856 (0.5229 -- 3.1653)  data: 0.2416 (0.0002 -- 2.6455)  max mem: 16735
[2023-09-01 00:40:00,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:40:00,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:40:00,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:40:00,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 1.9363 (2.0113)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4706 (8.0557)  time: 0.8581 (0.5346 -- 3.6571)  data: 0.3015 (0.0003 -- 3.1168)  max mem: 16735
[2023-09-01 00:40:08,959] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9591
[2023-09-01 00:40:08,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:40:08,959] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9591
[2023-09-01 00:40:08,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:40:08,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.9680 (2.0047)  loss_scale: 65536.0000 (37273.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6084 (8.0895)  time: 0.7135 (0.4961 -- 2.8218)  data: 0.1861 (0.0002 -- 2.2913)  max mem: 16735
Epoch: [59] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.9680 (1.9992)  loss_scale: 65536.0000 (37273.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6084 (8.0895)
[2023-09-01 00:40:14,416] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-01 00:40:14,418] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-01 00:40:14,419] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-01 00:40:14,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-01 00:40:15,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-01 00:40:15,427] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.5587 (0.5587)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3474 (2.3474 -- 2.3474)  data: 2.1413 (2.1413 -- 2.1413)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8041 (0.9582)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4163 (0.1947 -- 2.3474)  data: 0.2007 (0.0005 -- 2.1413)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7685 (0.8999)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (95.7672)  time: 0.2246 (0.1702 -- 0.4976)  data: 0.0188 (0.0001 -- 0.3057)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8051 (0.9724)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.2104 (0.1334 -- 0.4976)  data: 0.0185 (0.0001 -- 0.3057)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 75.726 Acc@5 95.021 loss 0.954
Accuracy of the network on the 482 val images: 75.73%
Max accuracy: 75.93%
Epoch: [60]  [  0/160]  eta: 0:23:03  lr: 0.000038  min_lr: 0.000001  loss: 2.1765 (2.1765)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0296 (10.0296)  time: 8.6491 (8.6491 -- 8.6491)  data: 8.1045 (8.1045 -- 8.1045)  max mem: 16735
Epoch: [60]  [ 20/160]  eta: 0:02:47  lr: 0.000038  min_lr: 0.000001  loss: 1.8921 (1.9763)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7840 (8.4516)  time: 0.8257 (0.5277 -- 2.3370)  data: 0.2727 (0.0002 -- 1.7934)  max mem: 16735
Epoch: [60]  [ 40/160]  eta: 0:02:03  lr: 0.000038  min_lr: 0.000001  loss: 2.1484 (2.0340)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8673 (8.4043)  time: 0.8538 (0.5216 -- 4.5620)  data: 0.3118 (0.0002 -- 4.0363)  max mem: 16735
Epoch: [60]  [ 60/160]  eta: 0:01:37  lr: 0.000038  min_lr: 0.000001  loss: 1.9619 (2.0362)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0955 (8.3363)  time: 0.8638 (0.5260 -- 3.1107)  data: 0.3194 (0.0004 -- 2.5525)  max mem: 16735
Epoch: [60]  [ 80/160]  eta: 0:01:14  lr: 0.000038  min_lr: 0.000001  loss: 2.0217 (2.0460)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2181 (8.1356)  time: 0.8134 (0.5400 -- 3.9922)  data: 0.2507 (0.0002 -- 3.4416)  max mem: 16735
Epoch: [60]  [100/160]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000001  loss: 2.0238 (2.0382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0494 (8.1262)  time: 0.8554 (0.5323 -- 4.2154)  data: 0.3005 (0.0005 -- 3.6846)  max mem: 16735
[2023-09-01 00:42:12,832] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:42:12,832] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:42:12,833] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:42:12,833] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000001  loss: 2.0740 (2.0390)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7090 (8.0738)  time: 0.8331 (0.5302 -- 3.3523)  data: 0.2718 (0.0003 -- 2.8109)  max mem: 16735
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 2.1185 (2.0425)  loss_scale: 65536.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5336 (8.0109)  time: 0.9292 (0.5250 -- 3.8810)  data: 0.2656 (0.0007 -- 3.3561)  max mem: 16735
[2023-09-01 00:42:38,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9747
[2023-09-01 00:42:38,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9747
[2023-09-01 00:42:38,051] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:42:38,051] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:42:38,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8689 (2.0348)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1058 (8.0032)  time: 0.7022 (0.5032 -- 2.6405)  data: 0.0330 (0.0002 -- 0.6440)  max mem: 16735
Epoch: [60] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8689 (2.0317)  loss_scale: 32768.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1058 (8.0032)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4855 (0.4855)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3763 (2.3763 -- 2.3763)  data: 2.1219 (2.1219 -- 2.1219)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7932 (0.9807)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4212 (0.2066 -- 2.3763)  data: 0.2028 (0.0008 -- 2.1219)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7705 (0.9078)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1697 -- 0.4406)  data: 0.0190 (0.0001 -- 0.2681)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8764 (0.9828)  acc1: 71.4286 (73.8589)  acc5: 100.0000 (94.6058)  time: 0.2091 (0.1323 -- 0.4406)  data: 0.0187 (0.0001 -- 0.2681)  max mem: 16735
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 75.311 Acc@5 95.228 loss 0.962
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 75.93%
Epoch: [61]  [  0/160]  eta: 0:20:03  lr: 0.000038  min_lr: 0.000001  loss: 2.3058 (2.3058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7200 (10.7200)  time: 7.5200 (7.5200 -- 7.5200)  data: 6.9449 (6.9449 -- 6.9449)  max mem: 16735
Epoch: [61]  [ 20/160]  eta: 0:02:47  lr: 0.000038  min_lr: 0.000001  loss: 2.1665 (2.0339)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7682 (7.9291)  time: 0.8835 (0.5285 -- 4.3364)  data: 0.1058 (0.0002 -- 2.0777)  max mem: 16735
Epoch: [61]  [ 40/160]  eta: 0:02:07  lr: 0.000038  min_lr: 0.000001  loss: 1.9835 (2.0063)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8554 (7.9553)  time: 0.9178 (0.5230 -- 3.1870)  data: 0.0030 (0.0003 -- 0.0379)  max mem: 16735
Epoch: [61]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000001  loss: 2.0007 (2.0031)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9233 (7.9156)  time: 0.8160 (0.5332 -- 2.9682)  data: 0.0290 (0.0007 -- 0.3068)  max mem: 16735
Epoch: [61]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000001  loss: 1.9284 (1.9812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7291 (7.8734)  time: 0.9715 (0.5192 -- 4.3474)  data: 0.0233 (0.0002 -- 0.4403)  max mem: 16735
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000001  loss: 2.1248 (1.9883)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5812 (7.9064)  time: 0.7793 (0.5232 -- 3.1673)  data: 0.0011 (0.0003 -- 0.0021)  max mem: 16735
[2023-09-01 00:44:40,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:44:40,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:44:40,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:44:40,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:44:44,907] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9880
[2023-09-01 00:44:44,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:44:44,907] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9880
[2023-09-01 00:44:44,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:44:44,907] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000001  loss: 1.8989 (1.9827)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8811 (7.7707)  time: 0.8604 (0.5191 -- 3.5084)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16735
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 2.0552 (1.9933)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2226 (7.8810)  time: 0.8495 (0.5264 -- 3.4632)  data: 0.0017 (0.0008 -- 0.0044)  max mem: 16735
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 2.0776 (2.0023)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3560 (7.8995)  time: 0.6698 (0.5016 -- 2.5629)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16735
Epoch: [61] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 2.0776 (2.0237)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3560 (7.8995)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4816 (0.4816)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3655 (2.3655 -- 2.3655)  data: 2.1169 (2.1169 -- 2.1169)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7652 (0.9869)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4238 (0.2098 -- 2.3655)  data: 0.2029 (0.0007 -- 2.1169)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7463 (0.9112)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2180 (0.1700 -- 0.3379)  data: 0.0095 (0.0001 -- 0.0961)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9157 (0.9856)  acc1: 71.4286 (74.2739)  acc5: 100.0000 (94.6058)  time: 0.2018 (0.1334 -- 0.3379)  data: 0.0092 (0.0001 -- 0.0961)  max mem: 16735
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 75.726 Acc@5 94.813 loss 0.968
Accuracy of the network on the 482 val images: 75.73%
Max accuracy: 75.93%
Epoch: [62]  [  0/160]  eta: 0:20:59  lr: 0.000038  min_lr: 0.000001  loss: 2.0575 (2.0575)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8466 (6.8466)  time: 7.8713 (7.8713 -- 7.8713)  data: 7.3462 (7.3462 -- 7.3462)  max mem: 16735
Epoch: [62]  [ 20/160]  eta: 0:02:37  lr: 0.000038  min_lr: 0.000001  loss: 1.9745 (2.0360)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2851 (7.4644)  time: 0.7841 (0.5203 -- 3.0815)  data: 0.2342 (0.0006 -- 2.5327)  max mem: 16735
Epoch: [62]  [ 40/160]  eta: 0:02:06  lr: 0.000038  min_lr: 0.000001  loss: 1.9198 (1.9899)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4268 (7.9106)  time: 0.9904 (0.5476 -- 3.6923)  data: 0.4316 (0.0004 -- 3.1540)  max mem: 16735
Epoch: [62]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000001  loss: 1.9911 (2.0392)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0816 (8.0971)  time: 0.8452 (0.5318 -- 3.5910)  data: 0.2975 (0.0003 -- 3.0517)  max mem: 16735
[2023-09-01 00:46:36,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=58, lr=[8.915777879211351e-07, 8.915777879211351e-07, 1.1887703838948467e-06, 1.1887703838948467e-06, 1.5850271785264623e-06, 1.5850271785264623e-06, 2.1133695713686163e-06, 2.1133695713686163e-06, 2.8178260951581553e-06, 2.8178260951581553e-06, 3.7571014602108737e-06, 3.7571014602108737e-06, 5.009468613614498e-06, 5.009468613614498e-06, 6.679291484819331e-06, 6.679291484819331e-06, 8.905721979759108e-06, 8.905721979759108e-06, 1.1874295973012144e-05, 1.1874295973012144e-05, 1.5832394630682858e-05, 1.5832394630682858e-05, 2.1109859507577145e-05, 2.1109859507577145e-05, 2.814647934343619e-05, 2.814647934343619e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 00:46:36,467] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.95624694612538, CurrSamplesPerSec=22.532375182876493, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [62]  [ 80/160]  eta: 0:01:17  lr: 0.000038  min_lr: 0.000001  loss: 1.9543 (2.0225)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8291 (8.0479)  time: 0.9119 (0.5274 -- 4.5656)  data: 0.2238 (0.0004 -- 1.7639)  max mem: 16735
[2023-09-01 00:46:47,755] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:46:47,755] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:46:47,755] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:46:47,755] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [62]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000001  loss: 2.0287 (2.0199)  loss_scale: 65536.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5585 (8.0071)  time: 0.8403 (0.5203 -- 4.5692)  data: 0.0015 (0.0001 -- 0.0031)  max mem: 16735
[2023-09-01 00:47:09,876] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10033
[2023-09-01 00:47:09,876] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10033
[2023-09-01 00:47:09,876] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:47:09,876] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:47:09,876] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000001  loss: 2.1501 (2.0378)  loss_scale: 65536.0000 (39267.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8033 (7.9465)  time: 0.8489 (0.5376 -- 3.8553)  data: 0.0013 (0.0004 -- 0.0032)  max mem: 16735
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 2.0429 (2.0310)  loss_scale: 32768.0000 (38345.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0104 (7.8828)  time: 0.7525 (0.5246 -- 3.7716)  data: 0.0021 (0.0006 -- 0.0116)  max mem: 16735
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.0680 (2.0261)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0379 (7.8123)  time: 0.6995 (0.5007 -- 2.2558)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16735
Epoch: [62] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.0680 (2.0032)  loss_scale: 32768.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0379 (7.8123)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4831 (0.4831)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4238 (2.4238 -- 2.4238)  data: 2.2007 (2.2007 -- 2.2007)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.8005 (1.0223)  acc1: 66.6667 (72.7273)  acc5: 100.0000 (93.9394)  time: 0.4358 (0.1905 -- 2.4238)  data: 0.2218 (0.0004 -- 2.2007)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.8000 (0.9440)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (95.2381)  time: 0.2196 (0.1683 -- 0.4573)  data: 0.0121 (0.0001 -- 0.2276)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9105 (1.0090)  acc1: 71.4286 (71.7842)  acc5: 100.0000 (94.6058)  time: 0.2055 (0.1326 -- 0.4573)  data: 0.0117 (0.0001 -- 0.2276)  max mem: 16735
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 72.614 Acc@5 95.021 loss 0.996
Accuracy of the network on the 482 val images: 72.61%
Max accuracy: 75.93%
Epoch: [63]  [  0/160]  eta: 0:19:46  lr: 0.000037  min_lr: 0.000001  loss: 2.2018 (2.2018)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6796 (7.6796)  time: 7.4135 (7.4135 -- 7.4135)  data: 5.8365 (5.8365 -- 5.8365)  max mem: 16735
Epoch: [63]  [ 20/160]  eta: 0:03:05  lr: 0.000037  min_lr: 0.000001  loss: 1.9556 (1.9168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0035 (7.3551)  time: 1.0218 (0.5247 -- 4.9292)  data: 0.0336 (0.0003 -- 0.5460)  max mem: 16735
Epoch: [63]  [ 40/160]  eta: 0:02:04  lr: 0.000037  min_lr: 0.000001  loss: 1.9778 (1.9854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5843 (7.6522)  time: 0.7414 (0.5266 -- 2.8623)  data: 0.0027 (0.0001 -- 0.0162)  max mem: 16735
Epoch: [63]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000001  loss: 2.0804 (1.9852)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4872 (7.9118)  time: 0.9108 (0.5193 -- 4.3986)  data: 0.0014 (0.0004 -- 0.0045)  max mem: 16735
Epoch: [63]  [ 80/160]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000001  loss: 1.9504 (1.9805)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5631 (7.7763)  time: 0.8065 (0.5351 -- 4.0515)  data: 0.0017 (0.0003 -- 0.0040)  max mem: 16735
[2023-09-01 00:49:11,299] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:49:11,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:49:11,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:49:11,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:49:14,768] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10167
[2023-09-01 00:49:14,769] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:49:14,770] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10167
[2023-09-01 00:49:14,770] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:49:14,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [63]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000001  loss: 2.1159 (2.0051)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2929 (7.8085)  time: 0.9462 (0.5291 -- 3.3688)  data: 0.0758 (0.0003 -- 0.7741)  max mem: 16735
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000001  loss: 2.0734 (2.0173)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5829 (7.6917)  time: 0.8789 (0.5019 -- 4.7438)  data: 0.0012 (0.0003 -- 0.0034)  max mem: 16735
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 2.0761 (2.0164)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4421 (7.8093)  time: 0.8259 (0.5319 -- 3.7567)  data: 0.0024 (0.0003 -- 0.0118)  max mem: 16735
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.9992 (2.0118)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6435 (7.8679)  time: 0.6736 (0.5016 -- 2.4744)  data: 0.0008 (0.0002 -- 0.0041)  max mem: 16735
Epoch: [63] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.9992 (2.0255)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6435 (7.8679)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4542 (0.4542)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4580 (2.4580 -- 2.4580)  data: 2.2222 (2.2222 -- 2.2222)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7567 (0.9776)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (93.9394)  time: 0.4291 (0.2096 -- 2.4580)  data: 0.2032 (0.0007 -- 2.2222)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7382 (0.9013)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (94.7090)  time: 0.2186 (0.1700 -- 0.2899)  data: 0.0062 (0.0001 -- 0.1082)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8746 (0.9742)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (93.7759)  time: 0.1998 (0.1330 -- 0.2899)  data: 0.0058 (0.0001 -- 0.1082)  max mem: 16735
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 74.481 Acc@5 94.398 loss 0.961
Accuracy of the network on the 482 val images: 74.48%
Max accuracy: 75.93%
Epoch: [64]  [  0/160]  eta: 0:18:23  lr: 0.000037  min_lr: 0.000001  loss: 1.7415 (1.7415)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7395 (11.7395)  time: 6.8951 (6.8951 -- 6.8951)  data: 6.3790 (6.3790 -- 6.3790)  max mem: 16735
Epoch: [64]  [ 20/160]  eta: 0:02:42  lr: 0.000037  min_lr: 0.000001  loss: 2.0861 (2.0887)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3155 (8.9294)  time: 0.8745 (0.5329 -- 3.5447)  data: 0.3163 (0.0006 -- 3.0147)  max mem: 16735
Epoch: [64]  [ 40/160]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000001  loss: 2.0391 (2.0565)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2360 (8.5047)  time: 0.8630 (0.5250 -- 2.7421)  data: 0.2403 (0.0004 -- 2.2119)  max mem: 16735
[2023-09-01 00:51:19,170] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:51:19,170] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:51:19,171] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:51:19,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:51:20,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10298
[2023-09-01 00:51:20,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:51:20,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10298
[2023-09-01 00:51:20,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:51:20,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [64]  [ 60/160]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000001  loss: 1.9393 (2.0185)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0037 (8.2938)  time: 0.8871 (0.5259 -- 4.0129)  data: 0.3212 (0.0005 -- 3.4588)  max mem: 16735
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000001  loss: 1.9592 (2.0070)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7164 (8.3124)  time: 0.8511 (0.5349 -- 3.2199)  data: 0.1397 (0.0004 -- 1.5565)  max mem: 16735
Epoch: [64]  [100/160]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000001  loss: 2.1440 (2.0292)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2865 (8.1901)  time: 0.8605 (0.5350 -- 3.0544)  data: 0.2401 (0.0004 -- 2.5280)  max mem: 16735
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000001  loss: 2.0862 (2.0476)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0648 (8.0694)  time: 0.8184 (0.5263 -- 3.0432)  data: 0.2505 (0.0002 -- 2.5107)  max mem: 16735
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 1.8946 (2.0232)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0427 (7.9963)  time: 0.8662 (0.5261 -- 3.6937)  data: 0.2690 (0.0007 -- 3.1596)  max mem: 16735
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.9560 (2.0188)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2901 (7.9427)  time: 0.7059 (0.5021 -- 1.9753)  data: 0.1045 (0.0002 -- 0.8008)  max mem: 16735
Epoch: [64] Total time: 0:02:20 (0.8807 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.9560 (2.0344)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2901 (7.9427)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4981 (0.4981)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3207 (2.3207 -- 2.3207)  data: 2.1008 (2.1008 -- 2.1008)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7744 (0.9667)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4187 (0.1883 -- 2.3207)  data: 0.2054 (0.0007 -- 2.1008)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7744 (0.9137)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2209 (0.1685 -- 0.3677)  data: 0.0133 (0.0001 -- 0.1461)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9253 (0.9711)  acc1: 71.4286 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2080 (0.1325 -- 0.3677)  data: 0.0131 (0.0001 -- 0.1461)  max mem: 16735
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 75.726 Acc@5 95.228 loss 0.957
Accuracy of the network on the 482 val images: 75.73%
Max accuracy: 75.93%
Epoch: [65]  [  0/160]  eta: 0:17:46  lr: 0.000037  min_lr: 0.000001  loss: 1.7846 (1.7846)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9869 (8.9869)  time: 6.6640 (6.6640 -- 6.6640)  data: 5.2329 (5.2329 -- 5.2329)  max mem: 16735
Epoch: [65]  [ 20/160]  eta: 0:02:40  lr: 0.000037  min_lr: 0.000001  loss: 1.9730 (1.9690)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4468 (7.6813)  time: 0.8670 (0.5313 -- 3.6489)  data: 0.0806 (0.0006 -- 0.7812)  max mem: 16735
[2023-09-01 00:53:22,889] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:53:22,889] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:53:22,892] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:53:22,893] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [65]  [ 40/160]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000001  loss: 1.7552 (1.8660)  loss_scale: 65536.0000 (43957.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0406 (7.5341)  time: 0.8837 (0.5160 -- 3.4810)  data: 0.2158 (0.0003 -- 2.9387)  max mem: 16735
Epoch: [65]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000001  loss: 2.0257 (1.8914)  loss_scale: 65536.0000 (51032.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2886 (7.6741)  time: 0.9428 (0.5238 -- 2.8325)  data: 0.3338 (0.0003 -- 2.3069)  max mem: 16735
Epoch: [65]  [ 80/160]  eta: 0:01:14  lr: 0.000037  min_lr: 0.000001  loss: 2.0020 (1.9176)  loss_scale: 65536.0000 (54613.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4037 (7.6512)  time: 0.7274 (0.5267 -- 2.5145)  data: 0.1778 (0.0001 -- 1.9929)  max mem: 16735
[2023-09-01 00:54:19,244] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10493
[2023-09-01 00:54:19,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:54:19,244] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10493
[2023-09-01 00:54:19,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 00:54:19,245] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000001  loss: 1.9342 (1.9310)  loss_scale: 65536.0000 (54180.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0176 (7.8423)  time: 0.9887 (0.5271 -- 4.5419)  data: 0.2708 (0.0005 -- 4.0160)  max mem: 16735
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000001  loss: 2.0588 (1.9467)  loss_scale: 32768.0000 (50641.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3559 (7.8825)  time: 0.7699 (0.5206 -- 3.3670)  data: 0.2204 (0.0003 -- 2.8263)  max mem: 16735
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.9461 (1.9457)  loss_scale: 32768.0000 (48106.2128)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9497 (8.0105)  time: 0.9802 (0.5306 -- 4.5959)  data: 0.4336 (0.0003 -- 4.0821)  max mem: 16735
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 2.1320 (1.9591)  loss_scale: 32768.0000 (46284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5491 (8.0507)  time: 0.7125 (0.5013 -- 2.6067)  data: 0.1871 (0.0002 -- 2.0866)  max mem: 16735
Epoch: [65] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 2.1320 (1.9888)  loss_scale: 32768.0000 (46284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5491 (8.0507)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4504 (0.4504)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3825 (2.3825 -- 2.3825)  data: 2.1123 (2.1123 -- 2.1123)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.7311 (0.9479)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4092 (0.1988 -- 2.3825)  data: 0.1960 (0.0005 -- 2.1123)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7310 (0.8804)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.7672)  time: 0.2187 (0.1692 -- 0.4813)  data: 0.0170 (0.0001 -- 0.2944)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8864 (0.9458)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2056 (0.1327 -- 0.4813)  data: 0.0167 (0.0001 -- 0.2944)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 76.349 Acc@5 95.643 loss 0.934
Accuracy of the network on the 482 val images: 76.35%
[2023-09-01 00:55:21,973] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 00:55:21,974] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 00:55:21,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 00:55:21,974] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 00:55:23,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 00:55:23,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.35%
Epoch: [66]  [  0/160]  eta: 0:15:35  lr: 0.000036  min_lr: 0.000001  loss: 1.4344 (1.4344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6838 (10.6838)  time: 5.8476 (5.8476 -- 5.8476)  data: 4.8571 (4.8571 -- 4.8571)  max mem: 16735
Epoch: [66]  [ 20/160]  eta: 0:02:44  lr: 0.000036  min_lr: 0.000001  loss: 2.0079 (1.9767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6299 (7.9916)  time: 0.9414 (0.5349 -- 2.7893)  data: 0.3262 (0.0006 -- 2.2598)  max mem: 16735
Epoch: [66]  [ 40/160]  eta: 0:01:59  lr: 0.000036  min_lr: 0.000001  loss: 1.9330 (1.9570)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2741 (7.6929)  time: 0.8090 (0.5218 -- 2.8710)  data: 0.2202 (0.0002 -- 2.3486)  max mem: 16735
Epoch: [66]  [ 60/160]  eta: 0:01:39  lr: 0.000036  min_lr: 0.000001  loss: 1.8832 (1.9444)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7785 (7.9000)  time: 0.9884 (0.5295 -- 3.7660)  data: 0.3912 (0.0003 -- 3.2529)  max mem: 16735
[2023-09-01 00:56:25,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:56:25,122] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:56:25,122] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:56:25,122] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [66]  [ 80/160]  eta: 0:01:15  lr: 0.000036  min_lr: 0.000001  loss: 2.1877 (1.9854)  loss_scale: 65536.0000 (40454.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1849 (7.7489)  time: 0.8143 (0.5235 -- 2.4855)  data: 0.2624 (0.0002 -- 1.9377)  max mem: 16735
Epoch: [66]  [100/160]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000001  loss: 1.7897 (1.9716)  loss_scale: 65536.0000 (45420.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1794 (7.6997)  time: 0.8674 (0.5329 -- 3.5128)  data: 0.3159 (0.0006 -- 2.9933)  max mem: 16735
[2023-09-01 00:56:58,195] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10661
[2023-09-01 00:56:58,196] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:56:58,196] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 00:56:58,198] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10661
[2023-09-01 00:56:58,198] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 2.0747 (1.9872)  loss_scale: 32768.0000 (43329.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0339 (7.8130)  time: 0.8990 (0.5290 -- 4.3775)  data: 0.3508 (0.0003 -- 3.8439)  max mem: 16735
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.9713 (1.9820)  loss_scale: 32768.0000 (41831.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5138 (7.9381)  time: 0.9510 (0.5326 -- 3.9531)  data: 0.3988 (0.0004 -- 3.4101)  max mem: 16735
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9317 (1.9753)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3384 (8.0235)  time: 0.5766 (0.5004 -- 1.5941)  data: 0.0530 (0.0002 -- 1.0483)  max mem: 16735
Epoch: [66] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9317 (2.0074)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3384 (8.0235)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5017 (0.5017)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4461 (2.4461 -- 2.4461)  data: 2.2329 (2.2329 -- 2.2329)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7550 (0.9640)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (93.9394)  time: 0.4142 (0.1916 -- 2.4461)  data: 0.2051 (0.0002 -- 2.2329)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7723 (0.9061)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (93.6508)  time: 0.2189 (0.1695 -- 0.4666)  data: 0.0177 (0.0001 -- 0.2804)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9360 (0.9621)  acc1: 71.4286 (74.2739)  acc5: 100.0000 (93.7759)  time: 0.2058 (0.1404 -- 0.4666)  data: 0.0168 (0.0001 -- 0.2804)  max mem: 16735
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 75.311 Acc@5 94.606 loss 0.953
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 76.35%
Epoch: [67]  [  0/160]  eta: 0:18:24  lr: 0.000036  min_lr: 0.000001  loss: 1.9445 (1.9445)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8367 (4.8367)  time: 6.9019 (6.9019 -- 6.9019)  data: 5.5980 (5.5980 -- 5.5980)  max mem: 16735
Epoch: [67]  [ 20/160]  eta: 0:02:40  lr: 0.000036  min_lr: 0.000001  loss: 2.0637 (2.0644)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1692 (8.1831)  time: 0.8593 (0.5412 -- 3.1978)  data: 0.3028 (0.0004 -- 2.6523)  max mem: 16735
Epoch: [67]  [ 40/160]  eta: 0:02:06  lr: 0.000036  min_lr: 0.000001  loss: 1.9496 (2.0098)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2173 (7.8254)  time: 0.9519 (0.5225 -- 3.7942)  data: 0.3941 (0.0003 -- 3.2126)  max mem: 16735
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000001  loss: 2.0245 (1.9929)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6729 (7.8405)  time: 0.7870 (0.5330 -- 3.1429)  data: 0.1769 (0.0004 -- 2.6040)  max mem: 16735
[2023-09-01 00:59:01,111] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:59:01,111] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:59:01,113] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 00:59:01,114] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 00:59:05,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10792
[2023-09-01 00:59:05,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 00:59:05,930] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 00:59:05,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10792
[2023-09-01 00:59:05,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [67]  [ 80/160]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000001  loss: 2.1152 (2.0001)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2764 (7.7518)  time: 0.9970 (0.5293 -- 4.2579)  data: 0.4488 (0.0003 -- 3.7547)  max mem: 16735
Epoch: [67]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000001  loss: 2.1347 (2.0078)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9168 (7.8792)  time: 0.8311 (0.5263 -- 3.9621)  data: 0.2874 (0.0003 -- 3.4451)  max mem: 16735
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.8826 (1.9914)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6254 (7.8944)  time: 0.9398 (0.5336 -- 3.5210)  data: 0.3919 (0.0002 -- 2.9595)  max mem: 16735
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.9970 (1.9880)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5712 (7.8664)  time: 0.7661 (0.5251 -- 3.2458)  data: 0.2180 (0.0003 -- 2.7217)  max mem: 16735
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 2.1154 (1.9934)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1253 (7.9135)  time: 0.6592 (0.5010 -- 2.2846)  data: 0.1313 (0.0002 -- 1.7501)  max mem: 16735
Epoch: [67] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 2.1154 (2.0019)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1253 (7.9135)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4703 (0.4703)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4546 (2.4546 -- 2.4546)  data: 2.2307 (2.2307 -- 2.2307)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7858 (0.9603)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (93.9394)  time: 0.4190 (0.2012 -- 2.4546)  data: 0.2037 (0.0005 -- 2.2307)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7406 (0.8834)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (94.7090)  time: 0.2122 (0.1686 -- 0.2994)  data: 0.0062 (0.0001 -- 0.1109)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8679 (0.9469)  acc1: 77.7778 (72.6141)  acc5: 100.0000 (94.1909)  time: 0.1961 (0.1333 -- 0.2994)  data: 0.0059 (0.0001 -- 0.1109)  max mem: 16735
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 75.934 Acc@5 94.606 loss 0.925
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 76.35%
Epoch: [68]  [  0/160]  eta: 0:19:39  lr: 0.000036  min_lr: 0.000001  loss: 2.0669 (2.0669)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5747 (8.5747)  time: 7.3702 (7.3702 -- 7.3702)  data: 6.8302 (6.8302 -- 6.8302)  max mem: 16735
Epoch: [68]  [ 20/160]  eta: 0:02:44  lr: 0.000036  min_lr: 0.000001  loss: 1.8228 (1.8321)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6913 (7.8636)  time: 0.8680 (0.5272 -- 3.8230)  data: 0.2530 (0.0003 -- 2.1527)  max mem: 16735
Epoch: [68]  [ 40/160]  eta: 0:02:05  lr: 0.000036  min_lr: 0.000001  loss: 2.0554 (1.8861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9191 (7.8492)  time: 0.9124 (0.5282 -- 2.5641)  data: 0.1038 (0.0004 -- 1.2418)  max mem: 16735
[2023-09-01 01:01:07,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:01:07,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:01:07,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:01:07,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:01:14,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10929
[2023-09-01 01:01:14,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:01:14,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:01:14,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10929
[2023-09-01 01:01:14,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [68]  [ 60/160]  eta: 0:01:37  lr: 0.000036  min_lr: 0.000001  loss: 1.9160 (1.8953)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9402 (7.9011)  time: 0.8334 (0.5319 -- 2.3361)  data: 0.1198 (0.0005 -- 1.5194)  max mem: 16735
Epoch: [68]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000001  loss: 2.1208 (1.9557)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7201 (7.9147)  time: 0.8923 (0.5295 -- 3.5793)  data: 0.2841 (0.0003 -- 3.0334)  max mem: 16735
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000001  loss: 2.0614 (1.9741)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0859 (7.9003)  time: 0.8905 (0.5174 -- 3.6935)  data: 0.3451 (0.0001 -- 3.1606)  max mem: 16735
[2023-09-01 01:02:15,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=65, lr=[8.451868793498704e-07, 8.451868793498704e-07, 1.1269158391331605e-06, 1.1269158391331605e-06, 1.5025544521775474e-06, 1.5025544521775474e-06, 2.0034059362367297e-06, 2.0034059362367297e-06, 2.6712079149823063e-06, 2.6712079149823063e-06, 3.561610553309742e-06, 3.561610553309742e-06, 4.748814071079656e-06, 4.748814071079656e-06, 6.3317520947728745e-06, 6.3317520947728745e-06, 8.442336126363833e-06, 8.442336126363833e-06, 1.125644816848511e-05, 1.125644816848511e-05, 1.5008597557980148e-05, 1.5008597557980148e-05, 2.0011463410640196e-05, 2.0011463410640196e-05, 2.668195121418693e-05, 2.668195121418693e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 01:02:15,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=18.017579279965805, CurrSamplesPerSec=19.74886760648266, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [68]  [120/160]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000001  loss: 2.0599 (1.9882)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6789 (7.9394)  time: 0.8310 (0.5291 -- 2.6820)  data: 0.1860 (0.0005 -- 2.1469)  max mem: 16735
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 2.2603 (2.0208)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7806 (7.9505)  time: 0.9685 (0.5303 -- 4.0521)  data: 0.4134 (0.0004 -- 3.5260)  max mem: 16735
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 2.0361 (2.0208)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3787 (7.9456)  time: 0.6216 (0.5005 -- 2.5716)  data: 0.1035 (0.0002 -- 2.0590)  max mem: 16735
Epoch: [68] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 2.0361 (2.0088)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3787 (7.9456)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4746 (0.4746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4686 (2.4686 -- 2.4686)  data: 2.2193 (2.2193 -- 2.2193)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7387 (0.9760)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4300 (0.2071 -- 2.4686)  data: 0.2030 (0.0008 -- 2.2193)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7387 (0.9030)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.2381)  time: 0.2147 (0.1695 -- 0.2544)  data: 0.0025 (0.0001 -- 0.0323)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9242 (0.9649)  acc1: 71.4286 (75.1037)  acc5: 100.0000 (94.6058)  time: 0.1965 (0.1332 -- 0.2463)  data: 0.0022 (0.0001 -- 0.0323)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 75.311 Acc@5 95.436 loss 0.955
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 76.35%
Epoch: [69]  [  0/160]  eta: 0:18:02  lr: 0.000035  min_lr: 0.000001  loss: 2.4886 (2.4886)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3497 (5.3497)  time: 6.7632 (6.7632 -- 6.7632)  data: 5.1282 (5.1282 -- 5.1282)  max mem: 16735
[2023-09-01 01:03:19,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:03:19,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:03:19,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:03:19,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [69]  [ 20/160]  eta: 0:02:54  lr: 0.000035  min_lr: 0.000001  loss: 2.0592 (2.0641)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7209 (7.7643)  time: 0.9714 (0.5322 -- 4.8216)  data: 0.4175 (0.0003 -- 4.2806)  max mem: 16735
Epoch: [69]  [ 40/160]  eta: 0:02:05  lr: 0.000035  min_lr: 0.000001  loss: 1.9863 (2.0217)  loss_scale: 65536.0000 (51150.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2915 (7.6509)  time: 0.8387 (0.5301 -- 4.8696)  data: 0.2835 (0.0006 -- 4.3050)  max mem: 16735
[2023-09-01 01:03:45,313] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11089
[2023-09-01 01:03:45,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:03:45,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:03:45,314] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11089
[2023-09-01 01:03:45,315] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [69]  [ 60/160]  eta: 0:01:38  lr: 0.000035  min_lr: 0.000001  loss: 2.1948 (2.0721)  loss_scale: 32768.0000 (49420.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1246 (7.6120)  time: 0.8617 (0.5486 -- 3.7995)  data: 0.2760 (0.0009 -- 3.2662)  max mem: 16735
Epoch: [69]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000001  loss: 2.0337 (2.0466)  loss_scale: 32768.0000 (45308.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0735 (7.7139)  time: 0.8385 (0.5335 -- 2.3756)  data: 0.1466 (0.0005 -- 1.6322)  max mem: 16735
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000001  loss: 2.1069 (2.0468)  loss_scale: 32768.0000 (42825.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0633 (7.7961)  time: 0.9144 (0.5324 -- 3.4806)  data: 0.1560 (0.0006 -- 1.6646)  max mem: 16735
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000001  loss: 2.0960 (2.0479)  loss_scale: 32768.0000 (41163.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8905 (7.8230)  time: 0.8738 (0.5234 -- 3.4957)  data: 0.0010 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 2.0331 (2.0404)  loss_scale: 32768.0000 (39972.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9146 (7.8250)  time: 0.8654 (0.5302 -- 3.9393)  data: 0.0018 (0.0005 -- 0.0054)  max mem: 16735
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8797 (2.0244)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7015 (7.8280)  time: 0.6864 (0.5016 -- 2.7884)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16735
Epoch: [69] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8797 (2.0086)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7015 (7.8280)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.4858 (0.4858)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0794 (2.0794 -- 2.0794)  data: 1.8829 (1.8829 -- 1.8829)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.7281 (0.9488)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4072 (0.2006 -- 2.0794)  data: 0.1884 (0.0006 -- 1.8829)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7285 (0.8861)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (95.7672)  time: 0.2388 (0.1682 -- 0.6293)  data: 0.0317 (0.0001 -- 0.4407)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8738 (0.9479)  acc1: 71.4286 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2213 (0.1331 -- 0.6293)  data: 0.0314 (0.0001 -- 0.4407)  max mem: 16735
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 76.971 Acc@5 95.436 loss 0.940
Accuracy of the network on the 482 val images: 76.97%
[2023-09-01 01:05:25,683] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 01:05:25,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 01:05:25,685] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 01:05:25,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 01:05:27,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 01:05:27,190] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.97%
Epoch: [70]  [  0/160]  eta: 0:18:47  lr: 0.000035  min_lr: 0.000001  loss: 2.1482 (2.1482)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1835 (5.1835)  time: 7.0489 (7.0489 -- 7.0489)  data: 6.4917 (6.4917 -- 6.4917)  max mem: 16735
[2023-09-01 01:05:51,817] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:05:51,817] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:05:51,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:05:51,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [70]  [ 20/160]  eta: 0:02:51  lr: 0.000035  min_lr: 0.000001  loss: 2.0254 (2.0047)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3635 (7.8541)  time: 0.9350 (0.5252 -- 3.8209)  data: 0.3599 (0.0008 -- 3.2942)  max mem: 16735
[2023-09-01 01:06:05,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11235
[2023-09-01 01:06:05,697] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:06:05,697] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:06:05,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11235
[2023-09-01 01:06:05,698] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [70]  [ 40/160]  eta: 0:02:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8864 (1.9497)  loss_scale: 65536.0000 (46354.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1581 (8.1902)  time: 0.7772 (0.5289 -- 3.3056)  data: 0.2251 (0.0002 -- 2.7695)  max mem: 16735
Epoch: [70]  [ 60/160]  eta: 0:01:37  lr: 0.000035  min_lr: 0.000001  loss: 2.0419 (1.9490)  loss_scale: 32768.0000 (41900.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3855 (8.3292)  time: 0.9212 (0.5372 -- 3.3628)  data: 0.3595 (0.0004 -- 2.8276)  max mem: 16735
Epoch: [70]  [ 80/160]  eta: 0:01:17  lr: 0.000035  min_lr: 0.000001  loss: 2.1084 (1.9930)  loss_scale: 32768.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7936 (8.3737)  time: 0.9372 (0.5229 -- 4.1762)  data: 0.0014 (0.0002 -- 0.0029)  max mem: 16735
Epoch: [70]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000001  loss: 2.1079 (2.0154)  loss_scale: 32768.0000 (38283.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5824 (8.2175)  time: 0.7388 (0.5215 -- 2.7593)  data: 0.0153 (0.0004 -- 0.2801)  max mem: 16735
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000001  loss: 2.0519 (2.0258)  loss_scale: 32768.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1744 (8.2469)  time: 0.8800 (0.5380 -- 3.3547)  data: 0.2113 (0.0005 -- 2.8288)  max mem: 16735
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 2.0686 (2.0316)  loss_scale: 32768.0000 (36718.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9284 (8.2194)  time: 0.8299 (0.5252 -- 2.4892)  data: 0.1310 (0.0002 -- 1.4545)  max mem: 16735
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 2.1384 (2.0382)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4706 (8.1523)  time: 0.6780 (0.5009 -- 1.9536)  data: 0.0569 (0.0002 -- 0.6062)  max mem: 16735
Epoch: [70] Total time: 0:02:20 (0.8779 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 2.1384 (2.0106)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4706 (8.1523)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4460 (0.4460)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3338 (2.3338 -- 2.3338)  data: 2.1159 (2.1159 -- 2.1159)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7522 (0.9806)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4326 (0.2016 -- 2.3338)  data: 0.2149 (0.0006 -- 2.1159)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7522 (0.9087)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.2381)  time: 0.2217 (0.1696 -- 0.4692)  data: 0.0157 (0.0001 -- 0.2376)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9206 (0.9686)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.0207)  time: 0.2064 (0.1323 -- 0.4692)  data: 0.0154 (0.0001 -- 0.2376)  max mem: 16735
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 74.689 Acc@5 95.643 loss 0.957
Accuracy of the network on the 482 val images: 74.69%
Max accuracy: 76.97%
Epoch: [71]  [  0/160]  eta: 0:17:32  lr: 0.000035  min_lr: 0.000001  loss: 1.6492 (1.6492)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1736 (7.1736)  time: 6.5790 (6.5790 -- 6.5790)  data: 6.0031 (6.0031 -- 6.0031)  max mem: 16735
[2023-09-01 01:08:04,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:08:04,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:08:04,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:08:04,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:08:19,783] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11380
[2023-09-01 01:08:19,783] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:08:19,783] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11380
[2023-09-01 01:08:19,784] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:08:19,784] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [71]  [ 20/160]  eta: 0:02:42  lr: 0.000035  min_lr: 0.000001  loss: 1.9499 (1.9115)  loss_scale: 65536.0000 (57734.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2703 (7.7757)  time: 0.8864 (0.5193 -- 3.0076)  data: 0.3392 (0.0002 -- 2.4686)  max mem: 16735
Epoch: [71]  [ 40/160]  eta: 0:02:03  lr: 0.000035  min_lr: 0.000001  loss: 1.8325 (1.8983)  loss_scale: 32768.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0623 (7.5738)  time: 0.8936 (0.5134 -- 4.1392)  data: 0.0883 (0.0003 -- 1.6775)  max mem: 16735
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000035  min_lr: 0.000001  loss: 2.1966 (1.9570)  loss_scale: 32768.0000 (41362.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1138 (7.8710)  time: 0.9446 (0.5081 -- 5.1047)  data: 0.2082 (0.0004 -- 2.4420)  max mem: 16735
Epoch: [71]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000001  loss: 1.7245 (1.9148)  loss_scale: 32768.0000 (39240.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1943 (7.7172)  time: 0.8276 (0.5344 -- 3.9190)  data: 0.2618 (0.0003 -- 3.3689)  max mem: 16735
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000001  loss: 2.0605 (1.9341)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7182 (7.7274)  time: 0.7888 (0.5303 -- 3.7196)  data: 0.2397 (0.0004 -- 3.1674)  max mem: 16735
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000001  loss: 2.0544 (1.9600)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6864 (7.7389)  time: 1.0146 (0.5378 -- 3.9611)  data: 0.4625 (0.0003 -- 3.4331)  max mem: 16735
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 2.1991 (1.9930)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1620 (7.7127)  time: 0.7736 (0.5178 -- 4.0453)  data: 0.2253 (0.0003 -- 3.5250)  max mem: 16735
[2023-09-01 01:10:12,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:10:12,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:10:12,132] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:10:12,132] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 2.1419 (2.0039)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6332 (7.7735)  time: 0.6685 (0.5007 -- 3.0949)  data: 0.1429 (0.0002 -- 2.5576)  max mem: 16735
Epoch: [71] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 2.1419 (1.9853)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6332 (7.7735)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.4140 (0.4140)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2019 (2.2019 -- 2.2019)  data: 1.9782 (1.9782 -- 1.9782)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7424 (0.9515)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4136 (0.2066 -- 2.2019)  data: 0.1885 (0.0008 -- 1.9782)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7424 (0.8823)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.7672)  time: 0.2256 (0.1718 -- 0.3253)  data: 0.0104 (0.0001 -- 0.0719)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8447 (0.9461)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (94.1909)  time: 0.2081 (0.1331 -- 0.3253)  data: 0.0093 (0.0001 -- 0.0719)  max mem: 16735
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 75.519 Acc@5 94.813 loss 0.928
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 76.97%
Epoch: [72]  [  0/160]  eta: 0:18:00  lr: 0.000035  min_lr: 0.000001  loss: 1.4976 (1.4976)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8755 (7.8755)  time: 6.7551 (6.7551 -- 6.7551)  data: 5.6118 (5.6118 -- 5.6118)  max mem: 16735
[2023-09-01 01:10:34,787] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11525
[2023-09-01 01:10:34,787] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:10:34,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11525
[2023-09-01 01:10:34,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:10:34,792] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [72]  [ 20/160]  eta: 0:02:38  lr: 0.000034  min_lr: 0.000001  loss: 1.8534 (1.9104)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4045 (8.0992)  time: 0.8475 (0.5376 -- 2.9809)  data: 0.2946 (0.0006 -- 2.4558)  max mem: 16735
Epoch: [72]  [ 40/160]  eta: 0:01:55  lr: 0.000034  min_lr: 0.000001  loss: 2.0308 (1.9543)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2238 (8.3641)  time: 0.7897 (0.5339 -- 2.2090)  data: 0.1763 (0.0003 -- 1.6745)  max mem: 16735
Epoch: [72]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000001  loss: 1.8287 (1.9214)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8881 (8.5331)  time: 0.9915 (0.5256 -- 2.6332)  data: 0.1957 (0.0011 -- 2.1046)  max mem: 16735
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000034  min_lr: 0.000001  loss: 2.0397 (1.9383)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1197 (8.5512)  time: 0.9050 (0.5277 -- 3.4689)  data: 0.1323 (0.0001 -- 1.7844)  max mem: 16735
Epoch: [72]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000001  loss: 1.8825 (1.9412)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0648 (8.4161)  time: 0.9245 (0.5223 -- 4.4426)  data: 0.3799 (0.0003 -- 3.8920)  max mem: 16735
Epoch: [72]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000001  loss: 2.0618 (1.9393)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6321 (8.4222)  time: 0.8809 (0.5118 -- 4.4636)  data: 0.3396 (0.0003 -- 3.9348)  max mem: 16735
[2023-09-01 01:12:29,335] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:12:29,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:12:29,336] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:12:29,336] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000001  loss: 2.1160 (1.9506)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2708 (8.4442)  time: 0.8666 (0.5382 -- 4.0367)  data: 0.3043 (0.0004 -- 3.4971)  max mem: 16735
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.9569 (1.9516)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8990 (8.3819)  time: 0.6218 (0.5006 -- 1.9245)  data: 0.0931 (0.0002 -- 1.4241)  max mem: 16735
Epoch: [72] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.9569 (1.9753)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8990 (8.3819)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.4781 (0.4781)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5980 (2.5980 -- 2.5980)  data: 2.3673 (2.3673 -- 2.3673)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7422 (0.9719)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (94.9495)  time: 0.4325 (0.1970 -- 2.5980)  data: 0.2162 (0.0005 -- 2.3673)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7422 (0.8948)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (94.7090)  time: 0.2146 (0.1696 -- 0.3587)  data: 0.0090 (0.0001 -- 0.1670)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8708 (0.9635)  acc1: 77.7778 (72.6141)  acc5: 100.0000 (93.7759)  time: 0.1981 (0.1329 -- 0.3587)  data: 0.0088 (0.0001 -- 0.1670)  max mem: 16735
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 74.274 Acc@5 94.606 loss 0.959
Accuracy of the network on the 482 val images: 74.27%
Max accuracy: 76.97%
Epoch: [73]  [  0/160]  eta: 0:20:06  lr: 0.000034  min_lr: 0.000001  loss: 2.3471 (2.3471)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7865 (7.7865)  time: 7.5399 (7.5399 -- 7.5399)  data: 5.6483 (5.6483 -- 5.6483)  max mem: 16735
Epoch: [73]  [ 20/160]  eta: 0:02:52  lr: 0.000034  min_lr: 0.000001  loss: 1.9241 (1.9861)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2684 (7.3903)  time: 0.9193 (0.5248 -- 2.8879)  data: 0.0016 (0.0001 -- 0.0043)  max mem: 16735
[2023-09-01 01:13:23,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11703
[2023-09-01 01:13:23,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11703
[2023-09-01 01:13:23,516] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:13:23,516] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:13:23,516] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [73]  [ 40/160]  eta: 0:02:09  lr: 0.000034  min_lr: 0.000001  loss: 1.9238 (1.9386)  loss_scale: 32768.0000 (51150.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1296 (7.9987)  time: 0.9099 (0.5224 -- 2.8379)  data: 0.0013 (0.0005 -- 0.0027)  max mem: 16735
Epoch: [73]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000001  loss: 2.0447 (1.9575)  loss_scale: 32768.0000 (45123.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3273 (8.1665)  time: 0.8744 (0.5165 -- 4.7851)  data: 0.0012 (0.0001 -- 0.0025)  max mem: 16735
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000001  loss: 1.8794 (1.9494)  loss_scale: 32768.0000 (42072.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4620 (8.1125)  time: 0.8607 (0.5248 -- 3.7014)  data: 0.0012 (0.0003 -- 0.0035)  max mem: 16735
Epoch: [73]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000001  loss: 1.9386 (1.9504)  loss_scale: 32768.0000 (40230.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0263 (8.0718)  time: 0.7489 (0.5297 -- 2.5978)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16735
Epoch: [73]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000001  loss: 1.8871 (1.9446)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0851 (8.1553)  time: 0.8795 (0.5320 -- 2.9720)  data: 0.0462 (0.0004 -- 0.6864)  max mem: 16735
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000001  loss: 1.9884 (1.9552)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5307 (8.2175)  time: 0.9474 (0.5271 -- 4.2555)  data: 0.1009 (0.0003 -- 1.6814)  max mem: 16735
[2023-09-01 01:15:14,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:15:14,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:15:14,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:15:14,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.8406 (1.9602)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3301 (8.1952)  time: 0.6293 (0.5020 -- 2.4998)  data: 0.0008 (0.0002 -- 0.0033)  max mem: 16735
Epoch: [73] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.8406 (1.9841)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3301 (8.1952)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4437 (0.4437)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5839 (2.5839 -- 2.5839)  data: 2.3612 (2.3612 -- 2.3612)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7170 (0.9368)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4400 (0.2027 -- 2.5839)  data: 0.2161 (0.0006 -- 2.3612)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7772 (0.8812)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2146 (0.1742 -- 0.2708)  data: 0.0010 (0.0001 -- 0.0055)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.9133 (0.9478)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (94.6058)  time: 0.1995 (0.1332 -- 0.2708)  data: 0.0004 (0.0001 -- 0.0017)  max mem: 16735
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 74.896 Acc@5 95.021 loss 0.936
Accuracy of the network on the 482 val images: 74.90%
Max accuracy: 76.97%
Epoch: [74]  [  0/160]  eta: 0:20:02  lr: 0.000034  min_lr: 0.000001  loss: 2.0461 (2.0461)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8015 (8.8015)  time: 7.5136 (7.5136 -- 7.5136)  data: 6.9569 (6.9569 -- 6.9569)  max mem: 16735
Epoch: [74]  [ 20/160]  eta: 0:02:51  lr: 0.000034  min_lr: 0.000001  loss: 1.9763 (1.9550)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1164 (7.7768)  time: 0.9105 (0.5289 -- 3.7674)  data: 0.3552 (0.0005 -- 3.2385)  max mem: 16735
Epoch: [74]  [ 40/160]  eta: 0:02:04  lr: 0.000034  min_lr: 0.000001  loss: 1.9519 (1.9542)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8676 (8.1535)  time: 0.8383 (0.5231 -- 4.0685)  data: 0.2885 (0.0003 -- 3.5272)  max mem: 16735
[2023-09-01 01:16:19,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11892
[2023-09-01 01:16:19,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11892
[2023-09-01 01:16:19,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:16:19,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:16:19,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [74]  [ 60/160]  eta: 0:01:40  lr: 0.000034  min_lr: 0.000001  loss: 2.0756 (1.9919)  loss_scale: 65536.0000 (60701.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5334 (8.0143)  time: 0.9338 (0.5321 -- 4.4257)  data: 0.3871 (0.0003 -- 3.8930)  max mem: 16735
Epoch: [74]  [ 80/160]  eta: 0:01:14  lr: 0.000034  min_lr: 0.000001  loss: 1.9717 (2.0010)  loss_scale: 32768.0000 (53804.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6454 (8.0260)  time: 0.7259 (0.5382 -- 2.3108)  data: 0.1710 (0.0005 -- 1.7913)  max mem: 16735
Epoch: [74]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000001  loss: 1.9903 (2.0062)  loss_scale: 32768.0000 (49638.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6950 (8.0038)  time: 0.8786 (0.5271 -- 3.7772)  data: 0.3314 (0.0004 -- 3.2475)  max mem: 16735
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000001  loss: 2.0114 (2.0079)  loss_scale: 32768.0000 (46850.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3218 (7.9149)  time: 0.8756 (0.5296 -- 4.7282)  data: 0.3251 (0.0005 -- 4.2132)  max mem: 16735
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000001  loss: 2.0455 (2.0179)  loss_scale: 32768.0000 (44852.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0213 (7.9771)  time: 0.8480 (0.5353 -- 3.3632)  data: 0.2580 (0.0007 -- 2.8246)  max mem: 16735
[2023-09-01 01:17:45,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=71, lr=[7.958802674087248e-07, 7.958802674087248e-07, 1.0611736898782997e-06, 1.0611736898782997e-06, 1.4148982531710663e-06, 1.4148982531710663e-06, 1.8865310042280885e-06, 1.8865310042280885e-06, 2.515374672304118e-06, 2.515374672304118e-06, 3.3538328964054906e-06, 3.3538328964054906e-06, 4.471777195207321e-06, 4.471777195207321e-06, 5.962369593609761e-06, 5.962369593609761e-06, 7.949826124813014e-06, 7.949826124813014e-06, 1.0599768166417353e-05, 1.0599768166417353e-05, 1.4133024221889804e-05, 1.4133024221889804e-05, 1.8844032295853073e-05, 1.8844032295853073e-05, 2.512537639447076e-05, 2.512537639447076e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 01:17:45,825] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=18.06097456300341, CurrSamplesPerSec=24.317425500428307, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7873 (2.0016)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9430 (8.0480)  time: 0.6191 (0.5049 -- 1.5341)  data: 0.0233 (0.0003 -- 0.4457)  max mem: 16735
Epoch: [74] Total time: 0:02:19 (0.8726 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.7873 (2.0082)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9430 (8.0480)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4115 (0.4115)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2996 (2.2996 -- 2.2996)  data: 2.0880 (2.0880 -- 2.0880)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7288 (0.9005)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4237 (0.2055 -- 2.2996)  data: 0.2049 (0.0008 -- 2.0880)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7288 (0.8480)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2209 (0.1698 -- 0.4087)  data: 0.0089 (0.0001 -- 0.1523)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8463 (0.9099)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.8506)  time: 0.2047 (0.1365 -- 0.4087)  data: 0.0085 (0.0001 -- 0.1523)  max mem: 16735
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 77.801 Acc@5 96.058 loss 0.897
Accuracy of the network on the 482 val images: 77.80%
[2023-09-01 01:17:53,571] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 01:17:53,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 01:17:53,573] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 01:17:53,573] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 01:17:54,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 01:17:54,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.80%
Epoch: [75]  [  0/160]  eta: 0:22:01  lr: 0.000033  min_lr: 0.000001  loss: 2.1069 (2.1069)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3103 (7.3103)  time: 8.2593 (8.2593 -- 8.2593)  data: 5.1046 (5.1046 -- 5.1046)  max mem: 16735
Epoch: [75]  [ 20/160]  eta: 0:03:01  lr: 0.000033  min_lr: 0.000001  loss: 2.0073 (1.9890)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6809 (8.5181)  time: 0.9457 (0.5245 -- 4.8776)  data: 0.1055 (0.0003 -- 2.0840)  max mem: 16735
[2023-09-01 01:18:22,670] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:18:22,670] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:18:22,671] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:18:22,671] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [75]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000001  loss: 2.1330 (2.0242)  loss_scale: 65536.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2664 (8.2912)  time: 0.8189 (0.5278 -- 4.3889)  data: 0.1702 (0.0003 -- 2.5371)  max mem: 16735
[2023-09-01 01:18:54,304] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12057
[2023-09-01 01:18:54,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:18:54,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:18:54,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12057
[2023-09-01 01:18:54,305] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [75]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000001  loss: 2.0290 (2.0047)  loss_scale: 65536.0000 (52106.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7090 (8.2592)  time: 0.8741 (0.5367 -- 2.2859)  data: 0.1434 (0.0005 -- 1.0491)  max mem: 16735
Epoch: [75]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000001  loss: 1.9437 (1.9878)  loss_scale: 32768.0000 (47331.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4396 (8.2059)  time: 0.9098 (0.5227 -- 2.8037)  data: 0.0793 (0.0003 -- 0.9203)  max mem: 16735
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000001  loss: 1.8126 (1.9562)  loss_scale: 32768.0000 (44447.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3152 (8.2133)  time: 0.8842 (0.5285 -- 3.6837)  data: 0.0711 (0.0004 -- 1.3961)  max mem: 16735
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000001  loss: 1.9978 (1.9479)  loss_scale: 32768.0000 (42517.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4538 (8.2549)  time: 0.8112 (0.5336 -- 3.0551)  data: 0.0016 (0.0003 -- 0.0043)  max mem: 16735
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 1.9000 (1.9467)  loss_scale: 32768.0000 (41134.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4312 (8.4162)  time: 0.8534 (0.5320 -- 4.4563)  data: 0.0026 (0.0002 -- 0.0154)  max mem: 16735
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.9133 (1.9403)  loss_scale: 32768.0000 (40140.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4370 (8.4017)  time: 0.6788 (0.5011 -- 2.8927)  data: 0.0008 (0.0002 -- 0.0037)  max mem: 16735
Epoch: [75] Total time: 0:02:23 (0.8954 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.9133 (1.9529)  loss_scale: 32768.0000 (40140.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4370 (8.4017)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3937 (0.3937)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5561 (2.5561 -- 2.5561)  data: 2.3270 (2.3270 -- 2.3270)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7289 (0.9195)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (94.9495)  time: 0.4441 (0.2101 -- 2.5561)  data: 0.2228 (0.0007 -- 2.3270)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7134 (0.8461)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2168 (0.1703 -- 0.3416)  data: 0.0096 (0.0001 -- 0.1113)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8040 (0.9131)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.4357)  time: 0.1998 (0.1329 -- 0.3416)  data: 0.0092 (0.0001 -- 0.1113)  max mem: 16735
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 75.519 Acc@5 95.851 loss 0.901
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 77.80%
Epoch: [76]  [  0/160]  eta: 0:16:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1740 (2.1740)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5698 (13.5698)  time: 6.1135 (6.1135 -- 6.1135)  data: 4.7819 (4.7819 -- 4.7819)  max mem: 16735
Epoch: [76]  [ 20/160]  eta: 0:02:54  lr: 0.000033  min_lr: 0.000001  loss: 2.0220 (2.0041)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0881 (8.8061)  time: 1.0067 (0.5357 -- 4.3461)  data: 0.4536 (0.0005 -- 3.8196)  max mem: 16735
[2023-09-01 01:20:58,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:20:58,769] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:20:58,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:20:58,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [76]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000001  loss: 1.7110 (1.8464)  loss_scale: 65536.0000 (44756.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4319 (8.6124)  time: 0.8242 (0.5258 -- 3.6098)  data: 0.2289 (0.0002 -- 3.0540)  max mem: 16735
Epoch: [76]  [ 60/160]  eta: 0:01:39  lr: 0.000033  min_lr: 0.000001  loss: 1.8976 (1.8896)  loss_scale: 65536.0000 (51569.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0561 (8.6509)  time: 0.9047 (0.5232 -- 3.2805)  data: 0.3569 (0.0004 -- 2.7382)  max mem: 16735
Epoch: [76]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000001  loss: 2.0450 (1.9312)  loss_scale: 65536.0000 (55017.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7190 (8.4744)  time: 0.7245 (0.5365 -- 2.2402)  data: 0.1727 (0.0004 -- 1.7107)  max mem: 16735
[2023-09-01 01:21:46,247] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12244
[2023-09-01 01:21:46,248] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:21:46,248] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:21:46,249] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12244
[2023-09-01 01:21:46,250] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [76]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.0410 (1.9616)  loss_scale: 32768.0000 (51585.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4193 (8.3361)  time: 0.9507 (0.5256 -- 3.4312)  data: 0.3962 (0.0009 -- 2.8892)  max mem: 16735
Epoch: [76]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 1.9934 (1.9740)  loss_scale: 32768.0000 (48474.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3379 (8.3368)  time: 0.7815 (0.5350 -- 1.5791)  data: 0.0596 (0.0004 -- 0.7573)  max mem: 16735
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.0863 (1.9811)  loss_scale: 32768.0000 (46247.0355)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7717 (8.2563)  time: 0.8483 (0.5367 -- 3.6542)  data: 0.2559 (0.0009 -- 3.1215)  max mem: 16735
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.8898 (1.9685)  loss_scale: 32768.0000 (44646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2814 (8.3068)  time: 0.7486 (0.5031 -- 3.8228)  data: 0.2187 (0.0001 -- 3.2041)  max mem: 16735
Epoch: [76] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.8898 (1.9832)  loss_scale: 32768.0000 (44646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2814 (8.3068)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4041 (0.4041)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4397 (2.4397 -- 2.4397)  data: 2.1824 (2.1824 -- 2.1824)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7241 (0.9142)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4353 (0.1985 -- 2.4397)  data: 0.2186 (0.0004 -- 2.1824)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7241 (0.8406)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2210 (0.1682 -- 0.4337)  data: 0.0177 (0.0001 -- 0.2128)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8319 (0.9037)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2056 (0.1321 -- 0.4337)  data: 0.0174 (0.0001 -- 0.2128)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 76.141 Acc@5 95.436 loss 0.888
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.80%
Epoch: [77]  [  0/160]  eta: 0:21:12  lr: 0.000033  min_lr: 0.000001  loss: 2.1910 (2.1910)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3199 (9.3199)  time: 7.9500 (7.9500 -- 7.9500)  data: 6.4251 (6.4251 -- 6.4251)  max mem: 16735
Epoch: [77]  [ 20/160]  eta: 0:02:50  lr: 0.000033  min_lr: 0.000001  loss: 2.1202 (2.0340)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4402 (7.8838)  time: 0.8789 (0.5184 -- 3.8980)  data: 0.2713 (0.0002 -- 3.2578)  max mem: 16735
Epoch: [77]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000001  loss: 1.8823 (1.9763)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8121 (7.7700)  time: 0.9004 (0.5279 -- 4.2325)  data: 0.2837 (0.0004 -- 2.4913)  max mem: 16735
[2023-09-01 01:23:48,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:23:48,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:23:48,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:23:48,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [77]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000001  loss: 1.8001 (1.9211)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4969 (7.6996)  time: 0.8242 (0.5299 -- 3.4753)  data: 0.0717 (0.0004 -- 0.9182)  max mem: 16735
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 1.7560 (1.9161)  loss_scale: 65536.0000 (44095.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8339 (7.8694)  time: 0.8668 (0.5317 -- 5.4487)  data: 0.0661 (0.0003 -- 1.0241)  max mem: 16735
[2023-09-01 01:24:23,926] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12415
[2023-09-01 01:24:23,927] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:24:23,926] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12415
[2023-09-01 01:24:23,927] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:24:23,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [77]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000001  loss: 1.8712 (1.9174)  loss_scale: 65536.0000 (46394.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8597 (7.9637)  time: 0.8204 (0.5282 -- 3.5077)  data: 0.0020 (0.0004 -- 0.0062)  max mem: 16735
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 1.7733 (1.9011)  loss_scale: 32768.0000 (44142.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8924 (8.0210)  time: 0.7645 (0.5479 -- 1.9806)  data: 0.0327 (0.0004 -- 0.6222)  max mem: 16735
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000033  min_lr: 0.000001  loss: 2.0075 (1.9119)  loss_scale: 32768.0000 (42528.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8030 (8.2084)  time: 0.8326 (0.5444 -- 4.5272)  data: 0.0029 (0.0003 -- 0.0153)  max mem: 16735
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.1359 (1.9318)  loss_scale: 32768.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8030 (8.3224)  time: 0.7856 (0.5011 -- 2.5353)  data: 0.0010 (0.0001 -- 0.0039)  max mem: 16735
Epoch: [77] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.1359 (1.9657)  loss_scale: 32768.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8030 (8.3224)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4144 (0.4144)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4362 (2.4362 -- 2.4362)  data: 2.1896 (2.1896 -- 2.1896)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7262 (0.9273)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4392 (0.2003 -- 2.4362)  data: 0.2211 (0.0006 -- 2.1896)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7482 (0.8641)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.8254)  time: 0.2182 (0.1723 -- 0.4584)  data: 0.0137 (0.0001 -- 0.2352)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8564 (0.9274)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (96.2656)  time: 0.2029 (0.1332 -- 0.4584)  data: 0.0134 (0.0001 -- 0.2352)  max mem: 16735
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 73.859 Acc@5 96.058 loss 0.912
Accuracy of the network on the 482 val images: 73.86%
Max accuracy: 77.80%
Epoch: [78]  [  0/160]  eta: 0:19:12  lr: 0.000032  min_lr: 0.000001  loss: 1.7428 (1.7428)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2240 (5.2240)  time: 7.2046 (7.2046 -- 7.2046)  data: 6.6456 (6.6456 -- 6.6456)  max mem: 16735
Epoch: [78]  [ 20/160]  eta: 0:02:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9455 (1.8843)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0632 (8.1055)  time: 0.8194 (0.5296 -- 2.3254)  data: 0.1920 (0.0006 -- 1.5791)  max mem: 16735
Epoch: [78]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000001  loss: 2.0929 (1.9817)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4476 (8.4143)  time: 0.8898 (0.5376 -- 2.3428)  data: 0.2694 (0.0001 -- 1.3820)  max mem: 16735
Epoch: [78]  [ 60/160]  eta: 0:01:35  lr: 0.000032  min_lr: 0.000001  loss: 1.9889 (1.9812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2939 (8.5839)  time: 0.8381 (0.5370 -- 2.4684)  data: 0.2269 (0.0006 -- 1.9088)  max mem: 16735
[2023-09-01 01:26:24,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:26:24,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:26:24,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:26:24,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:26:37,904] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12557
[2023-09-01 01:26:37,904] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12557
[2023-09-01 01:26:37,904] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:26:37,904] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:26:37,905] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [78]  [ 80/160]  eta: 0:01:14  lr: 0.000032  min_lr: 0.000001  loss: 1.9979 (1.9900)  loss_scale: 65536.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7370 (8.3486)  time: 0.8667 (0.5230 -- 3.7805)  data: 0.3218 (0.0005 -- 3.2453)  max mem: 16735
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.9074 (1.9931)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4332 (8.3409)  time: 0.9109 (0.5281 -- 3.9576)  data: 0.3633 (0.0004 -- 3.4314)  max mem: 16735
Epoch: [78]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0612 (2.0005)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9998 (8.3231)  time: 0.9018 (0.5292 -- 4.4034)  data: 0.3512 (0.0002 -- 3.8773)  max mem: 16735
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.9415 (1.9904)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9087 (8.2825)  time: 0.8867 (0.5106 -- 5.0028)  data: 0.3518 (0.0002 -- 4.4811)  max mem: 16735
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0561 (1.9853)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5506 (8.3265)  time: 0.7080 (0.5016 -- 3.7101)  data: 0.1856 (0.0002 -- 3.1715)  max mem: 16735
Epoch: [78] Total time: 0:02:23 (0.8945 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0561 (1.9933)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5506 (8.3265)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4028 (0.4028)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4393 (2.4393 -- 2.4393)  data: 2.2240 (2.2240 -- 2.2240)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7176 (0.8958)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4164 (0.1913 -- 2.4393)  data: 0.2037 (0.0005 -- 2.2240)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7119 (0.8360)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.8254)  time: 0.2178 (0.1683 -- 0.4131)  data: 0.0139 (0.0001 -- 0.2166)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8325 (0.9028)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (96.2656)  time: 0.2024 (0.1325 -- 0.4131)  data: 0.0134 (0.0001 -- 0.2166)  max mem: 16735
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 76.763 Acc@5 96.058 loss 0.891
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 77.80%
Epoch: [79]  [  0/160]  eta: 0:25:48  lr: 0.000032  min_lr: 0.000001  loss: 2.2790 (2.2790)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2835 (6.2835)  time: 9.6772 (9.6772 -- 9.6772)  data: 9.1000 (9.1000 -- 9.1000)  max mem: 16735
Epoch: [79]  [ 20/160]  eta: 0:02:43  lr: 0.000032  min_lr: 0.000001  loss: 2.0585 (2.0178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1130 (8.5500)  time: 0.7459 (0.5270 -- 3.0478)  data: 0.1997 (0.0003 -- 2.4988)  max mem: 16735
Epoch: [79]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000001  loss: 2.0458 (2.0604)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5884 (8.2715)  time: 0.8438 (0.5345 -- 2.7143)  data: 0.1922 (0.0004 -- 1.9433)  max mem: 16735
[2023-09-01 01:28:40,485] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:28:40,485] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:28:40,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:28:40,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [79]  [ 60/160]  eta: 0:01:35  lr: 0.000032  min_lr: 0.000001  loss: 2.1607 (2.0658)  loss_scale: 65536.0000 (40825.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0733 (8.1838)  time: 0.8517 (0.5221 -- 2.9068)  data: 0.0544 (0.0002 -- 1.0439)  max mem: 16735
Epoch: [79]  [ 80/160]  eta: 0:01:14  lr: 0.000032  min_lr: 0.000001  loss: 2.0665 (2.0449)  loss_scale: 65536.0000 (46927.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6690 (7.9097)  time: 0.8579 (0.5275 -- 2.8215)  data: 0.0726 (0.0002 -- 1.3340)  max mem: 16735
[2023-09-01 01:29:28,814] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12738
[2023-09-01 01:29:28,814] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12738
[2023-09-01 01:29:28,814] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:29:28,814] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:29:28,814] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [79]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000001  loss: 1.9899 (2.0440)  loss_scale: 65536.0000 (49638.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3066 (7.8707)  time: 0.9671 (0.5211 -- 3.1969)  data: 0.0167 (0.0003 -- 0.3152)  max mem: 16735
Epoch: [79]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.8283 (2.0062)  loss_scale: 32768.0000 (46850.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4778 (8.0622)  time: 0.7723 (0.5307 -- 3.4951)  data: 0.0014 (0.0002 -- 0.0048)  max mem: 16735
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.9947 (2.0003)  loss_scale: 32768.0000 (44852.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9363 (8.0739)  time: 0.8848 (0.5264 -- 3.9848)  data: 0.0092 (0.0002 -- 0.1619)  max mem: 16735
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.6165 (1.9744)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1669 (7.9684)  time: 0.6771 (0.5017 -- 3.3466)  data: 0.0006 (0.0001 -- 0.0025)  max mem: 16735
Epoch: [79] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.6165 (2.0044)  loss_scale: 32768.0000 (43417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1669 (7.9684)
[2023-09-01 01:30:16,119] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-01 01:30:16,162] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-01 01:30:16,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-01 01:30:16,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-01 01:30:17,246] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-01 01:30:17,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3912 (0.3912)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6003 (2.6003 -- 2.6003)  data: 2.3455 (2.3455 -- 2.3455)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7000 (0.9024)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4318 (0.1968 -- 2.6003)  data: 0.2142 (0.0006 -- 2.3455)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7000 (0.8441)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2130 (0.1687 -- 0.3333)  data: 0.0075 (0.0001 -- 0.1357)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8215 (0.9070)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (96.2656)  time: 0.1970 (0.1358 -- 0.3333)  data: 0.0072 (0.0001 -- 0.1357)  max mem: 16735
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 77.178 Acc@5 96.266 loss 0.897
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.80%
Epoch: [80]  [  0/160]  eta: 0:20:59  lr: 0.000032  min_lr: 0.000001  loss: 2.1316 (2.1316)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9153 (11.9153)  time: 7.8690 (7.8690 -- 7.8690)  data: 5.8202 (5.8202 -- 5.8202)  max mem: 16735
Epoch: [80]  [ 20/160]  eta: 0:02:41  lr: 0.000032  min_lr: 0.000001  loss: 2.1077 (2.0126)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3497 (8.4894)  time: 0.8158 (0.5190 -- 3.4208)  data: 0.0211 (0.0003 -- 0.3921)  max mem: 16735
Epoch: [80]  [ 40/160]  eta: 0:01:58  lr: 0.000032  min_lr: 0.000001  loss: 1.8920 (1.9927)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3058 (8.6042)  time: 0.8223 (0.5352 -- 1.9946)  data: 0.0497 (0.0001 -- 0.7783)  max mem: 16735
Epoch: [80]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0110 (1.9900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2212 (8.4632)  time: 0.9107 (0.5298 -- 2.6454)  data: 0.0021 (0.0002 -- 0.0157)  max mem: 16735
[2023-09-01 01:31:29,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:31:29,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:31:30,036] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:31:30,036] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:31:36,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12874
[2023-09-01 01:31:36,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12874
[2023-09-01 01:31:36,193] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:31:36,193] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:31:36,193] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [80]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000001  loss: 1.9684 (1.9911)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3067 (8.1816)  time: 0.9430 (0.5306 -- 3.8598)  data: 0.2580 (0.0001 -- 3.3052)  max mem: 16735
Epoch: [80]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000001  loss: 2.1679 (2.0012)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3348 (8.2210)  time: 0.8429 (0.5173 -- 2.9961)  data: 0.2906 (0.0002 -- 2.4528)  max mem: 16735
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 2.0579 (2.0171)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5428 (8.1220)  time: 0.7418 (0.5352 -- 3.1423)  data: 0.1828 (0.0001 -- 2.6157)  max mem: 16735
Epoch: [80]  [140/160]  eta: 0:00:17  lr: 0.000031  min_lr: 0.000001  loss: 1.6789 (1.9910)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5132 (8.1303)  time: 0.8659 (0.5339 -- 2.6790)  data: 0.2231 (0.0005 -- 2.1458)  max mem: 16735
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 2.0996 (2.0006)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6213 (8.1337)  time: 0.7246 (0.5013 -- 2.1227)  data: 0.1214 (0.0002 -- 1.6155)  max mem: 16735
Epoch: [80] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 2.0996 (1.9862)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6213 (8.1337)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3951 (0.3951)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3053 (2.3053 -- 2.3053)  data: 2.0722 (2.0722 -- 2.0722)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6816 (0.8903)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4020 (0.1998 -- 2.3053)  data: 0.1896 (0.0005 -- 2.0722)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6816 (0.8301)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2254 (0.1696 -- 0.4777)  data: 0.0208 (0.0001 -- 0.2385)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7933 (0.8986)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.2109 (0.1331 -- 0.4777)  data: 0.0205 (0.0001 -- 0.2385)  max mem: 16735
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 77.386 Acc@5 95.436 loss 0.895
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 77.80%
Epoch: [81]  [  0/160]  eta: 0:21:45  lr: 0.000031  min_lr: 0.000001  loss: 2.3387 (2.3387)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6963 (10.6963)  time: 8.1597 (8.1597 -- 8.1597)  data: 7.5918 (7.5918 -- 7.5918)  max mem: 16735
Epoch: [81]  [ 20/160]  eta: 0:02:41  lr: 0.000031  min_lr: 0.000001  loss: 2.1047 (2.0510)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7038 (8.6999)  time: 0.8010 (0.5454 -- 2.0175)  data: 0.2153 (0.0007 -- 1.3397)  max mem: 16735
[2023-09-01 01:33:34,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=77, lr=[7.441574443416659e-07, 7.441574443416659e-07, 9.922099257888878e-07, 9.922099257888878e-07, 1.3229465677185172e-06, 1.3229465677185172e-06, 1.763928756958023e-06, 1.763928756958023e-06, 2.3519050092773636e-06, 2.3519050092773636e-06, 3.135873345703152e-06, 3.135873345703152e-06, 4.181164460937536e-06, 4.181164460937536e-06, 5.574885947916714e-06, 5.574885947916714e-06, 7.433181263888952e-06, 7.433181263888952e-06, 9.910908351851937e-06, 9.910908351851937e-06, 1.3214544469135915e-05, 1.3214544469135915e-05, 1.7619392625514554e-05, 1.7619392625514554e-05, 2.349252350068607e-05, 2.349252350068607e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 01:33:34,190] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=18.03249483908867, CurrSamplesPerSec=22.502838135093086, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [81]  [ 40/160]  eta: 0:02:00  lr: 0.000031  min_lr: 0.000001  loss: 1.7305 (1.9162)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7537 (8.6366)  time: 0.8452 (0.5213 -- 2.8984)  data: 0.1098 (0.0003 -- 1.0493)  max mem: 16735
[2023-09-01 01:33:38,127] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:33:38,127] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:33:38,127] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:33:38,127] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [81]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000001  loss: 2.1200 (1.9745)  loss_scale: 65536.0000 (42437.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0044 (8.4156)  time: 0.9708 (0.5270 -- 5.1061)  data: 0.3611 (0.0004 -- 4.5524)  max mem: 16735
Epoch: [81]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000001  loss: 1.9832 (1.9648)  loss_scale: 65536.0000 (48140.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5490 (8.1826)  time: 0.8320 (0.5297 -- 3.3997)  data: 0.2836 (0.0003 -- 2.8723)  max mem: 16735
[2023-09-01 01:34:16,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13047
[2023-09-01 01:34:16,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13047
[2023-09-01 01:34:16,682] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:34:16,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:34:16,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [81]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000001  loss: 2.0170 (1.9693)  loss_scale: 32768.0000 (47043.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0635 (8.1714)  time: 0.8472 (0.5366 -- 2.6020)  data: 0.2059 (0.0002 -- 2.0728)  max mem: 16735
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.8700 (1.9564)  loss_scale: 32768.0000 (44683.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7277 (8.2538)  time: 0.7838 (0.5327 -- 3.0966)  data: 0.1019 (0.0005 -- 1.7285)  max mem: 16735
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 2.0369 (1.9638)  loss_scale: 32768.0000 (42993.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3960 (8.2119)  time: 0.9826 (0.5298 -- 4.0337)  data: 0.3884 (0.0004 -- 2.8224)  max mem: 16735
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9195 (1.9530)  loss_scale: 32768.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3260 (8.3756)  time: 0.7017 (0.5020 -- 2.8800)  data: 0.0476 (0.0002 -- 0.9425)  max mem: 16735
Epoch: [81] Total time: 0:02:22 (0.8934 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9195 (1.9483)  loss_scale: 32768.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3260 (8.3756)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3827 (0.3827)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3558 (2.3558 -- 2.3558)  data: 2.1251 (2.1251 -- 2.1251)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6598 (0.8879)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4127 (0.1976 -- 2.3558)  data: 0.1952 (0.0005 -- 2.1251)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6598 (0.8199)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2189 (0.1705 -- 0.4439)  data: 0.0144 (0.0002 -- 0.2617)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7496 (0.8988)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2034 (0.1330 -- 0.4439)  data: 0.0141 (0.0001 -- 0.2617)  max mem: 16735
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 76.141 Acc@5 95.228 loss 0.888
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.80%
Epoch: [82]  [  0/160]  eta: 0:20:41  lr: 0.000031  min_lr: 0.000001  loss: 2.2097 (2.2097)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.2128 (16.2128)  time: 7.7578 (7.7578 -- 7.7578)  data: 5.2580 (5.2580 -- 5.2580)  max mem: 16735
Epoch: [82]  [ 20/160]  eta: 0:02:45  lr: 0.000031  min_lr: 0.000001  loss: 2.0251 (2.0240)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1158 (8.7127)  time: 0.8526 (0.5286 -- 2.5492)  data: 0.2511 (0.0001 -- 1.9974)  max mem: 16735
Epoch: [82]  [ 40/160]  eta: 0:02:07  lr: 0.000031  min_lr: 0.000001  loss: 2.0079 (1.9827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5656 (8.1743)  time: 0.9456 (0.5275 -- 3.0494)  data: 0.3922 (0.0002 -- 2.4972)  max mem: 16735
[2023-09-01 01:36:20,938] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:36:20,939] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:36:20,940] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:36:20,940] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [82]  [ 60/160]  eta: 0:01:38  lr: 0.000031  min_lr: 0.000001  loss: 1.9482 (1.9786)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9956 (8.3050)  time: 0.8102 (0.5327 -- 2.5197)  data: 0.2578 (0.0002 -- 1.9919)  max mem: 16735
[2023-09-01 01:36:31,011] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13187
[2023-09-01 01:36:31,012] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:36:31,012] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13187
[2023-09-01 01:36:31,012] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:36:31,012] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000001  loss: 2.0437 (1.9951)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0806 (8.4459)  time: 0.7986 (0.5329 -- 3.2315)  data: 0.2380 (0.0005 -- 2.6971)  max mem: 16735
Epoch: [82]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000001  loss: 2.1184 (2.0141)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1815 (8.2455)  time: 0.8923 (0.5389 -- 2.4673)  data: 0.2507 (0.0007 -- 1.9298)  max mem: 16735
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.9640 (2.0005)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7167 (8.2520)  time: 0.8267 (0.5359 -- 3.3316)  data: 0.2791 (0.0003 -- 2.8157)  max mem: 16735
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.8033 (1.9853)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1592 (8.1512)  time: 0.9269 (0.5327 -- 2.8298)  data: 0.3267 (0.0004 -- 2.2910)  max mem: 16735
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9462 (1.9747)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7126 (8.1406)  time: 0.6782 (0.5025 -- 1.6867)  data: 0.0585 (0.0002 -- 1.1563)  max mem: 16735
Epoch: [82] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9462 (1.9740)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7126 (8.1406)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3977 (0.3977)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4462 (2.4462 -- 2.4462)  data: 2.2102 (2.2102 -- 2.2102)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6497 (0.8651)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4171 (0.2021 -- 2.4462)  data: 0.2019 (0.0007 -- 2.2102)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6924 (0.8152)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2180 (0.1711 -- 0.3331)  data: 0.0139 (0.0001 -- 0.1520)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7518 (0.8885)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2005 (0.1324 -- 0.3331)  data: 0.0137 (0.0001 -- 0.1520)  max mem: 16735
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.880
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 77.80%
Epoch: [83]  [  0/160]  eta: 0:22:21  lr: 0.000031  min_lr: 0.000001  loss: 2.1776 (2.1776)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6010 (4.6010)  time: 8.3843 (8.3843 -- 8.3843)  data: 7.8465 (7.8465 -- 7.8465)  max mem: 16735
Epoch: [83]  [ 20/160]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000001  loss: 1.8514 (1.9398)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1925 (8.3039)  time: 0.8134 (0.5283 -- 3.2171)  data: 0.2594 (0.0005 -- 2.6682)  max mem: 16735
[2023-09-01 01:38:31,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:38:31,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:38:31,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:38:31,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [83]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000001  loss: 1.8716 (1.8832)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3095 (8.5620)  time: 0.8170 (0.5244 -- 3.0329)  data: 0.1187 (0.0003 -- 1.1718)  max mem: 16735
Epoch: [83]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000001  loss: 2.1629 (1.9459)  loss_scale: 65536.0000 (46197.5082)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3558 (8.3824)  time: 0.9802 (0.5310 -- 3.8955)  data: 0.0046 (0.0005 -- 0.0641)  max mem: 16735
[2023-09-01 01:38:54,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13341
[2023-09-01 01:38:54,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13341
[2023-09-01 01:38:54,236] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:38:54,236] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:38:54,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [83]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000001  loss: 2.0011 (1.9524)  loss_scale: 32768.0000 (42881.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4185 (8.1709)  time: 0.8326 (0.5379 -- 4.1554)  data: 0.0173 (0.0005 -- 0.3037)  max mem: 16735
Epoch: [83]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000001  loss: 2.1932 (1.9943)  loss_scale: 32768.0000 (40878.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3245 (7.9977)  time: 0.8725 (0.5203 -- 2.4410)  data: 0.1366 (0.0005 -- 1.8992)  max mem: 16735
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.7851 (1.9696)  loss_scale: 32768.0000 (39538.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2007 (8.0638)  time: 0.8413 (0.5317 -- 5.0910)  data: 0.2951 (0.0005 -- 4.5685)  max mem: 16735
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.6832 (1.9453)  loss_scale: 32768.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1429 (8.1314)  time: 0.9677 (0.5334 -- 4.2084)  data: 0.3844 (0.0005 -- 3.6648)  max mem: 16735
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 2.1319 (1.9591)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2493 (8.1328)  time: 0.6331 (0.5009 -- 2.0753)  data: 0.0781 (0.0002 -- 1.5525)  max mem: 16735
Epoch: [83] Total time: 0:02:23 (0.8940 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 2.1319 (1.9844)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2493 (8.1328)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4228 (0.4228)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3219 (2.3219 -- 2.3219)  data: 2.0972 (2.0972 -- 2.0972)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6327 (0.8619)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4170 (0.1989 -- 2.3219)  data: 0.2044 (0.0008 -- 2.0972)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6977 (0.8214)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2176 (0.1693 -- 0.3692)  data: 0.0125 (0.0001 -- 0.1337)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7744 (0.8910)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2036 (0.1329 -- 0.3692)  data: 0.0118 (0.0001 -- 0.1337)  max mem: 16735
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 76.971 Acc@5 96.058 loss 0.889
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 77.80%
Epoch: [84]  [  0/160]  eta: 0:22:29  lr: 0.000030  min_lr: 0.000001  loss: 2.2881 (2.2881)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1398 (6.1398)  time: 8.4329 (8.4329 -- 8.4329)  data: 7.8890 (7.8890 -- 7.8890)  max mem: 16735
Epoch: [84]  [ 20/160]  eta: 0:02:49  lr: 0.000030  min_lr: 0.000001  loss: 1.8475 (1.8538)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0232 (8.2535)  time: 0.8520 (0.5330 -- 4.0031)  data: 0.2992 (0.0003 -- 3.4677)  max mem: 16735
[2023-09-01 01:40:56,417] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:40:56,417] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:40:56,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:40:56,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:40:58,551] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13472
[2023-09-01 01:40:58,551] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:40:58,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:40:58,551] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13472
[2023-09-01 01:40:58,551] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [84]  [ 40/160]  eta: 0:02:01  lr: 0.000030  min_lr: 0.000001  loss: 2.0738 (1.9288)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4392 (8.0742)  time: 0.8101 (0.5399 -- 2.3776)  data: 0.2514 (0.0004 -- 1.8392)  max mem: 16735
Epoch: [84]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000001  loss: 2.0392 (1.9611)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7185 (8.0559)  time: 0.8524 (0.5282 -- 3.7961)  data: 0.3020 (0.0005 -- 3.2203)  max mem: 16735
Epoch: [84]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000001  loss: 1.8571 (1.9344)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9668 (8.1699)  time: 0.9310 (0.5339 -- 3.6029)  data: 0.3737 (0.0003 -- 3.0673)  max mem: 16735
Epoch: [84]  [100/160]  eta: 0:00:54  lr: 0.000030  min_lr: 0.000001  loss: 1.8726 (1.9250)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7125 (8.1435)  time: 0.7602 (0.5331 -- 2.7182)  data: 0.2074 (0.0002 -- 2.1497)  max mem: 16735
Epoch: [84]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.9350 (1.9375)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4355 (8.0494)  time: 0.9058 (0.5210 -- 3.1676)  data: 0.3223 (0.0003 -- 2.6294)  max mem: 16735
Epoch: [84]  [140/160]  eta: 0:00:17  lr: 0.000030  min_lr: 0.000001  loss: 1.8898 (1.9333)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8540 (8.1777)  time: 0.8006 (0.5409 -- 1.8755)  data: 0.1539 (0.0004 -- 0.9132)  max mem: 16735
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9788 (1.9289)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2802 (8.1763)  time: 0.7252 (0.5041 -- 1.7175)  data: 0.1399 (0.0002 -- 1.2035)  max mem: 16735
Epoch: [84] Total time: 0:02:20 (0.8791 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9788 (1.9598)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2802 (8.1763)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3545 (0.3545)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5216 (2.5216 -- 2.5216)  data: 2.2743 (2.2743 -- 2.2743)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6691 (0.8989)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4212 (0.1981 -- 2.5216)  data: 0.2077 (0.0006 -- 2.2743)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6691 (0.8276)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.8254)  time: 0.2154 (0.1685 -- 0.4023)  data: 0.0112 (0.0001 -- 0.2068)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7924 (0.8964)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (96.2656)  time: 0.1996 (0.1330 -- 0.4023)  data: 0.0109 (0.0001 -- 0.2068)  max mem: 16735
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 76.141 Acc@5 96.058 loss 0.891
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.80%
Epoch: [85]  [  0/160]  eta: 0:23:15  lr: 0.000030  min_lr: 0.000001  loss: 1.6201 (1.6201)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2653 (7.2653)  time: 8.7224 (8.7224 -- 8.7224)  data: 6.4289 (6.4289 -- 6.4289)  max mem: 16735
[2023-09-01 01:43:01,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:43:01,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:43:01,609] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:43:01,609] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:43:15,662] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13619
[2023-09-01 01:43:15,663] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13619
[2023-09-01 01:43:15,704] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:43:15,704] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:43:15,704] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [85]  [ 20/160]  eta: 0:02:39  lr: 0.000030  min_lr: 0.000001  loss: 2.0151 (1.9350)  loss_scale: 65536.0000 (60854.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1646 (8.3140)  time: 0.7595 (0.5304 -- 1.9539)  data: 0.0039 (0.0004 -- 0.0421)  max mem: 16735
Epoch: [85]  [ 40/160]  eta: 0:01:57  lr: 0.000030  min_lr: 0.000001  loss: 1.9656 (1.9167)  loss_scale: 32768.0000 (47153.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3260 (8.3535)  time: 0.8061 (0.5267 -- 2.4060)  data: 0.0663 (0.0003 -- 0.6023)  max mem: 16735
Epoch: [85]  [ 60/160]  eta: 0:01:33  lr: 0.000030  min_lr: 0.000001  loss: 1.6352 (1.8637)  loss_scale: 32768.0000 (42437.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2512 (8.5721)  time: 0.8608 (0.5401 -- 3.1682)  data: 0.2823 (0.0009 -- 2.5931)  max mem: 16735
Epoch: [85]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 2.0053 (1.9019)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2704 (8.3692)  time: 0.9647 (0.5278 -- 2.9562)  data: 0.1301 (0.0003 -- 1.7476)  max mem: 16735
Epoch: [85]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 1.9041 (1.8964)  loss_scale: 32768.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0755 (8.3442)  time: 0.8492 (0.5308 -- 3.3034)  data: 0.1933 (0.0003 -- 2.7660)  max mem: 16735
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.9255 (1.9086)  loss_scale: 32768.0000 (37642.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0462 (8.3200)  time: 0.8800 (0.5334 -- 2.3154)  data: 0.2798 (0.0006 -- 1.7782)  max mem: 16735
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.9840 (1.9145)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4608 (8.2239)  time: 0.8692 (0.5357 -- 2.2321)  data: 0.3146 (0.0004 -- 1.6711)  max mem: 16735
[2023-09-01 01:45:08,046] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:45:08,046] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:45:08,047] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:45:08,047] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 2.0426 (1.9350)  loss_scale: 65536.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4322 (8.1521)  time: 0.7011 (0.5023 -- 2.4709)  data: 0.0990 (0.0002 -- 1.9424)  max mem: 16735
Epoch: [85] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 2.0426 (1.9714)  loss_scale: 65536.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4322 (8.1521)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3808 (0.3808)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3312 (2.3312 -- 2.3312)  data: 2.1207 (2.1207 -- 2.1207)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6435 (0.8858)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4162 (0.2105 -- 2.3312)  data: 0.1938 (0.0005 -- 2.1207)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7030 (0.8290)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2249 (0.1690 -- 0.5010)  data: 0.0158 (0.0001 -- 0.3022)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.8053 (0.8958)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2078 (0.1332 -- 0.5010)  data: 0.0155 (0.0001 -- 0.3022)  max mem: 16735
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 76.971 Acc@5 95.643 loss 0.887
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 77.80%
Epoch: [86]  [  0/160]  eta: 0:18:01  lr: 0.000030  min_lr: 0.000001  loss: 2.1101 (2.1101)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7630 (6.7630)  time: 6.7576 (6.7576 -- 6.7576)  data: 6.2152 (6.2152 -- 6.2152)  max mem: 16735
[2023-09-01 01:45:39,142] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13770
[2023-09-01 01:45:39,142] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:45:39,142] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:45:39,143] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13770
[2023-09-01 01:45:39,143] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [86]  [ 20/160]  eta: 0:02:46  lr: 0.000030  min_lr: 0.000001  loss: 2.1337 (2.0463)  loss_scale: 32768.0000 (48371.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9974 (8.1575)  time: 0.9101 (0.5295 -- 2.9330)  data: 0.2197 (0.0003 -- 1.6994)  max mem: 16735
Epoch: [86]  [ 40/160]  eta: 0:02:04  lr: 0.000030  min_lr: 0.000001  loss: 1.8689 (1.9671)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0618 (8.2552)  time: 0.8772 (0.5160 -- 4.1968)  data: 0.0013 (0.0002 -- 0.0026)  max mem: 16735
Epoch: [86]  [ 60/160]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000001  loss: 1.8039 (1.9322)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1444 (8.4851)  time: 0.9491 (0.5233 -- 4.6898)  data: 0.0016 (0.0004 -- 0.0069)  max mem: 16735
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000001  loss: 2.0632 (1.9677)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4167 (8.5330)  time: 0.7228 (0.5350 -- 2.6238)  data: 0.0015 (0.0005 -- 0.0034)  max mem: 16735
Epoch: [86]  [100/160]  eta: 0:00:54  lr: 0.000029  min_lr: 0.000001  loss: 1.9102 (1.9582)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7150 (8.5834)  time: 0.8003 (0.5271 -- 3.1322)  data: 0.0740 (0.0002 -- 1.3333)  max mem: 16735
Epoch: [86]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 2.0831 (1.9808)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8747 (8.5143)  time: 1.0893 (0.5236 -- 4.0469)  data: 0.5389 (0.0007 -- 3.5344)  max mem: 16735
[2023-09-01 01:47:32,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:47:32,129] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:47:32,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:47:32,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.9641 (1.9736)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3187 (8.3443)  time: 0.8365 (0.5268 -- 3.4037)  data: 0.2842 (0.0002 -- 2.8676)  max mem: 16735
[2023-09-01 01:47:39,960] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13909
[2023-09-01 01:47:39,960] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13909
[2023-09-01 01:47:39,961] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:47:39,961] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:47:39,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9923 (1.9649)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2925 (8.2671)  time: 0.6436 (0.5015 -- 3.0100)  data: 0.1239 (0.0002 -- 2.4675)  max mem: 16735
Epoch: [86] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9923 (1.9427)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2925 (8.2671)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3698 (0.3698)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4952 (2.4952 -- 2.4952)  data: 2.2620 (2.2620 -- 2.2620)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7654 (0.9152)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4408 (0.1940 -- 2.4952)  data: 0.2265 (0.0004 -- 2.2620)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7559 (0.8486)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (95.2381)  time: 0.2207 (0.1681 -- 0.4272)  data: 0.0151 (0.0001 -- 0.2169)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7816 (0.9258)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (94.1909)  time: 0.2049 (0.1361 -- 0.4272)  data: 0.0149 (0.0001 -- 0.2169)  max mem: 16735
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 75.726 Acc@5 95.021 loss 0.911
Accuracy of the network on the 482 val images: 75.73%
Max accuracy: 77.80%
Epoch: [87]  [  0/160]  eta: 0:22:15  lr: 0.000029  min_lr: 0.000001  loss: 1.9491 (1.9491)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5779 (11.5779)  time: 8.3493 (8.3493 -- 8.3493)  data: 7.7766 (7.7766 -- 7.7766)  max mem: 16735
Epoch: [87]  [ 20/160]  eta: 0:02:48  lr: 0.000029  min_lr: 0.000001  loss: 1.8548 (1.9124)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6673 (8.0834)  time: 0.8438 (0.5380 -- 3.7515)  data: 0.2787 (0.0009 -- 3.2232)  max mem: 16735
Epoch: [87]  [ 40/160]  eta: 0:02:02  lr: 0.000029  min_lr: 0.000001  loss: 1.9575 (1.9027)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8361 (8.1070)  time: 0.8269 (0.5374 -- 2.8125)  data: 0.2714 (0.0004 -- 2.3002)  max mem: 16735
Epoch: [87]  [ 60/160]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000001  loss: 1.9088 (1.9130)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4895 (8.2512)  time: 0.9004 (0.5262 -- 3.1849)  data: 0.3526 (0.0002 -- 2.6466)  max mem: 16735
[2023-09-01 01:49:08,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=84, lr=[6.905423794084148e-07, 6.905423794084148e-07, 9.207231725445531e-07, 9.207231725445531e-07, 1.2276308967260707e-06, 1.2276308967260707e-06, 1.636841195634761e-06, 1.636841195634761e-06, 2.1824549275130148e-06, 2.1824549275130148e-06, 2.909939903350686e-06, 2.909939903350686e-06, 3.8799198711342485e-06, 3.8799198711342485e-06, 5.173226494845664e-06, 5.173226494845664e-06, 6.897635326460885e-06, 6.897635326460885e-06, 9.196847101947847e-06, 9.196847101947847e-06, 1.226246280259713e-05, 1.226246280259713e-05, 1.634995040346284e-05, 1.634995040346284e-05, 2.1799933871283787e-05, 2.1799933871283787e-05, 2.906657849504505e-05, 2.906657849504505e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 01:49:08,408] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=18.11758506579939, CurrSamplesPerSec=21.661996126533246, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [87]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000001  loss: 2.1745 (1.9720)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8127 (8.1450)  time: 0.8125 (0.5342 -- 3.6692)  data: 0.2586 (0.0003 -- 3.1472)  max mem: 16735
Epoch: [87]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.9977 (1.9685)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1114 (8.2068)  time: 0.8570 (0.5373 -- 3.4266)  data: 0.0879 (0.0002 -- 0.8867)  max mem: 16735
[2023-09-01 01:49:39,903] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:49:39,903] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:49:39,903] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:49:39,903] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [87]  [120/160]  eta: 0:00:35  lr: 0.000029  min_lr: 0.000001  loss: 2.0405 (1.9759)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1883 (8.1107)  time: 0.7682 (0.5226 -- 1.7786)  data: 0.0616 (0.0006 -- 0.4771)  max mem: 16735
Epoch: [87]  [140/160]  eta: 0:00:17  lr: 0.000029  min_lr: 0.000001  loss: 2.0220 (1.9903)  loss_scale: 65536.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5988 (8.0443)  time: 0.8260 (0.5367 -- 1.8385)  data: 0.2304 (0.0004 -- 1.3248)  max mem: 16735
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 2.0017 (1.9919)  loss_scale: 65536.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9781 (8.0438)  time: 0.7747 (0.5025 -- 2.9327)  data: 0.1838 (0.0003 -- 2.3731)  max mem: 16735
Epoch: [87] Total time: 0:02:19 (0.8750 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 2.0017 (1.9444)  loss_scale: 65536.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9781 (8.0438)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3587 (0.3587)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3633 (2.3633 -- 2.3633)  data: 2.1511 (2.1511 -- 2.1511)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6379 (0.8845)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4086 (0.1977 -- 2.3633)  data: 0.1964 (0.0007 -- 2.1511)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7089 (0.8267)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.2381)  time: 0.2257 (0.1703 -- 0.5095)  data: 0.0203 (0.0001 -- 0.3272)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7674 (0.8934)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.0207)  time: 0.2117 (0.1327 -- 0.5095)  data: 0.0200 (0.0001 -- 0.3272)  max mem: 16735
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 77.178 Acc@5 95.643 loss 0.878
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.80%
Epoch: [88]  [  0/160]  eta: 0:16:43  lr: 0.000029  min_lr: 0.000001  loss: 1.9315 (1.9315)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1275 (5.1275)  time: 6.2741 (6.2741 -- 6.2741)  data: 5.6461 (5.6461 -- 5.6461)  max mem: 16735
Epoch: [88]  [ 20/160]  eta: 0:02:45  lr: 0.000029  min_lr: 0.000001  loss: 2.0601 (2.0032)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6168 (7.5938)  time: 0.9253 (0.5310 -- 3.3969)  data: 0.3632 (0.0003 -- 2.8496)  max mem: 16735
[2023-09-01 01:50:46,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14101
[2023-09-01 01:50:46,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:50:46,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14101
[2023-09-01 01:50:46,165] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:50:46,165] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [88]  [ 40/160]  eta: 0:01:58  lr: 0.000029  min_lr: 0.000001  loss: 1.9331 (1.9219)  loss_scale: 32768.0000 (49551.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5775 (8.0424)  time: 0.7798 (0.5344 -- 2.1758)  data: 0.1828 (0.0004 -- 1.6373)  max mem: 16735
Epoch: [88]  [ 60/160]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000001  loss: 2.0522 (1.9319)  loss_scale: 32768.0000 (44048.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4825 (8.1138)  time: 0.9419 (0.5305 -- 3.2862)  data: 0.3851 (0.0004 -- 2.7546)  max mem: 16735
Epoch: [88]  [ 80/160]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000001  loss: 1.9471 (1.9339)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7374 (8.0937)  time: 0.8042 (0.5179 -- 3.0877)  data: 0.2328 (0.0003 -- 2.5528)  max mem: 16735
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.9832 (1.9351)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7841 (8.0814)  time: 0.9359 (0.5284 -- 3.1513)  data: 0.2828 (0.0004 -- 2.6127)  max mem: 16735
Epoch: [88]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 2.0631 (1.9501)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1435 (8.1642)  time: 0.9244 (0.5280 -- 3.4962)  data: 0.3727 (0.0003 -- 2.9867)  max mem: 16735
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.7387 (1.9377)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8665 (8.3341)  time: 0.8576 (0.5267 -- 3.8632)  data: 0.3115 (0.0003 -- 3.3499)  max mem: 16735
[2023-09-01 01:52:37,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:52:37,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:52:37,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:52:37,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 2.0169 (1.9362)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4399 (8.3674)  time: 0.6675 (0.5025 -- 1.7986)  data: 0.1427 (0.0002 -- 1.2255)  max mem: 16735
Epoch: [88] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 2.0169 (1.9559)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4399 (8.3674)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3659 (0.3659)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5124 (2.5124 -- 2.5124)  data: 2.2488 (2.2488 -- 2.2488)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6780 (0.9118)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4225 (0.1925 -- 2.5124)  data: 0.2054 (0.0008 -- 2.2488)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6780 (0.8380)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2166 (0.1693 -- 0.4589)  data: 0.0142 (0.0001 -- 0.2708)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7957 (0.9074)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (94.6058)  time: 0.2045 (0.1331 -- 0.4589)  data: 0.0140 (0.0001 -- 0.2708)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 75.934 Acc@5 95.228 loss 0.898
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 77.80%
Epoch: [89]  [  0/160]  eta: 0:20:03  lr: 0.000029  min_lr: 0.000001  loss: 2.0893 (2.0893)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0757 (7.0757)  time: 7.5221 (7.5221 -- 7.5221)  data: 6.9969 (6.9969 -- 6.9969)  max mem: 16735
[2023-09-01 01:53:06,732] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14249
[2023-09-01 01:53:06,733] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:53:06,735] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14249
[2023-09-01 01:53:06,735] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:53:06,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [89]  [ 20/160]  eta: 0:02:44  lr: 0.000028  min_lr: 0.000001  loss: 1.8762 (1.9300)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1699 (8.3137)  time: 0.8607 (0.5359 -- 3.5884)  data: 0.2821 (0.0002 -- 3.0393)  max mem: 16735
Epoch: [89]  [ 40/160]  eta: 0:02:05  lr: 0.000028  min_lr: 0.000001  loss: 1.9668 (1.9633)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6795 (8.1326)  time: 0.9115 (0.5283 -- 3.2293)  data: 0.3621 (0.0002 -- 2.7022)  max mem: 16735
Epoch: [89]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000001  loss: 1.8759 (1.9622)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3854 (8.0963)  time: 0.8636 (0.5349 -- 3.7965)  data: 0.3105 (0.0003 -- 3.2582)  max mem: 16735
Epoch: [89]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000001  loss: 1.8529 (1.9603)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8807 (8.0944)  time: 0.7949 (0.5258 -- 2.3829)  data: 0.2414 (0.0002 -- 1.8485)  max mem: 16735
Epoch: [89]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000001  loss: 1.8476 (1.9533)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2805 (8.2922)  time: 0.9186 (0.5326 -- 4.5855)  data: 0.3766 (0.0004 -- 4.0651)  max mem: 16735
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.8404 (1.9284)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0402 (8.3642)  time: 0.8152 (0.5237 -- 2.8736)  data: 0.2599 (0.0003 -- 2.3198)  max mem: 16735
[2023-09-01 01:54:56,423] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:54:56,424] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:54:56,424] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:54:56,424] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 2.0626 (1.9535)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0739 (8.1631)  time: 1.0094 (0.5248 -- 5.1537)  data: 0.4603 (0.0005 -- 4.6178)  max mem: 16735
[2023-09-01 01:55:03,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14382
[2023-09-01 01:55:03,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14382
[2023-09-01 01:55:03,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:55:03,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:55:03,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9906 (1.9493)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1757 (8.1433)  time: 0.5274 (0.5020 -- 0.5855)  data: 0.0007 (0.0002 -- 0.0014)  max mem: 16735
Epoch: [89] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9906 (1.9802)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1757 (8.1433)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3905 (0.3905)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3841 (2.3841 -- 2.3841)  data: 2.1776 (2.1776 -- 2.1776)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6941 (0.9142)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4258 (0.2011 -- 2.3841)  data: 0.2187 (0.0007 -- 2.1776)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7169 (0.8464)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2260 (0.1689 -- 0.4371)  data: 0.0237 (0.0001 -- 0.2421)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7749 (0.9122)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (94.1909)  time: 0.2117 (0.1322 -- 0.4371)  data: 0.0233 (0.0001 -- 0.2421)  max mem: 16735
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 76.349 Acc@5 94.813 loss 0.901
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.80%
Epoch: [90]  [  0/160]  eta: 0:20:08  lr: 0.000028  min_lr: 0.000001  loss: 2.2100 (2.2100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8955 (9.8955)  time: 7.5510 (7.5510 -- 7.5510)  data: 7.0268 (7.0268 -- 7.0268)  max mem: 16735
Epoch: [90]  [ 20/160]  eta: 0:02:43  lr: 0.000028  min_lr: 0.000001  loss: 1.9546 (1.9795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4482 (7.9328)  time: 0.8457 (0.5341 -- 2.4472)  data: 0.2335 (0.0003 -- 1.8848)  max mem: 16735
Epoch: [90]  [ 40/160]  eta: 0:02:03  lr: 0.000028  min_lr: 0.000001  loss: 2.0799 (2.0358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1044 (8.1383)  time: 0.8829 (0.5273 -- 3.0459)  data: 0.2862 (0.0004 -- 2.2790)  max mem: 16735
Epoch: [90]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000001  loss: 2.1576 (2.0654)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4640 (8.2748)  time: 0.8991 (0.5354 -- 2.8748)  data: 0.2288 (0.0001 -- 2.0772)  max mem: 16735
Epoch: [90]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000001  loss: 2.0495 (2.0595)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7651 (8.3626)  time: 0.8387 (0.5132 -- 3.5077)  data: 0.2558 (0.0002 -- 2.9677)  max mem: 16735
Epoch: [90]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 1.9681 (2.0375)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9470 (8.2866)  time: 0.8320 (0.5279 -- 3.1909)  data: 0.2319 (0.0003 -- 2.6572)  max mem: 16735
[2023-09-01 01:57:03,075] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:57:03,076] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:57:03,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:57:03,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9373 (2.0268)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7777 (8.2526)  time: 0.8704 (0.5372 -- 3.4536)  data: 0.2875 (0.0003 -- 2.9294)  max mem: 16735
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.9217 (2.0195)  loss_scale: 65536.0000 (39739.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7339 (8.2218)  time: 0.8739 (0.5277 -- 3.6724)  data: 0.3212 (0.0003 -- 3.1091)  max mem: 16735
[2023-09-01 01:57:29,013] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14541
[2023-09-01 01:57:29,013] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:57:29,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:57:29,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14541
[2023-09-01 01:57:29,015] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.0441 (2.0187)  loss_scale: 32768.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0486 (8.3078)  time: 0.6021 (0.5031 -- 1.9843)  data: 0.0742 (0.0002 -- 1.4689)  max mem: 16735
Epoch: [90] Total time: 0:02:19 (0.8745 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.0441 (1.9913)  loss_scale: 32768.0000 (38912.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0486 (8.3078)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3683 (0.3683)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4041 (2.4041 -- 2.4041)  data: 2.1881 (2.1881 -- 2.1881)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7602 (0.9366)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4266 (0.2004 -- 2.4041)  data: 0.2052 (0.0005 -- 2.1881)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7602 (0.8634)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (95.2381)  time: 0.2194 (0.1686 -- 0.3209)  data: 0.0080 (0.0001 -- 0.0692)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7898 (0.9328)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (93.7759)  time: 0.2029 (0.1322 -- 0.3209)  data: 0.0077 (0.0001 -- 0.0692)  max mem: 16735
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 75.519 Acc@5 94.606 loss 0.920
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 77.80%
Epoch: [91]  [  0/160]  eta: 0:21:04  lr: 0.000028  min_lr: 0.000001  loss: 2.2507 (2.2507)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5824 (11.5824)  time: 7.9053 (7.9053 -- 7.9053)  data: 7.0716 (7.0716 -- 7.0716)  max mem: 16735
Epoch: [91]  [ 20/160]  eta: 0:02:45  lr: 0.000028  min_lr: 0.000001  loss: 1.9009 (1.9199)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0537 (8.7856)  time: 0.8484 (0.5219 -- 6.1269)  data: 0.1674 (0.0004 -- 3.3072)  max mem: 16735
Epoch: [91]  [ 40/160]  eta: 0:02:08  lr: 0.000028  min_lr: 0.000001  loss: 1.9894 (1.9403)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1433 (8.7628)  time: 0.9541 (0.5390 -- 4.3968)  data: 0.0849 (0.0004 -- 1.6678)  max mem: 16735
Epoch: [91]  [ 60/160]  eta: 0:01:37  lr: 0.000028  min_lr: 0.000001  loss: 2.1385 (1.9859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3794 (8.6428)  time: 0.7642 (0.5299 -- 2.7533)  data: 0.0023 (0.0002 -- 0.0122)  max mem: 16735
Epoch: [91]  [ 80/160]  eta: 0:01:18  lr: 0.000028  min_lr: 0.000001  loss: 1.8614 (1.9512)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8940 (8.5280)  time: 0.9867 (0.5267 -- 4.5920)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16735
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 1.8870 (1.9490)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4017 (8.6790)  time: 0.7301 (0.5213 -- 4.2272)  data: 0.0012 (0.0003 -- 0.0059)  max mem: 16735
[2023-09-01 01:59:34,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:59:34,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 01:59:34,604] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 01:59:34,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.7298 (1.9190)  loss_scale: 65536.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3284 (8.6523)  time: 0.9971 (0.5260 -- 4.5501)  data: 0.0014 (0.0002 -- 0.0029)  max mem: 16735
[2023-09-01 01:59:54,304] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14695
[2023-09-01 01:59:54,305] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 01:59:54,305] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 01:59:54,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14695
[2023-09-01 01:59:54,305] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.8093 (1.9072)  loss_scale: 65536.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8191 (8.6089)  time: 0.7868 (0.5267 -- 3.3030)  data: 0.0013 (0.0006 -- 0.0035)  max mem: 16735
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7878 (1.8966)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5171 (8.6070)  time: 0.6701 (0.5012 -- 1.9689)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16735
Epoch: [91] Total time: 0:02:22 (0.8883 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7878 (1.9193)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5171 (8.6070)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3593 (0.3593)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3797 (2.3797 -- 2.3797)  data: 2.1652 (2.1652 -- 2.1652)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6879 (0.9088)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4252 (0.2038 -- 2.3797)  data: 0.2068 (0.0004 -- 2.1652)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6879 (0.8354)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.2381)  time: 0.2212 (0.1718 -- 0.2995)  data: 0.0138 (0.0001 -- 0.1277)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7684 (0.9080)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (94.6058)  time: 0.2066 (0.1327 -- 0.2995)  data: 0.0134 (0.0001 -- 0.1277)  max mem: 16735
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 75.519 Acc@5 95.228 loss 0.899
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 77.80%
Epoch: [92]  [  0/160]  eta: 0:20:36  lr: 0.000027  min_lr: 0.000001  loss: 1.8501 (1.8501)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2985 (9.2985)  time: 7.7271 (7.7271 -- 7.7271)  data: 6.8798 (6.8798 -- 6.8798)  max mem: 16735
Epoch: [92]  [ 20/160]  eta: 0:02:52  lr: 0.000027  min_lr: 0.000001  loss: 2.0983 (2.0105)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6914 (7.8030)  time: 0.9097 (0.5378 -- 3.2518)  data: 0.0795 (0.0005 -- 1.0892)  max mem: 16735
Epoch: [92]  [ 40/160]  eta: 0:02:07  lr: 0.000027  min_lr: 0.000001  loss: 1.7877 (1.9414)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9467 (7.8228)  time: 0.8893 (0.5252 -- 4.3675)  data: 0.0311 (0.0002 -- 0.3107)  max mem: 16735
Epoch: [92]  [ 60/160]  eta: 0:01:42  lr: 0.000027  min_lr: 0.000001  loss: 1.7414 (1.9230)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7172 (7.9506)  time: 0.9283 (0.5267 -- 3.2304)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16735
Epoch: [92]  [ 80/160]  eta: 0:01:16  lr: 0.000027  min_lr: 0.000001  loss: 2.0609 (1.9668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0659 (8.0678)  time: 0.7385 (0.5242 -- 2.5122)  data: 0.0016 (0.0003 -- 0.0041)  max mem: 16735
Epoch: [92]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000001  loss: 1.8056 (1.9411)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2700 (8.1207)  time: 0.8937 (0.5241 -- 3.8402)  data: 0.0146 (0.0005 -- 0.1297)  max mem: 16735
[2023-09-01 02:01:54,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:01:54,873] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:01:54,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:01:54,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:02:00,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14830
[2023-09-01 02:02:00,501] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:02:00,501] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14830
[2023-09-01 02:02:00,542] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:02:00,542] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 1.7353 (1.9348)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9843 (8.1872)  time: 0.7636 (0.5281 -- 2.8071)  data: 0.0949 (0.0004 -- 1.8582)  max mem: 16735
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.9040 (1.9324)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4629 (8.1548)  time: 0.9408 (0.5371 -- 4.2773)  data: 0.3862 (0.0004 -- 3.7518)  max mem: 16735
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.8891 (1.9310)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2430 (8.0655)  time: 0.6676 (0.5015 -- 1.6728)  data: 0.0697 (0.0003 -- 1.1663)  max mem: 16735
Epoch: [92] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.8891 (1.9372)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2430 (8.0655)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3638 (0.3638)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2039 (2.2039 -- 2.2039)  data: 1.9648 (1.9648 -- 1.9648)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7119 (0.8964)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4198 (0.2009 -- 2.2039)  data: 0.1969 (0.0007 -- 1.9648)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7098 (0.8235)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.2381)  time: 0.2273 (0.1681 -- 0.4211)  data: 0.0134 (0.0001 -- 0.1894)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7435 (0.9012)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (94.1909)  time: 0.2095 (0.1326 -- 0.4211)  data: 0.0131 (0.0001 -- 0.1894)  max mem: 16735
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 76.141 Acc@5 94.813 loss 0.894
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.80%
Epoch: [93]  [  0/160]  eta: 0:18:18  lr: 0.000027  min_lr: 0.000001  loss: 1.5929 (1.5929)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5933 (8.5933)  time: 6.8662 (6.8662 -- 6.8662)  data: 6.2296 (6.2296 -- 6.2296)  max mem: 16735
Epoch: [93]  [ 20/160]  eta: 0:02:43  lr: 0.000027  min_lr: 0.000001  loss: 2.0491 (1.9957)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7229 (8.5526)  time: 0.8795 (0.5331 -- 4.4937)  data: 0.2016 (0.0004 -- 1.9835)  max mem: 16735
Epoch: [93]  [ 40/160]  eta: 0:02:07  lr: 0.000027  min_lr: 0.000001  loss: 1.7888 (1.9407)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0996 (8.5742)  time: 0.9489 (0.5207 -- 3.5095)  data: 0.2402 (0.0003 -- 1.8169)  max mem: 16735
Epoch: [93]  [ 60/160]  eta: 0:01:39  lr: 0.000027  min_lr: 0.000001  loss: 1.9565 (1.9312)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0465 (8.7219)  time: 0.8525 (0.5248 -- 3.7504)  data: 0.1974 (0.0002 -- 2.5393)  max mem: 16735
[2023-09-01 02:04:04,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:04:04,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:04:04,615] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:04:04,615] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [93]  [ 80/160]  eta: 0:01:16  lr: 0.000027  min_lr: 0.000001  loss: 1.9046 (1.9217)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3521 (8.6348)  time: 0.8639 (0.5305 -- 3.1515)  data: 0.0168 (0.0003 -- 0.3100)  max mem: 16735
[2023-09-01 02:04:13,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14970
[2023-09-01 02:04:13,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14970
[2023-09-01 02:04:13,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:04:13,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:04:13,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [93]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000001  loss: 1.8018 (1.9161)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1398 (8.6425)  time: 0.9332 (0.5244 -- 4.6948)  data: 0.0028 (0.0003 -- 0.0148)  max mem: 16735
[2023-09-01 02:04:38,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=91, lr=[6.355782109029989e-07, 6.355782109029989e-07, 8.474376145373319e-07, 8.474376145373319e-07, 1.129916819383109e-06, 1.129916819383109e-06, 1.5065557591774788e-06, 1.5065557591774788e-06, 2.008741012236638e-06, 2.008741012236638e-06, 2.678321349648851e-06, 2.678321349648851e-06, 3.571095132865135e-06, 3.571095132865135e-06, 4.761460177153514e-06, 4.761460177153514e-06, 6.348613569538018e-06, 6.348613569538018e-06, 8.464818092717356e-06, 8.464818092717356e-06, 1.1286424123623142e-05, 1.1286424123623142e-05, 1.504856549816419e-05, 1.504856549816419e-05, 2.0064753997552253e-05, 2.0064753997552253e-05, 2.675300533006967e-05, 2.675300533006967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 02:04:38,751] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=18.1263957981598, CurrSamplesPerSec=21.592724695553837, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 2.0693 (1.9322)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6943 (8.4865)  time: 0.7732 (0.5183 -- 2.8004)  data: 0.0015 (0.0001 -- 0.0031)  max mem: 16735
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 2.0980 (1.9327)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0575 (8.4967)  time: 0.9042 (0.5281 -- 4.3675)  data: 0.0019 (0.0002 -- 0.0153)  max mem: 16735
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.8591 (1.9215)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1726 (8.4907)  time: 0.6455 (0.4999 -- 2.0546)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [93] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.8591 (1.9522)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1726 (8.4907)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3545 (0.3545)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3165 (2.3165 -- 2.3165)  data: 2.0773 (2.0773 -- 2.0773)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6812 (0.8874)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4161 (0.1852 -- 2.3165)  data: 0.1911 (0.0007 -- 2.0773)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6812 (0.8162)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1712 -- 0.5115)  data: 0.0178 (0.0001 -- 0.3282)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7623 (0.8958)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.8506)  time: 0.2037 (0.1330 -- 0.5115)  data: 0.0168 (0.0001 -- 0.3282)  max mem: 16735
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 76.349 Acc@5 95.643 loss 0.890
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.80%
Epoch: [94]  [  0/160]  eta: 0:18:46  lr: 0.000027  min_lr: 0.000001  loss: 2.1505 (2.1505)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5247 (7.5247)  time: 7.0426 (7.0426 -- 7.0426)  data: 6.5201 (6.5201 -- 6.5201)  max mem: 16735
Epoch: [94]  [ 20/160]  eta: 0:02:42  lr: 0.000027  min_lr: 0.000001  loss: 1.8250 (1.9057)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7549 (9.1571)  time: 0.8652 (0.5240 -- 3.1404)  data: 0.0882 (0.0001 -- 1.4233)  max mem: 16735
Epoch: [94]  [ 40/160]  eta: 0:01:57  lr: 0.000027  min_lr: 0.000001  loss: 1.9343 (1.9023)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1660 (8.8511)  time: 0.7820 (0.5294 -- 2.9007)  data: 0.1294 (0.0005 -- 1.5335)  max mem: 16735
[2023-09-01 02:06:16,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:06:16,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:06:16,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:06:16,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [94]  [ 60/160]  eta: 0:01:37  lr: 0.000027  min_lr: 0.000001  loss: 2.0023 (1.9232)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0918 (8.4495)  time: 0.9655 (0.5337 -- 3.7640)  data: 0.0824 (0.0005 -- 1.4512)  max mem: 16735
Epoch: [94]  [ 80/160]  eta: 0:01:14  lr: 0.000026  min_lr: 0.000001  loss: 1.8365 (1.9110)  loss_scale: 65536.0000 (41667.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7800 (8.4514)  time: 0.8012 (0.5327 -- 2.9380)  data: 0.0099 (0.0002 -- 0.1691)  max mem: 16735
[2023-09-01 02:06:35,267] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15122
[2023-09-01 02:06:35,267] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15122
[2023-09-01 02:06:35,267] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:06:35,267] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:06:35,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [94]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000001  loss: 2.0133 (1.9169)  loss_scale: 32768.0000 (40230.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5785 (8.4289)  time: 0.9094 (0.5322 -- 3.4814)  data: 0.2655 (0.0004 -- 2.9492)  max mem: 16735
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 2.0554 (1.9310)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5798 (8.4334)  time: 0.8858 (0.5169 -- 4.4037)  data: 0.3356 (0.0001 -- 3.8688)  max mem: 16735
Epoch: [94]  [140/160]  eta: 0:00:17  lr: 0.000026  min_lr: 0.000001  loss: 2.1922 (1.9648)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7456 (8.3843)  time: 0.7700 (0.5421 -- 2.4756)  data: 0.2118 (0.0005 -- 1.9357)  max mem: 16735
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8759 (1.9569)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5770 (8.3597)  time: 0.6946 (0.5024 -- 2.2253)  data: 0.1322 (0.0002 -- 1.6774)  max mem: 16735
Epoch: [94] Total time: 0:02:20 (0.8750 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8759 (1.9440)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5770 (8.3597)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3677 (0.3677)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3033 (2.3033 -- 2.3033)  data: 2.0944 (2.0944 -- 2.0944)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.7041 (0.9184)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4074 (0.2089 -- 2.3033)  data: 0.1913 (0.0006 -- 2.0944)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7041 (0.8411)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.2381)  time: 0.2243 (0.1684 -- 0.5362)  data: 0.0170 (0.0001 -- 0.3252)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7479 (0.9147)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2084 (0.1328 -- 0.5362)  data: 0.0167 (0.0001 -- 0.3252)  max mem: 16735
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 76.763 Acc@5 95.228 loss 0.909
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 77.80%
Epoch: [95]  [  0/160]  eta: 0:18:34  lr: 0.000026  min_lr: 0.000001  loss: 1.8297 (1.8297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2330 (6.2330)  time: 6.9643 (6.9643 -- 6.9643)  data: 6.4156 (6.4156 -- 6.4156)  max mem: 16735
Epoch: [95]  [ 20/160]  eta: 0:02:45  lr: 0.000026  min_lr: 0.000001  loss: 2.0668 (2.0089)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7446 (8.0107)  time: 0.8939 (0.5332 -- 4.1947)  data: 0.2141 (0.0003 -- 2.5680)  max mem: 16735
Epoch: [95]  [ 40/160]  eta: 0:02:02  lr: 0.000026  min_lr: 0.000001  loss: 1.9545 (1.9767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7921 (8.0065)  time: 0.8548 (0.5294 -- 4.6613)  data: 0.0112 (0.0002 -- 0.1854)  max mem: 16735
[2023-09-01 02:08:29,134] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15242
[2023-09-01 02:08:29,135] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:08:29,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-01 02:08:29,136] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15242
[2023-09-01 02:08:29,136] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [95]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000001  loss: 1.8721 (1.9774)  loss_scale: 16384.0000 (27664.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9529 (7.9899)  time: 0.9202 (0.5276 -- 3.6486)  data: 0.3336 (0.0007 -- 3.1193)  max mem: 16735
Epoch: [95]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000001  loss: 2.0090 (1.9821)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7996 (8.0806)  time: 0.7893 (0.5270 -- 3.1537)  data: 0.2380 (0.0002 -- 2.6258)  max mem: 16735
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000001  loss: 2.0140 (1.9868)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5885 (8.1676)  time: 0.9338 (0.5180 -- 3.5658)  data: 0.3892 (0.0003 -- 3.0069)  max mem: 16735
Epoch: [95]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.9323 (1.9774)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7338 (8.1708)  time: 0.8385 (0.5190 -- 4.0824)  data: 0.2926 (0.0002 -- 3.5558)  max mem: 16735
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.8939 (1.9715)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9159 (8.2234)  time: 0.8778 (0.5351 -- 3.6376)  data: 0.3203 (0.0008 -- 3.1012)  max mem: 16735
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.9253 (1.9716)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3818 (8.3305)  time: 0.6532 (0.4999 -- 2.3541)  data: 0.1218 (0.0002 -- 1.8008)  max mem: 16735
Epoch: [95] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.9253 (1.9727)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3818 (8.3305)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3285 (0.3285)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3911 (2.3911 -- 2.3911)  data: 2.1487 (2.1487 -- 2.1487)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7402 (0.8866)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4233 (0.2052 -- 2.3911)  data: 0.2014 (0.0008 -- 2.1487)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6919 (0.8057)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2201 (0.1688 -- 0.3462)  data: 0.0113 (0.0001 -- 0.1566)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7132 (0.8792)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2035 (0.1328 -- 0.3462)  data: 0.0108 (0.0001 -- 0.1566)  max mem: 16735
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 76.763 Acc@5 95.643 loss 0.877
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 77.80%
Epoch: [96]  [  0/160]  eta: 0:17:44  lr: 0.000026  min_lr: 0.000001  loss: 1.9175 (1.9175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5379 (9.5379)  time: 6.6551 (6.6551 -- 6.6551)  data: 6.1333 (6.1333 -- 6.1333)  max mem: 16735
[2023-09-01 02:10:29,993] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:10:29,993] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 02:10:29,994] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:10:29,994] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [96]  [ 20/160]  eta: 0:02:39  lr: 0.000026  min_lr: 0.000001  loss: 2.0531 (2.0209)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9858 (8.3792)  time: 0.8633 (0.5290 -- 3.3524)  data: 0.2579 (0.0002 -- 2.8346)  max mem: 16735
Epoch: [96]  [ 40/160]  eta: 0:02:05  lr: 0.000026  min_lr: 0.000001  loss: 1.8072 (1.9294)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7132 (8.2946)  time: 0.9403 (0.5343 -- 3.1413)  data: 0.2803 (0.0003 -- 2.6125)  max mem: 16735
Epoch: [96]  [ 60/160]  eta: 0:01:39  lr: 0.000026  min_lr: 0.000001  loss: 1.8699 (1.9114)  loss_scale: 32768.0000 (29813.5082)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5114 (8.1513)  time: 0.8877 (0.5205 -- 3.6041)  data: 0.0506 (0.0003 -- 0.9896)  max mem: 16735
Epoch: [96]  [ 80/160]  eta: 0:01:16  lr: 0.000026  min_lr: 0.000001  loss: 1.8226 (1.9046)  loss_scale: 32768.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2077 (8.2468)  time: 0.8534 (0.5251 -- 3.7158)  data: 0.0016 (0.0004 -- 0.0049)  max mem: 16735
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000001  loss: 1.8608 (1.8995)  loss_scale: 32768.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7658 (8.1415)  time: 0.7686 (0.5394 -- 1.8962)  data: 0.0227 (0.0004 -- 0.3977)  max mem: 16735
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.8381 (1.8878)  loss_scale: 32768.0000 (31278.5455)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9564 (8.1663)  time: 0.8473 (0.5301 -- 2.8830)  data: 0.0701 (0.0004 -- 0.8110)  max mem: 16735
[2023-09-01 02:12:22,079] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:12:22,079] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:12:22,081] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:12:22,081] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.9088 (1.8979)  loss_scale: 32768.0000 (31954.6099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2389 (8.1168)  time: 0.8931 (0.5337 -- 3.6595)  data: 0.0438 (0.0003 -- 0.8207)  max mem: 16735
[2023-09-01 02:12:34,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15515
[2023-09-01 02:12:34,472] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:12:34,472] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 02:12:34,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15515
[2023-09-01 02:12:34,472] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 2.0008 (1.9211)  loss_scale: 65536.0000 (34918.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9894 (8.1179)  time: 0.7194 (0.4938 -- 3.7123)  data: 0.0464 (0.0002 -- 0.8675)  max mem: 16735
Epoch: [96] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 2.0008 (1.9242)  loss_scale: 65536.0000 (34918.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9894 (8.1179)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3457 (0.3457)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2772 (2.2772 -- 2.2772)  data: 2.0457 (2.0457 -- 2.0457)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6407 (0.8675)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4097 (0.2041 -- 2.2772)  data: 0.1910 (0.0007 -- 2.0457)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6675 (0.7998)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.7672)  time: 0.2240 (0.1689 -- 0.4144)  data: 0.0160 (0.0001 -- 0.1803)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7366 (0.8712)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2071 (0.1331 -- 0.4144)  data: 0.0152 (0.0001 -- 0.1803)  max mem: 16735
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 76.971 Acc@5 95.436 loss 0.870
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 77.80%
Epoch: [97]  [  0/160]  eta: 0:18:54  lr: 0.000026  min_lr: 0.000001  loss: 2.3506 (2.3506)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6335 (10.6335)  time: 7.0927 (7.0927 -- 7.0927)  data: 6.5549 (6.5549 -- 6.5549)  max mem: 16735
Epoch: [97]  [ 20/160]  eta: 0:02:40  lr: 0.000025  min_lr: 0.000001  loss: 2.0890 (1.9601)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7182 (8.3767)  time: 0.8510 (0.5237 -- 4.7865)  data: 0.2760 (0.0005 -- 4.2560)  max mem: 16735
Epoch: [97]  [ 40/160]  eta: 0:01:58  lr: 0.000025  min_lr: 0.000001  loss: 2.0654 (1.9733)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0419 (8.0607)  time: 0.8204 (0.5207 -- 2.0371)  data: 0.1050 (0.0003 -- 0.8243)  max mem: 16735
Epoch: [97]  [ 60/160]  eta: 0:01:40  lr: 0.000025  min_lr: 0.000001  loss: 2.0958 (1.9767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0717 (8.0157)  time: 1.0370 (0.5301 -- 4.6956)  data: 0.4874 (0.0005 -- 4.1664)  max mem: 16735
Epoch: [97]  [ 80/160]  eta: 0:01:16  lr: 0.000025  min_lr: 0.000001  loss: 1.9133 (1.9561)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2024 (8.1006)  time: 0.8225 (0.5103 -- 3.9442)  data: 0.2812 (0.0003 -- 3.4386)  max mem: 16735
Epoch: [97]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000001  loss: 1.9254 (1.9399)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6014 (8.0297)  time: 0.9602 (0.5136 -- 4.0388)  data: 0.4150 (0.0003 -- 3.5033)  max mem: 16735
Epoch: [97]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 2.0408 (1.9475)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9069 (8.1227)  time: 0.8395 (0.5202 -- 3.5744)  data: 0.2901 (0.0003 -- 3.0474)  max mem: 16735
[2023-09-01 02:14:40,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:14:40,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:14:40,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:14:40,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 2.0273 (1.9571)  loss_scale: 65536.0000 (36718.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5030 (8.0955)  time: 0.7517 (0.5255 -- 2.6410)  data: 0.1971 (0.0007 -- 2.1018)  max mem: 16735
[2023-09-01 02:14:53,678] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15661
[2023-09-01 02:14:53,678] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15661
[2023-09-01 02:14:53,679] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:14:53,679] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:14:53,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.9985 (1.9534)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1858 (8.1542)  time: 0.6739 (0.5011 -- 3.3886)  data: 0.1419 (0.0002 -- 2.8147)  max mem: 16735
Epoch: [97] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.9985 (1.9479)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1858 (8.1542)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3451 (0.3451)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3290 (2.3290 -- 2.3290)  data: 2.1213 (2.1213 -- 2.1213)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6719 (0.8845)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4130 (0.1993 -- 2.3290)  data: 0.1939 (0.0006 -- 2.1213)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6833 (0.8108)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.2381)  time: 0.2239 (0.1683 -- 0.4493)  data: 0.0171 (0.0001 -- 0.2495)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7789 (0.8839)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (94.6058)  time: 0.2050 (0.1333 -- 0.4493)  data: 0.0168 (0.0001 -- 0.2495)  max mem: 16735
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 77.178 Acc@5 95.228 loss 0.880
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.80%
Epoch: [98]  [  0/160]  eta: 0:17:07  lr: 0.000025  min_lr: 0.000001  loss: 1.4572 (1.4572)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5163 (8.5163)  time: 6.4205 (6.4205 -- 6.4205)  data: 5.7304 (5.7304 -- 5.7304)  max mem: 16735
Epoch: [98]  [ 20/160]  eta: 0:02:38  lr: 0.000025  min_lr: 0.000001  loss: 1.8378 (1.8839)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6943 (8.1314)  time: 0.8684 (0.5303 -- 3.7772)  data: 0.3132 (0.0006 -- 3.2554)  max mem: 16735
Epoch: [98]  [ 40/160]  eta: 0:02:03  lr: 0.000025  min_lr: 0.000001  loss: 2.0181 (1.9540)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7359 (8.0516)  time: 0.9241 (0.5444 -- 4.0367)  data: 0.3294 (0.0007 -- 2.8071)  max mem: 16735
Epoch: [98]  [ 60/160]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000001  loss: 1.9672 (1.9313)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8193 (8.2465)  time: 0.8751 (0.5195 -- 6.3620)  data: 0.3315 (0.0001 -- 5.8204)  max mem: 16735
Epoch: [98]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000001  loss: 1.9674 (1.9474)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9705 (8.1822)  time: 0.9375 (0.5242 -- 3.9964)  data: 0.3881 (0.0004 -- 3.4667)  max mem: 16735
Epoch: [98]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000001  loss: 1.8759 (1.9143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8150 (8.1771)  time: 0.8290 (0.5258 -- 3.8271)  data: 0.2773 (0.0003 -- 3.3166)  max mem: 16735
[2023-09-01 02:16:55,750] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:16:55,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:16:55,756] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:16:55,757] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 2.0564 (1.9265)  loss_scale: 65536.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7574 (8.0916)  time: 0.9086 (0.5452 -- 3.6130)  data: 0.3526 (0.0005 -- 3.0742)  max mem: 16735
[2023-09-01 02:17:09,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15804
[2023-09-01 02:17:09,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15804
[2023-09-01 02:17:09,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:17:09,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:17:09,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 2.0862 (1.9351)  loss_scale: 32768.0000 (36021.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1347 (8.1633)  time: 0.8185 (0.5233 -- 5.1716)  data: 0.2680 (0.0001 -- 4.6580)  max mem: 16735
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 2.0200 (1.9368)  loss_scale: 32768.0000 (35635.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0262 (8.2001)  time: 0.6823 (0.5014 -- 2.8913)  data: 0.1640 (0.0001 -- 2.3391)  max mem: 16735
Epoch: [98] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 2.0200 (1.9431)  loss_scale: 32768.0000 (35635.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0262 (8.2001)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.3398 (0.3398)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6999 (2.6999 -- 2.6999)  data: 2.4277 (2.4277 -- 2.4277)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6834 (0.9169)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4368 (0.1986 -- 2.6999)  data: 0.2215 (0.0005 -- 2.4277)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6848 (0.8391)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (94.1799)  time: 0.2075 (0.1701 -- 0.2968)  data: 0.0053 (0.0001 -- 0.0935)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7448 (0.9121)  acc1: 77.7778 (75.1037)  acc5: 88.8889 (92.9461)  time: 0.1941 (0.1324 -- 0.2968)  data: 0.0050 (0.0001 -- 0.0935)  max mem: 16735
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 76.349 Acc@5 94.398 loss 0.903
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.80%
Epoch: [99]  [  0/160]  eta: 0:18:20  lr: 0.000025  min_lr: 0.000001  loss: 2.2166 (2.2166)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2926 (8.2926)  time: 6.8782 (6.8782 -- 6.8782)  data: 4.8647 (4.8647 -- 4.8647)  max mem: 16735
Epoch: [99]  [ 20/160]  eta: 0:02:37  lr: 0.000025  min_lr: 0.000001  loss: 2.0343 (1.9473)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9116 (8.5979)  time: 0.8341 (0.5346 -- 2.3367)  data: 0.0932 (0.0006 -- 1.5912)  max mem: 16735
Epoch: [99]  [ 40/160]  eta: 0:02:04  lr: 0.000025  min_lr: 0.000001  loss: 1.9831 (1.9525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8694 (8.4943)  time: 0.9552 (0.5204 -- 3.1240)  data: 0.0483 (0.0006 -- 0.7305)  max mem: 16735
Epoch: [99]  [ 60/160]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000001  loss: 2.0028 (1.9842)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7647 (8.3721)  time: 0.8219 (0.5357 -- 2.9192)  data: 0.2459 (0.0003 -- 2.3785)  max mem: 16735
Epoch: [99]  [ 80/160]  eta: 0:01:15  lr: 0.000025  min_lr: 0.000001  loss: 1.8496 (1.9390)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2302 (8.4509)  time: 0.8782 (0.5291 -- 4.8478)  data: 0.3319 (0.0004 -- 4.3146)  max mem: 16735
[2023-09-01 02:19:14,089] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:19:14,089] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:19:14,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:19:14,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [99]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000001  loss: 1.9070 (1.9248)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0304 (8.3488)  time: 0.8945 (0.5316 -- 4.1958)  data: 0.3480 (0.0005 -- 3.6786)  max mem: 16735
[2023-09-01 02:19:26,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15946
[2023-09-01 02:19:26,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:19:26,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 02:19:26,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15946
[2023-09-01 02:19:26,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000001  loss: 1.8775 (1.9099)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0560 (8.1455)  time: 0.8109 (0.5333 -- 4.4084)  data: 0.2573 (0.0003 -- 3.9081)  max mem: 16735
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.9723 (1.9111)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4235 (8.0893)  time: 0.8563 (0.5320 -- 2.7563)  data: 0.2753 (0.0007 -- 1.9174)  max mem: 16735
[2023-09-01 02:20:05,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=97, lr=[5.798217439836669e-07, 5.798217439836669e-07, 7.730956586448891e-07, 7.730956586448891e-07, 1.0307942115265189e-06, 1.0307942115265189e-06, 1.3743922820353586e-06, 1.3743922820353586e-06, 1.8325230427138114e-06, 1.8325230427138114e-06, 2.4433640569517485e-06, 2.4433640569517485e-06, 3.2578187426023315e-06, 3.2578187426023315e-06, 4.343758323469775e-06, 4.343758323469775e-06, 5.791677764626367e-06, 5.791677764626367e-06, 7.722237019501822e-06, 7.722237019501822e-06, 1.029631602600243e-05, 1.029631602600243e-05, 1.372842136800324e-05, 1.372842136800324e-05, 1.830456182400432e-05, 1.830456182400432e-05, 2.440608243200576e-05, 2.440608243200576e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 02:20:05,518] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=18.191567789441446, CurrSamplesPerSec=24.415226685992533, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.9735 (1.9044)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7461 (8.0366)  time: 0.6834 (0.5002 -- 3.4160)  data: 0.1584 (0.0002 -- 2.8800)  max mem: 16735
Epoch: [99] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.9735 (1.9311)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7461 (8.0366)
[2023-09-01 02:20:05,524] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-09-01 02:20:05,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-09-01 02:20:05,529] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-09-01 02:20:05,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-09-01 02:20:06,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-09-01 02:20:06,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3473 (0.3473)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3973 (2.3973 -- 2.3973)  data: 2.1557 (2.1557 -- 2.1557)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6774 (0.8981)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4355 (0.1947 -- 2.3973)  data: 0.2134 (0.0005 -- 2.1557)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6774 (0.8214)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (94.1799)  time: 0.2239 (0.1712 -- 0.4055)  data: 0.0161 (0.0001 -- 0.1822)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7209 (0.8963)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (93.3610)  time: 0.2077 (0.1326 -- 0.4055)  data: 0.0158 (0.0001 -- 0.1822)  max mem: 16735
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 75.934 Acc@5 94.606 loss 0.890
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 77.80%
Epoch: [100]  [  0/160]  eta: 0:20:56  lr: 0.000024  min_lr: 0.000001  loss: 2.1731 (2.1731)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3313 (7.3313)  time: 7.8560 (7.8560 -- 7.8560)  data: 5.5777 (5.5777 -- 5.5777)  max mem: 16735
Epoch: [100]  [ 20/160]  eta: 0:02:41  lr: 0.000024  min_lr: 0.000001  loss: 1.9861 (1.9511)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4698 (8.1447)  time: 0.8167 (0.5295 -- 3.6555)  data: 0.0018 (0.0002 -- 0.0069)  max mem: 16735
Epoch: [100]  [ 40/160]  eta: 0:02:11  lr: 0.000024  min_lr: 0.000001  loss: 1.8365 (1.8915)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5042 (8.3338)  time: 1.0387 (0.5376 -- 4.6912)  data: 0.0018 (0.0003 -- 0.0048)  max mem: 16735
Epoch: [100]  [ 60/160]  eta: 0:01:39  lr: 0.000024  min_lr: 0.000001  loss: 1.9655 (1.9265)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7273 (8.2811)  time: 0.7965 (0.5365 -- 3.5211)  data: 0.0018 (0.0002 -- 0.0116)  max mem: 16735
[2023-09-01 02:21:30,309] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:21:30,309] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:21:30,309] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:21:30,310] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:21:32,452] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16079
[2023-09-01 02:21:32,453] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16079
[2023-09-01 02:21:32,453] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:21:32,453] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:21:32,453] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [100]  [ 80/160]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000001  loss: 2.0082 (1.9331)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8928 (8.2180)  time: 0.9924 (0.5208 -- 4.9301)  data: 0.0012 (0.0003 -- 0.0032)  max mem: 16735
[2023-09-01 02:21:39,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16087
[2023-09-01 02:21:39,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16087
[2023-09-01 02:21:39,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:21:39,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:21:39,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [100/160]  eta: 0:00:55  lr: 0.000024  min_lr: 0.000001  loss: 2.0066 (1.9395)  loss_scale: 16384.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8291 (8.0657)  time: 0.6549 (0.5198 -- 2.5784)  data: 0.0020 (0.0003 -- 0.0146)  max mem: 16735
Epoch: [100]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.9396 (1.9401)  loss_scale: 16384.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6920 (8.0755)  time: 0.9752 (0.5285 -- 5.4375)  data: 0.0324 (0.0005 -- 0.6178)  max mem: 16735
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 2.0744 (1.9477)  loss_scale: 16384.0000 (27422.8652)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3206 (8.0185)  time: 0.8132 (0.5218 -- 4.1659)  data: 0.0015 (0.0004 -- 0.0028)  max mem: 16735
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 2.0824 (1.9597)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9361 (8.0282)  time: 0.6623 (0.5016 -- 2.6952)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [100] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 2.0824 (1.9703)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9361 (8.0282)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3478 (0.3478)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3192 (2.3192 -- 2.3192)  data: 2.0956 (2.0956 -- 2.0956)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6487 (0.8680)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4178 (0.2128 -- 2.3192)  data: 0.1954 (0.0006 -- 2.0956)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7039 (0.8120)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2287 (0.1679 -- 0.5737)  data: 0.0220 (0.0001 -- 0.3843)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7544 (0.8802)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2105 (0.1324 -- 0.5737)  data: 0.0217 (0.0001 -- 0.3843)  max mem: 16735
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 78.423 Acc@5 95.851 loss 0.871
Accuracy of the network on the 482 val images: 78.42%
[2023-09-01 02:22:44,679] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 02:22:44,681] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 02:22:44,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 02:22:44,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 02:22:46,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 02:22:46,093] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.42%
Epoch: [101]  [  0/160]  eta: 0:18:42  lr: 0.000024  min_lr: 0.000001  loss: 1.5889 (1.5889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8351 (8.8351)  time: 7.0159 (7.0159 -- 7.0159)  data: 6.4899 (6.4899 -- 6.4899)  max mem: 16735
Epoch: [101]  [ 20/160]  eta: 0:02:50  lr: 0.000024  min_lr: 0.000001  loss: 1.8905 (1.8845)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4334 (8.2953)  time: 0.9264 (0.5266 -- 4.0755)  data: 0.1281 (0.0005 -- 1.3591)  max mem: 16735
Epoch: [101]  [ 40/160]  eta: 0:02:05  lr: 0.000024  min_lr: 0.000001  loss: 1.8569 (1.8775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3185 (8.2322)  time: 0.8734 (0.5155 -- 4.6801)  data: 0.0028 (0.0004 -- 0.0149)  max mem: 16735
[2023-09-01 02:23:42,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:23:42,403] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:23:42,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 02:23:42,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [ 60/160]  eta: 0:01:42  lr: 0.000024  min_lr: 0.000001  loss: 1.9779 (1.8847)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4823 (8.0170)  time: 0.9694 (0.5307 -- 4.4388)  data: 0.0013 (0.0004 -- 0.0026)  max mem: 16735
Epoch: [101]  [ 80/160]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000001  loss: 1.9136 (1.8928)  loss_scale: 32768.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8013 (8.2690)  time: 0.8488 (0.5154 -- 3.8529)  data: 0.0007 (0.0003 -- 0.0015)  max mem: 16735
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000001  loss: 1.9497 (1.9091)  loss_scale: 32768.0000 (23683.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7843 (8.3599)  time: 0.8393 (0.5416 -- 3.1037)  data: 0.0018 (0.0003 -- 0.0055)  max mem: 16735
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.7183 (1.8890)  loss_scale: 32768.0000 (25185.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9522 (8.3934)  time: 0.8535 (0.5347 -- 3.7031)  data: 0.0019 (0.0003 -- 0.0062)  max mem: 16735
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.8911 (1.8939)  loss_scale: 32768.0000 (26260.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4940 (8.4247)  time: 0.8958 (0.5202 -- 3.8282)  data: 0.0008 (0.0003 -- 0.0018)  max mem: 16735
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.9277 (1.8920)  loss_scale: 32768.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9373 (8.3859)  time: 0.6170 (0.5029 -- 2.2143)  data: 0.0007 (0.0002 -- 0.0034)  max mem: 16735
Epoch: [101] Total time: 0:02:23 (0.8939 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.9277 (1.9434)  loss_scale: 32768.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9373 (8.3859)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3276 (0.3276)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3707 (2.3707 -- 2.3707)  data: 2.1624 (2.1624 -- 2.1624)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6625 (0.8931)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4403 (0.2025 -- 2.3707)  data: 0.2129 (0.0006 -- 2.1624)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6892 (0.8190)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (95.7672)  time: 0.2273 (0.1689 -- 0.4289)  data: 0.0159 (0.0001 -- 0.1615)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7427 (0.8944)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.4357)  time: 0.2108 (0.1326 -- 0.4289)  data: 0.0156 (0.0001 -- 0.1615)  max mem: 16735
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 76.763 Acc@5 95.851 loss 0.884
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.42%
Epoch: [102]  [  0/160]  eta: 0:21:01  lr: 0.000024  min_lr: 0.000001  loss: 2.0238 (2.0238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3236 (6.3236)  time: 7.8854 (7.8854 -- 7.8854)  data: 7.3437 (7.3437 -- 7.3437)  max mem: 16735
Epoch: [102]  [ 20/160]  eta: 0:02:41  lr: 0.000024  min_lr: 0.000001  loss: 1.8565 (1.9292)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5595 (8.1662)  time: 0.8170 (0.5324 -- 3.8078)  data: 0.2253 (0.0003 -- 3.2397)  max mem: 16735
[2023-09-01 02:25:44,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:25:44,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:25:44,483] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:25:44,483] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:25:49,104] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16350
[2023-09-01 02:25:49,104] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:25:49,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 02:25:49,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16350
[2023-09-01 02:25:49,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [102]  [ 40/160]  eta: 0:01:57  lr: 0.000024  min_lr: 0.000001  loss: 1.9897 (1.9261)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3568 (8.2675)  time: 0.8044 (0.5315 -- 1.8255)  data: 0.2285 (0.0004 -- 1.2831)  max mem: 16735
Epoch: [102]  [ 60/160]  eta: 0:01:35  lr: 0.000024  min_lr: 0.000001  loss: 1.9488 (1.9134)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7085 (8.2448)  time: 0.8972 (0.5301 -- 2.7226)  data: 0.3181 (0.0004 -- 2.2073)  max mem: 16735
Epoch: [102]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.9831 (1.9322)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6985 (8.2066)  time: 0.8643 (0.5257 -- 2.0027)  data: 0.2967 (0.0004 -- 1.4562)  max mem: 16735
Epoch: [102]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8702 (1.9283)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7606 (8.0868)  time: 0.9032 (0.5262 -- 2.9031)  data: 0.3108 (0.0004 -- 2.3670)  max mem: 16735
Epoch: [102]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1355 (1.9435)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2253 (8.1707)  time: 0.8782 (0.5279 -- 2.7098)  data: 0.1993 (0.0007 -- 2.1676)  max mem: 16735
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9482 (1.9368)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9696 (8.1182)  time: 0.9012 (0.5388 -- 2.5648)  data: 0.3403 (0.0004 -- 2.0181)  max mem: 16735
[2023-09-01 02:27:38,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:27:38,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:27:38,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:27:38,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8880 (1.9329)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5032 (8.1012)  time: 0.6394 (0.5021 -- 2.8202)  data: 0.1163 (0.0002 -- 2.3100)  max mem: 16735
Epoch: [102] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8880 (1.9390)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5032 (8.1012)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3403 (0.3403)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6449 (2.6449 -- 2.6449)  data: 2.3779 (2.3779 -- 2.3779)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6876 (0.8898)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4330 (0.1819 -- 2.6449)  data: 0.2182 (0.0004 -- 2.3779)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7132 (0.8139)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (94.7090)  time: 0.2071 (0.1690 -- 0.3290)  data: 0.0092 (0.0001 -- 0.1585)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7690 (0.8874)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (94.1909)  time: 0.1920 (0.1326 -- 0.3290)  data: 0.0089 (0.0001 -- 0.1585)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 76.971 Acc@5 95.228 loss 0.883
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 78.42%
Epoch: [103]  [  0/160]  eta: 0:19:09  lr: 0.000023  min_lr: 0.000001  loss: 1.7917 (1.7917)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7886 (3.7886)  time: 7.1872 (7.1872 -- 7.1872)  data: 5.8179 (5.8179 -- 5.8179)  max mem: 16735
Epoch: [103]  [ 20/160]  eta: 0:02:38  lr: 0.000023  min_lr: 0.000001  loss: 1.9660 (1.9935)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7789 (7.9477)  time: 0.8327 (0.5383 -- 3.4304)  data: 0.2187 (0.0003 -- 2.8977)  max mem: 16735
[2023-09-01 02:28:18,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16511
[2023-09-01 02:28:18,519] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:28:18,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16511
[2023-09-01 02:28:18,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:28:18,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [103]  [ 40/160]  eta: 0:01:57  lr: 0.000023  min_lr: 0.000001  loss: 2.0149 (1.9963)  loss_scale: 32768.0000 (57543.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1630 (7.8027)  time: 0.8201 (0.5290 -- 2.9189)  data: 0.2414 (0.0002 -- 2.3829)  max mem: 16735
Epoch: [103]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 1.9323 (1.9875)  loss_scale: 32768.0000 (49420.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5873 (7.7995)  time: 0.9151 (0.5279 -- 2.1873)  data: 0.2963 (0.0005 -- 1.6493)  max mem: 16735
Epoch: [103]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 1.8620 (1.9467)  loss_scale: 32768.0000 (45308.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0647 (7.9455)  time: 0.8544 (0.5340 -- 3.0457)  data: 0.0676 (0.0006 -- 0.8607)  max mem: 16735
Epoch: [103]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0173 (1.9547)  loss_scale: 32768.0000 (42825.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8994 (8.0185)  time: 0.9131 (0.5350 -- 2.5744)  data: 0.0559 (0.0003 -- 0.7858)  max mem: 16735
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0736 (1.9602)  loss_scale: 32768.0000 (41163.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7460 (8.0471)  time: 0.7671 (0.5296 -- 2.3262)  data: 0.1134 (0.0005 -- 1.7856)  max mem: 16735
Epoch: [103]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.8484 (1.9534)  loss_scale: 32768.0000 (39972.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9478 (8.0409)  time: 0.8417 (0.5402 -- 2.1170)  data: 0.1312 (0.0003 -- 1.5558)  max mem: 16735
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9729 (1.9522)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1379 (8.0818)  time: 0.7528 (0.5032 -- 2.3369)  data: 0.1235 (0.0001 -- 1.7218)  max mem: 16735
Epoch: [103] Total time: 0:02:20 (0.8789 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9729 (1.9191)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1379 (8.0818)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3354 (0.3354)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3707 (2.3707 -- 2.3707)  data: 2.1427 (2.1427 -- 2.1427)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6416 (0.8533)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4149 (0.2075 -- 2.3707)  data: 0.2007 (0.0005 -- 2.1427)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6631 (0.7837)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2219 (0.1709 -- 0.4468)  data: 0.0165 (0.0001 -- 0.2624)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7047 (0.8690)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.2076 (0.1327 -- 0.4468)  data: 0.0162 (0.0001 -- 0.2624)  max mem: 16735
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 76.763 Acc@5 95.643 loss 0.860
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.42%
[2023-09-01 02:30:21,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:30:21,886] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:30:21,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:30:21,887] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [104]  [  0/160]  eta: 0:19:08  lr: 0.000023  min_lr: 0.000001  loss: 2.3011 (2.3011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7132 (9.7132)  time: 7.1772 (7.1772 -- 7.1772)  data: 6.1796 (6.1796 -- 6.1796)  max mem: 16735
[2023-09-01 02:30:39,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16660
[2023-09-01 02:30:39,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16660
[2023-09-01 02:30:39,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:30:39,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:30:39,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [104]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 1.8802 (1.9208)  loss_scale: 65536.0000 (63975.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5799 (8.2921)  time: 0.8838 (0.5139 -- 3.6054)  data: 0.0602 (0.0004 -- 1.1712)  max mem: 16735
Epoch: [104]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 1.8572 (1.9266)  loss_scale: 32768.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5282 (8.6091)  time: 0.9316 (0.5251 -- 3.1634)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16735
Epoch: [104]  [ 60/160]  eta: 0:01:42  lr: 0.000023  min_lr: 0.000001  loss: 2.0662 (1.9550)  loss_scale: 32768.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6620 (8.5755)  time: 0.9491 (0.5361 -- 4.3009)  data: 0.0014 (0.0004 -- 0.0035)  max mem: 16735
Epoch: [104]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.8193 (1.9300)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3999 (8.4958)  time: 0.7646 (0.5290 -- 3.2730)  data: 0.0018 (0.0003 -- 0.0089)  max mem: 16735
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.8693 (1.9202)  loss_scale: 32768.0000 (39256.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9847 (8.5441)  time: 0.8332 (0.5242 -- 3.7366)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16735
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1955 (1.9554)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1627 (8.5604)  time: 0.7561 (0.5406 -- 2.2660)  data: 0.0878 (0.0007 -- 1.7061)  max mem: 16735
Epoch: [104]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.8441 (1.9563)  loss_scale: 32768.0000 (37415.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2854 (8.6169)  time: 0.8576 (0.5368 -- 2.5132)  data: 0.0697 (0.0007 -- 1.1512)  max mem: 16735
[2023-09-01 02:32:28,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:32:28,726] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:32:28,727] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:32:28,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9053 (1.9476)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4150 (8.5783)  time: 0.7002 (0.5041 -- 2.2612)  data: 0.0010 (0.0003 -- 0.0037)  max mem: 16735
Epoch: [104] Total time: 0:02:20 (0.8761 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9053 (1.9359)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4150 (8.5783)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3247 (0.3247)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3365 (2.3365 -- 2.3365)  data: 2.1153 (2.1153 -- 2.1153)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6658 (0.8776)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (93.9394)  time: 0.4254 (0.2042 -- 2.3365)  data: 0.2060 (0.0006 -- 2.1153)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6664 (0.8040)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (94.7090)  time: 0.2247 (0.1721 -- 0.3890)  data: 0.0179 (0.0001 -- 0.2034)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7454 (0.8842)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (94.1909)  time: 0.2083 (0.1329 -- 0.3890)  data: 0.0176 (0.0001 -- 0.2034)  max mem: 16735
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 77.178 Acc@5 95.228 loss 0.876
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.42%
Epoch: [105]  [  0/160]  eta: 0:24:29  lr: 0.000023  min_lr: 0.000001  loss: 1.6475 (1.6475)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2098 (8.2098)  time: 9.1854 (9.1854 -- 9.1854)  data: 8.6569 (8.6569 -- 8.6569)  max mem: 16735
[2023-09-01 02:33:06,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16816
[2023-09-01 02:33:06,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:33:06,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16816
[2023-09-01 02:33:06,753] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:33:06,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [105]  [ 20/160]  eta: 0:02:54  lr: 0.000022  min_lr: 0.000001  loss: 1.7395 (1.7540)  loss_scale: 65536.0000 (57734.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3180 (7.6582)  time: 0.8496 (0.5212 -- 3.7765)  data: 0.3034 (0.0003 -- 3.2523)  max mem: 16735
Epoch: [105]  [ 40/160]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000001  loss: 2.0529 (1.9047)  loss_scale: 32768.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8871 (7.7178)  time: 0.8271 (0.5414 -- 3.0565)  data: 0.2478 (0.0004 -- 2.5154)  max mem: 16735
Epoch: [105]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9103 (1.9061)  loss_scale: 32768.0000 (41362.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9438 (7.8443)  time: 0.8030 (0.5212 -- 3.2064)  data: 0.2463 (0.0006 -- 2.6607)  max mem: 16735
Epoch: [105]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.1472 (1.9419)  loss_scale: 32768.0000 (39240.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6793 (7.9007)  time: 0.8312 (0.5161 -- 3.2936)  data: 0.2564 (0.0005 -- 2.7781)  max mem: 16735
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 1.8355 (1.9114)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7547 (7.9542)  time: 1.0165 (0.5318 -- 3.9966)  data: 0.0163 (0.0004 -- 0.2979)  max mem: 16735
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.8731 (1.9054)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6529 (8.0150)  time: 0.7607 (0.5279 -- 3.2536)  data: 0.0019 (0.0002 -- 0.0052)  max mem: 16735
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8410 (1.9030)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8479 (8.0608)  time: 0.9336 (0.5313 -- 3.7318)  data: 0.0019 (0.0003 -- 0.0069)  max mem: 16735
[2023-09-01 02:34:57,370] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:34:57,370] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:34:57,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:34:57,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:35:00,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16950
[2023-09-01 02:35:00,001] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:35:00,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16950
[2023-09-01 02:35:00,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 02:35:00,001] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0122 (1.9064)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4843 (8.0678)  time: 0.6362 (0.4937 -- 2.7830)  data: 0.0009 (0.0002 -- 0.0034)  max mem: 16735
Epoch: [105] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0122 (1.9098)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4843 (8.0678)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3319 (0.3319)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1613 (2.1613 -- 2.1613)  data: 1.9585 (1.9585 -- 1.9585)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7086 (0.8979)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (97.9798)  time: 0.4141 (0.1950 -- 2.1613)  data: 0.2019 (0.0007 -- 1.9585)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7086 (0.8146)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2292 (0.1728 -- 0.4645)  data: 0.0210 (0.0001 -- 0.2538)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7539 (0.8978)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.8506)  time: 0.2140 (0.1332 -- 0.4645)  data: 0.0207 (0.0001 -- 0.2538)  max mem: 16735
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 75.311 Acc@5 95.643 loss 0.894
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 78.42%
Epoch: [106]  [  0/160]  eta: 0:15:57  lr: 0.000022  min_lr: 0.000001  loss: 1.4064 (1.4064)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1205 (7.1205)  time: 5.9869 (5.9869 -- 5.9869)  data: 5.4086 (5.4086 -- 5.4086)  max mem: 16735
Epoch: [106]  [ 20/160]  eta: 0:02:42  lr: 0.000022  min_lr: 0.000001  loss: 1.8439 (1.8467)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1243 (8.3065)  time: 0.9181 (0.5206 -- 3.1841)  data: 0.0342 (0.0004 -- 0.5291)  max mem: 16735
[2023-09-01 02:35:51,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=104, lr=[5.238378100529603e-07, 5.238378100529603e-07, 6.984504134039471e-07, 6.984504134039471e-07, 9.312672178719295e-07, 9.312672178719295e-07, 1.2416896238292394e-06, 1.2416896238292394e-06, 1.6555861651056525e-06, 1.6555861651056525e-06, 2.20744822014087e-06, 2.20744822014087e-06, 2.94326429352116e-06, 2.94326429352116e-06, 3.9243523913615465e-06, 3.9243523913615465e-06, 5.232469855148729e-06, 5.232469855148729e-06, 6.9766264735316386e-06, 6.9766264735316386e-06, 9.302168631375518e-06, 9.302168631375518e-06, 1.240289150850069e-05, 1.240289150850069e-05, 1.653718867800092e-05, 1.653718867800092e-05, 2.2049584904001228e-05, 2.2049584904001228e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 02:35:51,845] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=18.071080758827826, CurrSamplesPerSec=22.22600184849985, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [106]  [ 40/160]  eta: 0:01:57  lr: 0.000022  min_lr: 0.000001  loss: 2.1160 (1.9447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1618 (8.7770)  time: 0.7849 (0.5331 -- 4.4005)  data: 0.0086 (0.0003 -- 0.1358)  max mem: 16735
Epoch: [106]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000001  loss: 1.8939 (1.9290)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8199 (8.5530)  time: 1.0017 (0.5392 -- 4.2083)  data: 0.1244 (0.0005 -- 1.9079)  max mem: 16735
Epoch: [106]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 1.8007 (1.9033)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4263 (8.5599)  time: 0.7267 (0.5427 -- 3.2660)  data: 0.0279 (0.0006 -- 0.5253)  max mem: 16735
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.7891 (1.8810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8857 (8.4669)  time: 0.9487 (0.5291 -- 4.4879)  data: 0.0665 (0.0004 -- 1.2981)  max mem: 16735
[2023-09-01 02:37:02,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:37:02,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:37:02,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:37:02,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.7958 (1.8752)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1493 (8.3477)  time: 0.8520 (0.5303 -- 2.5315)  data: 0.1428 (0.0002 -- 1.9768)  max mem: 16735
[2023-09-01 02:37:10,256] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17088
[2023-09-01 02:37:10,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17088
[2023-09-01 02:37:10,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:37:10,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:37:10,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9888 (1.8914)  loss_scale: 32768.0000 (34859.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4952 (8.3702)  time: 1.0175 (0.5274 -- 3.7350)  data: 0.3171 (0.0004 -- 3.2129)  max mem: 16735
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0113 (1.9070)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5066 (8.3060)  time: 0.5878 (0.5021 -- 1.2149)  data: 0.0653 (0.0002 -- 0.6785)  max mem: 16735
Epoch: [106] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0113 (1.9203)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5066 (8.3060)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3211 (0.3211)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2193 (2.2193 -- 2.2193)  data: 2.0142 (2.0142 -- 2.0142)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6843 (0.8759)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4141 (0.2047 -- 2.2193)  data: 0.1944 (0.0007 -- 2.0142)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7115 (0.8133)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.8254)  time: 0.2293 (0.1692 -- 0.4234)  data: 0.0200 (0.0001 -- 0.2458)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7519 (0.8900)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (96.2656)  time: 0.2118 (0.1327 -- 0.4234)  data: 0.0197 (0.0001 -- 0.2458)  max mem: 16735
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 76.141 Acc@5 96.058 loss 0.879
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 78.42%
Epoch: [107]  [  0/160]  eta: 0:18:28  lr: 0.000022  min_lr: 0.000001  loss: 1.6923 (1.6923)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4162 (11.4162)  time: 6.9282 (6.9282 -- 6.9282)  data: 6.3496 (6.3496 -- 6.3496)  max mem: 16735
Epoch: [107]  [ 20/160]  eta: 0:02:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9185 (1.9299)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9621 (8.2747)  time: 0.8243 (0.5381 -- 3.4097)  data: 0.2231 (0.0005 -- 2.8842)  max mem: 16735
Epoch: [107]  [ 40/160]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000001  loss: 2.0795 (1.9684)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3457 (8.5061)  time: 0.9718 (0.5442 -- 4.1657)  data: 0.3814 (0.0004 -- 3.6155)  max mem: 16735
Epoch: [107]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9858 (1.9610)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4283 (8.3724)  time: 0.7897 (0.5320 -- 1.9031)  data: 0.0965 (0.0003 -- 1.3280)  max mem: 16735
Epoch: [107]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.8409 (1.9406)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9877 (8.3274)  time: 0.8991 (0.5103 -- 2.5700)  data: 0.0462 (0.0003 -- 0.5790)  max mem: 16735
[2023-09-01 02:39:06,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17210
[2023-09-01 02:39:06,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17210
[2023-09-01 02:39:06,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:39:06,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:39:06,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9210 (1.9540)  loss_scale: 16384.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9749 (8.3100)  time: 0.8456 (0.5333 -- 3.4288)  data: 0.0858 (0.0002 -- 0.9641)  max mem: 16735
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.8393 (1.9405)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8365 (8.3653)  time: 0.8485 (0.5206 -- 3.2284)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16735
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 2.0909 (1.9541)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6275 (8.2909)  time: 0.8518 (0.5346 -- 3.0402)  data: 0.0585 (0.0004 -- 1.1411)  max mem: 16735
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.8353 (1.9352)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8455 (8.3062)  time: 0.6687 (0.5008 -- 1.6781)  data: 0.0397 (0.0002 -- 0.7724)  max mem: 16735
Epoch: [107] Total time: 0:02:20 (0.8775 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.8353 (1.9083)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8455 (8.3062)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3183 (0.3183)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4351 (2.4351 -- 2.4351)  data: 2.2059 (2.2059 -- 2.2059)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6248 (0.8438)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4309 (0.2012 -- 2.4351)  data: 0.2144 (0.0005 -- 2.2059)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7060 (0.7883)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2260 (0.1697 -- 0.4936)  data: 0.0232 (0.0001 -- 0.3088)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7293 (0.8664)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.2105 (0.1337 -- 0.4936)  data: 0.0229 (0.0001 -- 0.3088)  max mem: 16735
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 78.216 Acc@5 95.643 loss 0.853
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 78.42%
Epoch: [108]  [  0/160]  eta: 0:18:24  lr: 0.000021  min_lr: 0.000001  loss: 2.0544 (2.0544)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2027 (6.2027)  time: 6.9036 (6.9036 -- 6.9036)  data: 5.7009 (5.7009 -- 5.7009)  max mem: 16735
Epoch: [108]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000001  loss: 1.9869 (1.9701)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4038 (8.4966)  time: 0.9252 (0.5388 -- 4.5034)  data: 0.2829 (0.0005 -- 3.9852)  max mem: 16735
Epoch: [108]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000001  loss: 2.0935 (2.0028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6347 (8.6445)  time: 0.8410 (0.5295 -- 2.8816)  data: 0.1215 (0.0002 -- 2.3687)  max mem: 16735
[2023-09-01 02:41:10,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:41:10,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 02:41:10,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:41:10,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [108]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000001  loss: 2.0271 (1.9933)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4941 (8.5304)  time: 0.9229 (0.5340 -- 4.2652)  data: 0.0166 (0.0006 -- 0.2978)  max mem: 16735
Epoch: [108]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 1.9479 (1.9875)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0928 (8.5393)  time: 0.8506 (0.5314 -- 4.1406)  data: 0.0019 (0.0002 -- 0.0067)  max mem: 16735
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.7949 (1.9642)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0201 (8.4538)  time: 0.9118 (0.5286 -- 4.0869)  data: 0.0022 (0.0005 -- 0.0159)  max mem: 16735
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 2.0229 (1.9723)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5168 (8.4491)  time: 0.8155 (0.5327 -- 3.5840)  data: 0.0019 (0.0002 -- 0.0134)  max mem: 16735
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 2.0628 (1.9854)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6218 (8.4792)  time: 0.7673 (0.5375 -- 2.3796)  data: 0.0025 (0.0006 -- 0.0164)  max mem: 16735
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7724 (1.9790)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7007 (8.3892)  time: 0.6967 (0.5016 -- 3.4859)  data: 0.0010 (0.0002 -- 0.0035)  max mem: 16735
Epoch: [108] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7724 (1.9477)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7007 (8.3892)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3305 (0.3305)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6445 (2.6445 -- 2.6445)  data: 2.3871 (2.3871 -- 2.3871)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6835 (0.8638)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4453 (0.1931 -- 2.6445)  data: 0.2252 (0.0004 -- 2.3871)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6945 (0.8002)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2106 (0.1693 -- 0.3191)  data: 0.0059 (0.0001 -- 0.0816)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7579 (0.8799)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.1962 (0.1330 -- 0.3191)  data: 0.0056 (0.0001 -- 0.0816)  max mem: 16735
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 76.971 Acc@5 96.058 loss 0.871
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 78.42%
Epoch: [109]  [  0/160]  eta: 0:16:45  lr: 0.000021  min_lr: 0.000000  loss: 2.0288 (2.0288)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8373 (5.8373)  time: 6.2847 (6.2847 -- 6.2847)  data: 5.5351 (5.5351 -- 5.5351)  max mem: 16735
Epoch: [109]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000000  loss: 1.9873 (1.8620)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8122 (8.2311)  time: 0.9552 (0.5273 -- 3.5819)  data: 0.0688 (0.0007 -- 1.3417)  max mem: 16735
[2023-09-01 02:43:11,487] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:43:11,487] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:43:11,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:43:11,488] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [109]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000000  loss: 2.1335 (1.9389)  loss_scale: 65536.0000 (43957.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4276 (7.9887)  time: 0.8302 (0.5272 -- 3.0745)  data: 0.0712 (0.0003 -- 0.7140)  max mem: 16735
[2023-09-01 02:43:32,423] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17493
[2023-09-01 02:43:32,423] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17493
[2023-09-01 02:43:32,423] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:43:32,423] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:43:32,423] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [109]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000000  loss: 2.0613 (1.9534)  loss_scale: 65536.0000 (46734.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1002 (8.1353)  time: 0.8920 (0.5358 -- 2.2211)  data: 0.1476 (0.0004 -- 1.6662)  max mem: 16735
Epoch: [109]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000000  loss: 2.0002 (1.9572)  loss_scale: 32768.0000 (43286.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4899 (8.0515)  time: 0.8635 (0.5329 -- 2.4682)  data: 0.2633 (0.0004 -- 1.9181)  max mem: 16735
Epoch: [109]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.9779 (1.9578)  loss_scale: 32768.0000 (41203.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0276 (8.1475)  time: 0.8623 (0.5359 -- 2.7931)  data: 0.2437 (0.0005 -- 2.2584)  max mem: 16735
Epoch: [109]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 1.8636 (1.9555)  loss_scale: 32768.0000 (39809.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5915 (8.0941)  time: 0.8366 (0.5301 -- 2.4811)  data: 0.1611 (0.0003 -- 1.9308)  max mem: 16735
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0114 (1.9672)  loss_scale: 32768.0000 (38810.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0198 (8.0975)  time: 0.8114 (0.5334 -- 2.1375)  data: 0.0526 (0.0004 -- 0.6363)  max mem: 16735
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.8991 (1.9613)  loss_scale: 32768.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3005 (8.0706)  time: 0.7769 (0.5023 -- 2.8198)  data: 0.0117 (0.0002 -- 0.2064)  max mem: 16735
Epoch: [109] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.8991 (1.9152)  loss_scale: 32768.0000 (38092.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3005 (8.0706)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3070 (0.3070)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4576 (2.4576 -- 2.4576)  data: 2.2441 (2.2441 -- 2.2441)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7232 (0.8754)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (97.9798)  time: 0.4280 (0.1979 -- 2.4576)  data: 0.2128 (0.0005 -- 2.2441)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7232 (0.8087)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2206 (0.1687 -- 0.4337)  data: 0.0159 (0.0001 -- 0.2174)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7534 (0.8808)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (96.2656)  time: 0.2065 (0.1327 -- 0.4337)  data: 0.0156 (0.0001 -- 0.2174)  max mem: 16735
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 76.556 Acc@5 96.058 loss 0.862
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 78.42%
Epoch: [110]  [  0/160]  eta: 0:21:46  lr: 0.000021  min_lr: 0.000000  loss: 2.4379 (2.4379)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9896 (6.9896)  time: 8.1639 (8.1639 -- 8.1639)  data: 3.6591 (3.6591 -- 3.6591)  max mem: 16735
Epoch: [110]  [ 20/160]  eta: 0:02:52  lr: 0.000021  min_lr: 0.000000  loss: 1.9309 (1.9858)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2868 (8.0494)  time: 0.8826 (0.5214 -- 3.6718)  data: 0.0192 (0.0004 -- 0.3563)  max mem: 16735
[2023-09-01 02:45:36,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:45:36,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:45:36,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:45:36,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:45:41,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17625
[2023-09-01 02:45:41,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17625
[2023-09-01 02:45:41,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:45:41,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:45:41,119] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [110]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000000  loss: 1.9879 (1.9896)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4464 (8.2122)  time: 0.9049 (0.5258 -- 3.3465)  data: 0.0015 (0.0002 -- 0.0028)  max mem: 16735
Epoch: [110]  [ 60/160]  eta: 0:01:38  lr: 0.000020  min_lr: 0.000000  loss: 1.8476 (1.9634)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1829 (8.0878)  time: 0.8219 (0.5243 -- 3.1062)  data: 0.0318 (0.0004 -- 0.5959)  max mem: 16735
Epoch: [110]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000000  loss: 2.0317 (1.9655)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9373 (8.1262)  time: 0.8798 (0.5272 -- 2.7434)  data: 0.1782 (0.0002 -- 2.2016)  max mem: 16735
Epoch: [110]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.7358 (1.9415)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4173 (8.3985)  time: 0.7956 (0.5291 -- 2.7551)  data: 0.0485 (0.0004 -- 0.9294)  max mem: 16735
Epoch: [110]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.8850 (1.9371)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7160 (8.1969)  time: 1.0413 (0.5253 -- 4.4408)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16735
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.9789 (1.9301)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0218 (8.1560)  time: 0.8001 (0.5238 -- 3.2297)  data: 0.0010 (0.0003 -- 0.0027)  max mem: 16735
[2023-09-01 02:47:29,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:47:29,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:47:29,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:47:29,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0901 (1.9375)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2934 (8.1165)  time: 0.6243 (0.5008 -- 2.5186)  data: 0.0009 (0.0002 -- 0.0031)  max mem: 16735
Epoch: [110] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0901 (1.9288)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2934 (8.1165)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3369 (0.3369)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1574 (2.1574 -- 2.1574)  data: 1.9483 (1.9483 -- 1.9483)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6579 (0.8546)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4086 (0.2042 -- 2.1574)  data: 0.1970 (0.0005 -- 1.9483)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6579 (0.8024)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2315 (0.1690 -- 0.4786)  data: 0.0278 (0.0001 -- 0.2692)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7465 (0.8752)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2168 (0.1333 -- 0.4786)  data: 0.0276 (0.0001 -- 0.2692)  max mem: 16735
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 77.593 Acc@5 96.058 loss 0.864
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [111]  [  0/160]  eta: 0:20:26  lr: 0.000020  min_lr: 0.000000  loss: 1.7764 (1.7764)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5373 (9.5373)  time: 7.6668 (7.6668 -- 7.6668)  data: 7.0865 (7.0865 -- 7.0865)  max mem: 16735
[2023-09-01 02:47:56,225] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17768
[2023-09-01 02:47:56,225] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:47:56,226] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17768
[2023-09-01 02:47:56,226] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:47:56,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [111]  [ 20/160]  eta: 0:02:48  lr: 0.000020  min_lr: 0.000000  loss: 1.8982 (1.9086)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3577 (8.3677)  time: 0.8835 (0.5260 -- 4.4693)  data: 0.3378 (0.0004 -- 3.9216)  max mem: 16735
Epoch: [111]  [ 40/160]  eta: 0:02:03  lr: 0.000020  min_lr: 0.000000  loss: 1.9329 (1.8667)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1663 (8.5252)  time: 0.8463 (0.5326 -- 3.7020)  data: 0.2968 (0.0002 -- 3.1531)  max mem: 16735
Epoch: [111]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8283 (1.8540)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6205 (8.7897)  time: 0.8180 (0.5330 -- 2.2565)  data: 0.1558 (0.0005 -- 1.7253)  max mem: 16735
Epoch: [111]  [ 80/160]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 1.9742 (1.8775)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1391 (8.6786)  time: 0.7968 (0.5395 -- 1.9149)  data: 0.1404 (0.0002 -- 1.4021)  max mem: 16735
Epoch: [111]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.8846 (1.8809)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8781 (8.4952)  time: 0.9476 (0.5227 -- 3.7034)  data: 0.3847 (0.0005 -- 3.1589)  max mem: 16735
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9118 (1.8743)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4214 (8.4998)  time: 0.8061 (0.5318 -- 3.7467)  data: 0.2466 (0.0004 -- 3.1530)  max mem: 16735
[2023-09-01 02:49:46,001] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:49:46,001] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:49:46,004] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:49:46,004] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 2.1412 (1.8937)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5885 (8.4639)  time: 0.9065 (0.5374 -- 2.9541)  data: 0.3405 (0.0004 -- 2.3775)  max mem: 16735
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9606 (1.8987)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6701 (8.4445)  time: 0.7143 (0.5038 -- 3.4090)  data: 0.1798 (0.0002 -- 2.8833)  max mem: 16735
Epoch: [111] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9606 (1.9249)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6701 (8.4445)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3410 (0.3410)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4459 (2.4459 -- 2.4459)  data: 2.1945 (2.1945 -- 2.1945)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6991 (0.8938)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4188 (0.1961 -- 2.4459)  data: 0.2006 (0.0005 -- 2.1945)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6991 (0.8144)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (97.3545)  time: 0.2166 (0.1689 -- 0.3623)  data: 0.0094 (0.0001 -- 0.1734)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7647 (0.8922)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (96.6805)  time: 0.2000 (0.1325 -- 0.3623)  data: 0.0091 (0.0001 -- 0.1734)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 76.971 Acc@5 96.473 loss 0.886
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 78.42%
Epoch: [112]  [  0/160]  eta: 0:23:20  lr: 0.000020  min_lr: 0.000000  loss: 1.5011 (1.5011)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1315 (11.1315)  time: 8.7520 (8.7520 -- 8.7520)  data: 8.2138 (8.2138 -- 8.2138)  max mem: 16735
[2023-09-01 02:50:20,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17924
[2023-09-01 02:50:20,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:50:20,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 02:50:20,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17924
[2023-09-01 02:50:20,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [112]  [ 20/160]  eta: 0:02:45  lr: 0.000020  min_lr: 0.000000  loss: 2.0049 (1.8933)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8985 (8.7979)  time: 0.8071 (0.5137 -- 1.9807)  data: 0.1431 (0.0003 -- 1.4251)  max mem: 16735
Epoch: [112]  [ 40/160]  eta: 0:02:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7564 (1.8666)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1634 (8.7590)  time: 0.8159 (0.5290 -- 1.9001)  data: 0.1422 (0.0002 -- 1.3707)  max mem: 16735
Epoch: [112]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0776 (1.8894)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (8.7623)  time: 0.8688 (0.5163 -- 2.4910)  data: 0.0575 (0.0004 -- 0.9180)  max mem: 16735
[2023-09-01 02:51:23,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=110, lr=[4.6819354482928105e-07, 4.6819354482928105e-07, 6.242580597723747e-07, 6.242580597723747e-07, 8.323440796964997e-07, 8.323440796964997e-07, 1.1097921062619995e-06, 1.1097921062619995e-06, 1.4797228083493328e-06, 1.4797228083493328e-06, 1.972963744465777e-06, 1.972963744465777e-06, 2.630618325954369e-06, 2.630618325954369e-06, 3.5074911012724923e-06, 3.5074911012724923e-06, 4.676654801696657e-06, 4.676654801696657e-06, 6.235539735595542e-06, 6.235539735595542e-06, 8.314052980794057e-06, 8.314052980794057e-06, 1.1085403974392075e-05, 1.1085403974392075e-05, 1.4780538632522767e-05, 1.4780538632522767e-05, 1.970738484336369e-05, 1.970738484336369e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 02:51:23,457] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.99176366443332, CurrSamplesPerSec=22.929059820883122, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [112]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.9726 (1.9055)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4977 (8.8214)  time: 0.8997 (0.5311 -- 2.8401)  data: 0.0790 (0.0002 -- 1.5393)  max mem: 16735
[2023-09-01 02:51:36,482] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18013
[2023-09-01 02:51:36,482] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18013
[2023-09-01 02:51:36,482] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:51:36,482] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 02:51:36,482] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [112]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 2.0777 (1.9283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0175 (8.7738)  time: 0.8954 (0.5318 -- 4.4474)  data: 0.0016 (0.0003 -- 0.0060)  max mem: 16735
Epoch: [112]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.7676 (1.9122)  loss_scale: 16384.0000 (30059.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2368 (8.7691)  time: 0.9267 (0.5274 -- 3.8426)  data: 0.0014 (0.0004 -- 0.0029)  max mem: 16735
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8813 (1.9095)  loss_scale: 16384.0000 (28120.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5202 (8.7618)  time: 0.8160 (0.5198 -- 3.2694)  data: 0.0012 (0.0005 -- 0.0027)  max mem: 16735
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.1631 (1.9205)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1622 (8.6062)  time: 0.6271 (0.5010 -- 2.6407)  data: 0.0005 (0.0002 -- 0.0017)  max mem: 16735
Epoch: [112] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.1631 (1.9073)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1622 (8.6062)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3217 (0.3217)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4684 (2.4684 -- 2.4684)  data: 2.2453 (2.2453 -- 2.2453)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6733 (0.8751)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4370 (0.2042 -- 2.4684)  data: 0.2143 (0.0006 -- 2.2453)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6852 (0.8112)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2202 (0.1693 -- 0.3425)  data: 0.0108 (0.0001 -- 0.0999)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7440 (0.8834)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.2050 (0.1330 -- 0.3425)  data: 0.0105 (0.0001 -- 0.0999)  max mem: 16735
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.873
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [113]  [  0/160]  eta: 0:19:45  lr: 0.000020  min_lr: 0.000000  loss: 0.9806 (0.9806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3444 (5.3444)  time: 7.4081 (7.4081 -- 7.4081)  data: 6.8595 (6.8595 -- 6.8595)  max mem: 16735
Epoch: [113]  [ 20/160]  eta: 0:02:40  lr: 0.000019  min_lr: 0.000000  loss: 2.1052 (2.0253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8323 (8.1101)  time: 0.8369 (0.5273 -- 3.8519)  data: 0.2767 (0.0008 -- 3.3319)  max mem: 16735
Epoch: [113]  [ 40/160]  eta: 0:02:02  lr: 0.000019  min_lr: 0.000000  loss: 1.9431 (1.9337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9786 (8.2300)  time: 0.8878 (0.5256 -- 3.4974)  data: 0.2953 (0.0004 -- 2.9388)  max mem: 16735
Epoch: [113]  [ 60/160]  eta: 0:01:34  lr: 0.000019  min_lr: 0.000000  loss: 1.8567 (1.9553)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2008 (8.3891)  time: 0.7909 (0.5286 -- 2.6433)  data: 0.1334 (0.0006 -- 1.6303)  max mem: 16735
[2023-09-01 02:53:37,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:53:37,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:53:37,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 02:53:37,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [113]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000000  loss: 2.0793 (1.9807)  loss_scale: 32768.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3544 (8.6723)  time: 0.8387 (0.5320 -- 1.9070)  data: 0.0385 (0.0004 -- 0.4214)  max mem: 16735
Epoch: [113]  [100/160]  eta: 0:00:54  lr: 0.000019  min_lr: 0.000000  loss: 1.9165 (1.9700)  loss_scale: 32768.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0866 (8.6247)  time: 0.8595 (0.5331 -- 2.2882)  data: 0.0531 (0.0002 -- 1.0248)  max mem: 16735
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7262 (1.9435)  loss_scale: 32768.0000 (24372.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7054 (8.5524)  time: 0.8735 (0.5320 -- 2.4927)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16735
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.9582 (1.9524)  loss_scale: 32768.0000 (25563.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6712 (8.4604)  time: 0.9893 (0.5305 -- 2.7920)  data: 0.0015 (0.0003 -- 0.0083)  max mem: 16735
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.1713 (1.9712)  loss_scale: 32768.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7965 (8.4464)  time: 0.6380 (0.5025 -- 1.8788)  data: 0.0008 (0.0002 -- 0.0031)  max mem: 16735
Epoch: [113] Total time: 0:02:21 (0.8824 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.1713 (1.9417)  loss_scale: 32768.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7965 (8.4464)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3259 (0.3259)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3599 (2.3599 -- 2.3599)  data: 2.1437 (2.1437 -- 2.1437)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6578 (0.8805)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4173 (0.1998 -- 2.3599)  data: 0.2093 (0.0006 -- 2.1437)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6839 (0.8074)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1692 -- 0.4223)  data: 0.0199 (0.0001 -- 0.2360)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7408 (0.8868)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2086 (0.1327 -- 0.4223)  data: 0.0195 (0.0001 -- 0.2360)  max mem: 16735
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.874
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [114]  [  0/160]  eta: 0:19:47  lr: 0.000019  min_lr: 0.000000  loss: 2.0789 (2.0789)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1618 (8.1618)  time: 7.4218 (7.4218 -- 7.4218)  data: 4.5635 (4.5635 -- 4.5635)  max mem: 16735
Epoch: [114]  [ 20/160]  eta: 0:02:42  lr: 0.000019  min_lr: 0.000000  loss: 1.9562 (1.9591)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9706 (8.2470)  time: 0.8500 (0.5353 -- 1.8825)  data: 0.1248 (0.0010 -- 1.3256)  max mem: 16735
[2023-09-01 02:55:41,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:55:41,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:55:41,468] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:55:41,468] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [114]  [ 40/160]  eta: 0:02:03  lr: 0.000019  min_lr: 0.000000  loss: 1.9811 (1.9880)  loss_scale: 65536.0000 (41559.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1416 (8.0068)  time: 0.8855 (0.5332 -- 3.8251)  data: 0.0362 (0.0004 -- 0.6969)  max mem: 16735
Epoch: [114]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7770 (1.9362)  loss_scale: 65536.0000 (49420.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8237 (7.9831)  time: 0.8257 (0.5172 -- 3.7376)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16735
[2023-09-01 02:56:17,313] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18312
[2023-09-01 02:56:17,313] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18312
[2023-09-01 02:56:17,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:56:17,314] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:56:17,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [114]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000000  loss: 1.9047 (1.9283)  loss_scale: 65536.0000 (49758.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7803 (8.3379)  time: 0.8423 (0.5231 -- 4.5297)  data: 0.0016 (0.0004 -- 0.0075)  max mem: 16735
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.9032 (1.9227)  loss_scale: 32768.0000 (46394.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9564 (8.2627)  time: 0.8664 (0.5291 -- 2.1580)  data: 0.0708 (0.0003 -- 0.9291)  max mem: 16735
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9376 (1.9188)  loss_scale: 32768.0000 (44142.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1184 (8.3181)  time: 0.8317 (0.5390 -- 2.0726)  data: 0.0178 (0.0004 -- 0.1927)  max mem: 16735
Epoch: [114]  [140/160]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 1.9830 (1.9351)  loss_scale: 32768.0000 (42528.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7706 (8.3135)  time: 0.8366 (0.5315 -- 2.1453)  data: 0.2858 (0.0004 -- 1.6151)  max mem: 16735
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.0531 (1.9401)  loss_scale: 32768.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0021 (8.2523)  time: 0.7242 (0.5009 -- 2.8035)  data: 0.1973 (0.0002 -- 2.2771)  max mem: 16735
Epoch: [114] Total time: 0:02:20 (0.8761 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.0531 (1.9275)  loss_scale: 32768.0000 (41369.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0021 (8.2523)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3054 (0.3054)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3224 (2.3224 -- 2.3224)  data: 2.1053 (2.1053 -- 2.1053)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6700 (0.8681)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4080 (0.1912 -- 2.3224)  data: 0.1991 (0.0007 -- 2.1053)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6895 (0.7955)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.7672)  time: 0.2120 (0.1691 -- 0.3199)  data: 0.0084 (0.0001 -- 0.0796)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7493 (0.8756)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.0207)  time: 0.1991 (0.1334 -- 0.3199)  data: 0.0081 (0.0001 -- 0.0796)  max mem: 16735
Val: Total time: 0:00:07 (0.2796 s / it)
* Acc@1 77.386 Acc@5 95.643 loss 0.859
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [115]  [  0/160]  eta: 0:21:12  lr: 0.000019  min_lr: 0.000000  loss: 1.7953 (1.7953)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3905 (7.3905)  time: 7.9543 (7.9543 -- 7.9543)  data: 5.2410 (5.2410 -- 5.2410)  max mem: 16735
Epoch: [115]  [ 20/160]  eta: 0:02:45  lr: 0.000019  min_lr: 0.000000  loss: 1.8967 (1.8743)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9682 (8.6405)  time: 0.8452 (0.5260 -- 2.7100)  data: 0.0268 (0.0003 -- 0.5084)  max mem: 16735
Epoch: [115]  [ 40/160]  eta: 0:02:05  lr: 0.000019  min_lr: 0.000000  loss: 1.8513 (1.8655)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6618 (8.3467)  time: 0.8938 (0.5219 -- 3.9220)  data: 0.0354 (0.0004 -- 0.6822)  max mem: 16735
[2023-09-01 02:58:21,294] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:58:21,294] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:58:21,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 02:58:21,296] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 02:58:35,063] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18460
[2023-09-01 02:58:35,063] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18460
[2023-09-01 02:58:35,063] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:58:35,063] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 02:58:35,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [115]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8056 (1.8531)  loss_scale: 65536.0000 (42974.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1664 (8.3814)  time: 0.8176 (0.5284 -- 2.6590)  data: 0.0020 (0.0004 -- 0.0069)  max mem: 16735
Epoch: [115]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000000  loss: 1.8830 (1.8756)  loss_scale: 32768.0000 (40454.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9803 (8.2666)  time: 0.8535 (0.5335 -- 3.1816)  data: 0.0880 (0.0002 -- 1.1237)  max mem: 16735
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.8712 (1.8820)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7046 (8.2636)  time: 0.8938 (0.5312 -- 3.7463)  data: 0.0330 (0.0002 -- 0.6210)  max mem: 16735
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9856 (1.8901)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8854 (8.3338)  time: 0.8580 (0.5251 -- 2.5561)  data: 0.1397 (0.0003 -- 1.4900)  max mem: 16735
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9832 (1.8917)  loss_scale: 32768.0000 (37183.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3938 (8.3709)  time: 0.8377 (0.5413 -- 2.2080)  data: 0.1456 (0.0006 -- 1.6685)  max mem: 16735
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9959 (1.8947)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2892 (8.3501)  time: 0.7273 (0.5029 -- 3.8566)  data: 0.2045 (0.0002 -- 3.3370)  max mem: 16735
Epoch: [115] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9959 (1.9034)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2892 (8.3501)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3062 (0.3062)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6296 (2.6296 -- 2.6296)  data: 2.3650 (2.3650 -- 2.3650)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6678 (0.8625)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4585 (0.2053 -- 2.6296)  data: 0.2384 (0.0006 -- 2.3650)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7069 (0.7929)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2195 (0.1695 -- 0.4573)  data: 0.0130 (0.0001 -- 0.2487)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7650 (0.8740)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.4357)  time: 0.2036 (0.1324 -- 0.4573)  data: 0.0128 (0.0001 -- 0.2487)  max mem: 16735
Val: Total time: 0:00:08 (0.2964 s / it)
* Acc@1 76.971 Acc@5 95.851 loss 0.859
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 78.42%
Epoch: [116]  [  0/160]  eta: 0:20:39  lr: 0.000018  min_lr: 0.000000  loss: 1.9909 (1.9909)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3701 (5.3701)  time: 7.7462 (7.7462 -- 7.7462)  data: 7.2049 (7.2049 -- 7.2049)  max mem: 16735
[2023-09-01 03:00:27,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18577
[2023-09-01 03:00:27,523] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:00:27,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18577
[2023-09-01 03:00:27,523] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:00:27,523] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [ 20/160]  eta: 0:02:40  lr: 0.000018  min_lr: 0.000000  loss: 1.8366 (1.9065)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2803 (8.2817)  time: 0.8151 (0.5364 -- 2.1671)  data: 0.2222 (0.0004 -- 1.6170)  max mem: 16735
Epoch: [116]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 1.8185 (1.9082)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4959 (8.0568)  time: 0.9473 (0.5133 -- 3.8394)  data: 0.4018 (0.0003 -- 3.3149)  max mem: 16735
Epoch: [116]  [ 60/160]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 1.9154 (1.9100)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6608 (8.2067)  time: 0.8768 (0.5314 -- 3.1621)  data: 0.3250 (0.0005 -- 2.6034)  max mem: 16735
Epoch: [116]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 1.9398 (1.9026)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7827 (8.2328)  time: 0.7953 (0.5239 -- 3.6427)  data: 0.2429 (0.0002 -- 3.1030)  max mem: 16735
Epoch: [116]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.9462 (1.9096)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0978 (8.2750)  time: 0.8742 (0.5229 -- 3.2529)  data: 0.3244 (0.0004 -- 2.7227)  max mem: 16735
Epoch: [116]  [120/160]  eta: 0:00:35  lr: 0.000018  min_lr: 0.000000  loss: 1.6294 (1.8785)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4263 (8.1867)  time: 0.7439 (0.5395 -- 1.8793)  data: 0.1343 (0.0003 -- 1.3415)  max mem: 16735
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9609 (1.8889)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7008 (8.1746)  time: 0.9200 (0.5342 -- 2.7908)  data: 0.3170 (0.0006 -- 2.2691)  max mem: 16735
[2023-09-01 03:02:18,459] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:02:18,459] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 03:02:18,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:02:18,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8182 (1.8767)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7791 (8.1353)  time: 0.7237 (0.5001 -- 1.9898)  data: 0.1306 (0.0002 -- 1.4598)  max mem: 16735
Epoch: [116] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8182 (1.8927)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7791 (8.1353)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3027 (0.3027)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4439 (2.4439 -- 2.4439)  data: 2.2433 (2.2433 -- 2.2433)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6438 (0.8624)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4193 (0.2037 -- 2.4439)  data: 0.2052 (0.0008 -- 2.2433)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6465 (0.7897)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2154 (0.1698 -- 0.2811)  data: 0.0082 (0.0001 -- 0.0777)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7523 (0.8682)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2012 (0.1329 -- 0.2811)  data: 0.0079 (0.0001 -- 0.0777)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.853
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [117]  [  0/160]  eta: 0:16:13  lr: 0.000018  min_lr: 0.000000  loss: 1.6785 (1.6785)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1422 (9.1422)  time: 6.0846 (6.0846 -- 6.0846)  data: 5.5643 (5.5643 -- 5.5643)  max mem: 16735
Epoch: [117]  [ 20/160]  eta: 0:02:41  lr: 0.000018  min_lr: 0.000000  loss: 2.0146 (1.8527)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7590 (8.5818)  time: 0.9094 (0.5355 -- 2.4594)  data: 0.1392 (0.0006 -- 1.1592)  max mem: 16735
Epoch: [117]  [ 40/160]  eta: 0:02:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9276 (1.8897)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8413 (8.5371)  time: 0.8376 (0.5305 -- 3.5672)  data: 0.2673 (0.0003 -- 3.0118)  max mem: 16735
Epoch: [117]  [ 60/160]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 2.0682 (1.9530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7004 (8.6210)  time: 0.9830 (0.5235 -- 4.7679)  data: 0.2950 (0.0004 -- 2.5398)  max mem: 16735
Epoch: [117]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 2.0161 (1.9626)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0430 (8.5445)  time: 0.8082 (0.5302 -- 3.3309)  data: 0.0017 (0.0001 -- 0.0046)  max mem: 16735
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 1.9489 (1.9535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2316 (8.5800)  time: 1.0161 (0.5166 -- 4.5019)  data: 0.0010 (0.0002 -- 0.0030)  max mem: 16735
[2023-09-01 03:04:23,963] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:04:23,964] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:04:23,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:04:23,965] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [117]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.8798 (1.9426)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1460 (8.5150)  time: 0.7608 (0.5325 -- 3.2411)  data: 0.0012 (0.0003 -- 0.0035)  max mem: 16735
[2023-09-01 03:04:30,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18841
[2023-09-01 03:04:30,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18841
[2023-09-01 03:04:30,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:04:30,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:04:30,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 03:04:36,976] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18850
[2023-09-01 03:04:36,977] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:04:36,977] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18850
[2023-09-01 03:04:36,977] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:04:36,977] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9555 (1.9388)  loss_scale: 16384.0000 (33116.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4556 (8.3460)  time: 0.8888 (0.5294 -- 3.0794)  data: 0.0017 (0.0005 -- 0.0045)  max mem: 16735
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9769 (1.9340)  loss_scale: 16384.0000 (31129.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8440 (8.3144)  time: 0.7180 (0.4992 -- 4.3069)  data: 0.0008 (0.0003 -- 0.0024)  max mem: 16735
Epoch: [117] Total time: 0:02:23 (0.9000 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9769 (1.9226)  loss_scale: 16384.0000 (31129.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8440 (8.3144)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3136 (0.3136)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4680 (2.4680 -- 2.4680)  data: 2.2246 (2.2246 -- 2.2246)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6344 (0.8497)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4266 (0.2024 -- 2.4680)  data: 0.2084 (0.0006 -- 2.2246)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6990 (0.7847)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2219 (0.1694 -- 0.4714)  data: 0.0177 (0.0001 -- 0.2830)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7589 (0.8586)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2064 (0.1325 -- 0.4714)  data: 0.0174 (0.0001 -- 0.2830)  max mem: 16735
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.847
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [118]  [  0/160]  eta: 0:23:24  lr: 0.000018  min_lr: 0.000000  loss: 1.4976 (1.4976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7767 (7.7767)  time: 8.7792 (8.7792 -- 8.7792)  data: 8.2265 (8.2265 -- 8.2265)  max mem: 16735
Epoch: [118]  [ 20/160]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000000  loss: 2.0479 (1.9720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0183 (8.0096)  time: 0.7903 (0.5280 -- 2.8828)  data: 0.2207 (0.0003 -- 2.3406)  max mem: 16735
Epoch: [118]  [ 40/160]  eta: 0:01:59  lr: 0.000018  min_lr: 0.000000  loss: 1.8010 (1.9451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2658 (7.5832)  time: 0.8151 (0.5380 -- 2.2230)  data: 0.1181 (0.0005 -- 1.6106)  max mem: 16735
Epoch: [118]  [ 60/160]  eta: 0:01:34  lr: 0.000018  min_lr: 0.000000  loss: 1.6894 (1.9410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4656 (8.1236)  time: 0.8406 (0.5175 -- 3.3109)  data: 0.0515 (0.0004 -- 0.9222)  max mem: 16735
Epoch: [118]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.9690 (1.9533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8835 (8.2174)  time: 0.9500 (0.5359 -- 3.7392)  data: 0.3939 (0.0006 -- 3.2187)  max mem: 16735
[2023-09-01 03:06:38,910] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:06:38,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 03:06:38,911] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:06:38,912] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.7972 (1.9383)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0107 (8.3686)  time: 0.8007 (0.5254 -- 2.4571)  data: 0.2173 (0.0004 -- 1.9308)  max mem: 16735
[2023-09-01 03:06:56,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=116, lr=[4.134526430749807e-07, 4.134526430749807e-07, 5.51270190766641e-07, 5.51270190766641e-07, 7.35026921022188e-07, 7.35026921022188e-07, 9.800358946962506e-07, 9.800358946962506e-07, 1.3067145262616674e-06, 1.3067145262616674e-06, 1.7422860350155567e-06, 1.7422860350155567e-06, 2.3230480466874088e-06, 2.3230480466874088e-06, 3.097397395583212e-06, 3.097397395583212e-06, 4.12986319411095e-06, 4.12986319411095e-06, 5.506484258814599e-06, 5.506484258814599e-06, 7.341979011752798e-06, 7.341979011752798e-06, 9.789305349003732e-06, 9.789305349003732e-06, 1.3052407132004975e-05, 1.3052407132004975e-05, 1.7403209509339967e-05, 1.7403209509339967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 03:06:56,614] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.933356117860004, CurrSamplesPerSec=22.979665054840297, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [118]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.9580 (1.9261)  loss_scale: 32768.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3451 (8.4433)  time: 0.8842 (0.5339 -- 4.1242)  data: 0.3325 (0.0004 -- 3.5885)  max mem: 16735
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9139 (1.9204)  loss_scale: 32768.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9937 (8.3691)  time: 0.8943 (0.5346 -- 4.2128)  data: 0.3437 (0.0004 -- 3.6725)  max mem: 16735
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9139 (1.8966)  loss_scale: 32768.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3412 (8.4353)  time: 0.7535 (0.5020 -- 3.0509)  data: 0.2268 (0.0002 -- 2.5061)  max mem: 16735
Epoch: [118] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9139 (1.9054)  loss_scale: 32768.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3412 (8.4353)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2769 (0.2769)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4974 (2.4974 -- 2.4974)  data: 2.2376 (2.2376 -- 2.2376)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6467 (0.8483)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4322 (0.2020 -- 2.4974)  data: 0.2061 (0.0006 -- 2.2376)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6760 (0.7792)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (97.3545)  time: 0.2138 (0.1681 -- 0.2765)  data: 0.0071 (0.0001 -- 0.0563)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7530 (0.8624)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (96.6805)  time: 0.1934 (0.1328 -- 0.2765)  data: 0.0059 (0.0001 -- 0.0563)  max mem: 16735
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 77.178 Acc@5 96.680 loss 0.854
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.42%
Epoch: [119]  [  0/160]  eta: 0:20:09  lr: 0.000017  min_lr: 0.000000  loss: 1.9920 (1.9920)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8327 (9.8327)  time: 7.5609 (7.5609 -- 7.5609)  data: 7.0423 (7.0423 -- 7.0423)  max mem: 16735
Epoch: [119]  [ 20/160]  eta: 0:02:39  lr: 0.000017  min_lr: 0.000000  loss: 1.9270 (1.9723)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8060 (9.5060)  time: 0.8153 (0.5204 -- 2.4724)  data: 0.1447 (0.0003 -- 1.9164)  max mem: 16735
Epoch: [119]  [ 40/160]  eta: 0:02:03  lr: 0.000017  min_lr: 0.000000  loss: 2.0619 (2.0275)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0621 (9.0512)  time: 0.9217 (0.5249 -- 4.4503)  data: 0.1541 (0.0003 -- 1.2243)  max mem: 16735
Epoch: [119]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 2.0624 (2.0085)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4495 (8.7667)  time: 0.9490 (0.5312 -- 4.1314)  data: 0.0013 (0.0002 -- 0.0039)  max mem: 16735
[2023-09-01 03:08:45,762] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:08:45,762] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:08:45,763] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:08:45,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:08:47,382] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19110
[2023-09-01 03:08:47,382] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19110
[2023-09-01 03:08:47,382] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:08:47,382] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:08:47,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [119]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 2.0694 (2.0176)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6190 (8.6005)  time: 0.8474 (0.5163 -- 3.7936)  data: 0.0015 (0.0001 -- 0.0120)  max mem: 16735
Epoch: [119]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 1.7054 (1.9694)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1608 (8.5714)  time: 0.9361 (0.5295 -- 3.3822)  data: 0.0015 (0.0003 -- 0.0039)  max mem: 16735
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.8016 (1.9479)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4877 (8.5678)  time: 0.7755 (0.5302 -- 3.0549)  data: 0.0017 (0.0004 -- 0.0088)  max mem: 16735
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 2.0475 (1.9558)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8267 (8.5672)  time: 0.8223 (0.5283 -- 2.5953)  data: 0.0221 (0.0004 -- 0.4118)  max mem: 16735
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8928 (1.9476)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7112 (8.5403)  time: 0.6627 (0.5017 -- 2.2258)  data: 0.0158 (0.0002 -- 0.1540)  max mem: 16735
Epoch: [119] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8928 (1.9392)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7112 (8.5403)
[2023-09-01 03:09:59,024] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-09-01 03:09:59,026] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-09-01 03:09:59,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-09-01 03:09:59,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-09-01 03:09:59,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-09-01 03:09:59,894] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2910 (0.2910)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5059 (2.5059 -- 2.5059)  data: 2.3017 (2.3017 -- 2.3017)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6594 (0.8445)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4178 (0.1914 -- 2.5059)  data: 0.2103 (0.0006 -- 2.3017)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6780 (0.7779)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2248 (0.1694 -- 0.6264)  data: 0.0221 (0.0001 -- 0.4276)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7225 (0.8594)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2133 (0.1326 -- 0.6264)  data: 0.0218 (0.0001 -- 0.4276)  max mem: 16735
Val: Total time: 0:00:08 (0.2964 s / it)
* Acc@1 76.556 Acc@5 95.851 loss 0.853
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 78.42%
Epoch: [120]  [  0/160]  eta: 0:18:53  lr: 0.000017  min_lr: 0.000000  loss: 1.9515 (1.9515)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5432 (5.5432)  time: 7.0839 (7.0839 -- 7.0839)  data: 6.4843 (6.4843 -- 6.4843)  max mem: 16735
Epoch: [120]  [ 20/160]  eta: 0:02:41  lr: 0.000017  min_lr: 0.000000  loss: 1.9884 (2.0238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2007 (8.4871)  time: 0.8542 (0.5179 -- 3.1316)  data: 0.0013 (0.0004 -- 0.0031)  max mem: 16735
[2023-09-01 03:10:48,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:10:48,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:10:48,869] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:10:48,870] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [120]  [ 40/160]  eta: 0:02:10  lr: 0.000017  min_lr: 0.000000  loss: 2.1200 (1.9960)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1156 (7.9980)  time: 1.0218 (0.5228 -- 4.3150)  data: 0.4594 (0.0005 -- 3.7629)  max mem: 16735
[2023-09-01 03:11:02,817] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19252
[2023-09-01 03:11:02,818] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:11:02,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19252
[2023-09-01 03:11:02,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:11:02,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [120]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000000  loss: 2.0236 (1.9682)  loss_scale: 65536.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1970 (8.5104)  time: 0.7825 (0.5184 -- 4.2608)  data: 0.2294 (0.0003 -- 3.7291)  max mem: 16735
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.8234 (1.9138)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5484 (8.3844)  time: 0.8060 (0.5288 -- 2.4945)  data: 0.0026 (0.0005 -- 0.0194)  max mem: 16735
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.9362 (1.9294)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8755 (8.3713)  time: 0.8460 (0.5312 -- 2.7018)  data: 0.0420 (0.0005 -- 0.8111)  max mem: 16735
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.8518 (1.9231)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7030 (8.3742)  time: 0.8837 (0.5269 -- 2.6097)  data: 0.0509 (0.0005 -- 0.9929)  max mem: 16735
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9134 (1.9100)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1393 (8.3834)  time: 0.8646 (0.5165 -- 3.7052)  data: 0.3162 (0.0003 -- 3.1765)  max mem: 16735
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9417 (1.9141)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0568 (8.4796)  time: 0.7404 (0.5030 -- 3.8066)  data: 0.2170 (0.0002 -- 3.2966)  max mem: 16735
Epoch: [120] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9417 (1.9107)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0568 (8.4796)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.2958 (0.2958)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6299 (2.6299 -- 2.6299)  data: 2.4086 (2.4086 -- 2.4086)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6834 (0.8337)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4369 (0.2003 -- 2.6299)  data: 0.2262 (0.0007 -- 2.4086)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6834 (0.7680)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.2381)  time: 0.2131 (0.1715 -- 0.3059)  data: 0.0103 (0.0001 -- 0.1230)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7265 (0.8468)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.0207)  time: 0.1991 (0.1322 -- 0.3059)  data: 0.0100 (0.0001 -- 0.1230)  max mem: 16735
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 76.763 Acc@5 95.436 loss 0.844
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.42%
Epoch: [121]  [  0/160]  eta: 0:21:23  lr: 0.000017  min_lr: 0.000000  loss: 2.2520 (2.2520)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9774 (9.9774)  time: 8.0216 (8.0216 -- 8.0216)  data: 6.5441 (6.5441 -- 6.5441)  max mem: 16735
Epoch: [121]  [ 20/160]  eta: 0:02:46  lr: 0.000017  min_lr: 0.000000  loss: 1.8818 (1.9259)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2667 (9.3305)  time: 0.8494 (0.5302 -- 2.3285)  data: 0.0334 (0.0004 -- 0.6235)  max mem: 16735
[2023-09-01 03:13:03,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:13:03,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:13:03,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:13:03,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:13:04,937] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19383
[2023-09-01 03:13:04,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:13:04,937] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19383
[2023-09-01 03:13:04,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:13:04,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [121]  [ 40/160]  eta: 0:02:11  lr: 0.000016  min_lr: 0.000000  loss: 1.6580 (1.8316)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0304 (8.7852)  time: 0.9893 (0.5237 -- 3.1712)  data: 0.0018 (0.0002 -- 0.0078)  max mem: 16735
Epoch: [121]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 1.9860 (1.8799)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1419 (8.5949)  time: 0.7000 (0.5349 -- 1.7250)  data: 0.0013 (0.0002 -- 0.0041)  max mem: 16735
Epoch: [121]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.8229 (1.8892)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4596 (8.5668)  time: 0.8201 (0.5246 -- 3.0855)  data: 0.0018 (0.0003 -- 0.0068)  max mem: 16735
Epoch: [121]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 1.7806 (1.8908)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4775 (8.5394)  time: 0.9145 (0.5253 -- 3.2923)  data: 0.2720 (0.0004 -- 2.7518)  max mem: 16735
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.0077 (1.9112)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3572 (8.6437)  time: 0.7749 (0.5364 -- 2.9223)  data: 0.2205 (0.0002 -- 2.3805)  max mem: 16735
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 2.0301 (1.9236)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3895 (8.7962)  time: 0.9181 (0.5236 -- 3.4553)  data: 0.3667 (0.0004 -- 2.9278)  max mem: 16735
[2023-09-01 03:14:54,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:14:54,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:14:54,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:14:54,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8382 (1.9141)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6014 (8.7525)  time: 0.6722 (0.5011 -- 2.8749)  data: 0.1497 (0.0002 -- 2.3483)  max mem: 16735
Epoch: [121] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8382 (1.9184)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6014 (8.7525)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3142 (0.3142)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4057 (2.4057 -- 2.4057)  data: 2.1912 (2.1912 -- 2.1912)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6315 (0.8321)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4174 (0.2017 -- 2.4057)  data: 0.2004 (0.0003 -- 2.1912)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6833 (0.7712)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2190 (0.1707 -- 0.3408)  data: 0.0131 (0.0001 -- 0.1280)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7358 (0.8536)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.0207)  time: 0.2030 (0.1329 -- 0.3408)  data: 0.0128 (0.0001 -- 0.1280)  max mem: 16735
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 76.971 Acc@5 95.643 loss 0.848
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 78.42%
Epoch: [122]  [  0/160]  eta: 0:22:40  lr: 0.000016  min_lr: 0.000000  loss: 2.1268 (2.1268)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9136 (7.9136)  time: 8.5054 (8.5054 -- 8.5054)  data: 7.9540 (7.9540 -- 7.9540)  max mem: 16735
[2023-09-01 03:15:17,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19524
[2023-09-01 03:15:17,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19524
[2023-09-01 03:15:17,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:15:17,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:15:17,106] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [122]  [ 20/160]  eta: 0:02:47  lr: 0.000016  min_lr: 0.000000  loss: 1.8911 (1.8956)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9872 (7.5245)  time: 0.8314 (0.5290 -- 3.9092)  data: 0.2785 (0.0004 -- 3.3638)  max mem: 16735
Epoch: [122]  [ 40/160]  eta: 0:02:09  lr: 0.000016  min_lr: 0.000000  loss: 2.0078 (1.9568)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0756 (8.1302)  time: 0.9494 (0.5188 -- 4.0718)  data: 0.3999 (0.0004 -- 3.4929)  max mem: 16735
Epoch: [122]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000000  loss: 1.9961 (1.9763)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5453 (8.2194)  time: 0.7942 (0.5205 -- 3.4757)  data: 0.2411 (0.0004 -- 2.9545)  max mem: 16735
Epoch: [122]  [ 80/160]  eta: 0:01:19  lr: 0.000016  min_lr: 0.000000  loss: 1.8507 (1.9751)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7177 (8.2832)  time: 1.0159 (0.5329 -- 5.1125)  data: 0.4631 (0.0003 -- 4.5846)  max mem: 16735
Epoch: [122]  [100/160]  eta: 0:00:57  lr: 0.000016  min_lr: 0.000000  loss: 1.9461 (1.9553)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3533 (8.2010)  time: 0.7998 (0.5101 -- 3.7340)  data: 0.2602 (0.0003 -- 3.2199)  max mem: 16735
Epoch: [122]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000000  loss: 2.0013 (1.9559)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6815 (8.2061)  time: 0.8526 (0.5201 -- 2.8400)  data: 0.3058 (0.0003 -- 2.3093)  max mem: 16735
[2023-09-01 03:17:09,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:17:09,193] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:17:09,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:17:09,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:17:09,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19654
[2023-09-01 03:17:09,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:17:09,734] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 03:17:09,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19654
[2023-09-01 03:17:09,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8027 (1.9400)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7980 (8.2077)  time: 0.7192 (0.5274 -- 2.8472)  data: 0.1502 (0.0003 -- 2.3254)  max mem: 16735
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7606 (1.9236)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5893 (8.2934)  time: 0.7523 (0.5043 -- 1.6376)  data: 0.1739 (0.0001 -- 1.1059)  max mem: 16735
Epoch: [122] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7606 (1.8890)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5893 (8.2934)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2998 (0.2998)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5350 (2.5350 -- 2.5350)  data: 2.2555 (2.2555 -- 2.2555)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6641 (0.8238)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4272 (0.2011 -- 2.5350)  data: 0.2089 (0.0003 -- 2.2555)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6641 (0.7614)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2078 (0.1698 -- 0.2409)  data: 0.0038 (0.0001 -- 0.0353)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7236 (0.8424)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.1932 (0.1333 -- 0.2409)  data: 0.0037 (0.0001 -- 0.0353)  max mem: 16735
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 77.178 Acc@5 95.851 loss 0.840
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.42%
Epoch: [123]  [  0/160]  eta: 0:20:58  lr: 0.000016  min_lr: 0.000000  loss: 1.3784 (1.3784)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8721 (7.8721)  time: 7.8648 (7.8648 -- 7.8648)  data: 7.3180 (7.3180 -- 7.3180)  max mem: 16735
Epoch: [123]  [ 20/160]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000000  loss: 2.0392 (1.9473)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5255 (8.7288)  time: 0.8274 (0.5331 -- 2.6921)  data: 0.1615 (0.0005 -- 1.6878)  max mem: 16735
Epoch: [123]  [ 40/160]  eta: 0:02:01  lr: 0.000016  min_lr: 0.000000  loss: 1.9841 (1.9494)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3487 (8.3902)  time: 0.8616 (0.5342 -- 3.7876)  data: 0.2724 (0.0001 -- 3.2640)  max mem: 16735
Epoch: [123]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 2.0829 (1.9683)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1432 (8.3824)  time: 0.9064 (0.5390 -- 2.5033)  data: 0.1093 (0.0003 -- 1.1286)  max mem: 16735
Epoch: [123]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000000  loss: 1.9273 (1.9524)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0182 (8.5123)  time: 0.8317 (0.5162 -- 3.5080)  data: 0.2868 (0.0003 -- 2.9699)  max mem: 16735
Epoch: [123]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 2.0202 (1.9676)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7982 (8.4298)  time: 0.7936 (0.5248 -- 1.8767)  data: 0.2354 (0.0005 -- 1.3090)  max mem: 16735
[2023-09-01 03:19:09,954] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:19:09,954] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:19:09,957] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:19:09,958] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:19:12,282] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19784
[2023-09-01 03:19:12,283] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19784
[2023-09-01 03:19:12,324] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:19:12,324] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:19:12,324] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [123]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.0363 (1.9828)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3803 (8.4161)  time: 0.8485 (0.5192 -- 2.4748)  data: 0.2973 (0.0006 -- 1.9198)  max mem: 16735
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8280 (1.9634)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2786 (8.4420)  time: 0.9054 (0.5388 -- 2.7175)  data: 0.3529 (0.0009 -- 2.1827)  max mem: 16735
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9689 (1.9477)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4575 (8.4616)  time: 0.7352 (0.5027 -- 3.5461)  data: 0.2125 (0.0002 -- 3.0386)  max mem: 16735
Epoch: [123] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9689 (1.9248)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4575 (8.4616)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2881 (0.2881)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1655 (2.1655 -- 2.1655)  data: 1.9635 (1.9635 -- 1.9635)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6758 (0.8506)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4428 (0.2019 -- 2.1655)  data: 0.2181 (0.0008 -- 1.9635)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6758 (0.7841)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2343 (0.1691 -- 0.4596)  data: 0.0220 (0.0001 -- 0.2379)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7347 (0.8646)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.2108 (0.1324 -- 0.4596)  data: 0.0152 (0.0001 -- 0.2379)  max mem: 16735
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 76.763 Acc@5 96.266 loss 0.860
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.42%
Epoch: [124]  [  0/160]  eta: 0:22:03  lr: 0.000016  min_lr: 0.000000  loss: 2.4383 (2.4383)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9540 (9.9540)  time: 8.2705 (8.2705 -- 8.2705)  data: 6.8878 (6.8878 -- 6.8878)  max mem: 16735
Epoch: [124]  [ 20/160]  eta: 0:02:50  lr: 0.000015  min_lr: 0.000000  loss: 1.9395 (1.8825)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8997 (8.5669)  time: 0.8622 (0.5246 -- 3.8241)  data: 0.0508 (0.0001 -- 0.9743)  max mem: 16735
Epoch: [124]  [ 40/160]  eta: 0:02:08  lr: 0.000015  min_lr: 0.000000  loss: 1.9519 (1.9110)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1448 (8.4623)  time: 0.9238 (0.5275 -- 3.5083)  data: 0.2680 (0.0004 -- 2.9736)  max mem: 16735
Epoch: [124]  [ 60/160]  eta: 0:01:40  lr: 0.000015  min_lr: 0.000000  loss: 1.9945 (1.9247)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3629 (8.2489)  time: 0.8771 (0.5213 -- 4.9748)  data: 0.3417 (0.0002 -- 4.4593)  max mem: 16735
[2023-09-01 03:21:19,467] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:21:19,467] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:21:19,468] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:21:19,468] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:21:22,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19919
[2023-09-01 03:21:22,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:21:22,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19919
[2023-09-01 03:21:22,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:21:22,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [124]  [ 80/160]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8102 (1.9086)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1676 (8.3612)  time: 0.8812 (0.5191 -- 4.4454)  data: 0.3330 (0.0001 -- 3.9095)  max mem: 16735
Epoch: [124]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 1.9245 (1.9177)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2135 (8.3270)  time: 0.8724 (0.5305 -- 5.3314)  data: 0.3196 (0.0001 -- 4.7965)  max mem: 16735
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.9757 (1.9231)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2515 (8.3307)  time: 0.7796 (0.5237 -- 3.1854)  data: 0.2292 (0.0003 -- 2.6707)  max mem: 16735
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.9445 (1.9155)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3699 (8.4044)  time: 0.8218 (0.5226 -- 4.2689)  data: 0.2729 (0.0004 -- 3.7262)  max mem: 16735
[2023-09-01 03:22:27,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=123, lr=[3.601696481824693e-07, 3.601696481824693e-07, 4.802261975766258e-07, 4.802261975766258e-07, 6.403015967688343e-07, 6.403015967688343e-07, 8.537354623584458e-07, 8.537354623584458e-07, 1.1383139498112611e-06, 1.1383139498112611e-06, 1.5177519330816814e-06, 1.5177519330816814e-06, 2.0236692441089086e-06, 2.0236692441089086e-06, 2.698225658811878e-06, 2.698225658811878e-06, 3.5976342117491708e-06, 3.5976342117491708e-06, 4.796845615665561e-06, 4.796845615665561e-06, 6.395794154220747e-06, 6.395794154220747e-06, 8.527725538960998e-06, 8.527725538960998e-06, 1.1370300718614664e-05, 1.1370300718614664e-05, 1.5160400958152884e-05, 1.5160400958152884e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 03:22:27,660] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.97463842946799, CurrSamplesPerSec=24.36501892305592, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.6354 (1.8998)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6477 (8.4325)  time: 0.7088 (0.5026 -- 2.3240)  data: 0.1785 (0.0002 -- 1.7894)  max mem: 16735
Epoch: [124] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.6354 (1.8705)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6477 (8.4325)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2766 (0.2766)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3535 (2.3535 -- 2.3535)  data: 2.1177 (2.1177 -- 2.1177)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6289 (0.8264)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4136 (0.2007 -- 2.3535)  data: 0.1975 (0.0010 -- 2.1177)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6447 (0.7616)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2289 (0.1689 -- 0.6890)  data: 0.0283 (0.0001 -- 0.5091)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7322 (0.8419)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2117 (0.1328 -- 0.6890)  data: 0.0280 (0.0001 -- 0.5091)  max mem: 16735
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 78.008 Acc@5 96.680 loss 0.838
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 78.42%
Epoch: [125]  [  0/160]  eta: 0:19:29  lr: 0.000015  min_lr: 0.000000  loss: 2.1570 (2.1570)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8791 (7.8791)  time: 7.3088 (7.3088 -- 7.3088)  data: 5.4139 (5.4139 -- 5.4139)  max mem: 16735
Epoch: [125]  [ 20/160]  eta: 0:02:52  lr: 0.000015  min_lr: 0.000000  loss: 1.9833 (1.9541)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3678 (7.6689)  time: 0.9293 (0.5197 -- 3.2960)  data: 0.0020 (0.0003 -- 0.0106)  max mem: 16735
Epoch: [125]  [ 40/160]  eta: 0:02:04  lr: 0.000015  min_lr: 0.000000  loss: 2.0601 (1.9381)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9290 (8.0845)  time: 0.8380 (0.5260 -- 3.6504)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16735
[2023-09-01 03:23:25,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:23:25,355] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:23:25,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:23:25,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [125]  [ 60/160]  eta: 0:01:40  lr: 0.000015  min_lr: 0.000000  loss: 1.9868 (1.9369)  loss_scale: 65536.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8986 (7.8341)  time: 0.9399 (0.5149 -- 3.5948)  data: 0.0012 (0.0003 -- 0.0048)  max mem: 16735
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 2.1532 (1.9753)  loss_scale: 65536.0000 (46117.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6982 (8.0774)  time: 0.7440 (0.5190 -- 3.6320)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16735
Epoch: [125]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.8977 (1.9703)  loss_scale: 65536.0000 (49963.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7961 (8.0775)  time: 0.8822 (0.5298 -- 3.9402)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16735
[2023-09-01 03:24:23,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20115
[2023-09-01 03:24:23,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:24:23,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20115
[2023-09-01 03:24:23,075] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:24:23,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.7240 (1.9270)  loss_scale: 65536.0000 (50912.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7575 (8.1636)  time: 0.9407 (0.5163 -- 3.5850)  data: 0.0018 (0.0004 -- 0.0034)  max mem: 16735
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.9843 (1.9275)  loss_scale: 32768.0000 (48338.6099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5967 (8.1311)  time: 0.8372 (0.5182 -- 2.3959)  data: 0.0249 (0.0003 -- 0.4765)  max mem: 16735
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8445 (1.9237)  loss_scale: 32768.0000 (46489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2573 (8.2503)  time: 0.7112 (0.5011 -- 2.7270)  data: 0.0008 (0.0002 -- 0.0050)  max mem: 16735
Epoch: [125] Total time: 0:02:23 (0.8954 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8445 (1.9118)  loss_scale: 32768.0000 (46489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2573 (8.2503)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2896 (0.2896)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3781 (2.3781 -- 2.3781)  data: 2.1507 (2.1507 -- 2.1507)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6783 (0.8439)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4081 (0.1906 -- 2.3781)  data: 0.1978 (0.0006 -- 2.1507)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6783 (0.7673)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2194 (0.1725 -- 0.3454)  data: 0.0142 (0.0001 -- 0.1541)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7236 (0.8437)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2053 (0.1333 -- 0.3454)  data: 0.0132 (0.0001 -- 0.1541)  max mem: 16735
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.841
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.42%
Epoch: [126]  [  0/160]  eta: 0:22:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8638 (1.8638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7828 (6.7828)  time: 8.4834 (8.4834 -- 8.4834)  data: 7.9187 (7.9187 -- 7.9187)  max mem: 16735
Epoch: [126]  [ 20/160]  eta: 0:02:58  lr: 0.000015  min_lr: 0.000000  loss: 1.8107 (1.7972)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1742 (8.8979)  time: 0.9165 (0.5270 -- 5.2101)  data: 0.3583 (0.0004 -- 4.6431)  max mem: 16735
Epoch: [126]  [ 40/160]  eta: 0:02:13  lr: 0.000015  min_lr: 0.000000  loss: 2.0058 (1.8274)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2675 (8.3930)  time: 0.9370 (0.5404 -- 3.1996)  data: 0.3854 (0.0003 -- 2.6702)  max mem: 16735
Epoch: [126]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000000  loss: 1.9555 (1.8424)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9930 (8.5387)  time: 0.8082 (0.5311 -- 3.1721)  data: 0.2571 (0.0003 -- 2.6407)  max mem: 16735
Epoch: [126]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000000  loss: 1.9584 (1.8779)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7376 (8.3894)  time: 0.8166 (0.5362 -- 2.2148)  data: 0.1873 (0.0003 -- 1.6429)  max mem: 16735
[2023-09-01 03:26:27,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:26:27,848] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:26:27,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:26:27,889] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:26:37,574] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20256
[2023-09-01 03:26:37,575] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:26:37,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20256
[2023-09-01 03:26:37,575] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:26:37,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.8887 (1.8779)  loss_scale: 65536.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5463 (8.3782)  time: 0.8661 (0.5364 -- 2.8870)  data: 0.0173 (0.0003 -- 0.3037)  max mem: 16735
Epoch: [126]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.9557 (1.8987)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7828 (8.3550)  time: 0.8200 (0.5327 -- 3.5027)  data: 0.0021 (0.0003 -- 0.0109)  max mem: 16735
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 2.0739 (1.9148)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6074 (8.2745)  time: 0.8646 (0.5327 -- 2.9987)  data: 0.2068 (0.0001 -- 2.4768)  max mem: 16735
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8773 (1.9199)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4464 (8.2270)  time: 0.7021 (0.5013 -- 2.5915)  data: 0.0157 (0.0002 -- 0.3004)  max mem: 16735
Epoch: [126] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8773 (1.9116)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4464 (8.2270)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3049 (0.3049)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4333 (2.4333 -- 2.4333)  data: 2.2184 (2.2184 -- 2.2184)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6419 (0.8515)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4214 (0.1981 -- 2.4333)  data: 0.2042 (0.0006 -- 2.2184)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6967 (0.7825)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1686 -- 0.3421)  data: 0.0100 (0.0001 -- 0.1687)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7682 (0.8620)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2009 (0.1329 -- 0.3421)  data: 0.0097 (0.0001 -- 0.1687)  max mem: 16735
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 77.593 Acc@5 96.680 loss 0.859
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [127]  [  0/160]  eta: 0:20:31  lr: 0.000014  min_lr: 0.000000  loss: 2.2320 (2.2320)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8372 (7.8372)  time: 7.6992 (7.6992 -- 7.6992)  data: 7.1628 (7.1628 -- 7.1628)  max mem: 16735
Epoch: [127]  [ 20/160]  eta: 0:02:36  lr: 0.000014  min_lr: 0.000000  loss: 2.0975 (1.9900)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5963 (7.8626)  time: 0.7875 (0.5272 -- 2.8250)  data: 0.1577 (0.0008 -- 2.0629)  max mem: 16735
Epoch: [127]  [ 40/160]  eta: 0:02:06  lr: 0.000014  min_lr: 0.000000  loss: 1.4884 (1.8182)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0309 (7.8894)  time: 0.9950 (0.5409 -- 4.9653)  data: 0.4382 (0.0003 -- 4.4453)  max mem: 16735
Epoch: [127]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9809 (1.8670)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4379 (8.1140)  time: 0.7811 (0.5304 -- 2.6528)  data: 0.0585 (0.0004 -- 1.0063)  max mem: 16735
[2023-09-01 03:28:41,388] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:28:41,388] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:28:41,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:28:41,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:28:45,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20391
[2023-09-01 03:28:45,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20391
[2023-09-01 03:28:45,240] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:28:45,240] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:28:45,240] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [127]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000000  loss: 1.9621 (1.8914)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8498 (8.0511)  time: 0.8717 (0.5389 -- 1.9145)  data: 0.1588 (0.0004 -- 1.3637)  max mem: 16735
Epoch: [127]  [100/160]  eta: 0:00:54  lr: 0.000014  min_lr: 0.000000  loss: 1.8767 (1.8924)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2203 (8.1370)  time: 0.7948 (0.5363 -- 2.8201)  data: 0.0437 (0.0005 -- 0.4230)  max mem: 16735
Epoch: [127]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.8508 (1.8953)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6576 (8.2307)  time: 1.0010 (0.5219 -- 3.1271)  data: 0.2195 (0.0003 -- 2.5334)  max mem: 16735
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.9474 (1.9047)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6090 (8.3094)  time: 0.7842 (0.5355 -- 3.1182)  data: 0.1844 (0.0003 -- 2.5785)  max mem: 16735
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9789 (1.9195)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0036 (8.3013)  time: 0.6809 (0.5027 -- 2.2518)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16735
Epoch: [127] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9789 (1.9278)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0036 (8.3013)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3062 (0.3062)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2051 (2.2051 -- 2.2051)  data: 2.0054 (2.0054 -- 2.0054)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6859 (0.8456)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4167 (0.1919 -- 2.2051)  data: 0.2077 (0.0004 -- 2.0054)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6859 (0.7784)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (97.3545)  time: 0.2309 (0.1697 -- 0.4938)  data: 0.0296 (0.0001 -- 0.3094)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7837 (0.8570)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (97.0954)  time: 0.2166 (0.1321 -- 0.4938)  data: 0.0290 (0.0001 -- 0.3094)  max mem: 16735
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 77.801 Acc@5 96.680 loss 0.856
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.42%
Epoch: [128]  [  0/160]  eta: 0:21:21  lr: 0.000014  min_lr: 0.000000  loss: 2.3315 (2.3315)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5396 (9.5396)  time: 8.0099 (8.0099 -- 8.0099)  data: 7.4553 (7.4553 -- 7.4553)  max mem: 16735
Epoch: [128]  [ 20/160]  eta: 0:02:47  lr: 0.000014  min_lr: 0.000000  loss: 1.6994 (1.8652)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7391 (7.7252)  time: 0.8575 (0.5232 -- 3.9017)  data: 0.3074 (0.0003 -- 3.3707)  max mem: 16735
[2023-09-01 03:30:50,813] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:30:50,813] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:30:50,816] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:30:50,817] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [128]  [ 40/160]  eta: 0:02:11  lr: 0.000014  min_lr: 0.000000  loss: 2.0196 (1.9578)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0776 (8.0565)  time: 0.9891 (0.5213 -- 4.7486)  data: 0.4452 (0.0004 -- 4.2142)  max mem: 16735
[2023-09-01 03:30:59,187] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20529
[2023-09-01 03:30:59,187] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:30:59,187] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20529
[2023-09-01 03:30:59,187] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:30:59,187] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [128]  [ 60/160]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000000  loss: 1.9657 (1.9490)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7225 (7.9293)  time: 0.8028 (0.5092 -- 4.0422)  data: 0.2612 (0.0001 -- 3.5146)  max mem: 16735
Epoch: [128]  [ 80/160]  eta: 0:01:19  lr: 0.000014  min_lr: 0.000000  loss: 1.8321 (1.9176)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6143 (8.1480)  time: 0.9706 (0.5263 -- 4.0292)  data: 0.4247 (0.0004 -- 3.5083)  max mem: 16735
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 2.0949 (1.9368)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1053 (8.1852)  time: 0.7605 (0.5244 -- 2.6332)  data: 0.2121 (0.0004 -- 2.1028)  max mem: 16735
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.9207 (1.9498)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5671 (8.2022)  time: 0.8607 (0.5206 -- 3.2709)  data: 0.3107 (0.0003 -- 2.7092)  max mem: 16735
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.9553 (1.9471)  loss_scale: 32768.0000 (34859.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6326 (8.2825)  time: 0.9087 (0.5186 -- 3.1993)  data: 0.3566 (0.0006 -- 2.6769)  max mem: 16735
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7356 (1.9327)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7577 (8.3233)  time: 0.6674 (0.5023 -- 2.6650)  data: 0.1464 (0.0002 -- 2.1543)  max mem: 16735
Epoch: [128] Total time: 0:02:23 (0.8991 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7356 (1.9224)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7577 (8.3233)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3049 (0.3049)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3607 (2.3607 -- 2.3607)  data: 2.1259 (2.1259 -- 2.1259)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6579 (0.8433)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4376 (0.2005 -- 2.3607)  data: 0.2200 (0.0008 -- 2.1259)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6844 (0.7770)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2201 (0.1688 -- 0.4933)  data: 0.0149 (0.0001 -- 0.2847)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7424 (0.8571)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2044 (0.1327 -- 0.4933)  data: 0.0146 (0.0001 -- 0.2847)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.853
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [129]  [  0/160]  eta: 0:20:32  lr: 0.000014  min_lr: 0.000000  loss: 1.1649 (1.1649)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0127 (13.0127)  time: 7.7034 (7.7034 -- 7.7034)  data: 7.1217 (7.1217 -- 7.1217)  max mem: 16735
[2023-09-01 03:32:58,935] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20657
[2023-09-01 03:32:58,935] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20657
[2023-09-01 03:32:58,935] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:32:58,935] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:32:58,935] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [129]  [ 20/160]  eta: 0:02:47  lr: 0.000014  min_lr: 0.000000  loss: 1.9846 (1.9958)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6452 (8.4947)  time: 0.8735 (0.5284 -- 3.1967)  data: 0.3181 (0.0005 -- 2.6508)  max mem: 16735
Epoch: [129]  [ 40/160]  eta: 0:02:08  lr: 0.000014  min_lr: 0.000000  loss: 2.0697 (1.9927)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9261 (8.3399)  time: 0.9309 (0.5258 -- 2.2096)  data: 0.3026 (0.0006 -- 1.5736)  max mem: 16735
Epoch: [129]  [ 60/160]  eta: 0:01:41  lr: 0.000014  min_lr: 0.000000  loss: 1.9197 (1.9848)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3906 (8.3968)  time: 0.9175 (0.5281 -- 2.9735)  data: 0.3770 (0.0003 -- 2.4422)  max mem: 16735
Epoch: [129]  [ 80/160]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000000  loss: 2.0567 (1.9824)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8069 (8.3565)  time: 0.8448 (0.5280 -- 2.6676)  data: 0.2910 (0.0007 -- 2.1207)  max mem: 16735
Epoch: [129]  [100/160]  eta: 0:00:57  lr: 0.000014  min_lr: 0.000000  loss: 1.8883 (1.9680)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3979 (8.2570)  time: 0.8963 (0.5212 -- 4.2869)  data: 0.3528 (0.0004 -- 3.7856)  max mem: 16735
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 2.0350 (1.9797)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1519 (8.2559)  time: 0.7987 (0.5360 -- 3.1059)  data: 0.2515 (0.0003 -- 2.5730)  max mem: 16735
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.9304 (1.9688)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8001 (8.2098)  time: 0.9360 (0.5259 -- 3.3748)  data: 0.3858 (0.0003 -- 2.8496)  max mem: 16735
[2023-09-01 03:34:54,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:34:54,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 03:34:54,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:34:54,236] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8315 (1.9554)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2759 (8.1957)  time: 0.6586 (0.5017 -- 2.3054)  data: 0.1383 (0.0002 -- 1.7444)  max mem: 16735
Epoch: [129] Total time: 0:02:24 (0.9022 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8315 (1.9584)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2759 (8.1957)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3074 (0.3074)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3939 (2.3939 -- 2.3939)  data: 2.1833 (2.1833 -- 2.1833)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6522 (0.8403)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4153 (0.1929 -- 2.3939)  data: 0.2037 (0.0007 -- 2.1833)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6849 (0.7822)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2228 (0.1684 -- 0.4634)  data: 0.0180 (0.0001 -- 0.2818)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7692 (0.8550)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (97.0954)  time: 0.2069 (0.1330 -- 0.4634)  data: 0.0176 (0.0001 -- 0.2818)  max mem: 16735
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.857
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.42%
Epoch: [130]  [  0/160]  eta: 0:16:24  lr: 0.000013  min_lr: 0.000000  loss: 2.3705 (2.3705)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6759 (10.6759)  time: 6.1561 (6.1561 -- 6.1561)  data: 5.5991 (5.5991 -- 5.5991)  max mem: 16735
Epoch: [130]  [ 20/160]  eta: 0:02:51  lr: 0.000013  min_lr: 0.000000  loss: 2.0718 (1.9814)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9481 (8.0950)  time: 0.9749 (0.5187 -- 3.7304)  data: 0.2491 (0.0004 -- 3.1673)  max mem: 16735
Epoch: [130]  [ 40/160]  eta: 0:02:10  lr: 0.000013  min_lr: 0.000000  loss: 2.0793 (1.9920)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6824 (7.9563)  time: 0.9458 (0.5265 -- 3.2957)  data: 0.3761 (0.0002 -- 2.7721)  max mem: 16735
Epoch: [130]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000000  loss: 1.8997 (1.9324)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3965 (7.8623)  time: 0.8695 (0.5251 -- 3.9058)  data: 0.3143 (0.0003 -- 3.3626)  max mem: 16735
Epoch: [130]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.8736 (1.9276)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9104 (7.9686)  time: 0.8449 (0.5363 -- 3.1759)  data: 0.2466 (0.0001 -- 2.6404)  max mem: 16735
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.9745 (1.9350)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6007 (7.9779)  time: 0.8024 (0.5212 -- 4.4625)  data: 0.2079 (0.0004 -- 3.9257)  max mem: 16735
[2023-09-01 03:36:57,805] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:36:57,805] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:36:57,808] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:36:57,809] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:37:03,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20920
[2023-09-01 03:37:03,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20920
[2023-09-01 03:37:03,534] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:37:03,534] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:37:03,534] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [130]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000000  loss: 1.9951 (1.9395)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7960 (7.9753)  time: 0.9470 (0.5339 -- 3.1812)  data: 0.4019 (0.0001 -- 2.6521)  max mem: 16735
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7405 (1.9203)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8075 (8.1072)  time: 0.8617 (0.5286 -- 3.6501)  data: 0.3138 (0.0005 -- 3.1068)  max mem: 16735
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7405 (1.9069)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1945 (8.1761)  time: 0.6485 (0.5006 -- 2.3773)  data: 0.1222 (0.0002 -- 1.8510)  max mem: 16735
Epoch: [130] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7405 (1.9251)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1945 (8.1761)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3152 (0.3152)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4038 (2.4038 -- 2.4038)  data: 2.1736 (2.1736 -- 2.1736)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6289 (0.8332)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4132 (0.1922 -- 2.4038)  data: 0.1988 (0.0005 -- 2.1736)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6743 (0.7790)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2133 (0.1710 -- 0.2802)  data: 0.0072 (0.0001 -- 0.0641)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7597 (0.8520)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.1966 (0.1332 -- 0.2802)  data: 0.0068 (0.0001 -- 0.0641)  max mem: 16735
Val: Total time: 0:00:07 (0.2838 s / it)
* Acc@1 77.178 Acc@5 96.058 loss 0.855
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.42%
Epoch: [131]  [  0/160]  eta: 0:20:26  lr: 0.000013  min_lr: 0.000000  loss: 1.4152 (1.4152)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8557 (5.8557)  time: 7.6648 (7.6648 -- 7.6648)  data: 7.1192 (7.1192 -- 7.1192)  max mem: 16735
Epoch: [131]  [ 20/160]  eta: 0:02:48  lr: 0.000013  min_lr: 0.000000  loss: 1.8383 (1.8562)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9779 (9.1057)  time: 0.8839 (0.5246 -- 3.0578)  data: 0.2277 (0.0004 -- 2.5311)  max mem: 16735
[2023-09-01 03:38:21,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=129, lr=[3.0888433446672044e-07, 3.0888433446672044e-07, 4.118457792889606e-07, 4.118457792889606e-07, 5.491277057186142e-07, 5.491277057186142e-07, 7.321702742914855e-07, 7.321702742914855e-07, 9.762270323886474e-07, 9.762270323886474e-07, 1.3016360431848632e-06, 1.3016360431848632e-06, 1.7355147242464842e-06, 1.7355147242464842e-06, 2.3140196323286457e-06, 2.3140196323286457e-06, 3.0853595097715273e-06, 3.0853595097715273e-06, 4.11381267969537e-06, 4.11381267969537e-06, 5.48508357292716e-06, 5.48508357292716e-06, 7.3134447639028795e-06, 7.3134447639028795e-06, 9.75125968520384e-06, 9.75125968520384e-06, 1.3001679580271786e-05, 1.3001679580271786e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 03:38:21,044] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=18.020018538302377, CurrSamplesPerSec=21.587001387904813, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [131]  [ 40/160]  eta: 0:02:10  lr: 0.000013  min_lr: 0.000000  loss: 2.0431 (1.9243)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1383 (8.8640)  time: 0.9583 (0.5196 -- 4.3829)  data: 0.3857 (0.0004 -- 3.3565)  max mem: 16735
Epoch: [131]  [ 60/160]  eta: 0:01:38  lr: 0.000013  min_lr: 0.000000  loss: 1.8976 (1.9287)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0597 (8.7129)  time: 0.7840 (0.5316 -- 3.0519)  data: 0.2345 (0.0003 -- 2.5229)  max mem: 16735
Epoch: [131]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.9687 (1.9308)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0710 (8.7828)  time: 0.9016 (0.5215 -- 3.9727)  data: 0.3520 (0.0004 -- 3.4335)  max mem: 16735
[2023-09-01 03:39:07,760] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:39:07,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:39:07,762] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:39:07,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:39:08,832] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21051
[2023-09-01 03:39:08,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:39:08,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 03:39:08,832] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21051
[2023-09-01 03:39:08,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [131]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.9354 (1.9303)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9329 (8.6884)  time: 0.7372 (0.5199 -- 4.1134)  data: 0.1794 (0.0004 -- 3.5622)  max mem: 16735
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.8393 (1.9073)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2656 (8.6519)  time: 0.8674 (0.5330 -- 3.0246)  data: 0.3137 (0.0004 -- 2.4942)  max mem: 16735
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7984 (1.9013)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6244 (8.6663)  time: 0.8308 (0.5268 -- 3.3112)  data: 0.2774 (0.0003 -- 2.7724)  max mem: 16735
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8336 (1.8846)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6570 (8.7313)  time: 0.7113 (0.5012 -- 2.9156)  data: 0.1727 (0.0002 -- 2.1172)  max mem: 16735
Epoch: [131] Total time: 0:02:20 (0.8791 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8336 (1.9005)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6570 (8.7313)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3090 (0.3090)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2588 (2.2588 -- 2.2588)  data: 2.0456 (2.0456 -- 2.0456)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.7119 (0.8589)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4201 (0.1942 -- 2.2588)  data: 0.2076 (0.0007 -- 2.0456)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.7119 (0.7886)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1699 -- 0.4579)  data: 0.0159 (0.0001 -- 0.2273)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7717 (0.8600)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2059 (0.1348 -- 0.4579)  data: 0.0156 (0.0001 -- 0.2273)  max mem: 16735
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.858
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [132]  [  0/160]  eta: 0:14:24  lr: 0.000013  min_lr: 0.000000  loss: 1.7721 (1.7721)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8392 (10.8392)  time: 5.4019 (5.4019 -- 5.4019)  data: 4.8578 (4.8578 -- 4.8578)  max mem: 16735
Epoch: [132]  [ 20/160]  eta: 0:02:46  lr: 0.000013  min_lr: 0.000000  loss: 1.9493 (1.9044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4478 (8.7969)  time: 0.9760 (0.5360 -- 2.5999)  data: 0.3990 (0.0003 -- 1.5778)  max mem: 16735
Epoch: [132]  [ 40/160]  eta: 0:02:01  lr: 0.000013  min_lr: 0.000000  loss: 1.9030 (1.9290)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7448 (8.4662)  time: 0.8223 (0.5308 -- 2.0951)  data: 0.2713 (0.0004 -- 1.5332)  max mem: 16735
[2023-09-01 03:41:11,674] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:41:11,674] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:41:11,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:41:11,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [132]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000000  loss: 1.9266 (1.9197)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3178 (8.2025)  time: 1.0373 (0.5218 -- 4.4540)  data: 0.4928 (0.0003 -- 3.9224)  max mem: 16735
Epoch: [132]  [ 80/160]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000000  loss: 2.0670 (1.9417)  loss_scale: 65536.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3709 (8.2185)  time: 0.7227 (0.5229 -- 2.3806)  data: 0.1752 (0.0005 -- 1.8741)  max mem: 16735
[2023-09-01 03:41:40,671] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21217
[2023-09-01 03:41:40,672] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:41:40,672] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21217
[2023-09-01 03:41:40,673] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:41:40,673] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8987 (1.9347)  loss_scale: 65536.0000 (44772.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0774 (8.2263)  time: 0.8136 (0.5303 -- 3.1320)  data: 0.2558 (0.0003 -- 2.5800)  max mem: 16735
Epoch: [132]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7977 (1.9252)  loss_scale: 32768.0000 (42787.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6236 (8.1808)  time: 0.9375 (0.5301 -- 2.7207)  data: 0.2694 (0.0003 -- 2.1961)  max mem: 16735
Epoch: [132]  [140/160]  eta: 0:00:17  lr: 0.000012  min_lr: 0.000000  loss: 2.0649 (1.9432)  loss_scale: 32768.0000 (41366.6950)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4724 (8.1607)  time: 0.7578 (0.5404 -- 2.3731)  data: 0.0154 (0.0003 -- 0.2822)  max mem: 16735
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.9770 (1.9405)  loss_scale: 32768.0000 (40345.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6988 (8.1340)  time: 0.8140 (0.5014 -- 2.9408)  data: 0.0009 (0.0002 -- 0.0028)  max mem: 16735
Epoch: [132] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.9770 (1.9059)  loss_scale: 32768.0000 (40345.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6988 (8.1340)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2981 (0.2981)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3386 (2.3386 -- 2.3386)  data: 2.0984 (2.0984 -- 2.0984)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6545 (0.8533)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4217 (0.2073 -- 2.3386)  data: 0.1998 (0.0006 -- 2.0984)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6877 (0.7784)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2202 (0.1695 -- 0.3155)  data: 0.0116 (0.0001 -- 0.0973)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7397 (0.8563)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2030 (0.1332 -- 0.3155)  data: 0.0113 (0.0001 -- 0.0973)  max mem: 16735
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 77.593 Acc@5 96.058 loss 0.851
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [133]  [  0/160]  eta: 0:16:46  lr: 0.000012  min_lr: 0.000000  loss: 1.9924 (1.9924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0448 (9.0448)  time: 6.2881 (6.2881 -- 6.2881)  data: 5.3643 (5.3643 -- 5.3643)  max mem: 16735
Epoch: [133]  [ 20/160]  eta: 0:02:43  lr: 0.000012  min_lr: 0.000000  loss: 1.8592 (1.7683)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5769 (7.9030)  time: 0.9129 (0.5223 -- 2.6401)  data: 0.3104 (0.0004 -- 1.9160)  max mem: 16735
Epoch: [133]  [ 40/160]  eta: 0:02:05  lr: 0.000012  min_lr: 0.000000  loss: 1.7412 (1.7638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5305 (8.2141)  time: 0.9161 (0.5322 -- 2.1629)  data: 0.2236 (0.0002 -- 1.6094)  max mem: 16735
Epoch: [133]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 2.1054 (1.8545)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2206 (8.2165)  time: 0.8174 (0.5335 -- 2.3496)  data: 0.2717 (0.0003 -- 1.8139)  max mem: 16735
[2023-09-01 03:43:44,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:43:44,433] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:43:44,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:43:44,434] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:43:51,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21353
[2023-09-01 03:43:51,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:43:51,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21353
[2023-09-01 03:43:51,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:43:51,614] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [133]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 2.1222 (1.9092)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8557 (8.2172)  time: 0.8869 (0.5074 -- 3.8951)  data: 0.0327 (0.0003 -- 0.6217)  max mem: 16735
Epoch: [133]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 1.8560 (1.9066)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6863 (8.2119)  time: 0.8372 (0.5262 -- 3.0407)  data: 0.0016 (0.0006 -- 0.0054)  max mem: 16735
Epoch: [133]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.9962 (1.9156)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5536 (8.3626)  time: 0.8376 (0.5323 -- 3.2483)  data: 0.0655 (0.0003 -- 1.2791)  max mem: 16735
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.9410 (1.9035)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7617 (8.3180)  time: 0.8885 (0.5286 -- 3.4514)  data: 0.0018 (0.0003 -- 0.0075)  max mem: 16735
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8450 (1.8928)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2867 (8.3282)  time: 0.7323 (0.5018 -- 2.4639)  data: 0.0011 (0.0001 -- 0.0075)  max mem: 16735
Epoch: [133] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8450 (1.9155)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2867 (8.3282)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2798 (0.2798)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4709 (2.4709 -- 2.4709)  data: 2.2605 (2.2605 -- 2.2605)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6307 (0.8221)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4299 (0.1984 -- 2.4709)  data: 0.2143 (0.0007 -- 2.2605)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6506 (0.7626)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2163 (0.1699 -- 0.2818)  data: 0.0092 (0.0001 -- 0.0842)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7247 (0.8349)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (97.0954)  time: 0.2004 (0.1326 -- 0.2818)  data: 0.0089 (0.0001 -- 0.0842)  max mem: 16735
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 78.423 Acc@5 96.680 loss 0.828
Accuracy of the network on the 482 val images: 78.42%
[2023-09-01 03:45:08,435] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 03:45:08,437] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 03:45:08,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 03:45:08,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 03:45:09,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 03:45:09,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.42%
Epoch: [134]  [  0/160]  eta: 0:21:37  lr: 0.000012  min_lr: 0.000000  loss: 2.3832 (2.3832)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4894 (7.4894)  time: 8.1114 (8.1114 -- 8.1114)  data: 5.1431 (5.1431 -- 5.1431)  max mem: 16735
Epoch: [134]  [ 20/160]  eta: 0:02:39  lr: 0.000012  min_lr: 0.000000  loss: 1.9184 (1.9114)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5300 (7.9662)  time: 0.7891 (0.5187 -- 3.3853)  data: 0.0900 (0.0003 -- 0.8284)  max mem: 16735
Epoch: [134]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 2.0166 (1.9113)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6232 (8.1561)  time: 0.8936 (0.5369 -- 2.5244)  data: 0.0021 (0.0003 -- 0.0148)  max mem: 16735
[2023-09-01 03:45:53,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:45:53,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:45:53,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:45:53,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:46:02,905] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21492
[2023-09-01 03:46:02,905] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:46:02,905] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 03:46:02,905] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21492
[2023-09-01 03:46:02,906] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:46:09,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-09-01 03:46:09,536] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:46:09,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-09-01 03:46:09,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 03:46:09,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8564 (1.8907)  loss_scale: 32768.0000 (37871.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0736 (7.9342)  time: 0.9073 (0.4970 -- 4.0332)  data: 0.0013 (0.0002 -- 0.0047)  max mem: 16735
Epoch: [134]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.8486 (1.8755)  loss_scale: 16384.0000 (32565.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6557 (8.0506)  time: 0.8758 (0.5330 -- 4.0124)  data: 0.0018 (0.0001 -- 0.0089)  max mem: 16735
Epoch: [134]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.8727 (1.8679)  loss_scale: 16384.0000 (29361.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2295 (8.1898)  time: 0.8847 (0.5318 -- 3.5886)  data: 0.0015 (0.0003 -- 0.0044)  max mem: 16735
Epoch: [134]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.8475 (1.8509)  loss_scale: 16384.0000 (27216.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8538 (8.1901)  time: 0.7853 (0.5283 -- 3.6140)  data: 0.0011 (0.0003 -- 0.0028)  max mem: 16735
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 2.0337 (1.8715)  loss_scale: 16384.0000 (25679.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0391 (8.1830)  time: 0.9355 (0.5254 -- 3.9913)  data: 0.0017 (0.0005 -- 0.0032)  max mem: 16735
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7296 (1.8653)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9978 (8.2935)  time: 0.6368 (0.5009 -- 2.1394)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16735
Epoch: [134] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7296 (1.8742)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9978 (8.2935)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2852 (0.2852)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3997 (2.3997 -- 2.3997)  data: 2.1688 (2.1688 -- 2.1688)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6122 (0.8141)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4182 (0.1877 -- 2.3997)  data: 0.1981 (0.0003 -- 2.1688)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6609 (0.7649)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2217 (0.1694 -- 0.4240)  data: 0.0119 (0.0001 -- 0.2252)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7334 (0.8374)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (96.2656)  time: 0.2035 (0.1332 -- 0.4240)  data: 0.0116 (0.0001 -- 0.2252)  max mem: 16735
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.830
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.42%
Epoch: [135]  [  0/160]  eta: 0:18:27  lr: 0.000012  min_lr: 0.000000  loss: 1.4800 (1.4800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5311 (6.5311)  time: 6.9205 (6.9205 -- 6.9205)  data: 6.2134 (6.2134 -- 6.2134)  max mem: 16735
Epoch: [135]  [ 20/160]  eta: 0:02:36  lr: 0.000012  min_lr: 0.000000  loss: 1.8808 (1.8694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6314 (9.3445)  time: 0.8289 (0.5329 -- 3.6781)  data: 0.2685 (0.0006 -- 3.1514)  max mem: 16735
[2023-09-01 03:48:11,487] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:48:11,487] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 03:48:11,490] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:48:11,491] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [135]  [ 40/160]  eta: 0:02:01  lr: 0.000012  min_lr: 0.000000  loss: 1.8296 (1.8762)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0551 (8.8463)  time: 0.9055 (0.5278 -- 2.9409)  data: 0.3553 (0.0004 -- 2.3982)  max mem: 16735
Epoch: [135]  [ 60/160]  eta: 0:01:35  lr: 0.000012  min_lr: 0.000000  loss: 1.9435 (1.8745)  loss_scale: 32768.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7040 (8.6268)  time: 0.8417 (0.5282 -- 2.0157)  data: 0.2262 (0.0004 -- 1.4388)  max mem: 16735
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 2.0379 (1.8849)  loss_scale: 32768.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4864 (8.4387)  time: 0.9098 (0.5202 -- 4.3638)  data: 0.0548 (0.0003 -- 1.0641)  max mem: 16735
Epoch: [135]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.8304 (1.8800)  loss_scale: 32768.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6823 (8.3804)  time: 0.9077 (0.5283 -- 4.4320)  data: 0.0015 (0.0006 -- 0.0036)  max mem: 16735
Epoch: [135]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.9986 (1.8880)  loss_scale: 32768.0000 (28841.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4717 (8.3865)  time: 0.8489 (0.5227 -- 5.0891)  data: 0.0015 (0.0003 -- 0.0028)  max mem: 16735
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7540 (1.8718)  loss_scale: 32768.0000 (29398.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3582 (8.4946)  time: 0.8476 (0.5359 -- 4.0730)  data: 0.0029 (0.0004 -- 0.0153)  max mem: 16735
[2023-09-01 03:49:59,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:49:59,887] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:49:59,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:49:59,887] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9690 (1.8841)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0562 (8.4526)  time: 0.6742 (0.5014 -- 2.9483)  data: 0.0247 (0.0003 -- 0.4781)  max mem: 16735
Epoch: [135] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9690 (1.8969)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0562 (8.4526)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2766 (0.2766)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2876 (2.2876 -- 2.2876)  data: 2.0446 (2.0446 -- 2.0446)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6127 (0.8013)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4268 (0.1997 -- 2.2876)  data: 0.2085 (0.0009 -- 2.0446)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6674 (0.7527)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2224 (0.1692 -- 0.4306)  data: 0.0152 (0.0001 -- 0.2136)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7408 (0.8264)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (96.2656)  time: 0.2057 (0.1329 -- 0.4306)  data: 0.0149 (0.0001 -- 0.2136)  max mem: 16735
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 78.216 Acc@5 96.266 loss 0.815
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 78.42%
Epoch: [136]  [  0/160]  eta: 0:18:47  lr: 0.000011  min_lr: 0.000000  loss: 2.2591 (2.2591)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0017 (9.0017)  time: 7.0456 (7.0456 -- 7.0456)  data: 6.5164 (6.5164 -- 6.5164)  max mem: 16735
[2023-09-01 03:50:26,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21771
[2023-09-01 03:50:26,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:50:26,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21771
[2023-09-01 03:50:26,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:50:26,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [136]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 1.8880 (1.9140)  loss_scale: 32768.0000 (49932.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4495 (9.2385)  time: 0.8658 (0.5343 -- 3.4616)  data: 0.1160 (0.0004 -- 1.4907)  max mem: 16735
Epoch: [136]  [ 40/160]  eta: 0:02:02  lr: 0.000011  min_lr: 0.000000  loss: 1.9495 (1.9421)  loss_scale: 32768.0000 (41559.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2867 (8.7231)  time: 0.8678 (0.5262 -- 4.2413)  data: 0.0953 (0.0007 -- 0.7348)  max mem: 16735
Epoch: [136]  [ 60/160]  eta: 0:01:34  lr: 0.000011  min_lr: 0.000000  loss: 2.0813 (1.9905)  loss_scale: 32768.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4931 (8.3896)  time: 0.8059 (0.5235 -- 2.2339)  data: 0.1207 (0.0002 -- 1.7072)  max mem: 16735
Epoch: [136]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000000  loss: 1.6814 (1.9394)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5763 (8.3109)  time: 0.9154 (0.5288 -- 2.3596)  data: 0.0684 (0.0002 -- 0.8144)  max mem: 16735
Epoch: [136]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.9596 (1.9147)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3936 (8.3349)  time: 0.9210 (0.5273 -- 4.3521)  data: 0.0235 (0.0003 -- 0.4424)  max mem: 16735
Epoch: [136]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9338 (1.9358)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2916 (8.3068)  time: 0.8027 (0.5358 -- 3.1727)  data: 0.0394 (0.0005 -- 0.7604)  max mem: 16735
[2023-09-01 03:52:14,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:52:14,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:52:14,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:52:14,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [136]  [140/160]  eta: 0:00:17  lr: 0.000011  min_lr: 0.000000  loss: 1.7959 (1.9232)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5651 (8.3937)  time: 0.7628 (0.5263 -- 2.2191)  data: 0.1314 (0.0004 -- 1.6868)  max mem: 16735
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9999 (1.9267)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0050 (8.3490)  time: 0.7537 (0.5035 -- 3.4304)  data: 0.2153 (0.0002 -- 2.8781)  max mem: 16735
Epoch: [136] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9999 (1.9095)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0050 (8.3490)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2776 (0.2776)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4610 (2.4610 -- 2.4610)  data: 2.2302 (2.2302 -- 2.2302)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6340 (0.8215)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4250 (0.2016 -- 2.4610)  data: 0.2079 (0.0006 -- 2.2302)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6739 (0.7657)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2157 (0.1700 -- 0.3425)  data: 0.0108 (0.0001 -- 0.1564)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7468 (0.8411)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (96.6805)  time: 0.1980 (0.1330 -- 0.3425)  data: 0.0101 (0.0001 -- 0.1564)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.831
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [137]  [  0/160]  eta: 0:17:21  lr: 0.000011  min_lr: 0.000000  loss: 2.2922 (2.2922)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1881 (10.1881)  time: 6.5080 (6.5080 -- 6.5080)  data: 5.8757 (5.8757 -- 5.8757)  max mem: 16735
[2023-09-01 03:53:00,801] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21937
[2023-09-01 03:53:00,802] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:53:00,807] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21937
[2023-09-01 03:53:00,808] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:53:00,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [137]  [ 20/160]  eta: 0:02:51  lr: 0.000011  min_lr: 0.000000  loss: 1.9360 (2.0251)  loss_scale: 65536.0000 (59294.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3385 (7.8167)  time: 0.9575 (0.5275 -- 3.3124)  data: 0.0283 (0.0002 -- 0.4265)  max mem: 16735
Epoch: [137]  [ 40/160]  eta: 0:02:04  lr: 0.000011  min_lr: 0.000000  loss: 1.8713 (1.9634)  loss_scale: 32768.0000 (46354.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8359 (7.6809)  time: 0.8522 (0.5294 -- 3.3372)  data: 0.0484 (0.0004 -- 0.7597)  max mem: 16735
Epoch: [137]  [ 60/160]  eta: 0:01:38  lr: 0.000011  min_lr: 0.000000  loss: 1.8605 (1.9466)  loss_scale: 32768.0000 (41900.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7843 (7.8591)  time: 0.8723 (0.5207 -- 2.5215)  data: 0.0667 (0.0003 -- 1.3093)  max mem: 16735
[2023-09-01 03:53:52,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=136, lr=[2.6011623907340286e-07, 2.6011623907340286e-07, 3.468216520978705e-07, 3.468216520978705e-07, 4.6242886946382737e-07, 4.6242886946382737e-07, 6.165718259517698e-07, 6.165718259517698e-07, 8.220957679356931e-07, 8.220957679356931e-07, 1.096127690580924e-06, 1.096127690580924e-06, 1.461503587441232e-06, 1.461503587441232e-06, 1.9486714499216427e-06, 1.9486714499216427e-06, 2.598228599895524e-06, 2.598228599895524e-06, 3.4643047998606983e-06, 3.4643047998606983e-06, 4.619073066480931e-06, 4.619073066480931e-06, 6.158764088641242e-06, 6.158764088641242e-06, 8.211685451521655e-06, 8.211685451521655e-06, 1.0948913935362207e-05, 1.0948913935362207e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 03:53:52,763] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.969344879906828, CurrSamplesPerSec=21.830732136188427, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [137]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000000  loss: 1.8970 (1.9391)  loss_scale: 32768.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1295 (7.9763)  time: 0.8152 (0.5207 -- 4.0493)  data: 0.0014 (0.0005 -- 0.0028)  max mem: 16735
Epoch: [137]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 2.0128 (1.9495)  loss_scale: 32768.0000 (38283.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2923 (8.0319)  time: 0.9118 (0.5219 -- 4.0699)  data: 0.0017 (0.0002 -- 0.0047)  max mem: 16735
Epoch: [137]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9203 (1.9430)  loss_scale: 32768.0000 (37371.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0879 (8.1024)  time: 0.8419 (0.5124 -- 3.7942)  data: 0.0068 (0.0003 -- 0.1140)  max mem: 16735
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8668 (1.9280)  loss_scale: 32768.0000 (36718.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0375 (8.0927)  time: 0.9260 (0.5243 -- 3.8475)  data: 0.0012 (0.0003 -- 0.0028)  max mem: 16735
[2023-09-01 03:54:52,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:54:52,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:54:52,848] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:54:52,848] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:54:56,956] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22074
[2023-09-01 03:54:56,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:54:56,956] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22074
[2023-09-01 03:54:56,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:54:56,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 2.1911 (1.9458)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5972 (8.1433)  time: 0.6536 (0.4889 -- 3.1758)  data: 0.0005 (0.0002 -- 0.0014)  max mem: 16735
Epoch: [137] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 2.1911 (1.9104)  loss_scale: 32768.0000 (37888.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5972 (8.1433)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2943 (0.2943)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2304 (2.2304 -- 2.2304)  data: 2.0158 (2.0158 -- 2.0158)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6140 (0.8292)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4153 (0.2036 -- 2.2304)  data: 0.1961 (0.0006 -- 2.0158)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6657 (0.7664)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2299 (0.1689 -- 0.4016)  data: 0.0221 (0.0001 -- 0.2203)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7612 (0.8455)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (97.0954)  time: 0.2120 (0.1319 -- 0.4016)  data: 0.0218 (0.0001 -- 0.2203)  max mem: 16735
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 78.423 Acc@5 96.680 loss 0.838
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 78.42%
Epoch: [138]  [  0/160]  eta: 0:21:10  lr: 0.000011  min_lr: 0.000000  loss: 2.2650 (2.2650)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4461 (9.4461)  time: 7.9413 (7.9413 -- 7.9413)  data: 7.3824 (7.3824 -- 7.3824)  max mem: 16735
Epoch: [138]  [ 20/160]  eta: 0:02:45  lr: 0.000011  min_lr: 0.000000  loss: 1.8140 (1.8388)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2789 (8.7064)  time: 0.8410 (0.5306 -- 3.2234)  data: 0.2929 (0.0004 -- 2.6771)  max mem: 16735
Epoch: [138]  [ 40/160]  eta: 0:02:02  lr: 0.000011  min_lr: 0.000000  loss: 1.9156 (1.8341)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3532 (8.6219)  time: 0.8490 (0.5224 -- 3.6087)  data: 0.2866 (0.0005 -- 3.0690)  max mem: 16735
Epoch: [138]  [ 60/160]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000000  loss: 1.6987 (1.8161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9526 (8.6797)  time: 0.9994 (0.5337 -- 4.5375)  data: 0.4496 (0.0003 -- 4.0242)  max mem: 16735
Epoch: [138]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000000  loss: 1.7443 (1.8123)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1684 (8.5452)  time: 0.8260 (0.5156 -- 3.7957)  data: 0.2844 (0.0003 -- 3.2791)  max mem: 16735
Epoch: [138]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 2.0329 (1.8393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6725 (8.6135)  time: 0.8008 (0.5237 -- 3.3156)  data: 0.2440 (0.0008 -- 2.7451)  max mem: 16735
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.8424 (1.8446)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8891 (8.6289)  time: 0.8148 (0.5313 -- 3.5440)  data: 0.2564 (0.0008 -- 2.9874)  max mem: 16735
[2023-09-01 03:57:01,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:57:01,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:57:01,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:57:01,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:57:02,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22205
[2023-09-01 03:57:02,805] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:57:02,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22205
[2023-09-01 03:57:02,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 03:57:02,805] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8405 (1.8277)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1915 (8.4754)  time: 0.9332 (0.5275 -- 3.4329)  data: 0.3798 (0.0006 -- 2.9021)  max mem: 16735
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9011 (1.8283)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0213 (8.4392)  time: 0.6702 (0.5016 -- 1.7859)  data: 0.1444 (0.0002 -- 1.2505)  max mem: 16735
Epoch: [138] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9011 (1.8755)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0213 (8.4392)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2868 (0.2868)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3615 (2.3615 -- 2.3615)  data: 2.1496 (2.1496 -- 2.1496)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6029 (0.8164)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4119 (0.1910 -- 2.3615)  data: 0.2016 (0.0006 -- 2.1496)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6601 (0.7585)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2252 (0.1694 -- 0.5738)  data: 0.0225 (0.0001 -- 0.3794)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7283 (0.8381)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2102 (0.1328 -- 0.5738)  data: 0.0215 (0.0001 -- 0.3794)  max mem: 16735
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.828
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.42%
Epoch: [139]  [  0/160]  eta: 0:20:19  lr: 0.000010  min_lr: 0.000000  loss: 1.5336 (1.5336)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5917 (8.5917)  time: 7.6243 (7.6243 -- 7.6243)  data: 5.3959 (5.3959 -- 5.3959)  max mem: 16735
Epoch: [139]  [ 20/160]  eta: 0:02:38  lr: 0.000010  min_lr: 0.000000  loss: 1.8528 (1.8035)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1995 (8.9932)  time: 0.8065 (0.5317 -- 3.9302)  data: 0.0023 (0.0007 -- 0.0151)  max mem: 16735
Epoch: [139]  [ 40/160]  eta: 0:02:04  lr: 0.000010  min_lr: 0.000000  loss: 2.1382 (1.9374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4458 (8.9782)  time: 0.9363 (0.5241 -- 3.2242)  data: 0.1421 (0.0004 -- 1.6787)  max mem: 16735
Epoch: [139]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000000  loss: 1.7651 (1.9144)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2058 (8.8221)  time: 0.7745 (0.5369 -- 2.8664)  data: 0.2153 (0.0004 -- 2.3491)  max mem: 16735
Epoch: [139]  [ 80/160]  eta: 0:01:13  lr: 0.000010  min_lr: 0.000000  loss: 1.8072 (1.8965)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5199 (8.6052)  time: 0.8326 (0.5350 -- 2.8250)  data: 0.2776 (0.0007 -- 2.3020)  max mem: 16735
[2023-09-01 03:59:05,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:59:05,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 03:59:05,919] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 03:59:05,919] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [139]  [100/160]  eta: 0:00:54  lr: 0.000010  min_lr: 0.000000  loss: 2.0589 (1.9262)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9594 (8.5165)  time: 0.8693 (0.5323 -- 3.3230)  data: 0.2369 (0.0005 -- 2.7671)  max mem: 16735
[2023-09-01 03:59:14,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22344
[2023-09-01 03:59:14,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22344
[2023-09-01 03:59:14,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:59:14,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 03:59:14,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [139]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.8761 (1.9264)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6851 (8.5713)  time: 0.9441 (0.5371 -- 2.5011)  data: 0.1286 (0.0004 -- 1.5749)  max mem: 16735
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9219 (1.9245)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5450 (8.4871)  time: 0.8704 (0.5265 -- 2.4061)  data: 0.1882 (0.0004 -- 1.8847)  max mem: 16735
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 2.0625 (1.9370)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3783 (8.4986)  time: 0.6534 (0.5031 -- 2.0406)  data: 0.1078 (0.0001 -- 1.4833)  max mem: 16735
Epoch: [139] Total time: 0:02:20 (0.8802 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 2.0625 (1.9376)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3783 (8.4986)
[2023-09-01 03:59:58,083] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-09-01 03:59:58,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-09-01 03:59:58,087] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-09-01 03:59:58,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-09-01 03:59:59,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-09-01 03:59:59,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3048 (0.3048)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5072 (2.5072 -- 2.5072)  data: 2.2575 (2.2575 -- 2.2575)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6204 (0.8419)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4304 (0.2053 -- 2.5072)  data: 0.2065 (0.0004 -- 2.2575)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6854 (0.7818)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2173 (0.1694 -- 0.3027)  data: 0.0078 (0.0001 -- 0.1156)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7438 (0.8615)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.1980 (0.1329 -- 0.3027)  data: 0.0075 (0.0001 -- 0.1156)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.855
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 78.42%
Epoch: [140]  [  0/160]  eta: 0:16:55  lr: 0.000010  min_lr: 0.000000  loss: 2.0643 (2.0643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1473 (10.1473)  time: 6.3494 (6.3494 -- 6.3494)  data: 5.7838 (5.7838 -- 5.7838)  max mem: 16735
Epoch: [140]  [ 20/160]  eta: 0:02:50  lr: 0.000010  min_lr: 0.000000  loss: 2.0393 (1.9915)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3686 (7.6459)  time: 0.9649 (0.5227 -- 3.5573)  data: 0.1140 (0.0004 -- 1.2674)  max mem: 16735
[2023-09-01 04:00:48,747] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22438
[2023-09-01 04:00:48,747] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22438
[2023-09-01 04:00:48,747] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:00:48,747] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:00:48,747] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000000  loss: 1.9661 (1.9651)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0923 (7.9807)  time: 0.8609 (0.5232 -- 4.7379)  data: 0.0010 (0.0003 -- 0.0024)  max mem: 16735
Epoch: [140]  [ 60/160]  eta: 0:01:41  lr: 0.000010  min_lr: 0.000000  loss: 2.2071 (1.9970)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2675 (8.1493)  time: 0.9474 (0.5284 -- 3.9390)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16735
Epoch: [140]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.8039 (1.9537)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5390 (8.2830)  time: 0.7861 (0.5263 -- 3.6258)  data: 0.0018 (0.0004 -- 0.0054)  max mem: 16735
Epoch: [140]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.8285 (1.9356)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2924 (8.4413)  time: 0.8830 (0.5319 -- 4.2827)  data: 0.0013 (0.0003 -- 0.0062)  max mem: 16735
Epoch: [140]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 2.1585 (1.9740)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8318 (8.5203)  time: 0.8224 (0.5353 -- 3.2645)  data: 0.0015 (0.0003 -- 0.0054)  max mem: 16735
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9044 (1.9622)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7288 (8.5139)  time: 0.7927 (0.5227 -- 3.1374)  data: 0.0017 (0.0002 -- 0.0059)  max mem: 16735
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9802 (1.9565)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7246 (8.5165)  time: 0.7457 (0.5026 -- 3.2716)  data: 0.0033 (0.0002 -- 0.0524)  max mem: 16735
Epoch: [140] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9802 (1.9409)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7246 (8.5165)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2980 (0.2980)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4663 (2.4663 -- 2.4663)  data: 2.2061 (2.2061 -- 2.2061)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6760 (0.8243)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4267 (0.2085 -- 2.4663)  data: 0.2016 (0.0009 -- 2.2061)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6760 (0.7668)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2166 (0.1698 -- 0.3239)  data: 0.0082 (0.0001 -- 0.1488)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7584 (0.8431)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.1980 (0.1330 -- 0.3239)  data: 0.0079 (0.0001 -- 0.1488)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.839
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.42%
Epoch: [141]  [  0/160]  eta: 0:21:54  lr: 0.000010  min_lr: 0.000000  loss: 1.2566 (1.2566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0556 (6.0556)  time: 8.2129 (8.2129 -- 8.2129)  data: 6.3523 (6.3523 -- 6.3523)  max mem: 16735
[2023-09-01 04:02:48,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:02:48,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:02:48,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 04:02:48,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [ 20/160]  eta: 0:02:44  lr: 0.000010  min_lr: 0.000000  loss: 2.0162 (1.9182)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1468 (8.0687)  time: 0.8224 (0.5356 -- 3.5678)  data: 0.0522 (0.0007 -- 0.9950)  max mem: 16735
Epoch: [141]  [ 40/160]  eta: 0:02:02  lr: 0.000010  min_lr: 0.000000  loss: 1.7469 (1.8834)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3653 (8.3776)  time: 0.8596 (0.5333 -- 3.1080)  data: 0.0018 (0.0003 -- 0.0050)  max mem: 16735
Epoch: [141]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.9848 (1.8961)  loss_scale: 32768.0000 (30887.8689)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6338 (8.3898)  time: 0.9878 (0.5289 -- 3.4396)  data: 0.0090 (0.0004 -- 0.1511)  max mem: 16735
Epoch: [141]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000000  loss: 1.9541 (1.9141)  loss_scale: 32768.0000 (31352.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7514 (8.4007)  time: 0.7244 (0.5396 -- 1.9982)  data: 0.0020 (0.0001 -- 0.0042)  max mem: 16735
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000000  loss: 2.1643 (1.9353)  loss_scale: 32768.0000 (31632.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5051 (8.4109)  time: 1.0376 (0.5075 -- 3.1019)  data: 0.0216 (0.0003 -- 0.4092)  max mem: 16735
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.9159 (1.9156)  loss_scale: 32768.0000 (31820.1653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1690 (8.3860)  time: 0.8446 (0.5133 -- 3.3583)  data: 0.0022 (0.0003 -- 0.0148)  max mem: 16735
[2023-09-01 04:04:43,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:04:43,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:04:43,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:04:43,606] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.6578 (1.8835)  loss_scale: 32768.0000 (33348.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4310 (8.4042)  time: 0.8787 (0.5311 -- 3.0748)  data: 0.0016 (0.0004 -- 0.0110)  max mem: 16735
[2023-09-01 04:04:59,617] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22719
[2023-09-01 04:04:59,617] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:04:59,617] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22719
[2023-09-01 04:04:59,617] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:04:59,617] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8450 (1.8733)  loss_scale: 65536.0000 (36966.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9794 (8.4157)  time: 0.6921 (0.4908 -- 2.2476)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16735
Epoch: [141] Total time: 0:02:22 (0.8934 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8450 (1.8881)  loss_scale: 65536.0000 (36966.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9794 (8.4157)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2805 (0.2805)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3164 (2.3164 -- 2.3164)  data: 2.1019 (2.1019 -- 2.1019)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6431 (0.8287)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4251 (0.1948 -- 2.3164)  data: 0.2166 (0.0006 -- 2.1019)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6465 (0.7632)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2238 (0.1687 -- 0.4719)  data: 0.0184 (0.0001 -- 0.2654)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7171 (0.8429)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (96.2656)  time: 0.2115 (0.1328 -- 0.4719)  data: 0.0182 (0.0001 -- 0.2654)  max mem: 16735
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 77.386 Acc@5 96.266 loss 0.837
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.42%
Epoch: [142]  [  0/160]  eta: 0:18:07  lr: 0.000010  min_lr: 0.000000  loss: 2.4629 (2.4629)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3909 (5.3909)  time: 6.7975 (6.7975 -- 6.7975)  data: 6.2337 (6.2337 -- 6.2337)  max mem: 16735
Epoch: [142]  [ 20/160]  eta: 0:02:47  lr: 0.000010  min_lr: 0.000000  loss: 1.8980 (1.9404)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4469 (8.4595)  time: 0.9196 (0.5344 -- 2.8058)  data: 0.0696 (0.0004 -- 0.7634)  max mem: 16735
Epoch: [142]  [ 40/160]  eta: 0:01:57  lr: 0.000009  min_lr: 0.000000  loss: 2.0122 (1.9805)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8942 (8.6681)  time: 0.7468 (0.5403 -- 1.6229)  data: 0.0881 (0.0005 -- 0.8674)  max mem: 16735
Epoch: [142]  [ 60/160]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 1.8721 (1.9492)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0632 (8.5465)  time: 0.9177 (0.5275 -- 3.0218)  data: 0.0018 (0.0004 -- 0.0070)  max mem: 16735
Epoch: [142]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 2.0932 (1.9736)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4153 (8.5441)  time: 0.9867 (0.5244 -- 3.0747)  data: 0.0017 (0.0004 -- 0.0065)  max mem: 16735
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.9463 (1.9755)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9653 (8.5476)  time: 0.7622 (0.5308 -- 2.5059)  data: 0.0020 (0.0005 -- 0.0057)  max mem: 16735
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7551 (1.9422)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4174 (8.6038)  time: 0.8330 (0.5297 -- 2.0635)  data: 0.0014 (0.0003 -- 0.0032)  max mem: 16735
[2023-09-01 04:07:04,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:07:04,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:07:04,975] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:07:04,975] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:07:10,466] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22854
[2023-09-01 04:07:10,466] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:07:10,466] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:07:10,466] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22854
[2023-09-01 04:07:10,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9114 (1.9400)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2766 (8.5541)  time: 1.0018 (0.5303 -- 4.4141)  data: 0.0018 (0.0003 -- 0.0081)  max mem: 16735
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7664 (1.9189)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7217 (8.5714)  time: 0.6205 (0.5027 -- 2.5050)  data: 0.0006 (0.0001 -- 0.0020)  max mem: 16735
Epoch: [142] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7664 (1.9140)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7217 (8.5714)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2805 (0.2805)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2804 (2.2804 -- 2.2804)  data: 2.0759 (2.0759 -- 2.0759)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6478 (0.8185)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4076 (0.1988 -- 2.2804)  data: 0.1994 (0.0008 -- 2.0759)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6708 (0.7574)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2227 (0.1696 -- 0.3743)  data: 0.0199 (0.0001 -- 0.1535)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7241 (0.8326)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2100 (0.1331 -- 0.3743)  data: 0.0197 (0.0001 -- 0.1535)  max mem: 16735
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 79.046 Acc@5 96.266 loss 0.829
Accuracy of the network on the 482 val images: 79.05%
[2023-09-01 04:07:37,251] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-01 04:07:37,252] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-01 04:07:37,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-01 04:07:37,252] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-01 04:07:38,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-01 04:07:38,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.05%
Epoch: [143]  [  0/160]  eta: 0:20:53  lr: 0.000009  min_lr: 0.000000  loss: 1.9848 (1.9848)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4948 (8.4948)  time: 7.8352 (7.8352 -- 7.8352)  data: 6.1196 (6.1196 -- 6.1196)  max mem: 16735
Epoch: [143]  [ 20/160]  eta: 0:02:45  lr: 0.000009  min_lr: 0.000000  loss: 1.8133 (1.8813)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1147 (8.4055)  time: 0.8482 (0.5342 -- 3.3879)  data: 0.0870 (0.0004 -- 1.5765)  max mem: 16735
Epoch: [143]  [ 40/160]  eta: 0:02:00  lr: 0.000009  min_lr: 0.000000  loss: 2.0023 (1.9401)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8238 (8.3713)  time: 0.8268 (0.5462 -- 2.2571)  data: 0.0320 (0.0005 -- 0.3976)  max mem: 16735
Epoch: [143]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.8180 (1.8834)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6808 (8.5223)  time: 0.9336 (0.5451 -- 2.5416)  data: 0.0753 (0.0005 -- 0.7599)  max mem: 16735
Epoch: [143]  [ 80/160]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 1.9331 (1.9052)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1890 (8.4504)  time: 0.7760 (0.5381 -- 2.8282)  data: 0.0024 (0.0006 -- 0.0106)  max mem: 16735
Epoch: [143]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.9626 (1.9150)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1888 (8.5015)  time: 0.8817 (0.5207 -- 2.9746)  data: 0.0320 (0.0004 -- 0.5909)  max mem: 16735
[2023-09-01 04:09:15,265] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:09:15,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:09:15,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:09:15,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:09:24,704] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22996
[2023-09-01 04:09:24,704] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22996
[2023-09-01 04:09:24,705] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:09:24,705] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:09:24,705] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:09:29,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=143, lr=[2.1435939889621148e-07, 2.1435939889621148e-07, 2.858125318616153e-07, 2.858125318616153e-07, 3.810833758154871e-07, 3.810833758154871e-07, 5.081111677539828e-07, 5.081111677539828e-07, 6.774815570053104e-07, 6.774815570053104e-07, 9.033087426737471e-07, 9.033087426737471e-07, 1.2044116568983295e-06, 1.2044116568983295e-06, 1.6058822091977728e-06, 1.6058822091977728e-06, 2.1411762789303637e-06, 2.1411762789303637e-06, 2.8549017052404846e-06, 2.8549017052404846e-06, 3.806535606987313e-06, 3.806535606987313e-06, 5.0753808093164175e-06, 5.0753808093164175e-06, 6.7671744124218894e-06, 6.7671744124218894e-06, 9.02289921656252e-06, 9.02289921656252e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 04:09:29,998] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.87060824907319, CurrSamplesPerSec=2.8752580463886814, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [143]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.9998 (1.9179)  loss_scale: 65536.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8894 (8.4205)  time: 0.9450 (0.5228 -- 4.1884)  data: 0.0020 (0.0003 -- 0.0056)  max mem: 16735
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 2.0383 (1.9314)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1608 (8.4331)  time: 0.7811 (0.5328 -- 2.8353)  data: 0.0017 (0.0004 -- 0.0050)  max mem: 16735
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7936 (1.9230)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2657 (8.3302)  time: 0.8032 (0.4998 -- 3.0413)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16735
Epoch: [143] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7936 (1.9123)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2657 (8.3302)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2803 (0.2803)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3925 (2.3925 -- 2.3925)  data: 2.1840 (2.1840 -- 2.1840)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6381 (0.8242)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4196 (0.1959 -- 2.3925)  data: 0.2011 (0.0008 -- 2.1840)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6555 (0.7628)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2159 (0.1696 -- 0.2854)  data: 0.0050 (0.0001 -- 0.0668)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7248 (0.8382)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.1990 (0.1333 -- 0.2705)  data: 0.0046 (0.0001 -- 0.0668)  max mem: 16735
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.831
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [144]  [  0/160]  eta: 0:20:08  lr: 0.000009  min_lr: 0.000000  loss: 1.0134 (1.0134)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8732 (8.8732)  time: 7.5555 (7.5555 -- 7.5555)  data: 6.4703 (6.4703 -- 6.4703)  max mem: 16735
Epoch: [144]  [ 20/160]  eta: 0:02:38  lr: 0.000009  min_lr: 0.000000  loss: 1.8595 (1.8778)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7403 (8.6248)  time: 0.8097 (0.5348 -- 3.3034)  data: 0.0579 (0.0004 -- 0.4909)  max mem: 16735
Epoch: [144]  [ 40/160]  eta: 0:01:59  lr: 0.000009  min_lr: 0.000000  loss: 1.9488 (1.8905)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8994 (8.5807)  time: 0.8502 (0.5264 -- 2.5425)  data: 0.2106 (0.0004 -- 2.0187)  max mem: 16735
Epoch: [144]  [ 60/160]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 2.0360 (1.9296)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9430 (8.5938)  time: 0.9925 (0.5288 -- 2.6152)  data: 0.1949 (0.0009 -- 1.7200)  max mem: 16735
Epoch: [144]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 1.7793 (1.9015)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2381 (8.3964)  time: 0.8084 (0.5170 -- 3.8208)  data: 0.2625 (0.0002 -- 3.3011)  max mem: 16735
[2023-09-01 04:11:31,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:11:31,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:11:31,790] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:11:31,791] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:11:38,605] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23132
[2023-09-01 04:11:38,605] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23132
[2023-09-01 04:11:38,606] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:11:38,606] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:11:38,606] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [144]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000000  loss: 1.9240 (1.9194)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4427 (8.3742)  time: 0.9830 (0.5121 -- 3.5285)  data: 0.4394 (0.0003 -- 3.0111)  max mem: 16735
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8252 (1.9139)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4737 (8.3072)  time: 0.8372 (0.5235 -- 3.5836)  data: 0.2968 (0.0002 -- 3.0682)  max mem: 16735
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8234 (1.9003)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2462 (8.3055)  time: 0.9416 (0.5222 -- 3.5036)  data: 0.4014 (0.0002 -- 2.9914)  max mem: 16735
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7964 (1.8864)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6359 (8.3689)  time: 0.5411 (0.5020 -- 0.9517)  data: 0.0220 (0.0002 -- 0.4281)  max mem: 16735
Epoch: [144] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7964 (1.8895)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6359 (8.3689)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2820 (0.2820)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2786 (2.2786 -- 2.2786)  data: 2.0496 (2.0496 -- 2.0496)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6243 (0.8209)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4142 (0.1982 -- 2.2786)  data: 0.1958 (0.0005 -- 2.0496)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6587 (0.7661)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2273 (0.1715 -- 0.4564)  data: 0.0188 (0.0001 -- 0.2676)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7406 (0.8377)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2106 (0.1379 -- 0.4564)  data: 0.0183 (0.0001 -- 0.2676)  max mem: 16735
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 78.631 Acc@5 96.266 loss 0.832
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 79.05%
Epoch: [145]  [  0/160]  eta: 0:19:53  lr: 0.000009  min_lr: 0.000000  loss: 1.8447 (1.8447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7620 (7.7620)  time: 7.4623 (7.4623 -- 7.4623)  data: 6.0996 (6.0996 -- 6.0996)  max mem: 16735
Epoch: [145]  [ 20/160]  eta: 0:02:54  lr: 0.000009  min_lr: 0.000000  loss: 2.0592 (1.9610)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2094 (8.4969)  time: 0.9334 (0.5186 -- 3.9596)  data: 0.0934 (0.0003 -- 1.4353)  max mem: 16735
Epoch: [145]  [ 40/160]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000000  loss: 2.0409 (1.9584)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5022 (8.4715)  time: 0.8632 (0.5309 -- 3.0475)  data: 0.0016 (0.0007 -- 0.0044)  max mem: 16735
Epoch: [145]  [ 60/160]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 1.7582 (1.9102)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4157 (8.3800)  time: 0.9051 (0.5190 -- 2.2056)  data: 0.1407 (0.0003 -- 1.5584)  max mem: 16735
[2023-09-01 04:13:41,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:13:41,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:13:41,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:13:41,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:13:55,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23279
[2023-09-01 04:13:55,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23279
[2023-09-01 04:13:55,310] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:13:55,310] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:13:55,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [145]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 1.9229 (1.9296)  loss_scale: 65536.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7878 (8.2228)  time: 0.7400 (0.5233 -- 2.1460)  data: 0.0944 (0.0003 -- 1.5857)  max mem: 16735
Epoch: [145]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.8373 (1.8873)  loss_scale: 32768.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1786 (8.4994)  time: 0.8709 (0.5150 -- 2.5872)  data: 0.3027 (0.0004 -- 2.0538)  max mem: 16735
Epoch: [145]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 2.0167 (1.9125)  loss_scale: 32768.0000 (37642.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9158 (8.4938)  time: 0.8554 (0.5355 -- 3.1191)  data: 0.2257 (0.0002 -- 2.5831)  max mem: 16735
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.9232 (1.9170)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0464 (8.5579)  time: 0.8566 (0.5370 -- 3.6560)  data: 0.2920 (0.0003 -- 3.1255)  max mem: 16735
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7792 (1.9021)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0215 (8.6403)  time: 0.7379 (0.5024 -- 2.7969)  data: 0.1241 (0.0002 -- 2.2484)  max mem: 16735
Epoch: [145] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7792 (1.9091)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0215 (8.6403)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.2842 (0.2842)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1014 (2.1014 -- 2.1014)  data: 1.8944 (1.8944 -- 1.8944)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6078 (0.8042)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4077 (0.1990 -- 2.1014)  data: 0.1918 (0.0007 -- 1.8944)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6356 (0.7476)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2345 (0.1690 -- 0.4435)  data: 0.0281 (0.0001 -- 0.2511)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7121 (0.8223)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2188 (0.1358 -- 0.4435)  data: 0.0269 (0.0001 -- 0.2511)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 78.216 Acc@5 96.266 loss 0.819
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 79.05%
Epoch: [146]  [  0/160]  eta: 0:20:18  lr: 0.000008  min_lr: 0.000000  loss: 1.8848 (1.8848)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8300 (6.8300)  time: 7.6179 (7.6179 -- 7.6179)  data: 7.0790 (7.0790 -- 7.0790)  max mem: 16735
Epoch: [146]  [ 20/160]  eta: 0:02:54  lr: 0.000008  min_lr: 0.000000  loss: 1.7855 (1.8196)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1096 (7.9736)  time: 0.9267 (0.5283 -- 4.5715)  data: 0.3767 (0.0004 -- 4.0392)  max mem: 16735
Epoch: [146]  [ 40/160]  eta: 0:02:10  lr: 0.000008  min_lr: 0.000000  loss: 1.8710 (1.8044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3720 (8.0155)  time: 0.9168 (0.5290 -- 4.3313)  data: 0.3732 (0.0002 -- 3.8155)  max mem: 16735
[2023-09-01 04:16:01,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:16:01,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:16:01,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:16:01,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [146]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.8256 (1.8052)  loss_scale: 65536.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6575 (8.1096)  time: 0.7884 (0.5259 -- 3.5038)  data: 0.2352 (0.0003 -- 2.9306)  max mem: 16735
[2023-09-01 04:16:10,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23422
[2023-09-01 04:16:10,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23422
[2023-09-01 04:16:10,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:16:10,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:16:10,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [146]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000000  loss: 1.8836 (1.8201)  loss_scale: 32768.0000 (38431.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1242 (8.2584)  time: 0.9104 (0.5247 -- 3.1562)  data: 0.3614 (0.0004 -- 2.6243)  max mem: 16735
Epoch: [146]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 2.0548 (1.8610)  loss_scale: 32768.0000 (37310.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5580 (8.3572)  time: 0.7852 (0.5217 -- 2.0714)  data: 0.2350 (0.0002 -- 1.5410)  max mem: 16735
Epoch: [146]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 2.0086 (1.8762)  loss_scale: 32768.0000 (36559.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6317 (8.2771)  time: 0.8074 (0.5276 -- 3.1285)  data: 0.2587 (0.0006 -- 2.6120)  max mem: 16735
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.8206 (1.8567)  loss_scale: 32768.0000 (36021.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2261 (8.4270)  time: 0.8836 (0.5300 -- 2.1535)  data: 0.3372 (0.0008 -- 1.6041)  max mem: 16735
[2023-09-01 04:17:28,174] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23513
[2023-09-01 04:17:28,174] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:17:28,174] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23513
[2023-09-01 04:17:28,174] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:17:28,174] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9374 (1.8708)  loss_scale: 32768.0000 (34918.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7487 (8.4293)  time: 0.7077 (0.5016 -- 2.6112)  data: 0.1805 (0.0002 -- 2.0754)  max mem: 16735
Epoch: [146] Total time: 0:02:21 (0.8854 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9374 (1.8753)  loss_scale: 32768.0000 (34918.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7487 (8.4293)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2725 (0.2725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4617 (2.4617 -- 2.4617)  data: 2.2005 (2.2005 -- 2.2005)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6040 (0.8055)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4430 (0.2122 -- 2.4617)  data: 0.2153 (0.0008 -- 2.2005)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6459 (0.7496)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2211 (0.1698 -- 0.3845)  data: 0.0094 (0.0001 -- 0.1514)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7268 (0.8253)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.6805)  time: 0.2038 (0.1324 -- 0.3845)  data: 0.0091 (0.0001 -- 0.1514)  max mem: 16735
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 78.423 Acc@5 96.473 loss 0.820
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 79.05%
Epoch: [147]  [  0/160]  eta: 0:22:29  lr: 0.000008  min_lr: 0.000000  loss: 1.6789 (1.6789)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1467 (8.1467)  time: 8.4331 (8.4331 -- 8.4331)  data: 7.9080 (7.9080 -- 7.9080)  max mem: 16735
Epoch: [147]  [ 20/160]  eta: 0:02:52  lr: 0.000008  min_lr: 0.000000  loss: 2.0090 (1.8989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2765 (8.0901)  time: 0.8724 (0.5161 -- 5.4550)  data: 0.3214 (0.0003 -- 4.9240)  max mem: 16735
Epoch: [147]  [ 40/160]  eta: 0:02:10  lr: 0.000008  min_lr: 0.000000  loss: 1.9699 (1.9296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6909 (8.5182)  time: 0.9326 (0.5261 -- 3.4931)  data: 0.3758 (0.0004 -- 2.9689)  max mem: 16735
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.8878 (1.8965)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4295 (8.5682)  time: 0.7659 (0.5398 -- 2.5748)  data: 0.2125 (0.0002 -- 2.0592)  max mem: 16735
Epoch: [147]  [ 80/160]  eta: 0:01:19  lr: 0.000008  min_lr: 0.000000  loss: 2.0094 (1.9213)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8293 (8.4918)  time: 1.0402 (0.5256 -- 4.3811)  data: 0.4933 (0.0003 -- 3.8226)  max mem: 16735
Epoch: [147]  [100/160]  eta: 0:00:57  lr: 0.000008  min_lr: 0.000000  loss: 2.1399 (1.9483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0238 (8.4136)  time: 0.7701 (0.5225 -- 3.2170)  data: 0.2247 (0.0001 -- 2.6713)  max mem: 16735
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9277 (1.9381)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2576 (8.3713)  time: 0.8983 (0.5291 -- 3.1735)  data: 0.3493 (0.0004 -- 2.6472)  max mem: 16735
[2023-09-01 04:19:34,209] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:19:34,209] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 04:19:34,210] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:19:34,210] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7190 (1.9181)  loss_scale: 32768.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6380 (8.2662)  time: 0.8358 (0.5195 -- 4.2466)  data: 0.2844 (0.0004 -- 3.7168)  max mem: 16735
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9832 (1.9253)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6707 (8.3276)  time: 0.6384 (0.5024 -- 2.3846)  data: 0.1148 (0.0003 -- 1.8365)  max mem: 16735
Epoch: [147] Total time: 0:02:22 (0.8937 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9832 (1.8980)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6707 (8.3276)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2819 (0.2819)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4380 (2.4380 -- 2.4380)  data: 2.1968 (2.1968 -- 2.1968)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6144 (0.8087)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4225 (0.1895 -- 2.4380)  data: 0.2019 (0.0007 -- 2.1968)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6516 (0.7531)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (97.3545)  time: 0.2179 (0.1689 -- 0.3040)  data: 0.0082 (0.0001 -- 0.0988)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7251 (0.8313)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.1998 (0.1327 -- 0.3040)  data: 0.0072 (0.0001 -- 0.0988)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [148]  [  0/160]  eta: 0:20:35  lr: 0.000008  min_lr: 0.000000  loss: 1.5701 (1.5701)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8270 (9.8270)  time: 7.7239 (7.7239 -- 7.7239)  data: 7.0188 (7.0188 -- 7.0188)  max mem: 16735
Epoch: [148]  [ 20/160]  eta: 0:02:49  lr: 0.000008  min_lr: 0.000000  loss: 1.9393 (1.8512)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0901 (8.7593)  time: 0.8866 (0.5242 -- 3.6545)  data: 0.3344 (0.0003 -- 3.1172)  max mem: 16735
Epoch: [148]  [ 40/160]  eta: 0:02:01  lr: 0.000008  min_lr: 0.000000  loss: 1.9720 (1.8721)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0257 (8.4218)  time: 0.8048 (0.5363 -- 2.3270)  data: 0.1819 (0.0005 -- 1.7637)  max mem: 16735
Epoch: [148]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 2.0152 (1.8851)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2001 (8.4912)  time: 0.9949 (0.5222 -- 3.1619)  data: 0.3900 (0.0002 -- 2.6241)  max mem: 16735
Epoch: [148]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.8106 (1.8828)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5784 (8.4436)  time: 0.8104 (0.5169 -- 2.7497)  data: 0.2616 (0.0004 -- 2.2256)  max mem: 16735
[2023-09-01 04:21:34,809] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:21:34,809] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:21:34,810] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:21:34,810] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [148]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.9988 (1.8958)  loss_scale: 65536.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2888 (8.4364)  time: 0.8359 (0.5291 -- 2.3762)  data: 0.2780 (0.0005 -- 1.8358)  max mem: 16735
[2023-09-01 04:21:47,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23781
[2023-09-01 04:21:47,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:21:47,326] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23781
[2023-09-01 04:21:47,327] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:21:47,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [148]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.8211 (1.8844)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5869 (8.5549)  time: 0.9299 (0.5372 -- 3.2752)  data: 0.3304 (0.0004 -- 2.7266)  max mem: 16735
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.8548 (1.8852)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1745 (8.4717)  time: 0.8823 (0.5207 -- 4.4333)  data: 0.3384 (0.0002 -- 3.9193)  max mem: 16735
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9572 (1.8990)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6705 (8.5207)  time: 0.7081 (0.5000 -- 2.1711)  data: 0.1865 (0.0002 -- 1.6136)  max mem: 16735
Epoch: [148] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9572 (1.9039)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6705 (8.5207)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2746 (0.2746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2969 (2.2969 -- 2.2969)  data: 2.0753 (2.0753 -- 2.0753)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5993 (0.8131)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4274 (0.2040 -- 2.2969)  data: 0.2117 (0.0006 -- 2.0753)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6457 (0.7537)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2263 (0.1693 -- 0.4822)  data: 0.0235 (0.0001 -- 0.2439)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7075 (0.8316)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.2109 (0.1327 -- 0.4822)  data: 0.0232 (0.0001 -- 0.2439)  max mem: 16735
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 78.216 Acc@5 96.266 loss 0.826
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 79.05%
Epoch: [149]  [  0/160]  eta: 0:17:20  lr: 0.000008  min_lr: 0.000000  loss: 1.7000 (1.7000)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4096 (6.4096)  time: 6.5002 (6.5002 -- 6.5002)  data: 5.9334 (5.9334 -- 5.9334)  max mem: 16735
Epoch: [149]  [ 20/160]  eta: 0:02:39  lr: 0.000007  min_lr: 0.000000  loss: 1.7194 (1.8348)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3899 (8.3061)  time: 0.8746 (0.5380 -- 2.2783)  data: 0.2189 (0.0007 -- 1.4923)  max mem: 16735
Epoch: [149]  [ 40/160]  eta: 0:02:01  lr: 0.000007  min_lr: 0.000000  loss: 1.9490 (1.8859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0853 (8.5998)  time: 0.8762 (0.5378 -- 2.7383)  data: 0.1951 (0.0004 -- 2.2000)  max mem: 16735
Epoch: [149]  [ 60/160]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 1.9682 (1.9016)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0331 (8.5205)  time: 0.8702 (0.5275 -- 2.5713)  data: 0.0417 (0.0003 -- 0.6999)  max mem: 16735
[2023-09-01 04:23:49,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:23:49,769] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:23:49,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:23:49,769] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [149]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.9933 (1.9016)  loss_scale: 65536.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4257 (8.4995)  time: 0.8573 (0.5328 -- 3.0283)  data: 0.0014 (0.0002 -- 0.0034)  max mem: 16735
[2023-09-01 04:24:03,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23926
[2023-09-01 04:24:03,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:24:03,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:24:03,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23926
[2023-09-01 04:24:03,442] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [149]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.9245 (1.9074)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8184 (8.6127)  time: 0.9686 (0.5311 -- 3.9726)  data: 0.0015 (0.0004 -- 0.0057)  max mem: 16735
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.9180 (1.9115)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4679 (8.4735)  time: 0.7258 (0.5327 -- 2.3377)  data: 0.0016 (0.0004 -- 0.0044)  max mem: 16735
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8322 (1.8963)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4753 (8.5348)  time: 0.8649 (0.5231 -- 3.9988)  data: 0.0016 (0.0004 -- 0.0032)  max mem: 16735
[2023-09-01 04:25:03,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=149, lr=[1.720773458201662e-07, 1.720773458201662e-07, 2.2943646109355492e-07, 2.2943646109355492e-07, 3.0591528145807326e-07, 3.0591528145807326e-07, 4.078870419440976e-07, 4.078870419440976e-07, 5.438493892587968e-07, 5.438493892587968e-07, 7.251325190117291e-07, 7.251325190117291e-07, 9.668433586823055e-07, 9.668433586823055e-07, 1.289124478243074e-06, 1.289124478243074e-06, 1.7188326376574321e-06, 1.7188326376574321e-06, 2.2917768502099095e-06, 2.2917768502099095e-06, 3.055702466946546e-06, 3.055702466946546e-06, 4.074269955928728e-06, 4.074269955928728e-06, 5.4323599412383035e-06, 5.4323599412383035e-06, 7.243146588317738e-06, 7.243146588317738e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 04:25:03,018] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.928507047589697, CurrSamplesPerSec=24.060899378729964, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.9586 (1.9036)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9342 (8.5348)  time: 0.7196 (0.5007 -- 3.1846)  data: 0.0076 (0.0002 -- 0.1318)  max mem: 16735
Epoch: [149] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.9586 (1.9398)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9342 (8.5348)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2814 (0.2814)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4788 (2.4788 -- 2.4788)  data: 2.2403 (2.2403 -- 2.2403)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5879 (0.8146)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4308 (0.2011 -- 2.4788)  data: 0.2125 (0.0007 -- 2.2403)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6460 (0.7603)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2193 (0.1694 -- 0.3731)  data: 0.0146 (0.0001 -- 0.1924)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7204 (0.8343)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2027 (0.1328 -- 0.3731)  data: 0.0142 (0.0001 -- 0.1924)  max mem: 16735
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 78.631 Acc@5 96.266 loss 0.827
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 79.05%
Epoch: [150]  [  0/160]  eta: 0:20:58  lr: 0.000007  min_lr: 0.000000  loss: 2.1618 (2.1618)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8302 (7.8302)  time: 7.8673 (7.8673 -- 7.8673)  data: 7.3037 (7.3037 -- 7.3037)  max mem: 16735
Epoch: [150]  [ 20/160]  eta: 0:02:52  lr: 0.000007  min_lr: 0.000000  loss: 1.9133 (1.9529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6206 (7.6845)  time: 0.9028 (0.5349 -- 2.6920)  data: 0.2914 (0.0004 -- 2.1663)  max mem: 16735
Epoch: [150]  [ 40/160]  eta: 0:02:05  lr: 0.000007  min_lr: 0.000000  loss: 1.8620 (1.8744)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1884 (8.4710)  time: 0.8429 (0.5294 -- 2.4408)  data: 0.2157 (0.0001 -- 1.9108)  max mem: 16735
[2023-09-01 04:26:07,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:26:07,106] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:26:07,107] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:26:07,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [150]  [ 60/160]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 1.9761 (1.8758)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6239 (8.1037)  time: 0.9706 (0.5279 -- 3.1541)  data: 0.4190 (0.0004 -- 2.6337)  max mem: 16735
[2023-09-01 04:26:19,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24067
[2023-09-01 04:26:19,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:26:19,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:26:19,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24067
[2023-09-01 04:26:19,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [150]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.7006 (1.8594)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6666 (8.0861)  time: 0.7505 (0.5124 -- 2.7313)  data: 0.2041 (0.0002 -- 2.2010)  max mem: 16735
Epoch: [150]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.9687 (1.8855)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5280 (8.2034)  time: 0.8530 (0.5215 -- 2.9072)  data: 0.3049 (0.0003 -- 2.3742)  max mem: 16735
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.9487 (1.8724)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0824 (8.1802)  time: 0.8704 (0.5234 -- 2.9293)  data: 0.3196 (0.0005 -- 2.4033)  max mem: 16735
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.9258 (1.8835)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1943 (8.1283)  time: 0.9401 (0.5251 -- 3.3854)  data: 0.3939 (0.0004 -- 2.8647)  max mem: 16735
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.9107 (1.8807)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5621 (8.0751)  time: 0.6345 (0.4999 -- 1.8627)  data: 0.1089 (0.0002 -- 1.3586)  max mem: 16735
Epoch: [150] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.9107 (1.8755)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5621 (8.0751)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2808 (0.2808)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6294 (2.6294 -- 2.6294)  data: 2.3804 (2.3804 -- 2.3804)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6151 (0.8110)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4379 (0.2080 -- 2.6294)  data: 0.2175 (0.0007 -- 2.3804)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6367 (0.7588)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2147 (0.1698 -- 0.3399)  data: 0.0082 (0.0001 -- 0.1490)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7410 (0.8373)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.1978 (0.1321 -- 0.3399)  data: 0.0079 (0.0001 -- 0.1490)  max mem: 16735
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 78.216 Acc@5 96.473 loss 0.829
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 79.05%
Epoch: [151]  [  0/160]  eta: 0:17:18  lr: 0.000007  min_lr: 0.000000  loss: 1.5056 (1.5056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5175 (8.5175)  time: 6.4908 (6.4908 -- 6.4908)  data: 5.9374 (5.9374 -- 5.9374)  max mem: 16735
Epoch: [151]  [ 20/160]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000000  loss: 1.9395 (1.8993)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8546 (8.9553)  time: 0.8998 (0.5476 -- 2.4061)  data: 0.2986 (0.0008 -- 1.8732)  max mem: 16735
[2023-09-01 04:28:19,152] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:28:19,153] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:28:19,153] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:28:19,153] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:28:19,703] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24197
[2023-09-01 04:28:19,703] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24197
[2023-09-01 04:28:19,703] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:28:19,703] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:28:19,703] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [151]  [ 40/160]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000000  loss: 2.0390 (1.9174)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6767 (8.3695)  time: 0.8112 (0.5240 -- 2.4202)  data: 0.2132 (0.0003 -- 1.9086)  max mem: 16735
Epoch: [151]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 1.8070 (1.8788)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8052 (8.3102)  time: 0.8730 (0.5448 -- 2.8358)  data: 0.2748 (0.0002 -- 2.3188)  max mem: 16735
Epoch: [151]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.9765 (1.8829)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3484 (8.4831)  time: 0.8921 (0.5406 -- 2.7383)  data: 0.3416 (0.0003 -- 2.2198)  max mem: 16735
Epoch: [151]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.9331 (1.8836)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3376 (8.4902)  time: 0.9222 (0.5178 -- 2.5456)  data: 0.3688 (0.0002 -- 1.9794)  max mem: 16735
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 2.0561 (1.9036)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4247 (8.5268)  time: 0.8651 (0.5309 -- 2.9227)  data: 0.3140 (0.0004 -- 2.3961)  max mem: 16735
Epoch: [151]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7289 (1.8853)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8207 (8.5784)  time: 0.7740 (0.5261 -- 1.9424)  data: 0.2277 (0.0002 -- 1.4337)  max mem: 16735
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 2.1062 (1.9100)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5982 (8.6220)  time: 0.7021 (0.5019 -- 2.5604)  data: 0.1728 (0.0002 -- 2.0174)  max mem: 16735
Epoch: [151] Total time: 0:02:20 (0.8797 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 2.1062 (1.9041)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5982 (8.6220)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2830 (0.2830)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3674 (2.3674 -- 2.3674)  data: 2.1071 (2.1071 -- 2.1071)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6217 (0.8231)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4085 (0.1826 -- 2.3674)  data: 0.1925 (0.0003 -- 2.1071)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6468 (0.7632)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2156 (0.1692 -- 0.3211)  data: 0.0115 (0.0001 -- 0.1191)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7238 (0.8424)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2008 (0.1333 -- 0.3211)  data: 0.0111 (0.0001 -- 0.1191)  max mem: 16735
Val: Total time: 0:00:07 (0.2842 s / it)
* Acc@1 78.008 Acc@5 96.473 loss 0.834
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [152]  [  0/160]  eta: 0:19:22  lr: 0.000007  min_lr: 0.000000  loss: 1.7062 (1.7062)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1968 (9.1968)  time: 7.2659 (7.2659 -- 7.2659)  data: 6.4085 (6.4085 -- 6.4085)  max mem: 16735
[2023-09-01 04:30:22,075] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:30:22,075] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:30:22,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:30:22,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:30:22,609] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24327
[2023-09-01 04:30:22,609] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:30:22,609] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24327
[2023-09-01 04:30:22,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:30:22,652] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [152]  [ 20/160]  eta: 0:02:38  lr: 0.000007  min_lr: 0.000000  loss: 1.8682 (1.8744)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9510 (7.5531)  time: 0.8269 (0.5309 -- 2.5617)  data: 0.2755 (0.0003 -- 2.0161)  max mem: 16735
Epoch: [152]  [ 40/160]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000000  loss: 1.9845 (1.9203)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2621 (7.9292)  time: 0.8260 (0.5292 -- 2.4950)  data: 0.1806 (0.0002 -- 1.7213)  max mem: 16735
Epoch: [152]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000000  loss: 1.9908 (1.9179)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0997 (8.1528)  time: 1.0387 (0.5278 -- 3.2350)  data: 0.3715 (0.0002 -- 2.7066)  max mem: 16735
Epoch: [152]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.8072 (1.9050)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3709 (8.2574)  time: 0.8254 (0.5280 -- 3.1181)  data: 0.2748 (0.0004 -- 2.5989)  max mem: 16735
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.9407 (1.9003)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3546 (8.3923)  time: 0.9109 (0.5243 -- 4.5208)  data: 0.3581 (0.0004 -- 3.9812)  max mem: 16735
Epoch: [152]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7898 (1.8860)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0656 (8.3569)  time: 0.7826 (0.5335 -- 2.4890)  data: 0.1858 (0.0003 -- 1.9483)  max mem: 16735
[2023-09-01 04:32:15,044] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:32:15,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:32:15,045] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:32:15,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.8067 (1.8819)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8075 (8.4663)  time: 0.9000 (0.5307 -- 3.1404)  data: 0.3397 (0.0005 -- 2.5897)  max mem: 16735
[2023-09-01 04:32:22,092] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24464
[2023-09-01 04:32:22,092] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24464
[2023-09-01 04:32:22,092] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:32:22,092] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:32:22,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 2.0107 (1.8888)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7827 (8.4488)  time: 0.6758 (0.5053 -- 2.6832)  data: 0.1298 (0.0002 -- 2.1746)  max mem: 16735
Epoch: [152] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 2.0107 (1.8953)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7827 (8.4488)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2815 (0.2815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4319 (2.4319 -- 2.4319)  data: 2.2108 (2.2108 -- 2.2108)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6285 (0.8207)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4343 (0.1834 -- 2.4319)  data: 0.2253 (0.0004 -- 2.2108)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6463 (0.7604)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2264 (0.1707 -- 0.4775)  data: 0.0228 (0.0001 -- 0.2607)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7145 (0.8403)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (97.0954)  time: 0.2147 (0.1331 -- 0.4775)  data: 0.0226 (0.0001 -- 0.2607)  max mem: 16735
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 78.008 Acc@5 96.680 loss 0.831
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [153]  [  0/160]  eta: 0:22:52  lr: 0.000006  min_lr: 0.000000  loss: 1.6343 (1.6343)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7565 (7.7565)  time: 8.5799 (8.5799 -- 8.5799)  data: 7.5308 (7.5308 -- 7.5308)  max mem: 16735
Epoch: [153]  [ 20/160]  eta: 0:02:54  lr: 0.000006  min_lr: 0.000000  loss: 1.8791 (1.9044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3471 (9.0158)  time: 0.8799 (0.5335 -- 3.7892)  data: 0.3262 (0.0003 -- 3.2587)  max mem: 16735
Epoch: [153]  [ 40/160]  eta: 0:02:09  lr: 0.000006  min_lr: 0.000000  loss: 1.9457 (1.9023)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5806 (8.3292)  time: 0.9031 (0.5186 -- 4.7820)  data: 0.3626 (0.0001 -- 4.2447)  max mem: 16735
Epoch: [153]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000000  loss: 1.9362 (1.9180)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6784 (8.3506)  time: 0.8586 (0.5323 -- 4.0061)  data: 0.3136 (0.0005 -- 3.4829)  max mem: 16735
Epoch: [153]  [ 80/160]  eta: 0:01:17  lr: 0.000006  min_lr: 0.000000  loss: 1.9373 (1.9235)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6872 (8.5009)  time: 0.8378 (0.5186 -- 3.3623)  data: 0.2841 (0.0002 -- 2.8171)  max mem: 16735
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 2.0233 (1.9418)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6217 (8.4613)  time: 0.8086 (0.5335 -- 3.9730)  data: 0.2305 (0.0006 -- 3.4425)  max mem: 16735
[2023-09-01 04:34:28,025] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:34:28,025] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:34:28,026] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:34:28,026] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:34:31,911] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24600
[2023-09-01 04:34:31,911] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24600
[2023-09-01 04:34:31,912] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:34:31,912] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:34:31,912] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [153]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7858 (1.9324)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3331 (8.4387)  time: 0.8566 (0.5384 -- 4.2115)  data: 0.3055 (0.0003 -- 3.6600)  max mem: 16735
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.8604 (1.9230)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3384 (8.3728)  time: 0.9389 (0.5291 -- 4.0383)  data: 0.3849 (0.0004 -- 3.5195)  max mem: 16735
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.9593 (1.9157)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6876 (8.3700)  time: 0.6555 (0.5009 -- 2.6497)  data: 0.1304 (0.0002 -- 2.1283)  max mem: 16735
Epoch: [153] Total time: 0:02:22 (0.8927 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.9593 (1.9292)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6876 (8.3700)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2892 (0.2892)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3708 (2.3708 -- 2.3708)  data: 2.1444 (2.1444 -- 2.1444)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6110 (0.8149)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4288 (0.1931 -- 2.3708)  data: 0.2061 (0.0006 -- 2.1444)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6433 (0.7596)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2190 (0.1716 -- 0.3265)  data: 0.0063 (0.0001 -- 0.1127)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7190 (0.8367)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2032 (0.1327 -- 0.3265)  data: 0.0061 (0.0001 -- 0.1127)  max mem: 16735
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.833
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [154]  [  0/160]  eta: 0:20:23  lr: 0.000006  min_lr: 0.000000  loss: 1.5911 (1.5911)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0782 (9.0782)  time: 7.6438 (7.6438 -- 7.6438)  data: 4.6914 (4.6914 -- 4.6914)  max mem: 16735
Epoch: [154]  [ 20/160]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000000  loss: 2.0204 (1.9067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0082 (8.1724)  time: 0.8731 (0.5243 -- 2.1408)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16735
Epoch: [154]  [ 40/160]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000000  loss: 1.9129 (1.9225)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4274 (8.3877)  time: 0.8239 (0.5244 -- 3.7273)  data: 0.0016 (0.0004 -- 0.0029)  max mem: 16735
Epoch: [154]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.9386 (1.9297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6827 (8.5147)  time: 0.9262 (0.5400 -- 3.2986)  data: 0.3283 (0.0004 -- 2.7513)  max mem: 16735
Epoch: [154]  [ 80/160]  eta: 0:01:13  lr: 0.000006  min_lr: 0.000000  loss: 1.9435 (1.9279)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6587 (8.5349)  time: 0.7402 (0.5268 -- 2.5035)  data: 0.1811 (0.0002 -- 1.9609)  max mem: 16735
[2023-09-01 04:36:36,816] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:36:36,817] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:36:36,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:36:36,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:36:43,725] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24739
[2023-09-01 04:36:43,725] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:36:43,725] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24739
[2023-09-01 04:36:43,725] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:36:43,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.9629 (1.9282)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2556 (8.3889)  time: 0.9169 (0.5352 -- 3.7813)  data: 0.3654 (0.0004 -- 3.2434)  max mem: 16735
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 2.0308 (1.9428)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1543 (8.3644)  time: 0.8432 (0.5308 -- 4.5248)  data: 0.2921 (0.0003 -- 3.9776)  max mem: 16735
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.9905 (1.9440)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4424 (8.3830)  time: 0.9137 (0.5286 -- 3.0508)  data: 0.3611 (0.0005 -- 2.5055)  max mem: 16735
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8732 (1.9241)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0174 (8.4375)  time: 0.7029 (0.5015 -- 4.1715)  data: 0.1068 (0.0002 -- 2.1274)  max mem: 16735
Epoch: [154] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8732 (1.9059)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0174 (8.4375)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2787 (0.2787)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4028 (2.4028 -- 2.4028)  data: 2.1715 (2.1715 -- 2.1715)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6066 (0.8148)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4267 (0.2079 -- 2.4028)  data: 0.2084 (0.0006 -- 2.1715)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6431 (0.7571)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2168 (0.1705 -- 0.3424)  data: 0.0094 (0.0001 -- 0.1109)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7027 (0.8353)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2014 (0.1331 -- 0.3424)  data: 0.0091 (0.0001 -- 0.1109)  max mem: 16735
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.829
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [155]  [  0/160]  eta: 0:21:27  lr: 0.000006  min_lr: 0.000000  loss: 1.4912 (1.4912)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8810 (8.8810)  time: 8.0453 (8.0453 -- 8.0453)  data: 7.4635 (7.4635 -- 7.4635)  max mem: 16735
Epoch: [155]  [ 20/160]  eta: 0:02:59  lr: 0.000006  min_lr: 0.000000  loss: 1.9710 (1.8744)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8944 (8.1232)  time: 0.9419 (0.5201 -- 4.0147)  data: 0.3529 (0.0004 -- 3.4910)  max mem: 16735
Epoch: [155]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000000  loss: 1.8079 (1.8417)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9205 (8.0391)  time: 0.8245 (0.5288 -- 4.4973)  data: 0.2013 (0.0001 -- 3.9740)  max mem: 16735
Epoch: [155]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000000  loss: 2.0436 (1.8962)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6434 (8.0356)  time: 0.9086 (0.5212 -- 3.8748)  data: 0.3597 (0.0004 -- 3.3497)  max mem: 16735
[2023-09-01 04:38:49,100] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:38:49,100] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:38:49,100] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:38:49,100] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [155]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 1.8801 (1.8815)  loss_scale: 65536.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5657 (7.9328)  time: 0.7442 (0.5228 -- 2.8441)  data: 0.1935 (0.0002 -- 2.3171)  max mem: 16735
Epoch: [155]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 1.8891 (1.8862)  loss_scale: 65536.0000 (43474.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1109 (8.0017)  time: 0.9950 (0.5282 -- 4.1263)  data: 0.4396 (0.0004 -- 3.5944)  max mem: 16735
Epoch: [155]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8115 (1.8648)  loss_scale: 65536.0000 (47120.9256)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1852 (7.9551)  time: 0.7844 (0.5297 -- 4.4887)  data: 0.2331 (0.0002 -- 3.9664)  max mem: 16735
[2023-09-01 04:39:39,634] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24928
[2023-09-01 04:39:39,634] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24928
[2023-09-01 04:39:39,634] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:39:39,634] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:39:39,634] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.8730 (1.8686)  loss_scale: 32768.0000 (46711.8298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5450 (7.9309)  time: 0.8807 (0.5310 -- 2.9032)  data: 0.3235 (0.0004 -- 2.3899)  max mem: 16735
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 2.0420 (1.8910)  loss_scale: 32768.0000 (45056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4697 (7.9146)  time: 0.6239 (0.5023 -- 1.6114)  data: 0.0964 (0.0002 -- 1.1097)  max mem: 16735
Epoch: [155] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 2.0420 (1.9139)  loss_scale: 32768.0000 (45056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4697 (7.9146)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2750 (0.2750)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3832 (2.3832 -- 2.3832)  data: 2.1546 (2.1546 -- 2.1546)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6261 (0.8148)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4206 (0.2030 -- 2.3832)  data: 0.2033 (0.0009 -- 2.1546)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6465 (0.7586)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2209 (0.1695 -- 0.4624)  data: 0.0183 (0.0001 -- 0.2822)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7172 (0.8371)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2060 (0.1354 -- 0.4624)  data: 0.0180 (0.0001 -- 0.2822)  max mem: 16735
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.831
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [156]  [  0/160]  eta: 0:21:04  lr: 0.000006  min_lr: 0.000000  loss: 2.0934 (2.0934)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6963 (7.6963)  time: 7.9034 (7.9034 -- 7.9034)  data: 4.8534 (4.8534 -- 4.8534)  max mem: 16735
Epoch: [156]  [ 20/160]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000000  loss: 1.9274 (2.0529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8104 (7.7469)  time: 0.8304 (0.5314 -- 2.7129)  data: 0.2028 (0.0004 -- 2.1670)  max mem: 16735
[2023-09-01 04:40:49,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=156, lr=[1.336984109907071e-07, 1.336984109907071e-07, 1.7826454798760943e-07, 1.7826454798760943e-07, 2.3768606398347925e-07, 2.3768606398347925e-07, 3.1691475197797236e-07, 3.1691475197797236e-07, 4.2255300263729644e-07, 4.2255300263729644e-07, 5.634040035163953e-07, 5.634040035163953e-07, 7.512053380218604e-07, 7.512053380218604e-07, 1.0016071173624805e-06, 1.0016071173624805e-06, 1.3354761564833073e-06, 1.3354761564833073e-06, 1.7806348753110764e-06, 1.7806348753110764e-06, 2.374179833748102e-06, 2.374179833748102e-06, 3.165573111664136e-06, 3.165573111664136e-06, 4.220764148885515e-06, 4.220764148885515e-06, 5.627685531847353e-06, 5.627685531847353e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 04:40:49,574] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=18.031878967910764, CurrSamplesPerSec=22.142332747934976, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [156]  [ 40/160]  eta: 0:01:56  lr: 0.000006  min_lr: 0.000000  loss: 1.9366 (1.9397)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9871 (7.8366)  time: 0.7729 (0.5276 -- 2.2038)  data: 0.0937 (0.0006 -- 0.7607)  max mem: 16735
Epoch: [156]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.9341 (1.9390)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8002 (8.1595)  time: 0.9482 (0.5322 -- 2.4188)  data: 0.2365 (0.0004 -- 1.8784)  max mem: 16735
Epoch: [156]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.7552 (1.9172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1493 (8.3029)  time: 0.8393 (0.5285 -- 3.0739)  data: 0.1861 (0.0003 -- 2.5538)  max mem: 16735
[2023-09-01 04:41:40,153] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:41:40,153] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:41:40,154] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:41:40,154] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [156]  [100/160]  eta: 0:00:54  lr: 0.000006  min_lr: 0.000000  loss: 1.7661 (1.8908)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7473 (8.2609)  time: 0.8402 (0.5216 -- 3.0084)  data: 0.2097 (0.0006 -- 2.4494)  max mem: 16735
[2023-09-01 04:41:45,202] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25061
[2023-09-01 04:41:45,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:41:45,202] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 04:41:45,202] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25061
[2023-09-01 04:41:45,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [156]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.9401 (1.8794)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1138 (8.1278)  time: 0.8980 (0.5430 -- 3.0858)  data: 0.1505 (0.0004 -- 1.9482)  max mem: 16735
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.9968 (1.8935)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3374 (8.1364)  time: 0.8820 (0.5393 -- 3.2842)  data: 0.1838 (0.0004 -- 2.0361)  max mem: 16735
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9125 (1.8808)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2729 (8.1226)  time: 0.7172 (0.5022 -- 3.5579)  data: 0.1220 (0.0002 -- 2.4204)  max mem: 16735
Epoch: [156] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9125 (1.8801)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2729 (8.1226)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2721 (0.2721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3508 (2.3508 -- 2.3508)  data: 2.1285 (2.1285 -- 2.1285)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6294 (0.8185)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4394 (0.2019 -- 2.3508)  data: 0.2125 (0.0007 -- 2.1285)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6332 (0.7585)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2252 (0.1701 -- 0.4280)  data: 0.0137 (0.0001 -- 0.1828)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7112 (0.8376)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (96.2656)  time: 0.2037 (0.1326 -- 0.4280)  data: 0.0126 (0.0001 -- 0.1828)  max mem: 16735
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 76.971 Acc@5 96.266 loss 0.831
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 79.05%
Epoch: [157]  [  0/160]  eta: 0:19:11  lr: 0.000005  min_lr: 0.000000  loss: 1.4394 (1.4394)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9176 (5.9176)  time: 7.1941 (7.1941 -- 7.1941)  data: 6.6013 (6.6013 -- 6.6013)  max mem: 16735
Epoch: [157]  [ 20/160]  eta: 0:02:43  lr: 0.000005  min_lr: 0.000000  loss: 2.0173 (1.8784)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7377 (8.6320)  time: 0.8701 (0.5305 -- 4.7095)  data: 0.2070 (0.0005 -- 2.5459)  max mem: 16735
Epoch: [157]  [ 40/160]  eta: 0:02:11  lr: 0.000005  min_lr: 0.000000  loss: 1.9341 (1.8893)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4101 (8.5153)  time: 1.0093 (0.5231 -- 4.0885)  data: 0.0773 (0.0004 -- 0.7435)  max mem: 16735
Epoch: [157]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000000  loss: 1.9542 (1.8999)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2029 (8.3765)  time: 0.8192 (0.5260 -- 2.9187)  data: 0.0613 (0.0001 -- 0.8744)  max mem: 16735
[2023-09-01 04:43:48,601] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:43:48,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:43:48,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:43:48,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:43:57,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25200
[2023-09-01 04:43:57,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:43:57,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25200
[2023-09-01 04:43:57,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:43:57,840] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [157]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.8576 (1.8909)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6042 (8.4732)  time: 0.8341 (0.5292 -- 2.3875)  data: 0.1519 (0.0003 -- 1.8480)  max mem: 16735
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 2.1863 (1.9322)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1746 (8.4815)  time: 0.8475 (0.5257 -- 2.2779)  data: 0.2123 (0.0003 -- 1.5695)  max mem: 16735
Epoch: [157]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8563 (1.9328)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1266 (8.3404)  time: 0.8661 (0.5282 -- 3.7090)  data: 0.2956 (0.0003 -- 3.1895)  max mem: 16735
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8771 (1.9173)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4415 (8.3985)  time: 0.8053 (0.5197 -- 2.2272)  data: 0.2545 (0.0003 -- 1.6736)  max mem: 16735
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7286 (1.8983)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6811 (8.4306)  time: 0.7419 (0.5030 -- 1.9839)  data: 0.1600 (0.0002 -- 1.4165)  max mem: 16735
Epoch: [157] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7286 (1.9329)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6811 (8.4306)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2788 (0.2788)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5368 (2.5368 -- 2.5368)  data: 2.3057 (2.3057 -- 2.3057)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6270 (0.8177)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4457 (0.1998 -- 2.5368)  data: 0.2277 (0.0005 -- 2.3057)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6444 (0.7618)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1694 -- 0.4244)  data: 0.0149 (0.0001 -- 0.1912)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7203 (0.8412)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.1989 (0.1330 -- 0.4244)  data: 0.0147 (0.0001 -- 0.1912)  max mem: 16735
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.834
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [158]  [  0/160]  eta: 0:20:03  lr: 0.000005  min_lr: 0.000000  loss: 2.0322 (2.0322)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6288 (9.6288)  time: 7.5228 (7.5228 -- 7.5228)  data: 6.7279 (6.7279 -- 6.7279)  max mem: 16735
Epoch: [158]  [ 20/160]  eta: 0:02:45  lr: 0.000005  min_lr: 0.000000  loss: 1.9366 (1.9651)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6424 (7.7423)  time: 0.8685 (0.5269 -- 2.5510)  data: 0.2729 (0.0004 -- 2.0013)  max mem: 16735
Epoch: [158]  [ 40/160]  eta: 0:02:01  lr: 0.000005  min_lr: 0.000000  loss: 1.8433 (1.9143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3124 (8.0539)  time: 0.8261 (0.5306 -- 2.5190)  data: 0.1862 (0.0002 -- 1.9976)  max mem: 16735
[2023-09-01 04:46:00,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:46:00,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:46:00,288] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:46:00,289] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [158]  [ 60/160]  eta: 0:01:34  lr: 0.000005  min_lr: 0.000000  loss: 1.9194 (1.9319)  loss_scale: 65536.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5299 (8.0341)  time: 0.8151 (0.5322 -- 2.6584)  data: 0.0827 (0.0004 -- 0.8420)  max mem: 16735
[2023-09-01 04:46:10,651] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25342
[2023-09-01 04:46:10,651] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:46:10,653] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25342
[2023-09-01 04:46:10,654] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:46:10,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [158]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.9404 (1.9416)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3453 (8.1053)  time: 0.9730 (0.5259 -- 3.7537)  data: 0.1350 (0.0004 -- 1.4518)  max mem: 16735
Epoch: [158]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.6825 (1.9116)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7252 (8.0299)  time: 0.8585 (0.5364 -- 3.0372)  data: 0.0014 (0.0003 -- 0.0030)  max mem: 16735
Epoch: [158]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8719 (1.9046)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1722 (8.0803)  time: 0.7465 (0.5370 -- 3.9277)  data: 0.0524 (0.0003 -- 0.5252)  max mem: 16735
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7132 (1.8912)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9225 (8.0954)  time: 1.0385 (0.5277 -- 3.1503)  data: 0.3119 (0.0003 -- 2.6192)  max mem: 16735
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8668 (1.8991)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8757 (8.0563)  time: 0.6847 (0.5000 -- 2.2347)  data: 0.1590 (0.0002 -- 1.7369)  max mem: 16735
Epoch: [158] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8668 (1.8809)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8757 (8.0563)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2789 (0.2789)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5250 (2.5250 -- 2.5250)  data: 2.2588 (2.2588 -- 2.2588)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6143 (0.8127)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4300 (0.1884 -- 2.5250)  data: 0.2145 (0.0003 -- 2.2588)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6453 (0.7630)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2129 (0.1691 -- 0.3366)  data: 0.0126 (0.0001 -- 0.1489)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7416 (0.8401)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.1999 (0.1329 -- 0.3366)  data: 0.0125 (0.0001 -- 0.1489)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.833
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [159]  [  0/160]  eta: 0:19:10  lr: 0.000005  min_lr: 0.000000  loss: 2.2530 (2.2530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4210 (8.4210)  time: 7.1907 (7.1907 -- 7.1907)  data: 6.0299 (6.0299 -- 6.0299)  max mem: 16735
Epoch: [159]  [ 20/160]  eta: 0:02:42  lr: 0.000005  min_lr: 0.000000  loss: 1.9621 (1.8727)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4689 (8.9167)  time: 0.8622 (0.5355 -- 3.3997)  data: 0.2473 (0.0004 -- 2.1489)  max mem: 16735
[2023-09-01 04:48:14,750] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:48:14,750] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:48:14,750] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:48:14,750] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [159]  [ 40/160]  eta: 0:02:07  lr: 0.000005  min_lr: 0.000000  loss: 1.9201 (1.9234)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8338 (8.1328)  time: 0.9609 (0.5295 -- 3.3854)  data: 0.4032 (0.0004 -- 2.8374)  max mem: 16735
Epoch: [159]  [ 60/160]  eta: 0:01:34  lr: 0.000005  min_lr: 0.000000  loss: 1.6898 (1.8822)  loss_scale: 65536.0000 (48883.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1938 (8.3879)  time: 0.7105 (0.5269 -- 1.9493)  data: 0.0888 (0.0003 -- 0.9786)  max mem: 16735
[2023-09-01 04:48:47,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25509
[2023-09-01 04:48:47,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:48:47,076] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25509
[2023-09-01 04:48:47,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:48:47,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [159]  [ 80/160]  eta: 0:01:14  lr: 0.000005  min_lr: 0.000000  loss: 1.7130 (1.8651)  loss_scale: 32768.0000 (48140.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6064 (8.4851)  time: 0.8972 (0.5270 -- 3.6209)  data: 0.2922 (0.0003 -- 3.0834)  max mem: 16735
Epoch: [159]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.8248 (1.8690)  loss_scale: 32768.0000 (45096.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5577 (8.5734)  time: 0.9040 (0.5261 -- 4.3509)  data: 0.3522 (0.0002 -- 3.8181)  max mem: 16735
[2023-09-01 04:49:24,088] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25551
[2023-09-01 04:49:24,088] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25551
[2023-09-01 04:49:24,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:49:24,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 04:49:24,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.9293 (1.8798)  loss_scale: 16384.0000 (41704.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0625 (8.5681)  time: 0.7958 (0.5221 -- 2.1187)  data: 0.0758 (0.0003 -- 1.3878)  max mem: 16735
Epoch: [159]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.9244 (1.8878)  loss_scale: 16384.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2254 (8.5331)  time: 0.8845 (0.5318 -- 3.2371)  data: 0.0671 (0.0002 -- 1.3146)  max mem: 16735
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9891 (1.8872)  loss_scale: 16384.0000 (35532.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8508 (8.4944)  time: 0.6956 (0.5033 -- 2.5109)  data: 0.0264 (0.0001 -- 0.5071)  max mem: 16735
Epoch: [159] Total time: 0:02:20 (0.8807 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9891 (1.8884)  loss_scale: 16384.0000 (35532.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8508 (8.4944)
[2023-09-01 04:50:02,293] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-09-01 04:50:02,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-09-01 04:50:02,297] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-09-01 04:50:02,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-09-01 04:50:03,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-09-01 04:50:03,386] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2756 (0.2756)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4425 (2.4425 -- 2.4425)  data: 2.2188 (2.2188 -- 2.2188)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6107 (0.8121)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4308 (0.1913 -- 2.4425)  data: 0.2099 (0.0006 -- 2.2188)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6501 (0.7570)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1771 -- 0.3000)  data: 0.0128 (0.0001 -- 0.1097)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7188 (0.8365)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2029 (0.1325 -- 0.3000)  data: 0.0125 (0.0001 -- 0.1097)  max mem: 16735
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.829
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [160]  [  0/160]  eta: 0:23:31  lr: 0.000005  min_lr: 0.000000  loss: 2.2133 (2.2133)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8604 (6.8604)  time: 8.8248 (8.8248 -- 8.8248)  data: 5.5455 (5.5455 -- 5.5455)  max mem: 16735
Epoch: [160]  [ 20/160]  eta: 0:02:41  lr: 0.000005  min_lr: 0.000000  loss: 1.9564 (1.9775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7119 (8.8313)  time: 0.7728 (0.5270 -- 3.0711)  data: 0.1391 (0.0005 -- 2.5117)  max mem: 16735
Epoch: [160]  [ 40/160]  eta: 0:02:11  lr: 0.000005  min_lr: 0.000000  loss: 1.8196 (1.9071)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9445 (8.5809)  time: 1.0302 (0.5359 -- 4.8401)  data: 0.4549 (0.0003 -- 4.3065)  max mem: 16735
Epoch: [160]  [ 60/160]  eta: 0:01:38  lr: 0.000005  min_lr: 0.000000  loss: 2.0794 (1.9081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6131 (8.4034)  time: 0.7592 (0.5233 -- 2.9778)  data: 0.2149 (0.0004 -- 2.4527)  max mem: 16735
[2023-09-01 04:51:29,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:51:29,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 04:51:29,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:51:29,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [160]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000000  loss: 1.7854 (1.8696)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6301 (8.4049)  time: 0.9253 (0.5401 -- 3.7238)  data: 0.3740 (0.0004 -- 3.2087)  max mem: 16735
Epoch: [160]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7525 (1.8664)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3909 (8.3681)  time: 0.7337 (0.5284 -- 2.7674)  data: 0.1675 (0.0001 -- 2.1887)  max mem: 16735
Epoch: [160]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 2.0049 (1.8803)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6377 (8.3948)  time: 0.9842 (0.5247 -- 4.3753)  data: 0.4059 (0.0002 -- 3.8465)  max mem: 16735
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 2.0271 (1.8917)  loss_scale: 32768.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7579 (8.4532)  time: 0.7744 (0.5307 -- 4.0259)  data: 0.2266 (0.0004 -- 3.5059)  max mem: 16735
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8567 (1.8890)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3119 (8.4407)  time: 0.7218 (0.5027 -- 2.8269)  data: 0.1937 (0.0002 -- 2.2804)  max mem: 16735
Epoch: [160] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8567 (1.9031)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3119 (8.4407)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2746 (0.2746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4193 (2.4193 -- 2.4193)  data: 2.1832 (2.1832 -- 2.1832)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6161 (0.8210)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4233 (0.1952 -- 2.4193)  data: 0.2081 (0.0004 -- 2.1832)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6503 (0.7611)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1777 -- 0.3414)  data: 0.0146 (0.0001 -- 0.1411)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7096 (0.8412)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2067 (0.1324 -- 0.3414)  data: 0.0144 (0.0001 -- 0.1411)  max mem: 16735
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.831
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [161]  [  0/160]  eta: 0:18:00  lr: 0.000005  min_lr: 0.000000  loss: 2.5104 (2.5104)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6182 (9.6182)  time: 6.7530 (6.7530 -- 6.7530)  data: 6.1934 (6.1934 -- 6.1934)  max mem: 16735
Epoch: [161]  [ 20/160]  eta: 0:02:40  lr: 0.000004  min_lr: 0.000000  loss: 1.6970 (1.7608)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2779 (8.3059)  time: 0.8627 (0.5288 -- 3.0400)  data: 0.3081 (0.0004 -- 2.5136)  max mem: 16735
Epoch: [161]  [ 40/160]  eta: 0:01:58  lr: 0.000004  min_lr: 0.000000  loss: 1.7835 (1.7888)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9912 (8.5015)  time: 0.8229 (0.5323 -- 3.1255)  data: 0.2683 (0.0006 -- 2.5517)  max mem: 16735
[2023-09-01 04:53:29,029] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:53:29,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:53:29,031] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:53:29,032] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [161]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.8620 (1.8299)  loss_scale: 65536.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4188 (8.3150)  time: 0.9672 (0.5358 -- 4.4572)  data: 0.4137 (0.0006 -- 3.9328)  max mem: 16735
[2023-09-01 04:53:47,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25826
[2023-09-01 04:53:47,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25826
[2023-09-01 04:53:47,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:53:47,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:53:47,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [161]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.8357 (1.8502)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8389 (8.2602)  time: 0.8418 (0.5404 -- 3.5368)  data: 0.2866 (0.0004 -- 2.9732)  max mem: 16735
Epoch: [161]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 2.0238 (1.8743)  loss_scale: 32768.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9295 (8.2362)  time: 0.9509 (0.5390 -- 3.9265)  data: 0.3990 (0.0004 -- 3.3790)  max mem: 16735
Epoch: [161]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.9217 (1.8731)  loss_scale: 32768.0000 (37642.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1421 (8.2786)  time: 0.8171 (0.5207 -- 3.9237)  data: 0.2659 (0.0004 -- 3.4020)  max mem: 16735
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7892 (1.8503)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8739 (8.2895)  time: 0.9062 (0.5263 -- 3.4568)  data: 0.3374 (0.0005 -- 2.9262)  max mem: 16735
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8685 (1.8436)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3351 (8.2840)  time: 0.6783 (0.5041 -- 1.9126)  data: 0.0050 (0.0002 -- 0.0874)  max mem: 16735
Epoch: [161] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8685 (1.8804)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3351 (8.2840)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2803 (0.2803)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5008 (2.5008 -- 2.5008)  data: 2.2588 (2.2588 -- 2.2588)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6138 (0.8201)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4254 (0.2034 -- 2.5008)  data: 0.2062 (0.0005 -- 2.2588)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6387 (0.7608)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2163 (0.1691 -- 0.3183)  data: 0.0070 (0.0001 -- 0.1281)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7102 (0.8415)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2015 (0.1322 -- 0.3183)  data: 0.0067 (0.0001 -- 0.1281)  max mem: 16735
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 78.008 Acc@5 96.058 loss 0.835
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [162]  [  0/160]  eta: 0:19:13  lr: 0.000004  min_lr: 0.000000  loss: 1.9020 (1.9020)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0703 (7.0703)  time: 7.2118 (7.2118 -- 7.2118)  data: 5.3271 (5.3271 -- 5.3271)  max mem: 16735
Epoch: [162]  [ 20/160]  eta: 0:02:40  lr: 0.000004  min_lr: 0.000000  loss: 1.9314 (1.9658)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4819 (7.9414)  time: 0.8435 (0.5244 -- 2.7815)  data: 0.1089 (0.0004 -- 1.5874)  max mem: 16735
[2023-09-01 04:55:51,469] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:55:51,469] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:55:51,469] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:55:51,469] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [162]  [ 40/160]  eta: 0:02:02  lr: 0.000004  min_lr: 0.000000  loss: 1.7353 (1.8880)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6478 (8.0777)  time: 0.8823 (0.5254 -- 3.8552)  data: 0.2334 (0.0004 -- 3.1061)  max mem: 16735
Epoch: [162]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.6643 (1.8380)  loss_scale: 65536.0000 (46734.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6929 (8.0667)  time: 0.9285 (0.5422 -- 4.8859)  data: 0.2341 (0.0007 -- 1.7499)  max mem: 16735
[2023-09-01 04:56:27,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=162, lr=[9.961138567789659e-08, 9.961138567789659e-08, 1.3281518090386211e-07, 1.3281518090386211e-07, 1.7708690787181616e-07, 1.7708690787181616e-07, 2.3611587716242153e-07, 2.3611587716242153e-07, 3.148211695498954e-07, 3.148211695498954e-07, 4.197615593998605e-07, 4.197615593998605e-07, 5.59682079199814e-07, 5.59682079199814e-07, 7.462427722664186e-07, 7.462427722664186e-07, 9.949903630218917e-07, 9.949903630218917e-07, 1.326653817362522e-06, 1.326653817362522e-06, 1.768871756483363e-06, 1.768871756483363e-06, 2.3584956753111504e-06, 2.3584956753111504e-06, 3.144660900414867e-06, 3.144660900414867e-06, 4.192881200553156e-06, 4.192881200553156e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 04:56:27,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=18.065154457671383, CurrSamplesPerSec=22.26952723160302, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [162]  [ 80/160]  eta: 0:01:14  lr: 0.000004  min_lr: 0.000000  loss: 1.7987 (1.8533)  loss_scale: 65536.0000 (51376.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7088 (8.1799)  time: 0.7771 (0.5275 -- 2.7561)  data: 0.0556 (0.0004 -- 1.0652)  max mem: 16735
[2023-09-01 04:56:45,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26018
[2023-09-01 04:56:45,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26018
[2023-09-01 04:56:45,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:56:45,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:56:45,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [162]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8625 (1.8460)  loss_scale: 65536.0000 (53207.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5263 (8.1318)  time: 0.8985 (0.5173 -- 3.2326)  data: 0.0590 (0.0001 -- 1.1346)  max mem: 16735
Epoch: [162]  [120/160]  eta: 0:00:35  lr: 0.000004  min_lr: 0.000000  loss: 1.9119 (1.8493)  loss_scale: 32768.0000 (49829.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8424 (8.1476)  time: 0.7442 (0.5344 -- 2.6360)  data: 0.0191 (0.0004 -- 0.3602)  max mem: 16735
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8686 (1.8450)  loss_scale: 32768.0000 (47409.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2598 (8.2289)  time: 1.0048 (0.5343 -- 2.8066)  data: 0.3982 (0.0001 -- 2.2504)  max mem: 16735
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7889 (1.8473)  loss_scale: 32768.0000 (45670.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1451 (8.2458)  time: 0.6250 (0.5008 -- 1.8754)  data: 0.1004 (0.0002 -- 1.3508)  max mem: 16735
Epoch: [162] Total time: 0:02:20 (0.8800 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7889 (1.8923)  loss_scale: 32768.0000 (45670.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1451 (8.2458)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2773 (0.2773)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4208 (2.4208 -- 2.4208)  data: 2.2108 (2.2108 -- 2.2108)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5917 (0.8117)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4230 (0.2042 -- 2.4208)  data: 0.2021 (0.0006 -- 2.2108)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6463 (0.7581)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2206 (0.1699 -- 0.3452)  data: 0.0093 (0.0001 -- 0.1716)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7155 (0.8363)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2029 (0.1333 -- 0.3452)  data: 0.0090 (0.0001 -- 0.1716)  max mem: 16735
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.830
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [163]  [  0/160]  eta: 0:13:35  lr: 0.000004  min_lr: 0.000000  loss: 0.9843 (0.9843)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8995 (7.8995)  time: 5.0974 (5.0974 -- 5.0974)  data: 4.5596 (4.5596 -- 4.5596)  max mem: 16735
Epoch: [163]  [ 20/160]  eta: 0:02:50  lr: 0.000004  min_lr: 0.000000  loss: 1.9830 (1.8961)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2515 (8.4342)  time: 1.0243 (0.5319 -- 3.6082)  data: 0.4692 (0.0005 -- 3.0814)  max mem: 16735
Epoch: [163]  [ 40/160]  eta: 0:02:05  lr: 0.000004  min_lr: 0.000000  loss: 1.9018 (1.8961)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0730 (8.4185)  time: 0.8726 (0.5174 -- 3.8496)  data: 0.3282 (0.0003 -- 3.3414)  max mem: 16735
Epoch: [163]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000000  loss: 1.7063 (1.8639)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2912 (8.3586)  time: 0.9292 (0.5286 -- 3.9271)  data: 0.3849 (0.0005 -- 3.3915)  max mem: 16735
[2023-09-01 04:58:49,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:58:49,877] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:58:49,877] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 04:58:49,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 04:58:50,969] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26149
[2023-09-01 04:58:50,969] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26149
[2023-09-01 04:58:50,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:58:50,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 04:58:50,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [163]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.8230 (1.8543)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3035 (8.3896)  time: 0.7684 (0.5206 -- 3.8731)  data: 0.2229 (0.0003 -- 3.3308)  max mem: 16735
Epoch: [163]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8976 (1.8682)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8031 (8.3251)  time: 0.8364 (0.5261 -- 2.7618)  data: 0.2850 (0.0003 -- 2.2319)  max mem: 16735
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8616 (1.8670)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0254 (8.2833)  time: 0.8074 (0.5399 -- 2.1841)  data: 0.1452 (0.0003 -- 1.6242)  max mem: 16735
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8782 (1.8690)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3502 (8.3761)  time: 0.9294 (0.5213 -- 3.0469)  data: 0.3021 (0.0006 -- 2.5027)  max mem: 16735
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8147 (1.8606)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8842 (8.2803)  time: 0.7028 (0.5029 -- 1.7686)  data: 0.0566 (0.0001 -- 1.1082)  max mem: 16735
Epoch: [163] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8147 (1.8819)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8842 (8.2803)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2743 (0.2743)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3835 (2.3835 -- 2.3835)  data: 2.1376 (2.1376 -- 2.1376)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6042 (0.8156)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4063 (0.1899 -- 2.3835)  data: 0.1951 (0.0005 -- 2.1376)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6523 (0.7604)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2177 (0.1694 -- 0.4490)  data: 0.0147 (0.0001 -- 0.2702)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7136 (0.8386)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2026 (0.1330 -- 0.4490)  data: 0.0144 (0.0001 -- 0.2702)  max mem: 16735
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.832
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [164]  [  0/160]  eta: 0:21:18  lr: 0.000004  min_lr: 0.000000  loss: 2.2264 (2.2264)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4733 (7.4733)  time: 7.9919 (7.9919 -- 7.9919)  data: 6.8037 (6.8037 -- 6.8037)  max mem: 16735
Epoch: [164]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000000  loss: 2.0071 (1.9498)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1054 (8.8931)  time: 0.7899 (0.5357 -- 2.2223)  data: 0.0369 (0.0004 -- 0.7107)  max mem: 16735
[2023-09-01 05:00:50,976] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:00:50,976] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:00:50,977] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:00:50,977] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [164]  [ 40/160]  eta: 0:02:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8533 (1.9194)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1340 (8.5432)  time: 0.8719 (0.5277 -- 3.5970)  data: 0.2837 (0.0004 -- 3.0639)  max mem: 16735
[2023-09-01 05:00:56,594] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26284
[2023-09-01 05:00:56,594] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26284
[2023-09-01 05:00:56,636] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:00:56,636] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:00:56,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [164]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000000  loss: 1.9564 (1.9163)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0464 (8.6811)  time: 0.9606 (0.5258 -- 5.3235)  data: 0.4123 (0.0007 -- 4.7815)  max mem: 16735
Epoch: [164]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.9360 (1.9209)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7932 (8.5172)  time: 0.7945 (0.5312 -- 3.0130)  data: 0.2413 (0.0003 -- 2.4541)  max mem: 16735
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.9653 (1.9115)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5218 (8.6348)  time: 0.9623 (0.5262 -- 4.1096)  data: 0.4051 (0.0004 -- 3.5914)  max mem: 16735
Epoch: [164]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8517 (1.8970)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7855 (8.5668)  time: 0.7967 (0.5375 -- 3.0942)  data: 0.2445 (0.0002 -- 2.5595)  max mem: 16735
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.9808 (1.9083)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6918 (8.4823)  time: 0.9263 (0.5345 -- 3.4195)  data: 0.3813 (0.0005 -- 2.8956)  max mem: 16735
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7555 (1.8947)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1565 (8.4787)  time: 0.7118 (0.5019 -- 1.9700)  data: 0.1881 (0.0002 -- 1.4695)  max mem: 16735
Epoch: [164] Total time: 0:02:23 (0.8988 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7555 (1.8884)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1565 (8.4787)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2705 (0.2705)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3271 (2.3271 -- 2.3271)  data: 2.0873 (2.0873 -- 2.0873)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.6016 (0.8153)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4118 (0.1989 -- 2.3271)  data: 0.1965 (0.0005 -- 2.0873)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6549 (0.7601)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2213 (0.1690 -- 0.5147)  data: 0.0203 (0.0001 -- 0.3277)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7121 (0.8385)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2059 (0.1321 -- 0.5147)  data: 0.0200 (0.0001 -- 0.3277)  max mem: 16735
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.831
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [165]  [  0/160]  eta: 0:19:44  lr: 0.000004  min_lr: 0.000000  loss: 2.7020 (2.7020)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9999 (10.9999)  time: 7.4004 (7.4004 -- 7.4004)  data: 6.0410 (6.0410 -- 6.0410)  max mem: 16735
[2023-09-01 05:02:59,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:02:59,981] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:02:59,981] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:02:59,981] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:03:04,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26417
[2023-09-01 05:03:04,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:03:04,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:03:04,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26417
[2023-09-01 05:03:04,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [165]  [ 20/160]  eta: 0:02:36  lr: 0.000004  min_lr: 0.000000  loss: 2.0286 (1.9815)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0319 (8.4521)  time: 0.8074 (0.5314 -- 3.3828)  data: 0.0069 (0.0004 -- 0.1008)  max mem: 16735
Epoch: [165]  [ 40/160]  eta: 0:01:58  lr: 0.000004  min_lr: 0.000000  loss: 1.9202 (1.9125)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8740 (8.1524)  time: 0.8414 (0.5365 -- 2.5130)  data: 0.0279 (0.0008 -- 0.3700)  max mem: 16735
Epoch: [165]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.8819 (1.9055)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7630 (8.0385)  time: 0.9867 (0.5284 -- 4.8107)  data: 0.0123 (0.0005 -- 0.2108)  max mem: 16735
Epoch: [165]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.7019 (1.8783)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2921 (8.2732)  time: 0.8294 (0.5403 -- 2.7990)  data: 0.0028 (0.0004 -- 0.0159)  max mem: 16735
[2023-09-01 05:04:14,203] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26497
[2023-09-01 05:04:14,203] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:04:14,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26497
[2023-09-01 05:04:14,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:04:14,204] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [165]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.9896 (1.8863)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8145 (8.1997)  time: 0.8327 (0.5250 -- 3.4041)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16735
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 2.0045 (1.8961)  loss_scale: 16384.0000 (30601.5207)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2682 (8.2401)  time: 0.9416 (0.5315 -- 4.1583)  data: 0.0014 (0.0006 -- 0.0032)  max mem: 16735
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.9508 (1.8970)  loss_scale: 16384.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6714 (8.2739)  time: 0.7749 (0.5246 -- 3.6833)  data: 0.0020 (0.0002 -- 0.0140)  max mem: 16735
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9426 (1.8853)  loss_scale: 16384.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9352 (8.2822)  time: 0.7187 (0.5016 -- 2.3688)  data: 0.0007 (0.0002 -- 0.0033)  max mem: 16735
Epoch: [165] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9426 (1.9215)  loss_scale: 16384.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9352 (8.2822)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2714 (0.2714)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3685 (2.3685 -- 2.3685)  data: 2.1303 (2.1303 -- 2.1303)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.6042 (0.8140)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4224 (0.1926 -- 2.3685)  data: 0.2034 (0.0002 -- 2.1303)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6549 (0.7576)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2213 (0.1691 -- 0.3342)  data: 0.0108 (0.0001 -- 0.1043)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7102 (0.8358)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2071 (0.1333 -- 0.3342)  data: 0.0105 (0.0001 -- 0.1043)  max mem: 16735
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.829
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [166]  [  0/160]  eta: 0:16:15  lr: 0.000003  min_lr: 0.000000  loss: 2.2507 (2.2507)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5118 (9.5118)  time: 6.0943 (6.0943 -- 6.0943)  data: 5.5473 (5.5473 -- 5.5473)  max mem: 16735
Epoch: [166]  [ 20/160]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000000  loss: 2.0864 (1.9859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1005 (8.4788)  time: 0.9368 (0.5230 -- 2.9818)  data: 0.1354 (0.0002 -- 2.4244)  max mem: 16735
Epoch: [166]  [ 40/160]  eta: 0:01:59  lr: 0.000003  min_lr: 0.000000  loss: 2.0103 (1.9536)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0748 (8.3177)  time: 0.7999 (0.5239 -- 2.5428)  data: 0.0185 (0.0003 -- 0.3243)  max mem: 16735
Epoch: [166]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.7235 (1.8783)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9190 (8.3548)  time: 1.0381 (0.5368 -- 3.7883)  data: 0.0013 (0.0003 -- 0.0026)  max mem: 16735
[2023-09-01 05:06:17,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:06:17,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:06:17,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 05:06:17,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [166]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8387 (1.8618)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2726 (8.2838)  time: 0.8712 (0.5265 -- 3.7273)  data: 0.0011 (0.0002 -- 0.0035)  max mem: 16735
Epoch: [166]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.9332 (1.8918)  loss_scale: 32768.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8855 (8.4169)  time: 0.7252 (0.5346 -- 1.8694)  data: 0.0016 (0.0002 -- 0.0033)  max mem: 16735
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9252 (1.8884)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9371 (8.4189)  time: 0.8243 (0.5287 -- 3.4642)  data: 0.0017 (0.0004 -- 0.0078)  max mem: 16735
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8486 (1.8839)  loss_scale: 32768.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1258 (8.4238)  time: 0.8708 (0.5366 -- 3.0911)  data: 0.0974 (0.0003 -- 1.5266)  max mem: 16735
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9406 (1.8875)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5636 (8.4744)  time: 0.6989 (0.5039 -- 2.1275)  data: 0.0721 (0.0003 -- 1.0494)  max mem: 16735
Epoch: [166] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9406 (1.8952)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5636 (8.4744)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2689 (0.2689)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3731 (2.3731 -- 2.3731)  data: 2.1544 (2.1544 -- 2.1544)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5970 (0.8083)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4294 (0.2083 -- 2.3731)  data: 0.2072 (0.0007 -- 2.1544)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6470 (0.7534)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2215 (0.1689 -- 0.3443)  data: 0.0107 (0.0001 -- 0.1138)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7076 (0.8319)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2052 (0.1321 -- 0.3443)  data: 0.0104 (0.0001 -- 0.1138)  max mem: 16735
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [167]  [  0/160]  eta: 0:21:44  lr: 0.000003  min_lr: 0.000000  loss: 2.3234 (2.3234)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7721 (6.7721)  time: 8.1562 (8.1562 -- 8.1562)  data: 7.6277 (7.6277 -- 7.6277)  max mem: 16735
Epoch: [167]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 1.8574 (1.8957)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7889 (8.8645)  time: 0.8406 (0.5312 -- 3.8593)  data: 0.2896 (0.0003 -- 3.3145)  max mem: 16735
[2023-09-01 05:08:17,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:08:17,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:08:17,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:08:17,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [167]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7740 (1.8405)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7344 (8.3353)  time: 0.8035 (0.5136 -- 3.6061)  data: 0.2538 (0.0002 -- 3.0410)  max mem: 16735
[2023-09-01 05:08:34,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26775
[2023-09-01 05:08:34,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:08:34,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26775
[2023-09-01 05:08:34,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:08:34,018] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [167]  [ 60/160]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8842 (1.8505)  loss_scale: 65536.0000 (44048.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0188 (8.4376)  time: 0.9514 (0.5254 -- 3.7012)  data: 0.4071 (0.0004 -- 3.1679)  max mem: 16735
Epoch: [167]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8188 (1.8452)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7625 (8.3409)  time: 0.7499 (0.5308 -- 3.5633)  data: 0.2044 (0.0001 -- 3.0183)  max mem: 16735
Epoch: [167]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 2.1468 (1.8855)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0697 (8.5774)  time: 0.9599 (0.5286 -- 4.0984)  data: 0.1087 (0.0004 -- 1.1975)  max mem: 16735
Epoch: [167]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.8773 (1.8825)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3358 (8.5904)  time: 0.9340 (0.5195 -- 3.6623)  data: 0.0329 (0.0003 -- 0.6290)  max mem: 16735
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.9506 (1.8956)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4950 (8.5953)  time: 0.8100 (0.5267 -- 2.4862)  data: 0.2541 (0.0003 -- 1.9336)  max mem: 16735
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8291 (1.8974)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0354 (8.5812)  time: 0.7335 (0.4990 -- 2.5097)  data: 0.2072 (0.0002 -- 1.9673)  max mem: 16735
Epoch: [167] Total time: 0:02:23 (0.8957 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8291 (1.8933)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0354 (8.5812)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2716 (0.2716)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2875 (2.2875 -- 2.2875)  data: 2.0657 (2.0657 -- 2.0657)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5909 (0.8069)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4156 (0.1941 -- 2.2875)  data: 0.1991 (0.0005 -- 2.0657)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6425 (0.7541)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2262 (0.1691 -- 0.4346)  data: 0.0184 (0.0001 -- 0.2236)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7139 (0.8319)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2117 (0.1331 -- 0.4346)  data: 0.0182 (0.0001 -- 0.2236)  max mem: 16735
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [168]  [  0/160]  eta: 0:19:16  lr: 0.000003  min_lr: 0.000000  loss: 2.2807 (2.2807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8278 (6.8278)  time: 7.2300 (7.2300 -- 7.2300)  data: 6.1163 (6.1163 -- 6.1163)  max mem: 16735
Epoch: [168]  [ 20/160]  eta: 0:02:47  lr: 0.000003  min_lr: 0.000000  loss: 1.9217 (1.8805)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5172 (8.7936)  time: 0.8975 (0.5254 -- 4.1603)  data: 0.2126 (0.0003 -- 3.6239)  max mem: 16735
[2023-09-01 05:10:40,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:10:40,533] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:10:40,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:10:40,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:10:41,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26906
[2023-09-01 05:10:41,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:10:41,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26906
[2023-09-01 05:10:41,653] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:10:41,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [168]  [ 40/160]  eta: 0:01:59  lr: 0.000003  min_lr: 0.000000  loss: 1.8778 (1.9051)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7370 (8.3617)  time: 0.7894 (0.5341 -- 2.6955)  data: 0.0091 (0.0005 -- 0.1539)  max mem: 16735
Epoch: [168]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 2.0145 (1.9139)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7118 (8.3638)  time: 0.9382 (0.5423 -- 3.4593)  data: 0.1503 (0.0003 -- 1.4955)  max mem: 16735
Epoch: [168]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.9613 (1.9188)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1936 (8.5416)  time: 0.8581 (0.5228 -- 4.6394)  data: 0.2755 (0.0002 -- 4.1151)  max mem: 16735
Epoch: [168]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.8529 (1.9136)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7798 (8.6480)  time: 0.8725 (0.5358 -- 3.4781)  data: 0.3225 (0.0003 -- 2.9608)  max mem: 16735
[2023-09-01 05:12:00,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=169, lr=[7.016158269260065e-08, 7.016158269260065e-08, 9.354877692346754e-08, 9.354877692346754e-08, 1.247317025646234e-07, 1.247317025646234e-07, 1.6630893675283117e-07, 1.6630893675283117e-07, 2.217452490037749e-07, 2.217452490037749e-07, 2.956603320050332e-07, 2.956603320050332e-07, 3.9421377600671096e-07, 3.9421377600671096e-07, 5.256183680089479e-07, 5.256183680089479e-07, 7.008244906785972e-07, 7.008244906785972e-07, 9.344326542381296e-07, 9.344326542381296e-07, 1.2459102056508394e-06, 1.2459102056508394e-06, 1.6612136075344526e-06, 1.6612136075344526e-06, 2.2149514767126034e-06, 2.2149514767126034e-06, 2.9532686356168047e-06, 2.9532686356168047e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 05:12:00,469] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=18.05907591080508, CurrSamplesPerSec=22.197516588053954, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [168]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8666 (1.9111)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3349 (8.6824)  time: 0.7680 (0.5378 -- 3.8593)  data: 0.2114 (0.0003 -- 3.3289)  max mem: 16735
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6172 (1.8863)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3923 (8.6005)  time: 0.9707 (0.5271 -- 5.2151)  data: 0.4242 (0.0003 -- 4.7009)  max mem: 16735
[2023-09-01 05:12:31,056] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:12:31,056] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:12:31,057] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:12:31,057] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9724 (1.8926)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2930 (8.5241)  time: 0.6600 (0.5020 -- 3.2483)  data: 0.1376 (0.0002 -- 2.7373)  max mem: 16735
Epoch: [168] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9724 (1.8647)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2930 (8.5241)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2721 (0.2721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3902 (2.3902 -- 2.3902)  data: 2.1650 (2.1650 -- 2.1650)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5981 (0.8068)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4298 (0.1976 -- 2.3902)  data: 0.2098 (0.0004 -- 2.1650)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6504 (0.7549)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2240 (0.1685 -- 0.3513)  data: 0.0136 (0.0001 -- 0.1277)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7169 (0.8330)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2078 (0.1322 -- 0.3513)  data: 0.0133 (0.0001 -- 0.1277)  max mem: 16735
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [169]  [  0/160]  eta: 0:23:57  lr: 0.000003  min_lr: 0.000000  loss: 1.6939 (1.6939)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6238 (6.6238)  time: 8.9823 (8.9823 -- 8.9823)  data: 8.4224 (8.4224 -- 8.4224)  max mem: 16735
[2023-09-01 05:12:52,163] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27044
[2023-09-01 05:12:52,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:12:52,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27044
[2023-09-01 05:12:52,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:12:52,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [169]  [ 20/160]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000000  loss: 2.0010 (1.9767)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2048 (7.7001)  time: 0.7789 (0.5346 -- 3.0363)  data: 0.2233 (0.0003 -- 2.4904)  max mem: 16735
Epoch: [169]  [ 40/160]  eta: 0:02:02  lr: 0.000003  min_lr: 0.000000  loss: 1.7348 (1.8645)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3996 (7.8486)  time: 0.8725 (0.5466 -- 3.8449)  data: 0.3027 (0.0004 -- 3.2825)  max mem: 16735
Epoch: [169]  [ 60/160]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 2.0790 (1.9158)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5023 (8.0789)  time: 0.8940 (0.5311 -- 3.2518)  data: 0.3047 (0.0004 -- 2.7084)  max mem: 16735
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.9264 (1.9222)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7141 (8.3015)  time: 0.9346 (0.5268 -- 3.6774)  data: 0.3889 (0.0003 -- 3.1381)  max mem: 16735
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000000  loss: 1.8512 (1.9215)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9481 (8.3289)  time: 0.9314 (0.5217 -- 4.0156)  data: 0.3884 (0.0003 -- 3.4945)  max mem: 16735
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9878 (1.9331)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3956 (8.4099)  time: 0.6343 (0.5218 -- 1.5677)  data: 0.0908 (0.0002 -- 1.0506)  max mem: 16735
[2023-09-01 05:14:45,627] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:14:45,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:14:45,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:14:45,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7995 (1.9264)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7451 (8.3266)  time: 1.0361 (0.5296 -- 3.3322)  data: 0.4880 (0.0003 -- 2.7992)  max mem: 16735
[2023-09-01 05:15:01,890] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27196
[2023-09-01 05:15:01,890] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:15:01,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:15:01,890] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27196
[2023-09-01 05:15:01,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8201 (1.9275)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9189 (8.3096)  time: 0.6192 (0.4900 -- 2.4939)  data: 0.0987 (0.0002 -- 1.9612)  max mem: 16735
Epoch: [169] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8201 (1.9049)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9189 (8.3096)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2734 (0.2734)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1886 (2.1886 -- 2.1886)  data: 1.9873 (1.9873 -- 1.9873)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5959 (0.8105)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4104 (0.1945 -- 2.1886)  data: 0.2046 (0.0007 -- 1.9873)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6483 (0.7557)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2251 (0.1692 -- 0.3911)  data: 0.0230 (0.0001 -- 0.1622)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7123 (0.8360)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2137 (0.1328 -- 0.3911)  data: 0.0228 (0.0001 -- 0.1622)  max mem: 16735
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 77.386 Acc@5 96.266 loss 0.829
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [170]  [  0/160]  eta: 0:17:35  lr: 0.000003  min_lr: 0.000000  loss: 2.2945 (2.2945)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9750 (8.9750)  time: 6.5975 (6.5975 -- 6.5975)  data: 5.4541 (5.4541 -- 5.4541)  max mem: 16735
Epoch: [170]  [ 20/160]  eta: 0:02:34  lr: 0.000003  min_lr: 0.000000  loss: 1.7557 (1.8328)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4484 (8.7177)  time: 0.8311 (0.5312 -- 3.0648)  data: 0.1475 (0.0003 -- 1.7993)  max mem: 16735
Epoch: [170]  [ 40/160]  eta: 0:02:02  lr: 0.000003  min_lr: 0.000000  loss: 1.7426 (1.7524)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5478 (8.4701)  time: 0.9232 (0.5260 -- 2.8010)  data: 0.0030 (0.0003 -- 0.0310)  max mem: 16735
Epoch: [170]  [ 60/160]  eta: 0:01:39  lr: 0.000003  min_lr: 0.000000  loss: 2.0019 (1.8551)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2550 (8.4132)  time: 0.9382 (0.5296 -- 3.9594)  data: 0.0055 (0.0003 -- 0.0471)  max mem: 16735
Epoch: [170]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.9505 (1.8715)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4422 (8.4515)  time: 0.8119 (0.5222 -- 3.3130)  data: 0.0014 (0.0004 -- 0.0029)  max mem: 16735
Epoch: [170]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.9557 (1.8708)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7116 (8.7413)  time: 0.9628 (0.5249 -- 3.5945)  data: 0.0013 (0.0004 -- 0.0048)  max mem: 16735
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8541 (1.8664)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4431 (8.7184)  time: 0.7031 (0.5374 -- 2.7905)  data: 0.0019 (0.0003 -- 0.0046)  max mem: 16735
[2023-09-01 05:17:07,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:17:07,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:17:07,068] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:17:07,069] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:17:16,684] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27337
[2023-09-01 05:17:16,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:17:16,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27337
[2023-09-01 05:17:16,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:17:16,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8501 (1.8724)  loss_scale: 65536.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9266 (8.6351)  time: 0.9204 (0.5265 -- 3.6053)  data: 0.0014 (0.0007 -- 0.0022)  max mem: 16735
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9251 (1.8668)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3004 (8.7233)  time: 0.6597 (0.5045 -- 1.7958)  data: 0.0232 (0.0002 -- 0.4186)  max mem: 16735
Epoch: [170] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9251 (1.8750)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3004 (8.7233)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2700 (0.2700)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2193 (2.2193 -- 2.2193)  data: 2.0023 (2.0023 -- 2.0023)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5918 (0.8063)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4176 (0.1996 -- 2.2193)  data: 0.2006 (0.0007 -- 2.0023)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6428 (0.7514)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2295 (0.1694 -- 0.3928)  data: 0.0201 (0.0001 -- 0.1956)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7071 (0.8323)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2143 (0.1334 -- 0.3928)  data: 0.0198 (0.0001 -- 0.1956)  max mem: 16735
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 77.386 Acc@5 96.266 loss 0.825
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 79.05%
Epoch: [171]  [  0/160]  eta: 0:23:34  lr: 0.000003  min_lr: 0.000000  loss: 1.7499 (1.7499)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8046 (10.8046)  time: 8.8403 (8.8403 -- 8.8403)  data: 8.3144 (8.3144 -- 8.3144)  max mem: 16735
Epoch: [171]  [ 20/160]  eta: 0:02:42  lr: 0.000003  min_lr: 0.000000  loss: 1.8127 (1.8160)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6238 (8.8150)  time: 0.7780 (0.5346 -- 3.8655)  data: 0.2193 (0.0006 -- 3.3369)  max mem: 16735
Epoch: [171]  [ 40/160]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000000  loss: 2.0115 (1.8761)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1806 (8.5690)  time: 0.8125 (0.5299 -- 2.5733)  data: 0.1108 (0.0001 -- 1.2953)  max mem: 16735
Epoch: [171]  [ 60/160]  eta: 0:01:33  lr: 0.000002  min_lr: 0.000000  loss: 1.8832 (1.8532)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4375 (8.7147)  time: 0.8173 (0.5297 -- 3.5995)  data: 0.0970 (0.0008 -- 1.2498)  max mem: 16735
Epoch: [171]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.8198 (1.8519)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0659 (8.6311)  time: 0.9100 (0.5298 -- 2.7858)  data: 0.3584 (0.0006 -- 2.2285)  max mem: 16735
Epoch: [171]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7117 (1.8561)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2460 (8.7988)  time: 0.9085 (0.5282 -- 2.5490)  data: 0.3460 (0.0004 -- 2.0007)  max mem: 16735
[2023-09-01 05:19:19,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:19:19,177] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:19:19,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:19:19,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [171]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8946 (1.8673)  loss_scale: 65536.0000 (36830.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7671 (8.6846)  time: 0.8239 (0.5146 -- 2.4846)  data: 0.2743 (0.0005 -- 1.9503)  max mem: 16735
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8530 (1.8574)  loss_scale: 65536.0000 (40901.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6329 (8.6267)  time: 1.0232 (0.5266 -- 4.2212)  data: 0.4760 (0.0008 -- 3.6848)  max mem: 16735
[2023-09-01 05:19:57,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27509
[2023-09-01 05:19:57,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27509
[2023-09-01 05:19:57,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:19:57,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:19:57,828] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8812 (1.8616)  loss_scale: 32768.0000 (41574.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5565 (8.6606)  time: 0.6501 (0.4901 -- 3.0899)  data: 0.1292 (0.0002 -- 2.5706)  max mem: 16735
Epoch: [171] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8812 (1.8863)  loss_scale: 32768.0000 (41574.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5565 (8.6606)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2753 (0.2753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3843 (2.3843 -- 2.3843)  data: 2.1689 (2.1689 -- 2.1689)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5870 (0.8046)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4504 (0.1776 -- 2.3843)  data: 0.2389 (0.0006 -- 2.1689)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6450 (0.7516)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2251 (0.1694 -- 0.6847)  data: 0.0231 (0.0001 -- 0.4499)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7080 (0.8313)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2121 (0.1326 -- 0.6847)  data: 0.0228 (0.0001 -- 0.4499)  max mem: 16735
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.825
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [172]  [  0/160]  eta: 0:22:44  lr: 0.000002  min_lr: 0.000000  loss: 2.2273 (2.2273)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3595 (7.3595)  time: 8.5290 (8.5290 -- 8.5290)  data: 7.9883 (7.9883 -- 7.9883)  max mem: 16735
Epoch: [172]  [ 20/160]  eta: 0:02:56  lr: 0.000002  min_lr: 0.000000  loss: 2.0253 (1.9647)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5309 (8.5414)  time: 0.8950 (0.5299 -- 4.6158)  data: 0.1171 (0.0001 -- 1.5596)  max mem: 16735
Epoch: [172]  [ 40/160]  eta: 0:02:15  lr: 0.000002  min_lr: 0.000000  loss: 1.9203 (1.9066)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6853 (8.3473)  time: 0.9852 (0.5202 -- 4.9852)  data: 0.0013 (0.0002 -- 0.0048)  max mem: 16735
Epoch: [172]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.8559 (1.9155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5433 (8.4463)  time: 0.7374 (0.5412 -- 2.5569)  data: 0.0024 (0.0004 -- 0.0083)  max mem: 16735
Epoch: [172]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.9260 (1.9186)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0549 (8.6671)  time: 0.7134 (0.5313 -- 1.6730)  data: 0.0874 (0.0003 -- 1.1291)  max mem: 16735
Epoch: [172]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.9297 (1.9110)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4561 (8.5758)  time: 0.9172 (0.5383 -- 3.6163)  data: 0.3084 (0.0008 -- 3.0579)  max mem: 16735
[2023-09-01 05:22:00,950] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:22:00,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:22:00,951] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:22:00,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [172]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 2.0748 (1.9451)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6442 (8.5179)  time: 0.8887 (0.5408 -- 2.8331)  data: 0.2763 (0.0001 -- 2.2991)  max mem: 16735
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5862 (1.9205)  loss_scale: 65536.0000 (38113.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2916 (8.5044)  time: 0.9258 (0.5332 -- 3.3130)  data: 0.3687 (0.0002 -- 2.7673)  max mem: 16735
[2023-09-01 05:22:24,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27666
[2023-09-01 05:22:24,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27666
[2023-09-01 05:22:24,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:22:24,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:22:24,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8913 (1.9105)  loss_scale: 32768.0000 (38502.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3289 (8.4549)  time: 0.6655 (0.5016 -- 1.5830)  data: 0.1404 (0.0002 -- 1.0341)  max mem: 16735
Epoch: [172] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8913 (1.8856)  loss_scale: 32768.0000 (38502.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3289 (8.4549)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2753 (0.2753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2793 (2.2793 -- 2.2793)  data: 2.0600 (2.0600 -- 2.0600)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5814 (0.8056)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4067 (0.1941 -- 2.2793)  data: 0.1896 (0.0007 -- 2.0600)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6453 (0.7521)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2231 (0.1716 -- 0.4061)  data: 0.0147 (0.0001 -- 0.2058)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7087 (0.8308)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2062 (0.1323 -- 0.4061)  data: 0.0138 (0.0001 -- 0.2058)  max mem: 16735
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [173]  [  0/160]  eta: 0:19:57  lr: 0.000002  min_lr: 0.000000  loss: 1.9676 (1.9676)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0176 (7.0176)  time: 7.4847 (7.4847 -- 7.4847)  data: 6.0972 (6.0972 -- 6.0972)  max mem: 16735
Epoch: [173]  [ 20/160]  eta: 0:02:57  lr: 0.000002  min_lr: 0.000000  loss: 2.0810 (1.9860)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5824 (7.9437)  time: 0.9564 (0.5369 -- 3.6875)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16735
Epoch: [173]  [ 40/160]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 1.9548 (1.9470)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3829 (8.0712)  time: 0.8467 (0.5168 -- 2.9649)  data: 0.0017 (0.0001 -- 0.0081)  max mem: 16735
Epoch: [173]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.6744 (1.8846)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4098 (8.0743)  time: 0.8181 (0.5294 -- 2.6255)  data: 0.0014 (0.0003 -- 0.0046)  max mem: 16735
[2023-09-01 05:23:47,073] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27749
[2023-09-01 05:23:47,073] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:23:47,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-01 05:23:47,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27749
[2023-09-01 05:23:47,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [173]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.7563 (1.8624)  loss_scale: 16384.0000 (30340.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9083 (8.0181)  time: 0.8459 (0.5303 -- 2.5888)  data: 0.0022 (0.0006 -- 0.0144)  max mem: 16735
Epoch: [173]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.7790 (1.8460)  loss_scale: 16384.0000 (27577.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0426 (8.0329)  time: 0.9991 (0.5093 -- 5.4067)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16735
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9031 (1.8510)  loss_scale: 16384.0000 (25726.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7928 (8.2679)  time: 0.7626 (0.5360 -- 3.0322)  data: 0.0013 (0.0001 -- 0.0026)  max mem: 16735
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 2.0746 (1.8719)  loss_scale: 16384.0000 (24401.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8767 (8.2540)  time: 0.8576 (0.5368 -- 4.2428)  data: 0.0016 (0.0002 -- 0.0031)  max mem: 16735
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.9382 (1.8761)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1499 (8.2559)  time: 0.6526 (0.5023 -- 2.7273)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16735
Epoch: [173] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.9382 (1.8791)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1499 (8.2559)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2759 (0.2759)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5266 (2.5266 -- 2.5266)  data: 2.3044 (2.3044 -- 2.3044)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5871 (0.8046)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4286 (0.2012 -- 2.5266)  data: 0.2105 (0.0007 -- 2.3044)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6391 (0.7523)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2149 (0.1721 -- 0.2675)  data: 0.0077 (0.0001 -- 0.0852)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7099 (0.8303)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.1994 (0.1326 -- 0.2675)  data: 0.0074 (0.0001 -- 0.0852)  max mem: 16735
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [174]  [  0/160]  eta: 0:16:13  lr: 0.000002  min_lr: 0.000000  loss: 1.8962 (1.8962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7681 (5.7681)  time: 6.0871 (6.0871 -- 6.0871)  data: 5.5385 (5.5385 -- 5.5385)  max mem: 16735
Epoch: [174]  [ 20/160]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 1.9025 (1.8583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2438 (8.2287)  time: 0.8843 (0.5411 -- 3.2710)  data: 0.2405 (0.0009 -- 2.0753)  max mem: 16735
[2023-09-01 05:25:49,798] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:25:49,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 05:25:49,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:25:49,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [174]  [ 40/160]  eta: 0:01:57  lr: 0.000002  min_lr: 0.000000  loss: 2.0272 (1.9218)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2042 (8.3579)  time: 0.8258 (0.5293 -- 2.2618)  data: 0.1109 (0.0004 -- 1.3013)  max mem: 16735
Epoch: [174]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9422 (1.9137)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7543 (8.5432)  time: 0.9677 (0.5310 -- 3.7168)  data: 0.3306 (0.0002 -- 3.2005)  max mem: 16735
Epoch: [174]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.8600 (1.8822)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4547 (8.6358)  time: 0.8716 (0.5350 -- 4.0495)  data: 0.1369 (0.0004 -- 2.4919)  max mem: 16735
Epoch: [174]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8298 (1.8774)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1324 (8.6415)  time: 0.9300 (0.5291 -- 3.8338)  data: 0.0008 (0.0003 -- 0.0016)  max mem: 16735
Epoch: [174]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8957 (1.8866)  loss_scale: 32768.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5931 (8.6431)  time: 0.8194 (0.5274 -- 3.5373)  data: 0.0592 (0.0007 -- 0.6157)  max mem: 16735
Epoch: [174]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.9004 (1.8985)  loss_scale: 32768.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1197 (8.6737)  time: 0.8884 (0.5121 -- 4.7399)  data: 0.0678 (0.0002 -- 1.3363)  max mem: 16735
[2023-09-01 05:27:32,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=175, lr=[4.564733825380591e-08, 4.564733825380591e-08, 6.086311767174122e-08, 6.086311767174122e-08, 8.115082356232162e-08, 8.115082356232162e-08, 1.082010980830955e-07, 1.082010980830955e-07, 1.4426813077746065e-07, 1.4426813077746065e-07, 1.923575077032809e-07, 1.923575077032809e-07, 2.5647667693770787e-07, 2.5647667693770787e-07, 3.4196890258361044e-07, 3.4196890258361044e-07, 4.5595853677814725e-07, 4.5595853677814725e-07, 6.079447157041963e-07, 6.079447157041963e-07, 8.105929542722618e-07, 8.105929542722618e-07, 1.0807906056963492e-06, 1.0807906056963492e-06, 1.4410541409284656e-06, 1.4410541409284656e-06, 1.921405521237954e-06, 1.921405521237954e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 05:27:32,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=18.039743110868535, CurrSamplesPerSec=24.298489420147824, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.9093 (1.8902)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8951 (8.6840)  time: 0.6332 (0.5029 -- 2.2338)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16735
Epoch: [174] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.9093 (1.8757)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8951 (8.6840)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2743 (0.2743)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3856 (2.3856 -- 2.3856)  data: 2.1475 (2.1475 -- 2.1475)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5957 (0.8076)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4277 (0.1965 -- 2.3856)  data: 0.2085 (0.0005 -- 2.1475)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6339 (0.7517)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1708 -- 0.3801)  data: 0.0110 (0.0001 -- 0.1342)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7013 (0.8304)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2050 (0.1324 -- 0.3801)  data: 0.0107 (0.0001 -- 0.1342)  max mem: 16735
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 78.008 Acc@5 96.473 loss 0.824
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [175]  [  0/160]  eta: 0:16:44  lr: 0.000002  min_lr: 0.000000  loss: 1.8523 (1.8523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2001 (6.2001)  time: 6.2768 (6.2768 -- 6.2768)  data: 5.7427 (5.7427 -- 5.7427)  max mem: 16735
[2023-09-01 05:27:50,826] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:27:50,826] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:27:50,826] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:27:50,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:27:57,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28012
[2023-09-01 05:27:57,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:27:57,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:27:57,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28012
[2023-09-01 05:27:57,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [175]  [ 20/160]  eta: 0:02:35  lr: 0.000002  min_lr: 0.000000  loss: 1.7517 (1.8277)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1936 (8.6867)  time: 0.8540 (0.5298 -- 2.6615)  data: 0.1722 (0.0005 -- 2.1296)  max mem: 16735
Epoch: [175]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000000  loss: 1.7759 (1.8138)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9280 (8.5130)  time: 0.9807 (0.5385 -- 4.4300)  data: 0.4318 (0.0005 -- 3.8958)  max mem: 16735
Epoch: [175]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 2.0175 (1.8513)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2681 (8.4488)  time: 0.8751 (0.5252 -- 3.9546)  data: 0.3359 (0.0002 -- 3.4352)  max mem: 16735
[2023-09-01 05:28:55,912] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28078
[2023-09-01 05:28:55,912] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28078
[2023-09-01 05:28:55,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:28:55,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:28:55,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [175]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.9420 (1.8630)  loss_scale: 32768.0000 (34588.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1299 (8.4093)  time: 0.8092 (0.5228 -- 3.3591)  data: 0.2597 (0.0003 -- 2.8353)  max mem: 16735
Epoch: [175]  [100/160]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000000  loss: 1.8423 (1.8561)  loss_scale: 16384.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8197 (8.4056)  time: 1.0751 (0.5248 -- 5.2220)  data: 0.5221 (0.0004 -- 4.7142)  max mem: 16735
Epoch: [175]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8182 (1.8523)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5060 (8.4011)  time: 0.6178 (0.5372 -- 1.8459)  data: 0.0677 (0.0001 -- 1.3056)  max mem: 16735
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6887 (1.8269)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9170 (8.3186)  time: 0.9607 (0.5322 -- 3.9397)  data: 0.4044 (0.0004 -- 3.4189)  max mem: 16735
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.9803 (1.8358)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1614 (8.2921)  time: 0.6879 (0.5015 -- 3.0499)  data: 0.1631 (0.0002 -- 2.5194)  max mem: 16735
Epoch: [175] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.9803 (1.8629)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1614 (8.2921)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2736 (0.2736)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3168 (2.3168 -- 2.3168)  data: 2.0989 (2.0989 -- 2.0989)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5987 (0.8035)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4124 (0.2012 -- 2.3168)  data: 0.1928 (0.0010 -- 2.0989)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6348 (0.7497)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2200 (0.1721 -- 0.2916)  data: 0.0085 (0.0001 -- 0.0843)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6997 (0.8283)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2035 (0.1331 -- 0.2916)  data: 0.0081 (0.0001 -- 0.0843)  max mem: 16735
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.822
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [176]  [  0/160]  eta: 0:14:14  lr: 0.000002  min_lr: 0.000000  loss: 2.2363 (2.2363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6238 (10.6238)  time: 5.3390 (5.3390 -- 5.3390)  data: 4.6286 (4.6286 -- 4.6286)  max mem: 16735
Epoch: [176]  [ 20/160]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 1.9204 (1.9664)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4767 (7.8370)  time: 1.0079 (0.5239 -- 3.8092)  data: 0.4569 (0.0009 -- 3.2738)  max mem: 16735
Epoch: [176]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.8408 (1.8849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2307 (8.0251)  time: 0.8583 (0.5327 -- 3.7835)  data: 0.3050 (0.0004 -- 3.2641)  max mem: 16735
[2023-09-01 05:30:59,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:30:59,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 05:30:59,711] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:30:59,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [176]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.8554 (1.8630)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7190 (8.0500)  time: 0.8867 (0.5356 -- 2.6437)  data: 0.3441 (0.0005 -- 2.1294)  max mem: 16735
Epoch: [176]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.9920 (1.8860)  loss_scale: 32768.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0224 (8.0114)  time: 0.7508 (0.5349 -- 2.9403)  data: 0.1026 (0.0006 -- 0.9557)  max mem: 16735
Epoch: [176]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 2.0272 (1.9037)  loss_scale: 32768.0000 (25143.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3136 (8.1252)  time: 0.9012 (0.5266 -- 2.1980)  data: 0.1944 (0.0003 -- 1.6680)  max mem: 16735
Epoch: [176]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.9433 (1.9106)  loss_scale: 32768.0000 (26403.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3391 (8.2109)  time: 0.9000 (0.5401 -- 3.1675)  data: 0.0497 (0.0005 -- 0.5698)  max mem: 16735
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8282 (1.9030)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2684 (8.2885)  time: 0.8381 (0.5318 -- 3.9959)  data: 0.0056 (0.0003 -- 0.0850)  max mem: 16735
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 2.0125 (1.9016)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8646 (8.2540)  time: 0.7041 (0.5014 -- 3.2720)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16735
Epoch: [176] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 2.0125 (1.8734)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8646 (8.2540)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2725 (0.2725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1321 (2.1321 -- 2.1321)  data: 1.9240 (1.9240 -- 1.9240)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5921 (0.8010)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4334 (0.2013 -- 2.1321)  data: 0.2142 (0.0005 -- 1.9240)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6328 (0.7482)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2307 (0.1696 -- 0.4941)  data: 0.0224 (0.0001 -- 0.2554)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7009 (0.8277)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2162 (0.1323 -- 0.4941)  data: 0.0221 (0.0001 -- 0.2554)  max mem: 16735
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.822
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [177]  [  0/160]  eta: 0:22:36  lr: 0.000002  min_lr: 0.000000  loss: 2.1831 (2.1831)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3927 (8.3927)  time: 8.4765 (8.4765 -- 8.4765)  data: 7.1851 (7.1851 -- 7.1851)  max mem: 16735
[2023-09-01 05:33:01,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:33:01,019] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:33:01,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:33:01,019] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:33:06,029] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28340
[2023-09-01 05:33:06,029] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28340
[2023-09-01 05:33:06,029] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:33:06,029] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:33:06,029] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [177]  [ 20/160]  eta: 0:02:49  lr: 0.000002  min_lr: 0.000000  loss: 1.8758 (1.8676)  loss_scale: 32768.0000 (40569.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0081 (8.8000)  time: 0.8469 (0.5316 -- 4.2129)  data: 0.2973 (0.0004 -- 3.6907)  max mem: 16735
Epoch: [177]  [ 40/160]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000000  loss: 1.9338 (1.8755)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5459 (8.8471)  time: 0.8923 (0.5196 -- 3.9269)  data: 0.3470 (0.0002 -- 3.3836)  max mem: 16735
Epoch: [177]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 2.0160 (1.9173)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3693 (8.7846)  time: 0.8581 (0.5346 -- 2.7465)  data: 0.2411 (0.0004 -- 2.2152)  max mem: 16735
Epoch: [177]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.5900 (1.8599)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7053 (8.6389)  time: 0.8507 (0.5338 -- 3.5491)  data: 0.2935 (0.0002 -- 3.0317)  max mem: 16735
Epoch: [177]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 2.0216 (1.8864)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4913 (8.6761)  time: 0.8621 (0.5366 -- 2.8610)  data: 0.3071 (0.0010 -- 2.3350)  max mem: 16735
Epoch: [177]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7442 (1.8793)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2045 (8.8057)  time: 0.8136 (0.5315 -- 3.7562)  data: 0.2624 (0.0002 -- 3.1795)  max mem: 16735
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8495 (1.8772)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9738 (8.7372)  time: 0.9653 (0.5308 -- 4.5927)  data: 0.4212 (0.0003 -- 4.0663)  max mem: 16735
[2023-09-01 05:34:56,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:34:56,385] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:34:56,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:34:56,385] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:34:56,896] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28470
[2023-09-01 05:34:56,896] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28470
[2023-09-01 05:34:56,896] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:34:56,896] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:34:56,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8495 (1.8752)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5684 (8.6815)  time: 0.6064 (0.5012 -- 1.5489)  data: 0.0826 (0.0002 -- 1.0502)  max mem: 16735
Epoch: [177] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8495 (1.8841)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5684 (8.6815)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2727 (0.2727)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2822 (2.2822 -- 2.2822)  data: 2.0483 (2.0483 -- 2.0483)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5922 (0.8019)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4287 (0.1984 -- 2.2822)  data: 0.2099 (0.0005 -- 2.0483)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6358 (0.7488)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2266 (0.1726 -- 0.4914)  data: 0.0214 (0.0001 -- 0.2279)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6998 (0.8280)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2131 (0.1322 -- 0.4914)  data: 0.0210 (0.0001 -- 0.2279)  max mem: 16735
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.823
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [178]  [  0/160]  eta: 0:19:56  lr: 0.000002  min_lr: 0.000000  loss: 2.3103 (2.3103)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4448 (6.4448)  time: 7.4790 (7.4790 -- 7.4790)  data: 6.9341 (6.9341 -- 6.9341)  max mem: 16735
[2023-09-01 05:35:23,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28488
[2023-09-01 05:35:23,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28488
[2023-09-01 05:35:23,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:35:23,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:35:23,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [178]  [ 20/160]  eta: 0:02:39  lr: 0.000001  min_lr: 0.000000  loss: 1.9354 (1.9297)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1989 (8.2366)  time: 0.8232 (0.5288 -- 3.1679)  data: 0.1939 (0.0004 -- 2.6379)  max mem: 16735
Epoch: [178]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.9460 (1.9212)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4978 (8.1453)  time: 0.9635 (0.5396 -- 3.6064)  data: 0.4091 (0.0005 -- 3.0700)  max mem: 16735
Epoch: [178]  [ 60/160]  eta: 0:01:34  lr: 0.000001  min_lr: 0.000000  loss: 1.8849 (1.8838)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2079 (8.1605)  time: 0.7326 (0.5240 -- 2.3550)  data: 0.1755 (0.0001 -- 1.8233)  max mem: 16735
Epoch: [178]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9148 (1.8948)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7083 (8.3440)  time: 0.8742 (0.5357 -- 2.7247)  data: 0.3281 (0.0003 -- 2.1888)  max mem: 16735
Epoch: [178]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.8497 (1.8944)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1933 (8.4115)  time: 0.9770 (0.5260 -- 3.1106)  data: 0.2971 (0.0004 -- 2.5775)  max mem: 16735
Epoch: [178]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 2.0540 (1.9172)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8679 (8.5110)  time: 0.8091 (0.5175 -- 3.2451)  data: 0.2174 (0.0003 -- 2.7277)  max mem: 16735
[2023-09-01 05:37:17,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:37:17,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 05:37:17,655] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:37:17,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.0424 (1.9281)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (8.4222)  time: 0.8944 (0.5373 -- 2.9637)  data: 0.3467 (0.0006 -- 2.4329)  max mem: 16735
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9464 (1.9359)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2713 (8.4783)  time: 0.6699 (0.4994 -- 1.5997)  data: 0.1402 (0.0002 -- 1.0548)  max mem: 16735
Epoch: [178] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9464 (1.9113)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2713 (8.4783)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2738 (0.2738)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1217 (2.1217 -- 2.1217)  data: 1.9067 (1.9067 -- 1.9067)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5947 (0.8027)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4142 (0.2033 -- 2.1217)  data: 0.1964 (0.0003 -- 1.9067)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6353 (0.7489)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2302 (0.1690 -- 0.4838)  data: 0.0195 (0.0001 -- 0.2451)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6987 (0.8278)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2151 (0.1333 -- 0.4838)  data: 0.0192 (0.0001 -- 0.2451)  max mem: 16735
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.823
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [179]  [  0/160]  eta: 0:16:47  lr: 0.000001  min_lr: 0.000000  loss: 1.5267 (1.5267)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9970 (7.9970)  time: 6.2969 (6.2969 -- 6.2969)  data: 5.7384 (5.7384 -- 5.7384)  max mem: 16735
Epoch: [179]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 1.8633 (1.8831)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5650 (8.8786)  time: 0.9168 (0.5414 -- 2.4940)  data: 0.1470 (0.0009 -- 1.1901)  max mem: 16735
Epoch: [179]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.7456 (1.8657)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6588 (8.4659)  time: 0.9394 (0.5276 -- 4.4380)  data: 0.3774 (0.0004 -- 3.9062)  max mem: 16735
Epoch: [179]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9768 (1.8946)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4595 (8.6544)  time: 0.8248 (0.5144 -- 2.6973)  data: 0.2697 (0.0002 -- 2.1644)  max mem: 16735
Epoch: [179]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.9257 (1.9114)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4188 (8.6749)  time: 0.9172 (0.5288 -- 4.3417)  data: 0.3722 (0.0003 -- 3.8071)  max mem: 16735
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.9731 (1.9176)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6455 (8.5431)  time: 0.9470 (0.5132 -- 3.8341)  data: 0.4149 (0.0003 -- 3.3118)  max mem: 16735
[2023-09-01 05:39:19,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28744
[2023-09-01 05:39:19,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28744
[2023-09-01 05:39:19,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:39:19,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 05:39:19,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [179]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 2.1645 (1.9370)  loss_scale: 16384.0000 (30466.1157)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9162 (8.6726)  time: 0.8426 (0.5366 -- 3.8306)  data: 0.2913 (0.0002 -- 3.3191)  max mem: 16735
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5100 (1.9027)  loss_scale: 16384.0000 (28468.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5574 (8.6374)  time: 0.9812 (0.5194 -- 5.0400)  data: 0.4438 (0.0004 -- 4.5182)  max mem: 16735
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8059 (1.8882)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0580 (8.6609)  time: 0.6025 (0.5004 -- 1.6557)  data: 0.0840 (0.0001 -- 1.1486)  max mem: 16735
Epoch: [179] Total time: 0:02:25 (0.9077 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8059 (1.8924)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0580 (8.6609)
[2023-09-01 05:40:05,231] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-09-01 05:40:05,233] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-09-01 05:40:05,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-09-01 05:40:05,233] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-09-01 05:40:06,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-09-01 05:40:06,225] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:16  loss: 0.2753 (0.2753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.8152 (2.8152 -- 2.8152)  data: 2.5866 (2.5866 -- 2.5866)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5959 (0.8036)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4654 (0.2023 -- 2.8152)  data: 0.2458 (0.0006 -- 2.5866)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6345 (0.7497)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2083 (0.1695 -- 0.3327)  data: 0.0060 (0.0001 -- 0.1037)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6954 (0.8284)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (97.0954)  time: 0.1918 (0.1321 -- 0.3327)  data: 0.0055 (0.0001 -- 0.1037)  max mem: 16735
Val: Total time: 0:00:07 (0.2951 s / it)
* Acc@1 77.801 Acc@5 96.680 loss 0.824
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [180]  [  0/160]  eta: 0:22:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9294 (1.9294)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6918 (6.6918)  time: 8.3388 (8.3388 -- 8.3388)  data: 6.1827 (6.1827 -- 6.1827)  max mem: 16735
Epoch: [180]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 1.9986 (2.0015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9576 (8.2810)  time: 0.8389 (0.5234 -- 2.6289)  data: 0.2933 (0.0005 -- 2.0956)  max mem: 16735
Epoch: [180]  [ 40/160]  eta: 0:02:09  lr: 0.000001  min_lr: 0.000000  loss: 1.9835 (1.9612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5280 (8.4556)  time: 0.9637 (0.5156 -- 4.3434)  data: 0.4051 (0.0003 -- 3.7915)  max mem: 16735
Epoch: [180]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8470 (1.9407)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4115 (8.5852)  time: 0.8195 (0.5290 -- 2.0146)  data: 0.2191 (0.0003 -- 1.4937)  max mem: 16735
[2023-09-01 05:41:26,721] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:41:26,722] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:41:26,722] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 05:41:26,722] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [180]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.9492 (1.9386)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9223 (8.4552)  time: 0.9104 (0.5250 -- 3.3566)  data: 0.3473 (0.0002 -- 2.8120)  max mem: 16735
Epoch: [180]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7110 (1.9050)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5536 (8.5155)  time: 0.7885 (0.5181 -- 2.9137)  data: 0.2503 (0.0004 -- 2.3704)  max mem: 16735
Epoch: [180]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8285 (1.8887)  loss_scale: 32768.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7242 (8.5676)  time: 0.8214 (0.5303 -- 3.7924)  data: 0.2197 (0.0005 -- 3.0834)  max mem: 16735
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7003 (1.8751)  loss_scale: 32768.0000 (24285.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2532 (8.5616)  time: 0.8418 (0.5305 -- 3.4282)  data: 0.2860 (0.0003 -- 2.8594)  max mem: 16735
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8205 (1.8740)  loss_scale: 32768.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9445 (8.5439)  time: 0.7091 (0.5019 -- 2.5039)  data: 0.1526 (0.0002 -- 1.4695)  max mem: 16735
Epoch: [180] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8205 (1.8773)  loss_scale: 32768.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9445 (8.5439)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2748 (0.2748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2793 (2.2793 -- 2.2793)  data: 2.0559 (2.0559 -- 2.0559)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5916 (0.8052)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4088 (0.2000 -- 2.2793)  data: 0.1884 (0.0009 -- 2.0559)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6350 (0.7515)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2242 (0.1688 -- 0.4872)  data: 0.0166 (0.0001 -- 0.2935)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7000 (0.8307)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2062 (0.1323 -- 0.4872)  data: 0.0162 (0.0001 -- 0.2935)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [181]  [  0/160]  eta: 0:20:17  lr: 0.000001  min_lr: 0.000000  loss: 2.4192 (2.4192)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6034 (6.6034)  time: 7.6121 (7.6121 -- 7.6121)  data: 6.3486 (6.3486 -- 6.3486)  max mem: 16735
Epoch: [181]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 1.9528 (1.9535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2320 (8.7704)  time: 0.8760 (0.5285 -- 4.3055)  data: 0.3204 (0.0005 -- 3.7670)  max mem: 16735
[2023-09-01 05:43:23,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=181, lr=[2.6316989744306575e-08, 2.6316989744306575e-08, 3.508931965907543e-08, 3.508931965907543e-08, 4.678575954543391e-08, 4.678575954543391e-08, 6.238101272724522e-08, 6.238101272724522e-08, 8.317468363632695e-08, 8.317468363632695e-08, 1.1089957818176927e-07, 1.1089957818176927e-07, 1.47866104242359e-07, 1.47866104242359e-07, 1.971548056564787e-07, 1.971548056564787e-07, 2.6287307420863825e-07, 2.6287307420863825e-07, 3.5049743227818433e-07, 3.5049743227818433e-07, 4.6732990970424583e-07, 4.6732990970424583e-07, 6.231065462723277e-07, 6.231065462723277e-07, 8.308087283631036e-07, 8.308087283631036e-07, 1.1077449711508049e-06, 1.1077449711508049e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 05:43:23,503] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=18.125507949247005, CurrSamplesPerSec=21.365527025254472, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [181]  [ 40/160]  eta: 0:01:58  lr: 0.000001  min_lr: 0.000000  loss: 1.9226 (1.8767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3579 (8.3782)  time: 0.7704 (0.5298 -- 2.3686)  data: 0.2185 (0.0003 -- 1.8535)  max mem: 16735
[2023-09-01 05:43:25,734] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:43:25,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:43:25,737] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:43:25,737] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [181]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9243 (1.9123)  loss_scale: 65536.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0242 (8.2571)  time: 0.9851 (0.5287 -- 4.1833)  data: 0.4385 (0.0008 -- 3.6696)  max mem: 16735
[2023-09-01 05:43:44,404] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29021
[2023-09-01 05:43:44,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:43:44,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:43:44,405] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29021
[2023-09-01 05:43:44,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [181]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9110 (1.9352)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3460 (8.3902)  time: 0.7535 (0.5257 -- 2.0618)  data: 0.2029 (0.0005 -- 1.5305)  max mem: 16735
Epoch: [181]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9342 (1.9286)  loss_scale: 32768.0000 (39256.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5624 (8.5651)  time: 0.9585 (0.5319 -- 3.2932)  data: 0.3679 (0.0004 -- 2.7548)  max mem: 16735
Epoch: [181]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9882 (1.9294)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5283 (8.5565)  time: 0.8298 (0.5191 -- 3.8676)  data: 0.2811 (0.0004 -- 3.3451)  max mem: 16735
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7770 (1.9168)  loss_scale: 32768.0000 (37415.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8427 (8.5058)  time: 0.9239 (0.5241 -- 4.9120)  data: 0.3693 (0.0002 -- 4.3575)  max mem: 16735
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8719 (1.9073)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2217 (8.4917)  time: 0.5970 (0.5028 -- 2.0064)  data: 0.0746 (0.0002 -- 1.4774)  max mem: 16735
Epoch: [181] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8719 (1.8913)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2217 (8.4917)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2751 (0.2751)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3536 (2.3536 -- 2.3536)  data: 2.1161 (2.1161 -- 2.1161)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5918 (0.8043)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4230 (0.2110 -- 2.3536)  data: 0.1972 (0.0008 -- 2.1161)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6366 (0.7509)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2204 (0.1690 -- 0.3226)  data: 0.0075 (0.0001 -- 0.0937)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6991 (0.8302)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2028 (0.1370 -- 0.3226)  data: 0.0071 (0.0001 -- 0.0937)  max mem: 16735
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [182]  [  0/160]  eta: 0:19:24  lr: 0.000001  min_lr: 0.000000  loss: 1.8582 (1.8582)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6622 (10.6622)  time: 7.2776 (7.2776 -- 7.2776)  data: 6.7629 (6.7629 -- 6.7629)  max mem: 16735
Epoch: [182]  [ 20/160]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 1.9280 (1.9187)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4174 (8.9889)  time: 0.9064 (0.5300 -- 4.8780)  data: 0.3555 (0.0002 -- 4.3316)  max mem: 16735
[2023-09-01 05:45:45,914] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:45:45,914] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:45:45,915] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:45:45,915] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [182]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 2.0259 (1.9456)  loss_scale: 65536.0000 (41559.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1762 (8.8530)  time: 0.8160 (0.5357 -- 3.1030)  data: 0.2590 (0.0003 -- 2.5692)  max mem: 16735
[2023-09-01 05:46:11,721] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29178
[2023-09-01 05:46:11,721] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:46:11,722] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29178
[2023-09-01 05:46:11,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:46:11,762] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [182]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8262 (1.8901)  loss_scale: 65536.0000 (47809.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2057 (8.8035)  time: 0.9400 (0.5276 -- 3.8181)  data: 0.3881 (0.0008 -- 3.2940)  max mem: 16735
Epoch: [182]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9722 (1.9052)  loss_scale: 32768.0000 (44095.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7857 (8.6495)  time: 0.7593 (0.5353 -- 2.2950)  data: 0.1688 (0.0002 -- 1.7521)  max mem: 16735
Epoch: [182]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9164 (1.9005)  loss_scale: 32768.0000 (41852.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5029 (8.6729)  time: 0.9610 (0.5247 -- 4.0799)  data: 0.4120 (0.0005 -- 3.5521)  max mem: 16735
Epoch: [182]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9886 (1.9075)  loss_scale: 32768.0000 (40350.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3170 (8.5972)  time: 0.8149 (0.5224 -- 2.5783)  data: 0.1617 (0.0004 -- 1.5785)  max mem: 16735
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7462 (1.8926)  loss_scale: 32768.0000 (39275.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5208 (8.4466)  time: 0.9483 (0.5341 -- 4.4089)  data: 0.0387 (0.0003 -- 0.7043)  max mem: 16735
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9718 (1.9121)  loss_scale: 32768.0000 (38502.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2834 (8.4887)  time: 0.6347 (0.5017 -- 2.3046)  data: 0.0009 (0.0002 -- 0.0043)  max mem: 16735
Epoch: [182] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9718 (1.9004)  loss_scale: 32768.0000 (38502.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2834 (8.4887)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2753 (0.2753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1915 (2.1915 -- 2.1915)  data: 1.9720 (1.9720 -- 1.9720)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5970 (0.8054)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4072 (0.2031 -- 2.1915)  data: 0.1860 (0.0007 -- 1.9720)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6389 (0.7515)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2284 (0.1716 -- 0.4196)  data: 0.0168 (0.0001 -- 0.1988)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6982 (0.8308)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2097 (0.1334 -- 0.4196)  data: 0.0164 (0.0001 -- 0.1988)  max mem: 16735
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [183]  [  0/160]  eta: 0:18:20  lr: 0.000001  min_lr: 0.000000  loss: 1.9807 (1.9807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6507 (8.6507)  time: 6.8784 (6.8784 -- 6.8784)  data: 6.3245 (6.3245 -- 6.3245)  max mem: 16735
Epoch: [183]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 2.0294 (1.9363)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2743 (8.6338)  time: 0.9318 (0.5268 -- 3.2691)  data: 0.3715 (0.0002 -- 2.7364)  max mem: 16735
[2023-09-01 05:48:13,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:48:13,682] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:48:13,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:48:13,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [183]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.8261 (1.9067)  loss_scale: 65536.0000 (43957.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8988 (8.5294)  time: 0.7918 (0.5279 -- 2.2792)  data: 0.2087 (0.0005 -- 1.7367)  max mem: 16735
Epoch: [183]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 2.0109 (1.9370)  loss_scale: 65536.0000 (51032.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5045 (8.6260)  time: 0.9229 (0.5286 -- 2.5773)  data: 0.3753 (0.0005 -- 2.0293)  max mem: 16735
[2023-09-01 05:48:46,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29346
[2023-09-01 05:48:46,479] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:48:46,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:48:46,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29346
[2023-09-01 05:48:46,480] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [183]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.8947 (1.9203)  loss_scale: 32768.0000 (48545.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5031 (8.8231)  time: 0.8169 (0.5292 -- 2.8165)  data: 0.2734 (0.0005 -- 2.2902)  max mem: 16735
Epoch: [183]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.9157 (1.9211)  loss_scale: 32768.0000 (45420.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3401 (8.7459)  time: 0.9049 (0.5377 -- 3.2264)  data: 0.3242 (0.0004 -- 2.7116)  max mem: 16735
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9288 (1.9027)  loss_scale: 32768.0000 (43329.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7971 (8.7053)  time: 0.7348 (0.5297 -- 1.6303)  data: 0.1867 (0.0004 -- 1.1157)  max mem: 16735
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8749 (1.8989)  loss_scale: 32768.0000 (41831.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5773 (8.7010)  time: 0.9139 (0.5288 -- 2.6133)  data: 0.2828 (0.0004 -- 2.0832)  max mem: 16735
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8573 (1.8985)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3841 (8.7319)  time: 0.7329 (0.5029 -- 2.0943)  data: 0.1017 (0.0002 -- 1.1308)  max mem: 16735
Epoch: [183] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8573 (1.8849)  loss_scale: 32768.0000 (40755.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3841 (8.7319)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2748 (0.2748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5169 (2.5169 -- 2.5169)  data: 2.2732 (2.2732 -- 2.2732)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5957 (0.8035)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (98.9899)  time: 0.4379 (0.2027 -- 2.5169)  data: 0.2222 (0.0005 -- 2.2732)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6377 (0.7506)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2139 (0.1697 -- 0.3791)  data: 0.0110 (0.0001 -- 0.1630)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7036 (0.8298)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.1997 (0.1332 -- 0.3791)  data: 0.0107 (0.0001 -- 0.1630)  max mem: 16735
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.824
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [184]  [  0/160]  eta: 0:21:19  lr: 0.000001  min_lr: 0.000000  loss: 1.5441 (1.5441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1568 (13.1568)  time: 7.9956 (7.9956 -- 7.9956)  data: 7.4385 (7.4385 -- 7.4385)  max mem: 16735
Epoch: [184]  [ 20/160]  eta: 0:02:40  lr: 0.000001  min_lr: 0.000000  loss: 1.8470 (1.7618)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2577 (8.8129)  time: 0.8026 (0.5307 -- 3.1763)  data: 0.0789 (0.0003 -- 0.6090)  max mem: 16735
[2023-09-01 05:50:50,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:50:50,034] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:50:50,035] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:50:50,035] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [184]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8557 (1.8304)  loss_scale: 32768.0000 (37563.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3521 (8.2985)  time: 0.8571 (0.5279 -- 4.0778)  data: 0.0402 (0.0007 -- 0.6417)  max mem: 16735
[2023-09-01 05:51:06,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29496
[2023-09-01 05:51:06,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29496
[2023-09-01 05:51:06,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:51:06,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:51:06,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [184]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9468 (1.8526)  loss_scale: 65536.0000 (44048.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6373 (8.0979)  time: 0.9100 (0.5212 -- 3.7056)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16735
Epoch: [184]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8476 (1.8400)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3651 (8.1594)  time: 0.8242 (0.5441 -- 2.5049)  data: 0.0019 (0.0003 -- 0.0041)  max mem: 16735
Epoch: [184]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9183 (1.8398)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9439 (8.1695)  time: 0.9421 (0.5359 -- 3.3144)  data: 0.0897 (0.0002 -- 1.2417)  max mem: 16735
Epoch: [184]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9851 (1.8333)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0682 (8.1889)  time: 0.8534 (0.5186 -- 4.1905)  data: 0.0012 (0.0002 -- 0.0056)  max mem: 16735
Epoch: [184]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.0661 (1.8568)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4064 (8.2452)  time: 0.8989 (0.5330 -- 2.6626)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16735
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7463 (1.8425)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9924 (8.2731)  time: 0.7017 (0.5017 -- 2.6626)  data: 0.0006 (0.0001 -- 0.0020)  max mem: 16735
Epoch: [184] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7463 (1.8704)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9924 (8.2731)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2743 (0.2743)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4580 (2.4580 -- 2.4580)  data: 2.2294 (2.2294 -- 2.2294)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5991 (0.8048)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (98.9899)  time: 0.4257 (0.1973 -- 2.4580)  data: 0.2118 (0.0006 -- 2.2294)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6399 (0.7513)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2189 (0.1694 -- 0.3195)  data: 0.0171 (0.0001 -- 0.1475)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7009 (0.8309)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2037 (0.1324 -- 0.3195)  data: 0.0168 (0.0001 -- 0.1475)  max mem: 16735
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [185]  [  0/160]  eta: 0:20:50  lr: 0.000001  min_lr: 0.000000  loss: 2.5670 (2.5670)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1414 (11.1414)  time: 7.8146 (7.8146 -- 7.8146)  data: 7.2476 (7.2476 -- 7.2476)  max mem: 16735
Epoch: [185]  [ 20/160]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 1.6704 (1.7651)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (9.0674)  time: 0.8208 (0.5349 -- 2.8473)  data: 0.2471 (0.0003 -- 2.3180)  max mem: 16735
[2023-09-01 05:53:09,975] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:53:09,976] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:53:09,976] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:53:09,977] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:53:19,634] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29635
[2023-09-01 05:53:19,634] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29635
[2023-09-01 05:53:19,634] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:53:19,634] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:53:19,634] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [185]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.8828 (1.8086)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3734 (8.8692)  time: 0.8733 (0.5253 -- 4.1357)  data: 0.3257 (0.0001 -- 3.6127)  max mem: 16735
Epoch: [185]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8578 (1.8077)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6502 (8.9517)  time: 0.8863 (0.5332 -- 3.5594)  data: 0.3317 (0.0005 -- 3.0376)  max mem: 16735
Epoch: [185]  [ 80/160]  eta: 0:01:12  lr: 0.000001  min_lr: 0.000000  loss: 1.9619 (1.8520)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0706 (8.8056)  time: 0.7138 (0.5399 -- 2.4267)  data: 0.1532 (0.0007 -- 1.8712)  max mem: 16735
Epoch: [185]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 2.0346 (1.8614)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2152 (8.7177)  time: 1.0196 (0.5282 -- 4.2610)  data: 0.4515 (0.0005 -- 3.7420)  max mem: 16735
Epoch: [185]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9401 (1.8707)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5026 (8.6789)  time: 0.7968 (0.5194 -- 3.6556)  data: 0.2518 (0.0004 -- 3.1299)  max mem: 16735
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.0042 (1.8734)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2461 (8.6367)  time: 0.9797 (0.5215 -- 4.0150)  data: 0.3956 (0.0004 -- 3.4584)  max mem: 16735
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8710 (1.8710)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4059 (8.6753)  time: 0.6116 (0.5009 -- 1.7260)  data: 0.0853 (0.0002 -- 1.1875)  max mem: 16735
Epoch: [185] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8710 (1.8897)  loss_scale: 32768.0000 (34816.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4059 (8.6753)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2744 (0.2744)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3699 (2.3699 -- 2.3699)  data: 2.1659 (2.1659 -- 2.1659)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5990 (0.8056)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (98.9899)  time: 0.4156 (0.2037 -- 2.3699)  data: 0.2014 (0.0005 -- 2.1659)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6417 (0.7516)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2215 (0.1703 -- 0.4856)  data: 0.0170 (0.0001 -- 0.2863)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6986 (0.8311)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2074 (0.1332 -- 0.4856)  data: 0.0167 (0.0001 -- 0.2863)  max mem: 16735
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [186]  [  0/160]  eta: 0:18:58  lr: 0.000001  min_lr: 0.000000  loss: 1.5951 (1.5951)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2985 (9.2985)  time: 7.1165 (7.1165 -- 7.1165)  data: 6.5741 (6.5741 -- 6.5741)  max mem: 16735
[2023-09-01 05:55:19,805] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:55:19,805] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:55:19,805] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:55:19,805] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:55:20,347] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29765
[2023-09-01 05:55:20,347] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:55:20,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 05:55:20,348] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29765
[2023-09-01 05:55:20,348] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [186]  [ 20/160]  eta: 0:02:58  lr: 0.000001  min_lr: 0.000000  loss: 1.7504 (1.7740)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7080 (7.8155)  time: 0.9806 (0.5257 -- 3.8662)  data: 0.1720 (0.0003 -- 1.6803)  max mem: 16735
Epoch: [186]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 2.1228 (1.9389)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0976 (7.8545)  time: 0.7306 (0.5309 -- 2.5875)  data: 0.0013 (0.0002 -- 0.0024)  max mem: 16735
Epoch: [186]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9590 (1.9342)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9565 (8.2218)  time: 0.9282 (0.5411 -- 3.2718)  data: 0.1682 (0.0005 -- 1.1120)  max mem: 16735
Epoch: [186]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.9038 (1.9322)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0134 (8.3322)  time: 0.8530 (0.5272 -- 2.9028)  data: 0.2795 (0.0003 -- 2.3408)  max mem: 16735
Epoch: [186]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.8554 (1.9111)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7220 (8.4032)  time: 0.8247 (0.5249 -- 2.4983)  data: 0.2102 (0.0005 -- 1.9496)  max mem: 16735
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7724 (1.8894)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5681 (8.3287)  time: 0.8609 (0.5367 -- 2.4043)  data: 0.3042 (0.0006 -- 1.8694)  max mem: 16735
[2023-09-01 05:57:14,874] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:57:14,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:57:14,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:57:14,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:57:16,002] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29896
[2023-09-01 05:57:16,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:57:16,006] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29896
[2023-09-01 05:57:16,007] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:57:16,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7762 (1.8800)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1126 (8.3186)  time: 0.9634 (0.5111 -- 4.4528)  data: 0.4217 (0.0004 -- 3.9233)  max mem: 16735
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9321 (1.8846)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1959 (8.3297)  time: 0.7010 (0.5007 -- 4.0922)  data: 0.1836 (0.0002 -- 3.5818)  max mem: 16735
Epoch: [186] Total time: 0:02:23 (0.8967 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9321 (1.8996)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1959 (8.3297)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2746 (0.2746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4428 (2.4428 -- 2.4428)  data: 2.2148 (2.2148 -- 2.2148)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5971 (0.8056)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4195 (0.1926 -- 2.4428)  data: 0.2108 (0.0006 -- 2.2148)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6409 (0.7515)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2152 (0.1690 -- 0.3826)  data: 0.0146 (0.0001 -- 0.1858)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.6994 (0.8312)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2037 (0.1366 -- 0.3826)  data: 0.0144 (0.0001 -- 0.1858)  max mem: 16735
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.825
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [187]  [  0/160]  eta: 0:20:22  lr: 0.000001  min_lr: 0.000000  loss: 2.3521 (2.3521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2921 (11.2921)  time: 7.6408 (7.6408 -- 7.6408)  data: 6.3145 (6.3145 -- 6.3145)  max mem: 16735
Epoch: [187]  [ 20/160]  eta: 0:02:55  lr: 0.000001  min_lr: 0.000000  loss: 1.8117 (1.8873)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7864 (7.9889)  time: 0.9366 (0.5320 -- 3.1221)  data: 0.3879 (0.0003 -- 2.5849)  max mem: 16735
Epoch: [187]  [ 40/160]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 2.0948 (1.9656)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3383 (8.3409)  time: 0.8111 (0.5318 -- 3.7435)  data: 0.2629 (0.0004 -- 3.2232)  max mem: 16735
Epoch: [187]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.7750 (1.9053)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7501 (8.5331)  time: 0.8681 (0.5353 -- 3.7801)  data: 0.3141 (0.0002 -- 3.2404)  max mem: 16735
[2023-09-01 05:58:54,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=188, lr=[1.2366359971096707e-08, 1.2366359971096707e-08, 1.6488479961462277e-08, 1.6488479961462277e-08, 2.1984639948616367e-08, 2.1984639948616367e-08, 2.9312853264821822e-08, 2.9312853264821822e-08, 3.908380435309577e-08, 3.908380435309577e-08, 5.2111739137461016e-08, 5.2111739137461016e-08, 6.948231884994803e-08, 6.948231884994803e-08, 9.26430917999307e-08, 9.26430917999307e-08, 1.235241223999076e-07, 1.235241223999076e-07, 1.6469882986654348e-07, 1.6469882986654348e-07, 2.1959843982205795e-07, 2.1959843982205795e-07, 2.9279791976274395e-07, 2.9279791976274395e-07, 3.9039722635032525e-07, 3.9039722635032525e-07, 5.20529635133767e-07, 5.20529635133767e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 05:58:54,782] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=18.171558429725557, CurrSamplesPerSec=22.0886439649244, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [187]  [ 80/160]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 1.9281 (1.9374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4920 (8.6804)  time: 0.7193 (0.5314 -- 2.5096)  data: 0.1506 (0.0007 -- 1.9472)  max mem: 16735
Epoch: [187]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.8092 (1.9276)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3015 (8.6277)  time: 0.9296 (0.5330 -- 1.8291)  data: 0.1957 (0.0005 -- 1.2499)  max mem: 16735
[2023-09-01 05:59:17,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:59:17,432] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 05:59:17,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 05:59:17,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [187]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8528 (1.9146)  loss_scale: 65536.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9758 (8.5814)  time: 0.8778 (0.5277 -- 3.8761)  data: 0.3297 (0.0004 -- 3.3630)  max mem: 16735
[2023-09-01 05:59:45,746] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30056
[2023-09-01 05:59:45,746] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:59:45,746] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30056
[2023-09-01 05:59:45,747] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 05:59:45,747] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [187]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 2.0222 (1.9166)  loss_scale: 65536.0000 (39972.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6601 (8.6154)  time: 0.9324 (0.5294 -- 3.4161)  data: 0.3391 (0.0005 -- 2.8842)  max mem: 16735
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7701 (1.8988)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5412 (8.5708)  time: 0.6663 (0.5016 -- 2.1325)  data: 0.0270 (0.0002 -- 0.5268)  max mem: 16735
Epoch: [187] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7701 (1.9016)  loss_scale: 32768.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5412 (8.5708)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.2744 (0.2744)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6690 (2.6690 -- 2.6690)  data: 2.4446 (2.4446 -- 2.4446)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5957 (0.8066)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4433 (0.1978 -- 2.6690)  data: 0.2311 (0.0005 -- 2.4446)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6411 (0.7526)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2092 (0.1711 -- 0.3117)  data: 0.0070 (0.0001 -- 0.0889)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7025 (0.8322)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.1957 (0.1335 -- 0.3117)  data: 0.0067 (0.0001 -- 0.0889)  max mem: 16735
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [188]  [  0/160]  eta: 0:21:08  lr: 0.000000  min_lr: 0.000000  loss: 1.3130 (1.3130)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2687 (6.2687)  time: 7.9266 (7.9266 -- 7.9266)  data: 7.3758 (7.3758 -- 7.3758)  max mem: 16735
Epoch: [188]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 1.9742 (1.8729)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6262 (8.6614)  time: 0.7690 (0.5303 -- 3.1529)  data: 0.2103 (0.0003 -- 2.5946)  max mem: 16735
[2023-09-01 06:00:42,194] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30108
[2023-09-01 06:00:42,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 06:00:42,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-01 06:00:42,194] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30108
[2023-09-01 06:00:42,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [188]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 2.1106 (1.9230)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0121 (8.2332)  time: 0.9876 (0.5070 -- 4.2836)  data: 0.4094 (0.0003 -- 3.7630)  max mem: 16735
Epoch: [188]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 2.1144 (1.9494)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5014 (8.3121)  time: 0.8591 (0.5210 -- 4.2463)  data: 0.2872 (0.0003 -- 3.7100)  max mem: 16735
Epoch: [188]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.8776 (1.9346)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9767 (8.4378)  time: 0.8486 (0.5272 -- 3.1126)  data: 0.1304 (0.0002 -- 2.1120)  max mem: 16735
Epoch: [188]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 2.0718 (1.9409)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9253 (8.4438)  time: 0.8914 (0.5311 -- 3.0585)  data: 0.0009 (0.0004 -- 0.0020)  max mem: 16735
Epoch: [188]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8268 (1.9198)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3957 (8.5558)  time: 0.7725 (0.5236 -- 3.1260)  data: 0.0016 (0.0002 -- 0.0054)  max mem: 16735
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8852 (1.9267)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0511 (8.5608)  time: 0.8280 (0.5366 -- 3.1697)  data: 0.0024 (0.0004 -- 0.0123)  max mem: 16735
[2023-09-01 06:02:30,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:02:30,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:02:30,085] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 06:02:30,085] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5925 (1.8999)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5957 (8.5001)  time: 0.6876 (0.5021 -- 2.8204)  data: 0.0008 (0.0002 -- 0.0022)  max mem: 16735
Epoch: [188] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5925 (1.9125)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5957 (8.5001)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2750 (0.2750)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3131 (2.3131 -- 2.3131)  data: 2.1083 (2.1083 -- 2.1083)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5931 (0.8067)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (98.9899)  time: 0.4316 (0.2011 -- 2.3131)  data: 0.2180 (0.0005 -- 2.1083)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6409 (0.7529)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (97.3545)  time: 0.2303 (0.1684 -- 0.5098)  data: 0.0271 (0.0001 -- 0.2819)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7039 (0.8326)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2163 (0.1327 -- 0.5098)  data: 0.0268 (0.0001 -- 0.2819)  max mem: 16735
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [189]  [  0/160]  eta: 0:23:17  lr: 0.000000  min_lr: 0.000000  loss: 1.1810 (1.1810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9559 (8.9559)  time: 8.7339 (8.7339 -- 8.7339)  data: 8.1966 (8.1966 -- 8.1966)  max mem: 16735
Epoch: [189]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.9967 (1.9661)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5134 (8.4133)  time: 0.7661 (0.5361 -- 2.8805)  data: 0.2111 (0.0008 -- 2.3601)  max mem: 16735
Epoch: [189]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.8322 (1.8955)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5718 (8.1828)  time: 0.9831 (0.5241 -- 4.4590)  data: 0.4330 (0.0005 -- 3.9413)  max mem: 16735
Epoch: [189]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8710 (1.9014)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2668 (8.2095)  time: 0.7406 (0.5327 -- 2.2277)  data: 0.1696 (0.0004 -- 1.6983)  max mem: 16735
Epoch: [189]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9268 (1.9029)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7411 (8.5052)  time: 0.8362 (0.5210 -- 2.5542)  data: 0.2856 (0.0004 -- 2.0157)  max mem: 16735
Epoch: [189]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.9042 (1.9141)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1979 (8.2893)  time: 0.9292 (0.5209 -- 3.5081)  data: 0.3830 (0.0006 -- 2.9799)  max mem: 16735
Epoch: [189]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8736 (1.9115)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9126 (8.2448)  time: 0.9036 (0.5346 -- 3.8149)  data: 0.3586 (0.0002 -- 3.2614)  max mem: 16735
[2023-09-01 06:04:33,742] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:04:33,743] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:04:33,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:04:33,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:04:41,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30372
[2023-09-01 06:04:41,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30372
[2023-09-01 06:04:41,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:04:41,427] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:04:41,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9263 (1.9038)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5609 (8.1729)  time: 0.7932 (0.5327 -- 2.8853)  data: 0.1304 (0.0004 -- 1.3407)  max mem: 16735
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7519 (1.8880)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4939 (8.1790)  time: 0.7501 (0.5010 -- 3.8342)  data: 0.2250 (0.0002 -- 3.3147)  max mem: 16735
Epoch: [189] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7519 (1.8880)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4939 (8.1790)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2746 (0.2746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4056 (2.4056 -- 2.4056)  data: 2.1973 (2.1973 -- 2.1973)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5948 (0.8069)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (98.9899)  time: 0.4157 (0.1968 -- 2.4056)  data: 0.2010 (0.0004 -- 2.1973)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6405 (0.7528)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (97.3545)  time: 0.2225 (0.1683 -- 0.5239)  data: 0.0180 (0.0001 -- 0.3443)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7029 (0.8324)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2078 (0.1320 -- 0.5239)  data: 0.0178 (0.0001 -- 0.3443)  max mem: 16735
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 77.593 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.05%
Epoch: [190]  [  0/160]  eta: 0:18:19  lr: 0.000000  min_lr: 0.000000  loss: 1.1509 (1.1509)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4431 (8.4431)  time: 6.8723 (6.8723 -- 6.8723)  data: 5.6857 (5.6857 -- 5.6857)  max mem: 16735
Epoch: [190]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.6971 (1.7792)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8642 (9.3079)  time: 0.9519 (0.5248 -- 3.6593)  data: 0.3483 (0.0004 -- 3.0742)  max mem: 16735
Epoch: [190]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 2.0532 (1.8685)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7380 (9.3034)  time: 0.7620 (0.5327 -- 1.8301)  data: 0.0939 (0.0002 -- 0.8184)  max mem: 16735
Epoch: [190]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.7827 (1.8453)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2146 (9.0065)  time: 0.8612 (0.5248 -- 1.8713)  data: 0.2891 (0.0011 -- 1.3348)  max mem: 16735
Epoch: [190]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.8432 (1.8679)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6713 (8.9187)  time: 0.9831 (0.5287 -- 3.2482)  data: 0.4296 (0.0005 -- 2.7008)  max mem: 16735
Epoch: [190]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 2.0438 (1.8809)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3109 (8.8040)  time: 0.7980 (0.5292 -- 3.0156)  data: 0.2308 (0.0002 -- 2.4608)  max mem: 16735
[2023-09-01 06:06:43,677] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30501
[2023-09-01 06:06:43,677] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 06:06:43,677] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30501
[2023-09-01 06:06:43,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-01 06:06:43,678] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [190]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7970 (1.8766)  loss_scale: 16384.0000 (30059.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6725 (8.7356)  time: 0.9329 (0.5268 -- 3.6972)  data: 0.3807 (0.0003 -- 3.1123)  max mem: 16735
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9079 (1.8704)  loss_scale: 16384.0000 (28120.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0807 (8.6763)  time: 0.7870 (0.5344 -- 2.5815)  data: 0.2338 (0.0004 -- 2.0319)  max mem: 16735
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9975 (1.8802)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9173 (8.6607)  time: 0.7626 (0.5002 -- 3.3467)  data: 0.2433 (0.0002 -- 2.8307)  max mem: 16735
Epoch: [190] Total time: 0:02:23 (0.8945 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9975 (1.8788)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9173 (8.6607)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2745 (0.2745)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5223 (2.5223 -- 2.5223)  data: 2.2888 (2.2888 -- 2.2888)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5937 (0.8068)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4259 (0.1994 -- 2.5223)  data: 0.2112 (0.0004 -- 2.2888)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6405 (0.7528)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2150 (0.1685 -- 0.2833)  data: 0.0115 (0.0001 -- 0.1012)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7030 (0.8325)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.1996 (0.1326 -- 0.2833)  data: 0.0107 (0.0001 -- 0.1012)  max mem: 16735
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [191]  [  0/160]  eta: 0:20:11  lr: 0.000000  min_lr: 0.000000  loss: 1.3268 (1.3268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7289 (7.7289)  time: 7.5693 (7.5693 -- 7.5693)  data: 6.9995 (6.9995 -- 6.9995)  max mem: 16735
Epoch: [191]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.8504 (1.8630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7697 (9.1079)  time: 0.8369 (0.5297 -- 2.6017)  data: 0.2838 (0.0006 -- 2.0511)  max mem: 16735
Epoch: [191]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 2.0883 (1.9239)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1641 (9.0128)  time: 0.9582 (0.5277 -- 3.9844)  data: 0.4088 (0.0005 -- 3.4678)  max mem: 16735
Epoch: [191]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8014 (1.9070)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5004 (8.5577)  time: 0.7785 (0.5286 -- 2.0193)  data: 0.2294 (0.0001 -- 1.4822)  max mem: 16735
[2023-09-01 06:08:48,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:08:48,321] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 06:08:48,322] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:08:48,323] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [191]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7933 (1.8832)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8226 (8.4381)  time: 0.8229 (0.5475 -- 3.0852)  data: 0.0058 (0.0003 -- 0.0679)  max mem: 16735
Epoch: [191]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 2.0093 (1.9154)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2199 (8.4613)  time: 0.8385 (0.5317 -- 2.6041)  data: 0.1713 (0.0004 -- 1.7184)  max mem: 16735
Epoch: [191]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7283 (1.8985)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5810 (8.6336)  time: 0.8812 (0.5283 -- 3.4088)  data: 0.2935 (0.0002 -- 2.8758)  max mem: 16735
Epoch: [191]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8939 (1.8783)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3770 (8.6248)  time: 0.9676 (0.5233 -- 3.0245)  data: 0.4193 (0.0005 -- 2.4861)  max mem: 16735
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7693 (1.8720)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3555 (8.5499)  time: 0.6667 (0.5002 -- 1.9041)  data: 0.1428 (0.0001 -- 1.3897)  max mem: 16735
Epoch: [191] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7693 (1.8681)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3555 (8.5499)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2745 (0.2745)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3207 (2.3207 -- 2.3207)  data: 2.1142 (2.1142 -- 2.1142)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5928 (0.8065)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4215 (0.1984 -- 2.3207)  data: 0.2027 (0.0004 -- 2.1142)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6408 (0.7526)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2188 (0.1696 -- 0.3127)  data: 0.0094 (0.0001 -- 0.0857)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7021 (0.8325)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2036 (0.1326 -- 0.3127)  data: 0.0087 (0.0001 -- 0.0857)  max mem: 16735
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [192]  [  0/160]  eta: 0:19:38  lr: 0.000000  min_lr: 0.000000  loss: 1.1030 (1.1030)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3667 (7.3667)  time: 7.3664 (7.3664 -- 7.3664)  data: 6.8086 (6.8086 -- 6.8086)  max mem: 16735
Epoch: [192]  [ 20/160]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 1.8952 (1.8388)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6919 (7.7023)  time: 0.8653 (0.5283 -- 3.1880)  data: 0.2601 (0.0005 -- 2.6605)  max mem: 16735
[2023-09-01 06:10:50,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:10:50,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:10:50,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:10:50,889] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [192]  [ 40/160]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 1.9300 (1.8869)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5033 (8.4640)  time: 1.0118 (0.5267 -- 3.7051)  data: 0.4632 (0.0003 -- 3.1427)  max mem: 16735
[2023-09-01 06:11:01,223] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30770
[2023-09-01 06:11:01,223] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30770
[2023-09-01 06:11:01,223] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:11:01,223] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:11:01,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [192]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 2.0662 (1.8986)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6006 (8.3511)  time: 0.6223 (0.5138 -- 1.1274)  data: 0.0750 (0.0006 -- 0.5591)  max mem: 16735
Epoch: [192]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.9279 (1.9121)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7978 (8.3448)  time: 0.9477 (0.5289 -- 2.8270)  data: 0.0856 (0.0006 -- 0.9343)  max mem: 16735
Epoch: [192]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 2.0177 (1.9310)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8889 (8.3702)  time: 0.8774 (0.5341 -- 3.6258)  data: 0.2733 (0.0004 -- 3.0552)  max mem: 16735
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8534 (1.9229)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5512 (8.4238)  time: 0.7761 (0.5448 -- 3.5960)  data: 0.2033 (0.0004 -- 3.0611)  max mem: 16735
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9500 (1.9277)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6711 (8.3339)  time: 0.9673 (0.5410 -- 2.9904)  data: 0.3948 (0.0006 -- 2.4287)  max mem: 16735
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8979 (1.9263)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2212 (8.4476)  time: 0.6980 (0.5017 -- 3.1475)  data: 0.1565 (0.0001 -- 2.6048)  max mem: 16735
Epoch: [192] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8979 (1.9279)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2212 (8.4476)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2746 (0.2746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3776 (2.3776 -- 2.3776)  data: 2.1652 (2.1652 -- 2.1652)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5937 (0.8066)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4173 (0.1950 -- 2.3776)  data: 0.1980 (0.0007 -- 2.1652)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6407 (0.7526)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2198 (0.1691 -- 0.4053)  data: 0.0161 (0.0001 -- 0.2191)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7021 (0.8326)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2039 (0.1321 -- 0.4053)  data: 0.0158 (0.0001 -- 0.2191)  max mem: 16735
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [193]  [  0/160]  eta: 0:21:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6821 (1.6821)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1899 (6.1899)  time: 8.2210 (8.2210 -- 8.2210)  data: 4.6343 (4.6343 -- 4.6343)  max mem: 16735
[2023-09-01 06:13:05,194] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:13:05,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:13:05,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:13:05,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [193]  [ 20/160]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 1.8475 (1.8617)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9150 (7.8821)  time: 0.8764 (0.5370 -- 3.7074)  data: 0.2311 (0.0006 -- 3.1640)  max mem: 16735
[2023-09-01 06:13:20,786] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30919
[2023-09-01 06:13:20,786] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30919
[2023-09-01 06:13:20,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:13:20,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:13:20,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [193]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9299 (1.8904)  loss_scale: 65536.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8423 (8.1486)  time: 0.7795 (0.5312 -- 2.9687)  data: 0.2328 (0.0003 -- 2.4041)  max mem: 16735
Epoch: [193]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.9028 (1.8869)  loss_scale: 32768.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7795 (8.2851)  time: 0.8775 (0.5222 -- 3.6026)  data: 0.3276 (0.0004 -- 3.0754)  max mem: 16735
Epoch: [193]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.7601 (1.8604)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1530 (8.3175)  time: 0.9058 (0.5305 -- 3.0681)  data: 0.3648 (0.0002 -- 2.5430)  max mem: 16735
[2023-09-01 06:14:12,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30975
[2023-09-01 06:14:12,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30975
[2023-09-01 06:14:12,002] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 06:14:12,002] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-01 06:14:12,002] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 2.0571 (1.8809)  loss_scale: 32768.0000 (38283.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1010 (8.3341)  time: 0.9529 (0.5132 -- 3.7687)  data: 0.4100 (0.0004 -- 3.2325)  max mem: 16735
[2023-09-01 06:14:31,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=195, lr=[3.936773415739102e-09, 3.936773415739102e-09, 5.249031220985469e-09, 5.249031220985469e-09, 6.998708294647292e-09, 6.998708294647292e-09, 9.331611059529723e-09, 9.331611059529723e-09, 1.2442148079372963e-08, 1.2442148079372963e-08, 1.6589530772497285e-08, 1.6589530772497285e-08, 2.211937436332971e-08, 2.211937436332971e-08, 2.949249915110628e-08, 2.949249915110628e-08, 3.9323332201475046e-08, 3.9323332201475046e-08, 5.243110960196673e-08, 5.243110960196673e-08, 6.990814613595564e-08, 6.990814613595564e-08, 9.321086151460751e-08, 9.321086151460751e-08, 1.2428114868614336e-07, 1.2428114868614336e-07, 1.6570819824819113e-07, 1.6570819824819113e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 06:14:31,815] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=18.22903178013897, CurrSamplesPerSec=21.128850704558822, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [193]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8168 (1.8793)  loss_scale: 16384.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8531 (8.4115)  time: 0.8146 (0.5286 -- 4.6011)  data: 0.2633 (0.0001 -- 4.0758)  max mem: 16735
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8737 (1.8789)  loss_scale: 16384.0000 (32070.8085)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8812 (8.3568)  time: 0.8491 (0.5258 -- 3.4286)  data: 0.2983 (0.0004 -- 2.8956)  max mem: 16735
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9609 (1.8963)  loss_scale: 16384.0000 (30208.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7893 (8.4253)  time: 0.6578 (0.5039 -- 2.3452)  data: 0.1319 (0.0003 -- 1.8218)  max mem: 16735
Epoch: [193] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9609 (1.8953)  loss_scale: 16384.0000 (30208.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7893 (8.4253)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2749 (0.2749)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2977 (2.2977 -- 2.2977)  data: 2.0734 (2.0734 -- 2.0734)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5929 (0.8067)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4099 (0.1995 -- 2.2977)  data: 0.1903 (0.0004 -- 2.0734)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6407 (0.7527)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2279 (0.1715 -- 0.5828)  data: 0.0202 (0.0002 -- 0.3811)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7014 (0.8327)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2124 (0.1329 -- 0.5828)  data: 0.0198 (0.0001 -- 0.3811)  max mem: 16735
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [194]  [  0/160]  eta: 0:22:53  lr: 0.000000  min_lr: 0.000000  loss: 2.0971 (2.0971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6310 (9.6310)  time: 8.5842 (8.5842 -- 8.5842)  data: 6.4386 (6.4386 -- 6.4386)  max mem: 16735
Epoch: [194]  [ 20/160]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 1.9851 (1.9710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7785 (8.5582)  time: 0.8019 (0.5229 -- 2.5034)  data: 0.0018 (0.0002 -- 0.0092)  max mem: 16735
Epoch: [194]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.9016 (1.9250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9325 (8.6483)  time: 0.8649 (0.5245 -- 3.7745)  data: 0.0013 (0.0003 -- 0.0023)  max mem: 16735
Epoch: [194]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 2.0211 (1.9328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2961 (8.6515)  time: 0.9026 (0.5251 -- 2.6248)  data: 0.0270 (0.0002 -- 0.4212)  max mem: 16735
[2023-09-01 06:16:11,983] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:16:11,984] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-01 06:16:11,984] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:16:11,985] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [194]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.9307 (1.9434)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3480 (8.7051)  time: 0.8548 (0.5359 -- 4.1336)  data: 0.3024 (0.0006 -- 3.5848)  max mem: 16735
Epoch: [194]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 2.0458 (1.9619)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8037 (8.5972)  time: 0.9225 (0.5196 -- 3.5875)  data: 0.3790 (0.0003 -- 3.0598)  max mem: 16735
Epoch: [194]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8997 (1.9578)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5250 (8.6350)  time: 0.7087 (0.5325 -- 2.9775)  data: 0.1517 (0.0002 -- 2.4497)  max mem: 16735
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8556 (1.9364)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6959 (8.6343)  time: 0.9994 (0.5262 -- 4.5682)  data: 0.4511 (0.0003 -- 4.0413)  max mem: 16735
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8472 (1.9278)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0927 (8.6524)  time: 0.5955 (0.5004 -- 1.1936)  data: 0.0664 (0.0002 -- 0.6857)  max mem: 16735
Epoch: [194] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8472 (1.9307)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0927 (8.6524)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3924 (2.3924 -- 2.3924)  data: 2.1543 (2.1543 -- 2.1543)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5935 (0.8069)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4240 (0.1917 -- 2.3924)  data: 0.2105 (0.0007 -- 2.1543)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6408 (0.7527)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2205 (0.1697 -- 0.3544)  data: 0.0168 (0.0001 -- 0.1725)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7010 (0.8326)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2066 (0.1326 -- 0.3544)  data: 0.0165 (0.0001 -- 0.1725)  max mem: 16735
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.826
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [195]  [  0/160]  eta: 0:24:10  lr: 0.000000  min_lr: 0.000000  loss: 1.4764 (1.4764)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4538 (6.4538)  time: 9.0640 (9.0640 -- 9.0640)  data: 6.2989 (6.2989 -- 6.2989)  max mem: 16735
Epoch: [195]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7112 (1.7660)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8419 (8.3265)  time: 0.7537 (0.5318 -- 3.1561)  data: 0.0742 (0.0004 -- 1.3525)  max mem: 16735
[2023-09-01 06:18:15,493] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:18:15,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:18:15,493] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:18:15,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:18:17,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31235
[2023-09-01 06:18:17,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31235
[2023-09-01 06:18:17,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:18:17,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:18:17,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [195]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 2.0399 (1.8917)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6714 (8.1649)  time: 0.9461 (0.5234 -- 4.3428)  data: 0.3914 (0.0004 -- 3.8171)  max mem: 16735
Epoch: [195]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9422 (1.9221)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2718 (8.5136)  time: 0.8197 (0.5260 -- 2.1010)  data: 0.0867 (0.0002 -- 1.4388)  max mem: 16735
Epoch: [195]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8867 (1.9267)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0792 (8.4553)  time: 0.8659 (0.5293 -- 3.2103)  data: 0.0015 (0.0002 -- 0.0026)  max mem: 16735
Epoch: [195]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8761 (1.9157)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9951 (8.8156)  time: 0.8319 (0.5284 -- 4.2747)  data: 0.0078 (0.0003 -- 0.1133)  max mem: 16735
Epoch: [195]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.9809 (1.9219)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2598 (8.6843)  time: 0.8227 (0.5304 -- 2.8149)  data: 0.0564 (0.0002 -- 1.1015)  max mem: 16735
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 2.1157 (1.9432)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1066 (8.6542)  time: 0.8802 (0.5377 -- 2.6122)  data: 0.1175 (0.0005 -- 1.1582)  max mem: 16735
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7300 (1.9205)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6404 (8.5778)  time: 0.6686 (0.5024 -- 1.7083)  data: 0.0625 (0.0002 -- 0.9168)  max mem: 16735
Epoch: [195] Total time: 0:02:20 (0.8769 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7300 (1.9148)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6404 (8.5778)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2748 (0.2748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2804 (2.2804 -- 2.2804)  data: 2.0410 (2.0410 -- 2.0410)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5940 (0.8071)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4130 (0.2020 -- 2.2804)  data: 0.1878 (0.0004 -- 2.0410)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6413 (0.7529)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2249 (0.1694 -- 0.3870)  data: 0.0186 (0.0001 -- 0.2010)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7013 (0.8328)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2082 (0.1333 -- 0.3870)  data: 0.0177 (0.0001 -- 0.2010)  max mem: 16735
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [196]  [  0/160]  eta: 0:16:05  lr: 0.000000  min_lr: 0.000000  loss: 1.8148 (1.8148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9324 (7.9324)  time: 6.0363 (6.0363 -- 6.0363)  data: 5.4704 (5.4704 -- 5.4704)  max mem: 16735
[2023-09-01 06:20:16,486] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:20:16,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:20:16,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:20:16,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:20:23,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31371
[2023-09-01 06:20:23,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31371
[2023-09-01 06:20:23,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:20:23,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:20:23,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [196]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.8548 (1.8836)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3880 (9.0644)  time: 0.9432 (0.5204 -- 3.8187)  data: 0.3861 (0.0003 -- 3.2732)  max mem: 16735
Epoch: [196]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.8974 (1.8921)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4359 (8.3901)  time: 0.8777 (0.5178 -- 3.5093)  data: 0.3121 (0.0003 -- 2.9819)  max mem: 16735
Epoch: [196]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 2.0241 (1.9227)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1659 (8.2907)  time: 0.9240 (0.5239 -- 4.8566)  data: 0.3446 (0.0004 -- 4.3114)  max mem: 16735
Epoch: [196]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 2.0074 (1.9542)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2164 (8.4083)  time: 0.8886 (0.5267 -- 4.5859)  data: 0.3390 (0.0004 -- 4.0325)  max mem: 16735
Epoch: [196]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8760 (1.9374)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3541 (8.4556)  time: 0.8260 (0.5303 -- 3.0750)  data: 0.2532 (0.0001 -- 2.5445)  max mem: 16735
Epoch: [196]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9472 (1.9359)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2743 (8.3509)  time: 0.8902 (0.5196 -- 3.3836)  data: 0.1651 (0.0002 -- 1.9413)  max mem: 16735
[2023-09-01 06:22:17,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:22:17,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:22:17,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:22:17,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9687 (1.9346)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0217 (8.3715)  time: 0.8823 (0.5395 -- 3.5383)  data: 0.0653 (0.0003 -- 1.1815)  max mem: 16735
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8685 (1.9266)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7486 (8.3955)  time: 0.6864 (0.5002 -- 2.5097)  data: 0.1339 (0.0001 -- 1.9621)  max mem: 16735
Epoch: [196] Total time: 0:02:23 (0.8990 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8685 (1.9108)  loss_scale: 65536.0000 (38297.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7486 (8.3955)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4965 (2.4965 -- 2.4965)  data: 2.2404 (2.2404 -- 2.2404)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5939 (0.8072)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4312 (0.1992 -- 2.4965)  data: 0.2150 (0.0004 -- 2.2404)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6407 (0.7528)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2157 (0.1691 -- 0.3560)  data: 0.0104 (0.0001 -- 0.1162)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7010 (0.8327)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2015 (0.1326 -- 0.3560)  data: 0.0102 (0.0001 -- 0.1162)  max mem: 16735
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [197]  [  0/160]  eta: 0:19:30  lr: 0.000000  min_lr: 0.000000  loss: 1.8519 (1.8519)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7866 (7.7866)  time: 7.3127 (7.3127 -- 7.3127)  data: 6.7405 (6.7405 -- 6.7405)  max mem: 16735
[2023-09-01 06:22:47,393] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31523
[2023-09-01 06:22:47,393] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31523
[2023-09-01 06:22:47,394] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:22:47,394] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:22:47,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [197]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 1.9068 (1.8762)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6030 (8.4711)  time: 0.8959 (0.5218 -- 3.9050)  data: 0.0045 (0.0001 -- 0.0438)  max mem: 16735
Epoch: [197]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.9329 (1.8851)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1381 (8.7886)  time: 0.8873 (0.5331 -- 3.9056)  data: 0.0016 (0.0004 -- 0.0046)  max mem: 16735
Epoch: [197]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 2.0464 (1.9308)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3706 (8.6609)  time: 0.8498 (0.5309 -- 3.0336)  data: 0.0013 (0.0002 -- 0.0027)  max mem: 16735
Epoch: [197]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8420 (1.9015)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8638 (8.7839)  time: 0.8084 (0.5291 -- 2.7494)  data: 0.0017 (0.0004 -- 0.0049)  max mem: 16735
Epoch: [197]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6810 (1.8791)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6025 (8.7995)  time: 0.9774 (0.5283 -- 3.8189)  data: 0.0010 (0.0003 -- 0.0019)  max mem: 16735
Epoch: [197]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 2.0647 (1.9051)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9222 (8.7380)  time: 0.8025 (0.5171 -- 4.6239)  data: 0.0011 (0.0003 -- 0.0026)  max mem: 16735
[2023-09-01 06:24:43,919] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:24:43,920] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:24:43,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:24:43,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:24:44,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31653
[2023-09-01 06:24:44,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31653
[2023-09-01 06:24:44,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:24:44,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:24:44,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8640 (1.9073)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3926 (8.7176)  time: 0.9096 (0.5109 -- 5.7727)  data: 0.0012 (0.0002 -- 0.0042)  max mem: 16735
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 2.0259 (1.9118)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7749 (8.7610)  time: 0.6090 (0.5009 -- 1.3742)  data: 0.0008 (0.0002 -- 0.0028)  max mem: 16735
Epoch: [197] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 2.0259 (1.8798)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7749 (8.7610)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1375 (2.1375 -- 2.1375)  data: 1.9382 (1.9382 -- 1.9382)  max mem: 16735
Val:  [10/27]  eta: 0:00:06  loss: 0.5938 (0.8070)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4042 (0.1882 -- 2.1375)  data: 0.1947 (0.0004 -- 1.9382)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6409 (0.7527)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2286 (0.1703 -- 0.4230)  data: 0.0228 (0.0001 -- 0.1956)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7007 (0.8326)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2154 (0.1327 -- 0.4230)  data: 0.0226 (0.0001 -- 0.1956)  max mem: 16735
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [198]  [  0/160]  eta: 0:18:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7418 (1.7418)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0696 (9.0696)  time: 7.0043 (7.0043 -- 7.0043)  data: 6.4632 (6.4632 -- 6.4632)  max mem: 16735
Epoch: [198]  [ 20/160]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 2.0618 (1.9267)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0776 (8.7902)  time: 0.9489 (0.5235 -- 4.2136)  data: 0.0525 (0.0002 -- 0.8547)  max mem: 16735
Epoch: [198]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 2.0296 (1.9787)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0109 (8.7599)  time: 0.8584 (0.5214 -- 2.2750)  data: 0.1454 (0.0001 -- 1.4328)  max mem: 16735
Epoch: [198]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.7836 (1.9103)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7146 (8.5347)  time: 0.9234 (0.5219 -- 3.6425)  data: 0.3353 (0.0004 -- 3.1088)  max mem: 16735
Epoch: [198]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 2.0402 (1.9149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0023 (8.6557)  time: 0.7677 (0.5293 -- 3.0056)  data: 0.1957 (0.0001 -- 2.4601)  max mem: 16735
Epoch: [198]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 2.0745 (1.9210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1810 (8.5690)  time: 0.9308 (0.5296 -- 3.3438)  data: 0.3867 (0.0003 -- 2.8240)  max mem: 16735
[2023-09-01 06:26:44,581] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:26:44,581] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:26:44,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:26:44,586] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:26:58,599] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31797
[2023-09-01 06:26:58,600] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:26:58,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-01 06:26:58,600] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31797
[2023-09-01 06:26:58,601] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [198]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 2.0217 (1.9410)  loss_scale: 65536.0000 (36830.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5182 (8.4080)  time: 0.8389 (0.5270 -- 5.0963)  data: 0.2899 (0.0002 -- 4.5873)  max mem: 16735
Epoch: [198]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8755 (1.9301)  loss_scale: 32768.0000 (36253.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2635 (8.4391)  time: 0.9567 (0.5296 -- 3.6488)  data: 0.4166 (0.0003 -- 3.1285)  max mem: 16735
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8626 (1.9274)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7588 (8.5428)  time: 0.6400 (0.5029 -- 2.8184)  data: 0.1157 (0.0001 -- 2.2978)  max mem: 16735
Epoch: [198] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8626 (1.9136)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7588 (8.5428)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3352 (2.3352 -- 2.3352)  data: 2.1100 (2.1100 -- 2.1100)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5940 (0.8073)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4160 (0.2042 -- 2.3352)  data: 0.1979 (0.0005 -- 2.1100)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6411 (0.7528)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2275 (0.1696 -- 0.5494)  data: 0.0222 (0.0001 -- 0.3729)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7009 (0.8327)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2116 (0.1323 -- 0.5494)  data: 0.0219 (0.0001 -- 0.3729)  max mem: 16735
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Epoch: [199]  [  0/160]  eta: 0:18:33  lr: 0.000000  min_lr: 0.000000  loss: 1.6407 (1.6407)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9304 (6.9304)  time: 6.9587 (6.9587 -- 6.9587)  data: 5.1176 (5.1176 -- 5.1176)  max mem: 16735
Epoch: [199]  [ 20/160]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8446 (1.8238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8782 (8.7498)  time: 0.8406 (0.5240 -- 3.0076)  data: 0.1399 (0.0003 -- 2.0270)  max mem: 16735
Epoch: [199]  [ 40/160]  eta: 0:01:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8415 (1.8484)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9033 (8.5674)  time: 0.7932 (0.5228 -- 1.8271)  data: 0.0752 (0.0004 -- 1.2874)  max mem: 16735
Epoch: [199]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 1.9074 (1.8445)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5763 (8.6709)  time: 0.9143 (0.5293 -- 4.0654)  data: 0.0478 (0.0004 -- 0.4600)  max mem: 16735
Epoch: [199]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 2.0840 (1.9065)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8128 (8.6877)  time: 0.8635 (0.5260 -- 3.2710)  data: 0.1316 (0.0008 -- 0.7898)  max mem: 16735
[2023-09-01 06:29:00,558] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:29:00,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:29:00,560] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-01 06:29:00,560] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-01 06:29:04,600] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31930
[2023-09-01 06:29:04,602] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31930
[2023-09-01 06:29:04,642] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:29:04,642] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-01 06:29:04,642] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [199]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8896 (1.8990)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0548 (8.7076)  time: 0.9460 (0.5293 -- 3.2443)  data: 0.3995 (0.0006 -- 2.7076)  max mem: 16735
Epoch: [199]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9533 (1.9046)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6626 (8.6838)  time: 0.9066 (0.5171 -- 4.0580)  data: 0.3596 (0.0003 -- 3.5514)  max mem: 16735
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7156 (1.8874)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9851 (8.7010)  time: 0.9977 (0.5149 -- 5.2115)  data: 0.4479 (0.0004 -- 4.6643)  max mem: 16735
[2023-09-01 06:30:01,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=201, lr=[1.1136245707468657e-09, 1.1136245707468657e-09, 1.4848327609958208e-09, 1.4848327609958208e-09, 1.9797770146610946e-09, 1.9797770146610946e-09, 2.6397026862147927e-09, 2.6397026862147927e-09, 3.5196035816197234e-09, 3.5196035816197234e-09, 4.6928047754929645e-09, 4.6928047754929645e-09, 6.257073033990619e-09, 6.257073033990619e-09, 8.342764045320827e-09, 8.342764045320827e-09, 1.1123685393761101e-08, 1.1123685393761101e-08, 1.4831580525014802e-08, 1.4831580525014802e-08, 1.9775440700019736e-08, 1.9775440700019736e-08, 2.636725426669298e-08, 2.636725426669298e-08, 3.515633902225731e-08, 3.515633902225731e-08, 4.687511869634308e-08, 4.687511869634308e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-01 06:30:01,716] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=18.218708781100787, CurrSamplesPerSec=24.284174888582566, MemAllocated=1.21GB, MaxMemAllocated=16.34GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8132 (1.8741)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1340 (8.6340)  time: 0.5227 (0.5014 -- 0.5648)  data: 0.0037 (0.0002 -- 0.0634)  max mem: 16735
Epoch: [199] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8132 (1.8829)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1340 (8.6340)
[2023-09-01 06:30:01,719] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-09-01 06:30:01,721] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-09-01 06:30:01,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-09-01 06:30:01,721] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-09-01 06:30:02,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-09-01 06:30:02,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4001 (2.4001 -- 2.4001)  data: 2.1418 (2.1418 -- 2.1418)  max mem: 16735
Val:  [10/27]  eta: 0:00:07  loss: 0.5940 (0.8073)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4502 (0.2059 -- 2.4001)  data: 0.2223 (0.0004 -- 2.1418)  max mem: 16735
Val:  [20/27]  eta: 0:00:02  loss: 0.6409 (0.7529)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2293 (0.1699 -- 0.4181)  data: 0.0154 (0.0001 -- 0.1866)  max mem: 16735
Val:  [26/27]  eta: 0:00:00  loss: 0.7008 (0.8328)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2105 (0.1320 -- 0.4181)  data: 0.0150 (0.0001 -- 0.1866)  max mem: 16735
Val: Total time: 0:00:07 (0.2957 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.827
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.05%
Test:  [  0/603]  eta: 1:01:24  loss: 0.3992 (0.3992)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.1104 (6.1104 -- 6.1104)  data: 5.8719 (5.8719 -- 5.8719)  max mem: 16735
Test:  [ 10/603]  eta: 0:08:59  loss: 0.7370 (0.7784)  acc1: 83.3333 (80.3030)  acc5: 100.0000 (98.4848)  time: 0.9095 (0.1359 -- 6.1104)  data: 0.7510 (0.0006 -- 5.8719)  max mem: 16735
Test:  [ 20/603]  eta: 0:08:37  loss: 1.0710 (1.0559)  acc1: 66.6667 (69.0476)  acc5: 100.0000 (95.2381)  time: 0.6268 (0.1318 -- 3.8660)  data: 0.4828 (0.0005 -- 3.7318)  max mem: 16735
Test:  [ 30/603]  eta: 0:06:13  loss: 0.7243 (0.8881)  acc1: 66.6667 (74.7312)  acc5: 100.0000 (96.7742)  time: 0.5091 (0.1309 -- 3.8660)  data: 0.3707 (0.0002 -- 3.7318)  max mem: 16735
Test:  [ 40/603]  eta: 0:05:43  loss: 0.5892 (0.8956)  acc1: 83.3333 (74.7968)  acc5: 100.0000 (96.7480)  time: 0.3182 (0.1260 -- 3.0141)  data: 0.1805 (0.0002 -- 2.8668)  max mem: 16735
Test:  [ 50/603]  eta: 0:05:06  loss: 0.7821 (0.8802)  acc1: 66.6667 (75.4902)  acc5: 100.0000 (97.0588)  time: 0.4049 (0.1260 -- 3.0141)  data: 0.2663 (0.0002 -- 2.8668)  max mem: 16735
Test:  [ 60/603]  eta: 0:05:50  loss: 0.9796 (0.9548)  acc1: 66.6667 (72.6776)  acc5: 100.0000 (96.1749)  time: 0.7171 (0.1295 -- 5.5001)  data: 0.5755 (0.0003 -- 5.3815)  max mem: 16735
Test:  [ 70/603]  eta: 0:05:04  loss: 0.6746 (0.8971)  acc1: 83.3333 (74.4131)  acc5: 100.0000 (96.7136)  time: 0.6154 (0.1119 -- 5.5001)  data: 0.4826 (0.0001 -- 5.3815)  max mem: 16735
Test:  [ 80/603]  eta: 0:04:53  loss: 0.6408 (0.9017)  acc1: 83.3333 (74.4856)  acc5: 100.0000 (96.7078)  time: 0.3018 (0.1119 -- 3.4651)  data: 0.1667 (0.0001 -- 3.3232)  max mem: 16735
Test:  [ 90/603]  eta: 0:04:30  loss: 0.7053 (0.8924)  acc1: 83.3333 (74.9084)  acc5: 100.0000 (96.8864)  time: 0.3706 (0.1221 -- 3.4651)  data: 0.2311 (0.0003 -- 3.3232)  max mem: 16735
Test:  [100/603]  eta: 0:04:53  loss: 0.8971 (0.9354)  acc1: 66.6667 (73.2673)  acc5: 100.0000 (96.3696)  time: 0.6793 (0.1221 -- 4.9508)  data: 0.5396 (0.0003 -- 4.7499)  max mem: 16735
Test:  [110/603]  eta: 0:04:29  loss: 0.6855 (0.9006)  acc1: 83.3333 (74.3243)  acc5: 100.0000 (96.6967)  time: 0.6387 (0.1292 -- 4.9508)  data: 0.4981 (0.0003 -- 4.7499)  max mem: 16735
Test:  [120/603]  eta: 0:04:19  loss: 0.6385 (0.9036)  acc1: 83.3333 (74.3802)  acc5: 100.0000 (96.6942)  time: 0.2974 (0.1249 -- 2.6078)  data: 0.1638 (0.0003 -- 2.4718)  max mem: 16735
Test:  [130/603]  eta: 0:04:08  loss: 0.7143 (0.8987)  acc1: 83.3333 (74.6819)  acc5: 100.0000 (96.9466)  time: 0.3968 (0.1240 -- 2.6078)  data: 0.2603 (0.0003 -- 2.4718)  max mem: 16735
Test:  [140/603]  eta: 0:04:16  loss: 0.7143 (0.8934)  acc1: 83.3333 (75.1773)  acc5: 100.0000 (96.9267)  time: 0.6605 (0.1240 -- 4.6258)  data: 0.5224 (0.0003 -- 4.5040)  max mem: 16735
Test:  [150/603]  eta: 0:03:59  loss: 0.6848 (0.8731)  acc1: 83.3333 (75.8278)  acc5: 100.0000 (97.0199)  time: 0.5495 (0.1221 -- 4.6258)  data: 0.4142 (0.0002 -- 4.5040)  max mem: 16735
Test:  [160/603]  eta: 0:03:51  loss: 0.7619 (0.8805)  acc1: 66.6667 (75.1553)  acc5: 100.0000 (96.7909)  time: 0.2965 (0.1221 -- 2.9609)  data: 0.1594 (0.0002 -- 2.8077)  max mem: 16735
Test:  [170/603]  eta: 0:03:40  loss: 0.7705 (0.8774)  acc1: 66.6667 (75.3411)  acc5: 100.0000 (96.9786)  time: 0.3664 (0.1316 -- 2.9609)  data: 0.2261 (0.0003 -- 2.8077)  max mem: 16735
Test:  [180/603]  eta: 0:03:47  loss: 0.7342 (0.8749)  acc1: 83.3333 (75.6906)  acc5: 100.0000 (96.9613)  time: 0.6615 (0.1322 -- 4.8438)  data: 0.5215 (0.0004 -- 4.7092)  max mem: 16735
Test:  [190/603]  eta: 0:03:33  loss: 0.6103 (0.8600)  acc1: 83.3333 (76.1780)  acc5: 100.0000 (97.0332)  time: 0.5887 (0.1188 -- 4.8438)  data: 0.4507 (0.0003 -- 4.7092)  max mem: 16735
Test:  [200/603]  eta: 0:03:27  loss: 0.7303 (0.8658)  acc1: 83.3333 (75.6219)  acc5: 100.0000 (96.8491)  time: 0.3023 (0.1188 -- 3.2732)  data: 0.1645 (0.0003 -- 3.1346)  max mem: 16735
Test:  [210/603]  eta: 0:03:18  loss: 0.8695 (0.8644)  acc1: 66.6667 (75.7504)  acc5: 100.0000 (96.9984)  time: 0.3895 (0.1275 -- 3.2732)  data: 0.2518 (0.0003 -- 3.1346)  max mem: 16735
Test:  [220/603]  eta: 0:03:19  loss: 0.7782 (0.8606)  acc1: 83.3333 (76.0935)  acc5: 100.0000 (97.0588)  time: 0.5998 (0.1221 -- 4.5369)  data: 0.4650 (0.0003 -- 4.4122)  max mem: 16735
Test:  [230/603]  eta: 0:03:10  loss: 0.6161 (0.8515)  acc1: 83.3333 (76.4069)  acc5: 100.0000 (97.0418)  time: 0.5628 (0.1221 -- 4.5369)  data: 0.4274 (0.0003 -- 4.4122)  max mem: 16735
Test:  [240/603]  eta: 0:03:05  loss: 0.6293 (0.8563)  acc1: 83.3333 (75.9336)  acc5: 100.0000 (96.8880)  time: 0.3767 (0.1249 -- 2.5925)  data: 0.2406 (0.0003 -- 2.4508)  max mem: 16735
Test:  [250/603]  eta: 0:02:55  loss: 0.7319 (0.8569)  acc1: 66.6667 (76.0292)  acc5: 100.0000 (96.9456)  time: 0.3579 (0.1196 -- 2.5925)  data: 0.2269 (0.0002 -- 2.4508)  max mem: 16735
Test:  [260/603]  eta: 0:02:50  loss: 0.7319 (0.8547)  acc1: 83.3333 (76.1175)  acc5: 100.0000 (96.9349)  time: 0.3524 (0.1195 -- 1.8203)  data: 0.2245 (0.0001 -- 1.6919)  max mem: 16735
Test:  [270/603]  eta: 0:02:47  loss: 0.5269 (0.8472)  acc1: 83.3333 (76.2608)  acc5: 100.0000 (96.8635)  time: 0.5565 (0.1180 -- 4.7972)  data: 0.4266 (0.0001 -- 4.6830)  max mem: 16735
Test:  [280/603]  eta: 0:02:42  loss: 0.8397 (0.8573)  acc1: 83.3333 (75.9193)  acc5: 100.0000 (96.7972)  time: 0.5695 (0.1180 -- 4.7972)  data: 0.4361 (0.0001 -- 4.6830)  max mem: 16735
Test:  [290/603]  eta: 0:02:35  loss: 0.8919 (0.8577)  acc1: 66.6667 (76.0023)  acc5: 100.0000 (96.8499)  time: 0.4240 (0.1260 -- 3.9543)  data: 0.2886 (0.0002 -- 3.8096)  max mem: 16735
Test:  [300/603]  eta: 0:02:29  loss: 0.6781 (0.8543)  acc1: 83.3333 (76.1351)  acc5: 100.0000 (96.8992)  time: 0.3728 (0.1293 -- 2.9005)  data: 0.2367 (0.0002 -- 2.7704)  max mem: 16735
Test:  [310/603]  eta: 0:02:26  loss: 0.6269 (0.8493)  acc1: 83.3333 (76.2058)  acc5: 100.0000 (96.7846)  time: 0.5306 (0.1201 -- 4.9288)  data: 0.3970 (0.0001 -- 4.8083)  max mem: 16735
Test:  [320/603]  eta: 0:02:21  loss: 0.8181 (0.8562)  acc1: 66.6667 (75.9605)  acc5: 100.0000 (96.7809)  time: 0.5857 (0.1201 -- 4.9288)  data: 0.4513 (0.0001 -- 4.8083)  max mem: 16735
Test:  [330/603]  eta: 0:02:14  loss: 0.9459 (0.8579)  acc1: 66.6667 (76.0322)  acc5: 100.0000 (96.7774)  time: 0.3888 (0.1284 -- 3.9908)  data: 0.2527 (0.0003 -- 3.8545)  max mem: 16735
Test:  [340/603]  eta: 0:02:09  loss: 0.7141 (0.8551)  acc1: 83.3333 (76.0997)  acc5: 100.0000 (96.8231)  time: 0.3968 (0.1163 -- 4.2598)  data: 0.2667 (0.0002 -- 4.1369)  max mem: 16735
Test:  [350/603]  eta: 0:02:05  loss: 0.6788 (0.8508)  acc1: 83.3333 (76.1633)  acc5: 100.0000 (96.7236)  time: 0.5455 (0.1163 -- 4.2598)  data: 0.4139 (0.0002 -- 4.1369)  max mem: 16735
Test:  [360/603]  eta: 0:01:59  loss: 0.8753 (0.8570)  acc1: 66.6667 (75.9464)  acc5: 100.0000 (96.7221)  time: 0.4692 (0.1223 -- 4.0733)  data: 0.3339 (0.0002 -- 3.9458)  max mem: 16735
Test:  [370/603]  eta: 0:01:53  loss: 1.0693 (0.8587)  acc1: 66.6667 (75.9659)  acc5: 100.0000 (96.6757)  time: 0.3598 (0.1223 -- 2.6759)  data: 0.2238 (0.0002 -- 2.5373)  max mem: 16735
Test:  [380/603]  eta: 0:01:49  loss: 0.7228 (0.8605)  acc1: 83.3333 (75.8968)  acc5: 100.0000 (96.5442)  time: 0.4704 (0.1347 -- 4.8160)  data: 0.3301 (0.0003 -- 4.6620)  max mem: 16735
Test:  [390/603]  eta: 0:01:44  loss: 0.5456 (0.8562)  acc1: 83.3333 (76.0443)  acc5: 100.0000 (96.5047)  time: 0.5545 (0.1214 -- 4.8160)  data: 0.4188 (0.0002 -- 4.6620)  max mem: 16735
Test:  [400/603]  eta: 0:01:39  loss: 0.6570 (0.8594)  acc1: 83.3333 (75.9352)  acc5: 100.0000 (96.4256)  time: 0.4674 (0.1214 -- 3.6928)  data: 0.3361 (0.0002 -- 3.5746)  max mem: 16735
Test:  [410/603]  eta: 0:01:33  loss: 0.7798 (0.8591)  acc1: 83.3333 (76.0341)  acc5: 100.0000 (96.4315)  time: 0.3595 (0.1221 -- 3.1222)  data: 0.2227 (0.0002 -- 3.0044)  max mem: 16735
Test:  [420/603]  eta: 0:01:29  loss: 0.7605 (0.8608)  acc1: 83.3333 (75.9699)  acc5: 100.0000 (96.3183)  time: 0.4864 (0.1295 -- 5.6236)  data: 0.3466 (0.0002 -- 5.4843)  max mem: 16735
Test:  [430/603]  eta: 0:01:25  loss: 0.6090 (0.8575)  acc1: 83.3333 (76.0634)  acc5: 100.0000 (96.2877)  time: 0.6200 (0.1212 -- 5.6236)  data: 0.4847 (0.0001 -- 5.4843)  max mem: 16735
Test:  [440/603]  eta: 0:01:19  loss: 0.5665 (0.8596)  acc1: 83.3333 (76.0015)  acc5: 100.0000 (96.2207)  time: 0.5013 (0.1212 -- 4.0687)  data: 0.3658 (0.0001 -- 3.9468)  max mem: 16735
Test:  [450/603]  eta: 0:01:14  loss: 0.8071 (0.8600)  acc1: 83.3333 (76.0532)  acc5: 100.0000 (96.2306)  time: 0.3555 (0.1247 -- 3.2652)  data: 0.2203 (0.0003 -- 3.1072)  max mem: 16735
Test:  [460/603]  eta: 0:01:09  loss: 0.8162 (0.8616)  acc1: 83.3333 (75.9942)  acc5: 100.0000 (96.1316)  time: 0.4476 (0.1247 -- 4.9808)  data: 0.3071 (0.0002 -- 4.8451)  max mem: 16735
Test:  [470/603]  eta: 0:01:05  loss: 0.7063 (0.8585)  acc1: 83.3333 (76.0793)  acc5: 100.0000 (96.1076)  time: 0.6831 (0.1284 -- 4.9808)  data: 0.5391 (0.0002 -- 4.8451)  max mem: 16735
Test:  [480/603]  eta: 0:01:00  loss: 0.6304 (0.8603)  acc1: 83.3333 (76.0222)  acc5: 100.0000 (96.0499)  time: 0.6187 (0.1245 -- 3.5342)  data: 0.4805 (0.0002 -- 3.4171)  max mem: 16735
Test:  [490/603]  eta: 0:00:55  loss: 0.9558 (0.8621)  acc1: 66.6667 (76.0014)  acc5: 100.0000 (95.9946)  time: 0.4010 (0.1245 -- 3.5342)  data: 0.2624 (0.0002 -- 3.4171)  max mem: 16735
Test:  [500/603]  eta: 0:00:50  loss: 0.9628 (0.8670)  acc1: 66.6667 (75.8150)  acc5: 100.0000 (95.9415)  time: 0.4377 (0.1278 -- 4.5071)  data: 0.3002 (0.0003 -- 4.3748)  max mem: 16735
Test:  [510/603]  eta: 0:00:46  loss: 0.7194 (0.8657)  acc1: 83.3333 (75.8969)  acc5: 100.0000 (95.9230)  time: 0.6432 (0.1151 -- 5.4944)  data: 0.5098 (0.0002 -- 5.3821)  max mem: 16735
Test:  [520/603]  eta: 0:00:41  loss: 0.7252 (0.8689)  acc1: 83.3333 (75.6558)  acc5: 100.0000 (95.9053)  time: 0.5806 (0.1151 -- 5.4944)  data: 0.4480 (0.0002 -- 5.3821)  max mem: 16735
Test:  [530/603]  eta: 0:00:35  loss: 1.0738 (0.8708)  acc1: 66.6667 (75.6121)  acc5: 100.0000 (95.8255)  time: 0.3507 (0.1217 -- 3.3429)  data: 0.2179 (0.0003 -- 3.1957)  max mem: 16735
Test:  [540/603]  eta: 0:00:31  loss: 0.9703 (0.8753)  acc1: 66.6667 (75.4467)  acc5: 100.0000 (95.7794)  time: 0.4249 (0.1274 -- 4.7650)  data: 0.2897 (0.0003 -- 4.6309)  max mem: 16735
Test:  [550/603]  eta: 0:00:26  loss: 0.6861 (0.8740)  acc1: 83.3333 (75.5293)  acc5: 100.0000 (95.7653)  time: 0.6020 (0.1109 -- 4.9475)  data: 0.4728 (0.0001 -- 4.8165)  max mem: 16735
Test:  [560/603]  eta: 0:00:21  loss: 0.7497 (0.8768)  acc1: 83.3333 (75.3119)  acc5: 100.0000 (95.7516)  time: 0.5261 (0.1109 -- 4.9475)  data: 0.3961 (0.0001 -- 4.8165)  max mem: 16735
Test:  [570/603]  eta: 0:00:16  loss: 1.0728 (0.8784)  acc1: 66.6667 (75.2773)  acc5: 100.0000 (95.6801)  time: 0.3742 (0.1206 -- 3.2722)  data: 0.2380 (0.0002 -- 3.0985)  max mem: 16735
Test:  [580/603]  eta: 0:00:11  loss: 0.8817 (0.8825)  acc1: 66.6667 (75.1291)  acc5: 100.0000 (95.6397)  time: 0.4594 (0.1206 -- 4.9000)  data: 0.3203 (0.0003 -- 4.7418)  max mem: 16735
Test:  [590/603]  eta: 0:00:06  loss: 0.7581 (0.8811)  acc1: 83.3333 (75.2115)  acc5: 100.0000 (95.6289)  time: 0.6083 (0.1232 -- 4.9000)  data: 0.4692 (0.0001 -- 4.7418)  max mem: 16735
Test:  [600/603]  eta: 0:00:01  loss: 0.8569 (0.8836)  acc1: 66.6667 (75.0139)  acc5: 100.0000 (95.6184)  time: 0.4262 (0.1134 -- 4.7578)  data: 0.2999 (0.0001 -- 4.6305)  max mem: 16735
Test:  [602/603]  eta: 0:00:00  loss: 0.8569 (0.8839)  acc1: 66.6667 (75.0207)  acc5: 100.0000 (95.6017)  time: 0.1909 (0.0860 -- 1.4836)  data: 0.0683 (0.0001 -- 1.3616)  max mem: 16735
Test: Total time: 0:04:55 (0.4895 s / it)
* Acc@1 76.598 Acc@5 95.477 loss 0.864
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 7230 test videos: Top-1: 82.16%, Top-5: 99.17%
Training time 8:25:10
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
